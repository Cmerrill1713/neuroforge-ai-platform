#!/usr/bin/env python3
"""
Test MCP Tools with Pydantic AI and Ollama Integration

This test demonstrates how to use your existing MCP tools with Pydantic AI
and Ollama models in a proper, working integration.
"""

import asyncio
import logging
import sys
from pathlib import Path
from typing import Dict, Any, Optional

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from src.core.tools.pydantic_ai_mcp import (
    PydanticAIMCPAgent, 
    create_pydantic_ai_mcp_agent,
    create_enhanced_tool_call,
    OllamaGenerationRequest,
    OllamaGenerationResult
)
from src.core.tools.mcp_adapter import MCPAdapter, MCPTransportType
from src.core.memory.vector_pg import VectorStore

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MockMCPAdapter(MCPAdapter):
    """Mock MCP adapter for testing."""
    
    def __init__(self):
        # Create a mock transport that implements the abstract methods
        from src.core.tools.mcp_adapter import MCPTransport
        from abc import ABC
        
        class MockTransport(MCPTransport):
            async def send_message(self, message):
                pass
            
            async def receive_message(self):
                return None
            
            async def close(self):
                pass
        
        super().__init__(MockTransport())
        self.logger = logging.getLogger(__name__)
    
    async def send_request(self, method: str, params: Dict[str, Any], request_id: Optional[str] = None):
        """Mock MCP request handling."""
        # Mock responses for different MCP methods
        if method == "tools/list":
            return {
                "tools": [
                    {
                        "name": "file_read",
                        "description": "Read file contents",
                        "inputSchema": {
                            "type": "object",
                            "properties": {
                                "file_path": {"type": "string"},
                                "encoding": {"type": "string", "default": "utf-8"}
                            },
                            "required": ["file_path"]
                        }
                    },
                    {
                        "name": "file_write", 
                        "description": "Write file contents",
                        "inputSchema": {
                            "type": "object",
                            "properties": {
                                "file_path": {"type": "string"},
                                "content": {"type": "string"},
                                "encoding": {"type": "string", "default": "utf-8"}
                            },
                            "required": ["file_path", "content"]
                        }
                    }
                ]
            }
        elif method == "tools/call":
            tool_name = params.get("name", "")
            arguments = params.get("arguments", {})
            
            if tool_name == "file_read":
                file_path = arguments.get("file_path", "")
                return {
                    "content": [
                        {
                            "type": "text",
                            "text": f"Mock content from {file_path}"
                        }
                    ]
                }
            elif tool_name == "file_write":
                file_path = arguments.get("file_path", "")
                content = arguments.get("content", "")
                return {
                    "content": [
                        {
                            "type": "text", 
                            "text": f"Successfully wrote {len(content)} characters to {file_path}"
                        }
                    ]
                }
        
        return {"error": f"Unknown method: {method}"}

async def test_mcp_pydantic_ollama_integration():
    """Test the complete MCP + Pydantic AI + Ollama integration."""
    logger.info("üß™ Testing MCP Tools with Pydantic AI and Ollama")
    logger.info("=" * 60)
    
    try:
        # Create mock MCP adapter
        mcp_adapter = MockMCPAdapter()
        
        # Create Pydantic AI MCP agent with Ollama integration
        agent = await create_pydantic_ai_mcp_agent(
            mcp_adapter=mcp_adapter,
            vector_store=None,  # No vector store for this test
            agent_name="test_mcp_ollama_agent",
            ollama_config_path="configs/policies.yaml"
        )
        
        logger.info("‚úÖ Agent created successfully")
        
        # Test 1: MCP Tool Execution
        logger.info("\nüìù Test 1: MCP Tool Execution")
        logger.info("-" * 40)
        
        tool_call = create_enhanced_tool_call(
            tool_name="file_read",
            parameters={"file_path": "README.md"},
            knowledge_base_query="What is the project about?"
        )
        
        result = await agent._execute_tool_call(tool_call, None)
        
        if result.success:
            logger.info(f"‚úÖ MCP tool executed successfully")
            logger.info(f"   Result: {result.result}")
            logger.info(f"   Execution time: {result.execution_time:.3f}s")
        else:
            logger.error(f"‚ùå MCP tool failed: {result.error_message}")
        
        # Test 2: Ollama Text Generation
        logger.info("\nüí¨ Test 2: Ollama Text Generation")
        logger.info("-" * 40)
        
        ollama_result = await agent._ollama_generate_text(
            prompt="Explain how MCP tools work with Pydantic AI",
            max_tokens=200,
            temperature=0.7
        )
        
        if ollama_result.get("success"):
            logger.info(f"‚úÖ Ollama generation successful")
            logger.info(f"   Model: {ollama_result['model_used']}")
            logger.info(f"   Content: {ollama_result['content'][:100]}...")
            logger.info(f"   Processing time: {ollama_result['processing_time']:.2f}s")
        else:
            logger.error(f"‚ùå Ollama generation failed: {ollama_result.get('error')}")
        
        # Test 3: Ollama Code Generation
        logger.info("\nüíª Test 3: Ollama Code Generation")
        logger.info("-" * 40)
        
        code_result = await agent._ollama_generate_code(
            prompt="Create a simple Python function to calculate fibonacci numbers",
            language="python"
        )
        
        if code_result.get("success"):
            logger.info(f"‚úÖ Code generation successful")
            logger.info(f"   Model: {code_result['model_used']}")
            logger.info(f"   Code: {code_result['code'][:100]}...")
            logger.info(f"   Processing time: {code_result['processing_time']:.2f}s")
        else:
            logger.error(f"‚ùå Code generation failed: {code_result.get('error')}")
        
        # Test 4: Ollama Text Analysis
        logger.info("\nüîç Test 4: Ollama Text Analysis")
        logger.info("-" * 40)
        
        analysis_result = await agent._ollama_analyze_text(
            text="The Agentic LLM Core system provides a comprehensive framework for building intelligent agents with MCP tool integration and Pydantic AI validation.",
            analysis_type="technical"
        )
        
        if analysis_result.get("success"):
            logger.info(f"‚úÖ Text analysis successful")
            logger.info(f"   Model: {analysis_result['model_used']}")
            logger.info(f"   Analysis: {analysis_result['analysis'][:100]}...")
            logger.info(f"   Processing time: {analysis_result['processing_time']:.2f}s")
        else:
            logger.error(f"‚ùå Text analysis failed: {analysis_result.get('error')}")
        
        # Test 5: Ollama Quick Response
        logger.info("\n‚ö° Test 5: Ollama Quick Response")
        logger.info("-" * 40)
        
        quick_result = await agent._ollama_quick_response(
            prompt="What is 15 + 27?"
        )
        
        if quick_result.get("success"):
            logger.info(f"‚úÖ Quick response successful")
            logger.info(f"   Model: {quick_result['model_used']}")
            logger.info(f"   Response: {quick_result['response']}")
            logger.info(f"   Processing time: {quick_result['processing_time']:.2f}s")
        else:
            logger.error(f"‚ùå Quick response failed: {quick_result.get('error')}")
        
        # Test 6: Combined MCP + Ollama Workflow
        logger.info("\nüîÑ Test 6: Combined MCP + Ollama Workflow")
        logger.info("-" * 40)
        
        # Step 1: Read a file using MCP
        read_call = create_enhanced_tool_call(
            tool_name="file_read",
            parameters={"file_path": "configs/policies.yaml"}
        )
        read_result = await agent._execute_tool_call(read_call, None)
        
        if read_result.success:
            logger.info("‚úÖ Step 1: File read successful")
            
            # Step 2: Analyze the content using Ollama
            analysis_result = await agent._ollama_analyze_text(
                text=str(read_result.result),
                analysis_type="configuration"
            )
            
            if analysis_result.get("success"):
                logger.info("‚úÖ Step 2: Content analysis successful")
                logger.info(f"   Analysis: {analysis_result['analysis'][:150]}...")
            else:
                logger.error("‚ùå Step 2: Content analysis failed")
        else:
            logger.error("‚ùå Step 1: File read failed")
        
        # Show performance statistics
        stats = agent.get_performance_stats()
        logger.info(f"\nüìä Performance Statistics:")
        logger.info(f"   Total requests: {stats['total_requests']}")
        logger.info(f"   MCP tool calls: {stats['mcp_tool_calls']}")
        logger.info(f"   Ollama generations: {stats['ollama_generations']}")
        logger.info(f"   Average time: {stats['average_time']:.2f}s")
        
        # Show tool catalog
        catalog = agent.get_tool_catalog()
        logger.info(f"\nüõ†Ô∏è Available Tools ({len(catalog)}):")
        for tool_name, tool_spec in catalog.items():
            logger.info(f"   ‚Ä¢ {tool_name}: {tool_spec.description}")
        
        logger.info("\nüéâ All tests completed!")
        logger.info("‚úÖ MCP tools successfully integrated with Pydantic AI")
        logger.info("‚úÖ Ollama models working with MCP tool execution")
        logger.info("‚úÖ Complete agentic system operational")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Test failed with exception: {e}")
        import traceback
        traceback.print_exc()
        return False

async def main():
    """Main test function."""
    logger.info("üöÄ Starting MCP + Pydantic AI + Ollama Integration Test")
    
    success = await test_mcp_pydantic_ollama_integration()
    
    if success:
        logger.info("\nüéâ Integration test completed successfully!")
        logger.info("‚úÖ Your MCP tools are now working with Pydantic AI and Ollama")
    else:
        logger.error("\n‚ùå Integration test failed")

if __name__ == "__main__":
    asyncio.run(main())
