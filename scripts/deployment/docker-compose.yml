version: '3.8'

services:
  # Backend API Server
  backend:
    build:
      context: .
      dockerfile: Dockerfile.backend
    container_name: ai-backend
    ports:
      - "8000:8000"
    environment:
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
      - PORT=8000
      - BACKEND_API_URL=http://backend:8000
    volumes:
      - ./logs:/app/logs
      - ./archive:/app/archive
      - ./configs:/app/configs
      - ./knowledge_base:/app/knowledge_base
    networks:
      - ai-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Frontend Next.js App
  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    container_name: ai-frontend
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - NEXT_PUBLIC_API_URL=http://localhost:8000
      - NEXT_PUBLIC_WS_URL=ws://localhost:8000
      - NEXT_PUBLIC_TTS_SERVER_URL=http://localhost:8086
      - NEXT_PUBLIC_TTS_ENABLED=true
      - BACKEND_API_URL=http://backend:8000
    depends_on:
      backend:
        condition: service_healthy
    networks:
      - ai-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # TTS Server (if needed)
  tts-server:
    image: python:3.11-slim
    container_name: ai-tts-server
    ports:
      - "8086:8086"
    working_dir: /app
    command: ["python", "production_tts_server.py"]
    volumes:
      - ./production_tts_server.py:/app/production_tts_server.py
      - ./requirements.txt:/app/requirements.txt
    networks:
      - ai-network
    restart: unless-stopped
    profiles:
      - tts  # Optional service, use --profile tts to include

  # Redis for caching (optional)
  redis:
    image: redis:7-alpine
    container_name: ai-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    networks:
      - ai-network
    restart: unless-stopped
    profiles:
      - cache  # Optional service, use --profile cache to include

  # PostgreSQL for data persistence (optional)
  postgres:
    image: postgres:15-alpine
    container_name: ai-postgres
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_DB=ai_system
      - POSTGRES_USER=ai_user
      - POSTGRES_PASSWORD=ai_password
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - ai-network
    restart: unless-stopped
    profiles:
      - database  # Optional service, use --profile database to include

  # MCP Transcript Service
  transcript-mcp:
    build:
      context: .
      dockerfile: Dockerfile.transcript-mcp
    container_name: ai-transcript-mcp
    ports:
      - "8087:8087"
    environment:
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
      - GITHUB_TOKEN=${GITHUB_TOKEN:-}
    volumes:
      - ./knowledge_base:/app/knowledge_base
      - ./indydevdan_content_crawler.py:/app/indydevdan_content_crawler.py
      - ./mcp_transcript_service.py:/app/mcp_transcript_service.py
      - ./src:/app/src
    networks:
      - ai-network
    restart: unless-stopped
    profiles:
      - mcp  # Optional service, use --profile mcp to include

networks:
  ai-network:
    driver: bridge

volumes:
  redis-data:
  postgres-data: