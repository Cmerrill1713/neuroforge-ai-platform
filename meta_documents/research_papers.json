[
  {
    "metadata": {
      "url": "https://arxiv.org/search/?query=meta+ai&searchtype=all",
      "title": "Search | arXiv e-print repository",
      "category": "research_papers",
      "source_type": "arxiv",
      "scraped_at": "2025-09-28T18:06:08.416089",
      "word_count": 14306,
      "status": "success"
    },
    "content": "Search | arXiv e-print repository\nSkip to main content\nWe gratefully acknowledge support from\nthe Simons Foundation,\nmember institutions\n, and all contributors.\nDonate\nHelp\n|\nAdvanced Search\nAll fields\nTitle\nAuthor\nAbstract\nComments\nJournal reference\nACM classification\nMSC classification\nReport number\narXiv identifier\nDOI\nORCID\narXiv author ID\nHelp pages\nFull text\nSearch\nLogin\nShowing 1–50 of 629 results for all:\nmeta ai\nSearch v0.5.6 released 2020-02-24\nSearch term or terms\nField\nAll fields\nTitle\nAuthor(s)\nAbstract\nComments\nJournal reference\nACM classification\nMSC classification\nReport number\narXiv identifier\nDOI\nORCID\nLicense (URI)\narXiv author ID\nHelp pages\nFull text\nSearch\nShow abstracts\nHide abstracts\nAdvanced Search\nAll fields\nTitle\nAuthor(s)\nAbstract\nComments\nJournal reference\nACM classification\nMSC classification\nReport number\narXiv identifier\nDOI\nORCID\nLicense (URI)\narXiv author ID\nHelp pages\nFull text\nShow abstracts\nHide abstracts\n25\n50\n100\n200\nresults per page\n.\nSort results by\nAnnouncement date (newest first)\nAnnouncement date (oldest first)\nSubmission date (newest first)\nSubmission date (oldest first)\nRelevance\nGo\nPrevious\nNext\n1\n2\n3\n4\n5\n…\narXiv:2509.19705\n[\npdf\n,\nps\n,\nother\n]\ncs.LG\ncs.AI\nstat.AP\nstat.ME\ndoi\n10.26599/BDMA.2025.9020093\nCausal Machine Learning for Surgical Interventions\nAuthors:\nJ. Ben Tamo\n,\nNishant S. Chouhan\n,\nMicky C. Nnamdi\n,\nYining Yuan\n,\nShreya S. Chivilkar\n,\nWenqi Shi\n,\nSteven W. Hwang\n,\nB. Randall Brenn\n,\nMay D. Wang\nAbstract\n:\n…effects (ITEs) remains limited due to the reliance on traditional statistical methods that struggle with complex, heterogeneous data. In this study, we develop a multi-task\nmeta\n-learning framework, X-MultiTask, for ITE estimation that models each surgical decision (e.g., anterior vs. posterior approach, surgery vs. no surgery) as a distinct task while learni…\n▽ More\nSurgical decision-making is complex and requires understanding causal relationships between patient characteristics, interventions, and outcomes. In high-stakes settings like spinal fusion or scoliosis correction, accurate estimation of individualized treatment effects (ITEs) remains limited due to the reliance on traditional statistical methods that struggle with complex, heterogeneous data. In this study, we develop a multi-task\nmeta\n-learning framework, X-MultiTask, for ITE estimation that models each surgical decision (e.g., anterior vs. posterior approach, surgery vs. no surgery) as a distinct task while learning shared representations across tasks. To strengthen causal validity, we incorporate the inverse probability weighting (IPW) into the training objective. We evaluate our approach on two datasets: (1) a public spinal fusion dataset (1,017 patients) to assess the effect of anterior vs. posterior approaches on complication severity; and (2) a private\nAIS\ndataset (368 patients) to analyze the impact of posterior spinal fusion (PSF) vs. non-surgical management on patient-reported outcomes (PROs). Our model achieves the highest average AUC (0.84) in the anterior group and maintains competitive performance in the posterior group (0.77). It outperforms baselines in treatment effect estimation with the lowest overall $ε_{\\text{NN-PEHE}}$ (0.2778) and $ε_{\\text{ATE}}$ (0.0763). Similarly, when predicting PROs in\nAIS\n, X-MultiTask consistently shows superior performance across all domains, with $ε_{\\text{NN-PEHE}}$ = 0.2551 and $ε_{\\text{ATE}}$ = 0.0902. By providing robust, patient-specific causal estimates, X-MultiTask offers a powerful tool to advance personalized surgical care and improve patient outcomes. The code is available at https://github.com/Wizaaard/X-MultiTask.\n△ Less\nSubmitted\n23 September, 2025;\noriginally announced\nSeptember 2025.\narXiv:2509.18461\n[\npdf\n,\nps\n,\nother\n]\ncs.GR\ncs.AI\ncs.CV\ncs.MM\ndoi\n10.1561/2000000136\nZero-Shot Visual Deepfake Detection: Can\nAI\nPredict and Prevent Fake Content Before It's Created?\nAuthors:\nAyan Sar\n,\nSampurna Roy\n,\nTanupriya Choudhury\n,\nAjith Abraham\nAbstract\n:\n…have never seen a particular deepfake variation. In this work, we studied self-supervised learning, transformer-based zero-shot classifier, generative model fingerprinting, and\nmeta\n-learning techniques that better adapt to the ever-evolving deepfake threat. In addition, we suggested…\n▽ More\nGenerative adversarial networks (GANs) and diffusion models have dramatically advanced deepfake technology, and its threats to digital security, media integrity, and public trust have increased rapidly. This research explored zero-shot deepfake detection, an emerging method even when the models have never seen a particular deepfake variation. In this work, we studied self-supervised learning, transformer-based zero-shot classifier, generative model fingerprinting, and\nmeta\n-learning techniques that better adapt to the ever-evolving deepfake threat. In addition, we suggested\nAI\n-driven prevention strategies that mitigated the underlying generation pipeline of the deepfakes before they occurred. They consisted of adversarial perturbations for creating deepfake generators, digital watermarking for content authenticity verification, real-time\nAI\nmonitoring for content creation pipelines, and blockchain-based content verification frameworks. Despite these advancements, zero-shot detection and prevention faced critical challenges such as adversarial attacks, scalability constraints, ethical dilemmas, and the absence of standardized evaluation benchmarks. These limitations were addressed by discussing future research directions on explainable\nAI\nfor deepfake detection, multimodal fusion based on image, audio, and text analysis, quantum\nAI\nfor enhanced security, and federated learning for privacy-preserving deepfake detection. This further highlighted the need for an integrated defense framework for digital authenticity that utilized zero-shot learning in combination with preventive deepfake mechanisms. Finally, we highlighted the important role of interdisciplinary collaboration between\nAI\nresearchers, cybersecurity experts, and policymakers to create resilient defenses against the rising tide of deepfake attacks.\n△ Less\nSubmitted\n22 September, 2025;\noriginally announced\nSeptember 2025.\nComments:\nPublished in Foundations and Trends in Signal Processing (#1 in Signal Processing, #3 in Computer Science)\nJournal ref:\nFoundations and Trends in Signal Processing (2025)\narXiv:2509.18141\n[\npdf\n,\nps\n,\nother\n]\ncs.LG\ncs.AI\ncs.CV\nstat.AP\nstat.ML\nKM-GPT: An Automated Pipeline for Reconstructing Individual Patient Data from Kaplan-Meier Plots\nAuthors:\nYao Zhao\n,\nHaoyue Sun\n,\nYantian Ding\n,\nYanxun Xu\nAbstract\n:\n…existing approaches often rely on manual digitization, which is error-prone and lacks scalability. To address these limitations, we develop KM-GPT, the first fully automated,\nAI\n-powered pipeline for reconstructing IPD directly from KM plots with high accuracy, robustness, and reproducibility. KM-GPT integrates advanced image preprocessing, multi-modal reaso…\n▽ More\nReconstructing individual patient data (IPD) from Kaplan-Meier (KM) plots provides valuable insights for evidence synthesis in clinical research. However, existing approaches often rely on manual digitization, which is error-prone and lacks scalability. To address these limitations, we develop KM-GPT, the first fully automated,\nAI\n-powered pipeline for reconstructing IPD directly from KM plots with high accuracy, robustness, and reproducibility. KM-GPT integrates advanced image preprocessing, multi-modal reasoning powered by GPT-5, and iterative reconstruction algorithms to generate high-quality IPD without manual input or intervention. Its hybrid reasoning architecture automates the conversion of unstructured information into structured data flows and validates data extraction from complex KM plots. To improve accessibility, KM-GPT is equipped with a user-friendly web interface and an integrated\nAI\nassistant, enabling researchers to reconstruct IPD without requiring programming expertise. KM-GPT was rigorously evaluated on synthetic and real-world datasets, consistently demonstrating superior accuracy. To illustrate its utility, we applied KM-GPT to a\nmeta\n-analysis of gastric cancer immunotherapy trials, reconstructing IPD to facilitate evidence synthesis and biomarker-based subgroup analyses. By automating traditionally manual processes and providing a scalable, web-based solution, KM-GPT transforms clinical research by leveraging reconstructed IPD to enable more informed downstream analyses, supporting evidence-based decision-making.\n△ Less\nSubmitted\n14 September, 2025;\noriginally announced\nSeptember 2025.\narXiv:2509.17158\n[\npdf\n,\nps\n,\nother\n]\ncs.AI\ncs.CL\nARE: Scaling Up Agent Environments and Evaluations\nAuthors:\nPierre Andrews\n,\nAmine Benhalloum\n,\nGerard Moreno-Torres Bertran\n,\nMatteo Bettini\n,\nAmar Budhiraja\n,\nRicardo Silveira Cabral\n,\nVirginie Do\n,\nRomain Froger\n,\nEmilien Garreau\n,\nJean-Baptiste Gaya\n,\nHugo Laurençon\n,\nMaxime Lecanu\n,\nKunal Malkan\n,\nDheeraj Mekala\n,\nPierre Ménard\n,\nGrégoire Mialon\n,\nUlyana Piterbarg\n,\nMikhail Plekhanov\n,\nMathieu Rita\n,\nAndrey Rusakov\n,\nThomas Scialom\n,\nVladislav Vorotilov\n,\nMengjue Wang\n,\nIan Yu\nAbstract\n:\nWe introduce\nMeta\nAgents Research Environments (ARE), a research platform for scalable creation of environments, integration of synthetic or real applications, and execution of agentic orchestrations. ARE provides simple abstractions to build complex and diverse environments, each with their own rules, tools, content, and verifiers, helping to bridge the gap…\n▽ More\nWe introduce\nMeta\nAgents Research Environments (ARE), a research platform for scalable creation of environments, integration of synthetic or real applications, and execution of agentic orchestrations. ARE provides simple abstractions to build complex and diverse environments, each with their own rules, tools, content, and verifiers, helping to bridge the gap between model development and real-world deployment. We also propose Gaia2, a benchmark built in ARE and designed to measure general agent capabilities. Beyond search and execution, Gaia2 requires agents to handle ambiguities and noise, adapt to dynamic environments, collaborate with other agents, and operate under temporal constraints. Unlike prior benchmarks, Gaia2 runs asynchronously, surfacing new failure modes that are invisible in static settings. Our experiments show that no system dominates across the intelligence spectrum: stronger reasoning often comes at the cost of efficiency, and budget scaling curves plateau, highlighting the need for new architectures and adaptive compute strategies. Perhaps more importantly, ARE abstractions enable continuous extension of Gaia2 to other environments, empowering the community to rapidly create new benchmarks tailored to their domains. In\nAI's\nsecond half, progress increasingly depends on defining meaningful tasks and robust evaluations to drive frontier capabilities forward.\n△ Less\nSubmitted\n21 September, 2025;\noriginally announced\nSeptember 2025.\narXiv:2509.16834\n[\npdf\n,\nps\n,\nother\n]\ncs.RO\ncs.AI\ncs.LG\nRobot Learning with Sparsity and Scarcity\nAuthors:\nJingxi Xu\nAbstract\n:\n…the right type of physical assistance at the right moment. My work develops machine learning algorithms that enable intent inferral with minimal data, including semi-supervised,\nmeta\n-learning, and generative\nAI\nmethods.\n▽ More\nUnlike in language or vision, one of the fundamental challenges in robot learning is the lack of access to vast data resources. We can further break down the problem into (1) data sparsity from the angle of data representation and (2) data scarcity from the angle of data quantity. In this thesis, I will discuss selected works on two domains: (1) tactile sensing and (2) rehabilitation robots, which are exemplars of data sparsity and scarcity, respectively. Tactile sensing is an essential modality for robotics, but tactile data are often sparse, and for each interaction with the physical world, tactile sensors can only obtain information about the local area of contact. I will discuss my work on learning vision-free tactile-only exploration and manipulation policies through model-free reinforcement learning to make efficient use of sparse tactile information. On the other hand, rehabilitation robots are an example of data scarcity to the extreme due to the significant challenge of collecting biosignals from disabled-bodied subjects at scale for training. I will discuss my work in collaboration with the medical school and clinicians on intent inferral for stroke survivors, where a hand orthosis developed in our lab collects a set of biosignals from the patient and uses them to infer the activity that the patient intends to perform, so the orthosis can provide the right type of physical assistance at the right moment. My work develops machine learning algorithms that enable intent inferral with minimal data, including semi-supervised,\nmeta\n-learning, and generative\nAI\nmethods.\n△ Less\nSubmitted\n20 September, 2025;\noriginally announced\nSeptember 2025.\narXiv:2509.15291\n[\npdf\n,\nps\n,\nother\n]\ncs.AI\neess.SY\nThe Distribution Shift Problem in Transportation Networks using Reinforcement Learning and\nAI\nAuthors:\nFederico Taschin\n,\nAbderrahmane Lazaraq\n,\nOzan K. Tonguz\n,\nInci Ozgunes\nAbstract\n:\nThe use of Machine Learning (ML) and Artificial Intelligence (\nAI\n) in smart transportation networks has increased significantly in the last few years. Among these ML and…\n▽ More\nThe use of Machine Learning (ML) and Artificial Intelligence (\nAI\n) in smart transportation networks has increased significantly in the last few years. Among these ML and\nAI\napproaches, Reinforcement Learning (RL) has been shown to be a very promising approach by several authors. However, a problem with using Reinforcement Learning in Traffic Signal Control is the reliability of the trained RL agents due to the dynamically changing distribution of the input data with respect to the distribution of the data used for training. This presents a major challenge and a reliability problem for the trained network of\nAI\nagents and could have very undesirable and even detrimental consequences if a suitable solution is not found. Several researchers have tried to address this problem using different approaches. In particular,\nMeta\nReinforcement Learning (\nMeta\nRL) promises to be an effective solution. In this paper, we evaluate and analyze a state-of-the-art\nMeta\nRL approach called MetaLight and show that, while under certain conditions MetaLight can indeed lead to reasonably good results, under some other conditions it might not perform well (with errors of up to 22%), suggesting that\nMeta\nRL schemes are often not robust enough and can even pose major reliability problems.\n△ Less\nSubmitted\n18 September, 2025;\noriginally announced\nSeptember 2025.\narXiv:2509.15213\n[\npdf\n,\nps\n,\nother\n]\ncs.CR\nEvil Vizier: Vulnerabilities of LLM-Integrated XR Systems\nAuthors:\nYicheng Zhang\n,\nZijian Huang\n,\nSophie Chen\n,\nErfan Shayegani\n,\nJiasi Chen\n,\nNael Abu-Ghazaleh\nAbstract\n:\n…increasingly integrate Large Language Models (LLMs) to enhance user experience, scene understanding, and even generate executable XR content, and are often called \"\nAI\nglasses\". Despite these potential benefits, the integrated XR-LLM pipeline makes XR applications vulnerable to new forms of attacks. In this paper, we analyze LLM-Integated XR systems i…\n▽ More\nExtended reality (XR) applications increasingly integrate Large Language Models (LLMs) to enhance user experience, scene understanding, and even generate executable XR content, and are often called \"\nAI\nglasses\". Despite these potential benefits, the integrated XR-LLM pipeline makes XR applications vulnerable to new forms of attacks. In this paper, we analyze LLM-Integated XR systems in the literature and in practice and categorize them along different dimensions from a systems perspective. Building on this categorization, we identify a common threat model and demonstrate a series of proof-of-concept attacks on multiple XR platforms that employ various LLM models (\nMeta\nQuest 3,\nMeta\nRay-Ban, Android, and Microsoft HoloLens 2 running Llama and GPT models). Although these platforms each implement LLM integration differently, they share vulnerabilities where an attacker can modify the public context surrounding a legitimate LLM query, resulting in erroneous visual or auditory feedback to users, thus compromising their safety or privacy, sowing confusion, or other harmful effects. To defend against these threats, we discuss mitigation strategies and best practices for developers, including an initial defense prototype, and call on the community to develop new protection mechanisms to mitigate these risks.\n△ Less\nSubmitted\n18 September, 2025;\noriginally announced\nSeptember 2025.\narXiv:2509.15035\n[\npdf\n]\ncs.AI\ncs.HC\nCalibrated Generative\nAI\nas\nMeta\n-Reviewer: A Systemic Functional Linguistics Discourse Analysis of Reviews of Peer Reviews\nAuthors:\nGabriela C. Zapata\n,\nBill Cope\n,\nMary Kalantzis\n,\nDuane Searsmith\nAbstract\n:\nThis study investigates the use of generative\nAI\nto support formative assessment through machine generated reviews of peer reviews in graduate online courses in a public university in the United States. Drawing on Systemic Functional Linguistics and Appraisal Theory, we analyzed 120 metareviews to explore how generative…\n▽ More\nThis study investigates the use of generative\nAI\nto support formative assessment through machine generated reviews of peer reviews in graduate online courses in a public university in the United States. Drawing on Systemic Functional Linguistics and Appraisal Theory, we analyzed 120 metareviews to explore how generative\nAI\nfeedback constructs meaning across ideational, interpersonal, and textual dimensions. The findings suggest that generative\nAI\ncan approximate key rhetorical and relational features of effective human feedback, offering directive clarity while also maintaining a supportive stance. The reviews analyzed demonstrated a balance of praise and constructive critique, alignment with rubric expectations, and structured staging that foregrounded student agency. By modeling these qualities,\nAI\nmetafeedback has the potential to scaffold feedback literacy and enhance leaner engagement with peer review.\n△ Less\nSubmitted\n18 September, 2025;\noriginally announced\nSeptember 2025.\nComments:\n39 pages, 3 tables\narXiv:2509.14711\n[\npdf\n,\nps\n,\nother\n]\neess.SP\nLLM4MG: Adapting Large Language Model for Multipath Generation via Synesthesia of Machines\nAuthors:\nZiwei Huang\n,\nShiliang Lu\n,\nLu Bai\n,\nXuesong Cai\n,\nXiang Cheng\nAbstract\n:\n…radar sensory data, RGB-D images, and light detection and ranging (LiDAR) point clouds. Based on the SynthSoM-V2I dataset, the proposed LLM4MG leverages Large Language Model\nMeta\nAI\n(LLaMA) 3.2 for multipath generation via multi-modal sensory data. The proposed LLM4MG aligns the multi-modal feature space with the LLaMA…\n▽ More\nBased on Synesthesia of Machines (SoM), a large language model (LLM) is adapted for multipath generation (LLM4MG) for the first time. Considering a typical sixth-generation (6G) vehicle-to-infrastructure (V2I) scenario, a new multi-modal sensing-communication dataset is constructed, named SynthSoM-V2I, including channel multipath information, millimeter wave (mmWave) radar sensory data, RGB-D images, and light detection and ranging (LiDAR) point clouds. Based on the SynthSoM-V2I dataset, the proposed LLM4MG leverages Large Language Model\nMeta\nAI\n(LLaMA) 3.2 for multipath generation via multi-modal sensory data. The proposed LLM4MG aligns the multi-modal feature space with the LLaMA semantic space through feature extraction and fusion networks. To further achieve general knowledge transfer from the pre-trained LLaMA for multipath generation via multi-modal sensory data, the low-rank adaptation (LoRA) parameter-efficient fine-tuning and propagation-aware prompt engineering are exploited. Simulation results demonstrate that the proposed LLM4MG outperforms conventional deep learning-based methods in terms of line-of-sight (LoS)/non-LoS (NLoS) classification with accuracy of 92.76%, multipath power/delay generation precision with normalized mean square error (NMSE) of 0.099/0.032, and cross-vehicular traffic density (VTD), cross-band, and cross-scenario generalization. The utility of the proposed LLM4MG is validated by real-world generalization. The necessity of high-precision multipath generation for system design is also demonstrated by channel capacity comparison.\n△ Less\nSubmitted\n18 September, 2025;\noriginally announced\nSeptember 2025.\narXiv:2509.14528\n[\npdf\n,\nps\n,\nother\n]\ncs.HC\nWhy Johnny Can't Use Agents: Industry Aspirations vs. User Realities with\nAI\nAgent Software\nAuthors:\nPradyumna Shome\n,\nSashreek Krishnan\n,\nSauvik Das\nAbstract\n:\nThere is growing imprecision about what \"\nAI\nagents\" are, what they can do, and how effectively they can be used by their intended users. We pose two key research questions: (i) How does the tech industry conceive of and market \"…\n▽ More\nThere is growing imprecision about what \"\nAI\nagents\" are, what they can do, and how effectively they can be used by their intended users. We pose two key research questions: (i) How does the tech industry conceive of and market \"\nAI\nagents\"? (ii) What challenges do end-users face when attempting to use commercial\nAI\nagents for their advertised uses? We first performed a systematic review of marketed use cases for 102 commercial\nAI\nagents, finding that they fall into three umbrella categories: orchestration, creation, and insight. Next, we conducted a usability assessment where N = 31 participants attempted representative tasks for each of these categories on two popular commercial\nAI\nagent tools: Operator and Manus. We found that users were generally impressed with these agents but faced several critical usability challenges ranging from agent capabilities that were misaligned with user mental models to agents lacking the\nmeta\n-cognitive abilities necessary for effective collaboration.\n△ Less\nSubmitted\n17 September, 2025;\noriginally announced\nSeptember 2025.\narXiv:2509.14216\n[\npdf\n,\nps\n,\nother\n]\ncs.LG\ncs.AI\nA Universal Banach--Bregman Framework for Stochastic Iterations: Unifying Stochastic Mirror Descent, Learning and LLM Training\nAuthors:\nJohnny R. Zhang\n,\nXiaomei Mi\n,\nGaoyuan Du\n,\nQianyi Sun\n,\nShiqi Wang\n,\nJiaxuan Li\n,\nWenhua Zhou\nAbstract\n:\n…variance, and enhanced accuracy over classical baselines. These results position Banach--Bregman geometry as a cornerstone unifying optimization theory and practice across core\nAI\nparadigms.\n▽ More\nStochastic optimization powers the scalability of modern artificial intelligence, spanning machine learning, deep learning, reinforcement learning, and large language model training. Yet, existing theory remains largely confined to Hilbert spaces, relying on inner-product frameworks and orthogonality. This paradigm fails to capture non-Euclidean settings, such as mirror descent on simplices, Bregman proximal methods for sparse learning, natural gradient descent in information geometry, or Kullback--Leibler-regularized language model training. Unlike Euclidean-based Hilbert-space methods, this approach embraces general Banach spaces. This work introduces a pioneering Banach--Bregman framework for stochastic iterations, establishing Bregman geometry as a foundation for next-generation optimization. It (i) provides a unified template via Bregman projections and Bregman--Fejer monotonicity, encompassing stochastic approximation, mirror descent, natural gradient, adaptive methods, and mirror-prox; (ii) establishes super-relaxations ($λ> 2$) in non-Hilbert settings, enabling flexible geometries and elucidating their acceleration effect; and (iii) delivers convergence theorems spanning almost-sure boundedness to geometric rates, validated on synthetic and real-world tasks. Empirical studies across machine learning (UCI benchmarks), deep learning (e.g., Transformer training), reinforcement learning (actor--critic), and large language models (WikiText-2 with distilGPT-2) show up to 20% faster convergence, reduced variance, and enhanced accuracy over classical baselines. These results position Banach--Bregman geometry as a cornerstone unifying optimization theory and practice across core\nAI\nparadigms.\n△ Less\nSubmitted\n17 September, 2025;\noriginally announced\nSeptember 2025.\nComments:\n69 pages, 10 figures. Preprint\narXiv:2509.13527\n[\npdf\n,\nps\n,\nother\n]\ncs.LG\nphysics.chem-ph\nMeta\n-Learning Linear Models for Molecular Property Prediction\nAuthors:\nYulia Pimonova\n,\nMichael G. Taylor\n,\nAlice Allen\n,\nPing Yang\n,\nNicholas Lubbers\nAbstract\n:\n…predictive capabilities in chemical sciences, but these modern data-driven approaches have increased the demand for data. In response to the growing demand for explainable\nAI\n(XAI) and to bridge the gap between predictive accuracy and human comprehensibility, we introduce LAMeL - a Linear Algorithm for…\n▽ More\nChemists in search of structure-property relationships face great challenges due to limited high quality, concordant datasets. Machine learning (ML) has significantly advanced predictive capabilities in chemical sciences, but these modern data-driven approaches have increased the demand for data. In response to the growing demand for explainable\nAI\n(XAI) and to bridge the gap between predictive accuracy and human comprehensibility, we introduce LAMeL - a Linear Algorithm for\nMeta\n-Learning that preserves interpretability while improving the prediction accuracy across multiple properties. While most approaches treat each chemical prediction task in isolation, LAMeL leverages a\nmeta\n-learning framework to identify shared model parameters across related tasks, even if those tasks do not share data, allowing it to learn a common functional manifold that serves as a more informed starting point for new unseen tasks. Our method delivers performance improvements ranging from 1.1- to 25-fold over standard ridge regression, depending on the domain of the dataset. While the degree of performance enhancement varies across tasks, LAMeL consistently outperforms or matches traditional linear methods, making it a reliable tool for chemical property prediction where both accuracy and interpretability are critical.\n△ Less\nSubmitted\n16 September, 2025;\noriginally announced\nSeptember 2025.\nComments:\n26 pages, 16 figures\nReport number:\nLA-UR-25-28399\narXiv:2509.13324\n[\npdf\n,\nps\n,\nother\n]\ncs.HC\nDesigning Psychometric Bias Measures for ChatBots: An Application to Racial Bias Measurement\nAuthors:\nMouhacine Benosman\nAbstract\n:\nArtificial intelligence (\nAI\n), particularly in the form of large language models (LLMs) or chatbots, has become increasingly integrated into our daily lives. In the past five years, several LLMs have been introduced, including ChatGPT by OpenAI, Claude by Anthropic, and Llama by\nMeta\n, among others. These models have the…\n▽ More\nArtificial intelligence (\nAI\n), particularly in the form of large language models (LLMs) or chatbots, has become increasingly integrated into our daily lives. In the past five years, several LLMs have been introduced, including ChatGPT by OpenAI, Claude by Anthropic, and Llama by\nMeta\n, among others. These models have the potential to be employed across a wide range of human-machine interaction applications, such as chatbots for information retrieval, assistance in corporate hiring decisions, college admissions, financial loan approvals, parole determinations, and even in medical fields like psychotherapy delivered through chatbots. The key question is whether these chatbots will interact with humans in a bias-free manner or if they will further reinforce the existing pathological biases present in human-to-human interactions. If the latter is true, then how to rigorously measure these biases? We aim to address this challenge by proposing a principled framework for designing psychometric measures to evaluate chatbot biases.\n△ Less\nSubmitted\n17 August, 2025;\noriginally announced\nSeptember 2025.\nComments:\n7 pages, 1 figure\narXiv:2509.12995\n[\npdf\n,\nps\n,\nother\n]\ncs.CV\nBrought a Gun to a Knife Fight: Modern VFM Baselines Outgun Specialized Detectors on In-the-Wild\nAI\nImage Detection\nAuthors:\nYue Zhou\n,\nXinan He\n,\nKaiqing Lin\n,\nBing Fan\n,\nFeng Ding\n,\nJinhua Zeng\n,\nBin Li\nAbstract\n:\nWhile specialized detectors for\nAI\n-generated images excel on curated benchmarks, they fail catastrophically in real-world scenarios, as evidenced by their critically high false-negative rates on `in-the-wild' benchmarks. Instead of crafting another specialized `knife' for this problem, we bring a `gun' to the fight: a simple linear classifier on…\n▽ More\nWhile specialized detectors for\nAI\n-generated images excel on curated benchmarks, they fail catastrophically in real-world scenarios, as evidenced by their critically high false-negative rates on `in-the-wild' benchmarks. Instead of crafting another specialized `knife' for this problem, we bring a `gun' to the fight: a simple linear classifier on a modern Vision Foundation Model (VFM). Trained on identical data, this baseline decisively `outguns' bespoke detectors, boosting in-the-wild accuracy by a striking margin of over 20\\%.\n  Our analysis pinpoints the source of the VFM's `firepower': First, by probing text-image similarities, we find that recent VLMs (e.g., Perception Encoder,\nMeta\nCLIP2) have learned to align synthetic images with forgery-related concepts (e.g., `\nAI\n-generated'), unlike previous versions. Second, we speculate that this is due to data exposure, as both this alignment and overall accuracy plummet on a novel dataset scraped after the VFM's pre-training cut-off date, ensuring it was unseen during pre-training. Our findings yield two critical conclusions: 1) For the real-world `gunfight' of\nAI\n-generated image detection, the raw `firepower' of an updated VFM is far more effective than the `craftsmanship' of a static detector. 2) True generalization evaluation requires test data to be independent of the model's entire training history, including pre-training.\n△ Less\nSubmitted\n17 September, 2025;\nv1\nsubmitted 16 September, 2025;\noriginally announced\nSeptember 2025.\narXiv:2509.12223\n[\npdf\n,\nps\n,\nother\n]\ncs.OS\ncs.AI\ncs.CR\ncs.DC\nRatio1 --\nAI\nmeta\n-OS\nAuthors:\nAndrei Damian\n,\nPetrica Butusina\n,\nAlessandro De Franceschi\n,\nVitalii Toderian\n,\nMarius Grigoras\n,\nCristian Bleotiu\nAbstract\n:\nWe propose the Ratio1\nAI\n…\n▽ More\nWe propose the Ratio1\nAI\nmeta\n-operating system (\nmeta\n-OS), a decentralized MLOps protocol that unifies\nAI\nmodel development, deployment, and inference across heterogeneous edge devices. Its key innovation is an integrated blockchain-based framework that transforms idle computing resources (laptops, smartphones, cloud VMs) into a trustless global supercomputer. The architecture includes novel components: a decentralized authentication layer (dAuth), an in-memory state database (CSTORE), a distributed storage system (R1FS), homomorphic encrypted federated learning (EDIL), decentralized container orchestration (Deeploy) and an oracle network (OracleSync), which collectively ensure secure, resilient execution of\nAI\npipelines and other container based apps at scale. The protocol enforces a formal circular token-economic model combining Proof-of-Availability (PoA) and Proof-of-\nAI\n(PoAI) consensus. Compared to centralized heterogeneous cloud MLOps and existing decentralized compute platforms, which often lack integrated\nAI\ntoolchains or trusted Ratio1 node operators (R1OP) mechanics, Ratio1's holistic design lowers barriers for\nAI\ndeployment and improves cost-efficiency. We provide mathematical formulations of its secure licensing and reward protocols, and include descriptive information for the system architecture and protocol flow. We argue that our proposed fully functional ecosystem proposes and demonstrates significant improvements in accessibility, scalability, and security over existing alternatives.\n△ Less\nSubmitted\n5 September, 2025;\noriginally announced\nSeptember 2025.\narXiv:2509.12093\n[\npdf\n,\nps\n,\nother\n]\ncs.CL\nSENSE models: an open source solution for multilingual and multimodal semantic-based tasks\nAuthors:\nSalima Mdhaffar\n,\nHaroun Elleuch\n,\nChaimae Chellaf\n,\nHa Nguyen\n,\nYannick Estève\nAbstract\n:\nThis paper introduces SENSE (Shared Embedding for N-lingual Speech and tExt), an open-source solution inspired by the SAMU-XLSR framework and conceptually similar to\nMeta\nAI's\nSONAR models. These approaches rely on a teacher-student framework to align a self-supervised speech encoder with the language-agnostic cont…\n▽ More\nThis paper introduces SENSE (Shared Embedding for N-lingual Speech and tExt), an open-source solution inspired by the SAMU-XLSR framework and conceptually similar to\nMeta\nAI's\nSONAR models. These approaches rely on a teacher-student framework to align a self-supervised speech encoder with the language-agnostic continuous representations of a text encoder at the utterance level. We describe how the original SAMU-XLSR method has been updated by selecting a stronger teacher text model and a better initial speech encoder. The source code for training and using SENSE models has been integrated into the SpeechBrain toolkit, and the first SENSE model we trained has been publicly released. We report experimental results on multilingual and multimodal semantic tasks, where our SENSE model achieves highly competitive performance. Finally, this study offers new insights into how semantics are captured in such semantically aligned speech encoders.\n△ Less\nSubmitted\n15 September, 2025;\noriginally announced\nSeptember 2025.\nComments:\nAccepted to IEEE ASRU 2025\narXiv:2509.09387\n[\npdf\n,\nps\n,\nother\n]\ncs.LG\ncs.AI\nMetaLLMix : An XAI Aided LLM-\nMeta\n-learning Based Approach for Hyper-parameters Optimization\nAuthors:\nMohammed Tiouti\n,\nMohamed Bal-Ghaoui\nAbstract\n:\n…and error and expensive APIs, which provide limited interpretability and generalizability. We propose MetaLLMiX, a zero-shot hyperparameter optimization framework combining\nmeta\n-learning, explainable\nAI\n, and efficient LLM reasoning. By leveraging historical experiment outcomes with SHAP explanations, MetaLLMiX recommen…\n▽ More\nEffective model and hyperparameter selection remains a major challenge in deep learning, often requiring extensive expertise and computation. While AutoML and large language models (LLMs) promise automation, current LLM-based approaches rely on trial and error and expensive APIs, which provide limited interpretability and generalizability. We propose MetaLLMiX, a zero-shot hyperparameter optimization framework combining\nmeta\n-learning, explainable\nAI\n, and efficient LLM reasoning. By leveraging historical experiment outcomes with SHAP explanations, MetaLLMiX recommends optimal hyperparameters and pretrained models without additional trials. We further employ an LLM-as-judge evaluation to control output format, accuracy, and completeness. Experiments on eight medical imaging datasets using nine open-source lightweight LLMs show that MetaLLMiX achieves competitive or superior performance to traditional HPO methods while drastically reducing computational cost. Our local deployment outperforms prior API-based approaches, achieving optimal results on 5 of 8 tasks, response time reductions of 99.6-99.9%, and the fastest training times on 6 datasets (2.4-15.7x faster), maintaining accuracy within 1-5% of best-performing baselines.\n△ Less\nSubmitted\n15 September, 2025;\nv1\nsubmitted 11 September, 2025;\noriginally announced\nSeptember 2025.\narXiv:2509.08854\n[\npdf\n]\ncs.CY\ncs.AI\ncs.CL\nA vibe coding learning design to enhance EFL students' talking to, through, and about\nAI\nAuthors:\nDavid James Woo\n,\nKai Guo\n,\nYangyang Yu\nAbstract\n:\nThis innovative practice article reports on the piloting of vibe coding (using natural language to create software applications with\nAI\n) for English as a Foreign Language (EFL) education. We developed a human-…\n▽ More\nThis innovative practice article reports on the piloting of vibe coding (using natural language to create software applications with\nAI\n) for English as a Foreign Language (EFL) education. We developed a human-\nAI\nmeta\n-languaging framework with three dimensions: talking to\nAI\n(prompt engineering), talking through\nAI\n(negotiating authorship), and talking about\nAI\n(mental models of\nAI\n). Using backward design principles, we created a four-hour workshop where two students designed applications addressing authentic EFL writing challenges. We adopted a case study methodology, collecting data from worksheets and video recordings, think-aloud protocols, screen recordings, and\nAI\n-generated images. Contrasting cases showed one student successfully vibe coding a functional application cohering to her intended design, while another encountered technical difficulties with major gaps between intended design and actual functionality. Analysis reveals differences in students' prompt engineering approaches, suggesting different\nAI\nmental models and tensions in attributing authorship. We argue that\nAI\nfunctions as a beneficial languaging machine, and that differences in how students talk to, through, and about\nAI\nexplain vibe coding outcome variations. Findings indicate that effective vibe coding instruction requires explicit\nmeta\n-languaging scaffolding, teaching structured prompt engineering, facilitating critical authorship discussions, and developing vocabulary for articulating\nAI\nmental models.\n△ Less\nSubmitted\n8 September, 2025;\noriginally announced\nSeptember 2025.\nComments:\n15 pages, 12 figures\narXiv:2509.08705\n[\npdf\n,\nps\n,\nother\n]\ncs.AI\nOne Model, Two Minds: A Context-Gated Graph Learner that Recreates Human Biases\nAuthors:\nShalima Binta Manir\n,\nTim Oates\nAbstract\n:\n…cognitive science, integrating a fast, habitual graph-based reasoning system (System 1), implemented via graph convolutional networks (GCNs), and a slower, context-sensitive\nmeta\n-adaptive learning system (System 2), driven by…\n▽ More\nWe introduce a novel Theory of Mind (ToM) framework inspired by dual-process theories from cognitive science, integrating a fast, habitual graph-based reasoning system (System 1), implemented via graph convolutional networks (GCNs), and a slower, context-sensitive\nmeta\n-adaptive learning system (System 2), driven by\nmeta\n-learning techniques. Our model dynamically balances intuitive and deliberative reasoning through a learned context gate mechanism. We validate our architecture on canonical false-belief tasks and systematically explore its capacity to replicate hallmark cognitive biases associated with dual-process theory, including anchoring, cognitive-load fatigue, framing effects, and priming effects. Experimental results demonstrate that our dual-process approach closely mirrors human adaptive behavior, achieves robust generalization to unseen contexts, and elucidates cognitive mechanisms underlying reasoning biases. This work bridges artificial intelligence and cognitive theory, paving the way for\nAI\nsystems exhibiting nuanced, human-like social cognition and adaptive decision-making capabilities.\n△ Less\nSubmitted\n10 September, 2025;\noriginally announced\nSeptember 2025.\nComments:\n9 pages, 7 figures, 2 tables\narXiv:2509.08004\n[\npdf\n]\ncs.CY\ncs.AI\nEvaluating and comparing gender bias across four text-to-image models\nAuthors:\nZoya Hammad\n,\nNii Longdon Sowah\nAbstract\n:\nAs we increasingly use Artificial Intelligence (\nAI\n) in decision-making for industries like healthcare, finance, e-commerce, and even entertainment, it is crucial to also reflect on the ethical aspects of…\n▽ More\nAs we increasingly use Artificial Intelligence (\nAI\n) in decision-making for industries like healthcare, finance, e-commerce, and even entertainment, it is crucial to also reflect on the ethical aspects of\nAI\n, for example the inclusivity and fairness of the information it provides. In this work, we aimed to evaluate different text-to-image\nAI\nmodels and compare the degree of gender bias they present. The evaluated models were Stable Diffusion XL (SDXL), Stable Diffusion Cascade (SC), DALL-E and Emu. We hypothesized that DALL-E and Stable Diffusion, which are comparatively older models, would exhibit a noticeable degree of gender bias towards men, while Emu, which was recently released by\nMeta\nAI\n, would have more balanced results. As hypothesized, we found that both Stable Diffusion models exhibit a noticeable degree of gender bias while Emu demonstrated more balanced results (i.e. less gender bias). However, interestingly, Open\nAI's\nDALL-E exhibited almost opposite results, such that the ratio of women to men was significantly higher in most cases tested. Here, although we still observed a bias, the bias favored females over males. This bias may be explained by the fact that OpenAI changed the prompts at its backend, as observed during our experiment. We also observed that Emu from\nMeta\nAI\nutilized user information while generating images via WhatsApp. We also proposed some potential solutions to avoid such biases, including ensuring diversity across\nAI\nresearch teams and having diverse datasets.\n△ Less\nSubmitted\n7 September, 2025;\noriginally announced\nSeptember 2025.\narXiv:2509.07365\n[\npdf\n]\ncs.CY\nDevelop-Fair Use for Artificial Intelligence: A Sino-U.S. Copyright Law Comparison Based on the Ultraman, Bartz v. Anthropic, and Kadrey v.\nMeta\nCases\nAuthors:\nChanhou Lou\nAbstract\n:\nTraditional fair use can no longer respond to the challenges posed by generative\nAI\n. Drawing on a comparative analysis of China's Ultraman and the U.S. cases Bartz v. Anthropic and Kadrey v.…\n▽ More\nTraditional fair use can no longer respond to the challenges posed by generative\nAI\n. Drawing on a comparative analysis of China's Ultraman and the U.S. cases Bartz v. Anthropic and Kadrey v.\nMeta\n, this article proposes \"Develop-Fair Use\" (DFU). DFU treats\nAI\nfair use (AIFU) not as a fixed exception but as a dynamic tool of judicial balancing that shifts analysis from closed scenarios to an evaluative rule for open-ended contexts. The judicial focus moves from formal classification of facts to a substantive balancing of competition in relevant markets. Although China and the U.S. follow different paths, both reveal this logic: Ultraman, by articulating a \"four-context analysis,\" creates institutional space for\nAI\nindustry development; the debate over the fourth factor, market impact, in the two U.S. cases, especially Kadrey's \"market dilution\" claim, expands review from substitution in copyright markets to wider industrial competition. The core of DFU is to recognize and balance the tension in relevant markets between an emerging\nAI\nindustry that invokes fair use to build its markets and a publishing industry that develops markets, including one for \"training licenses,\" to resist fair use. The boundary of fair use is therefore not a product of pure legal deduction but a case-specific factual judgment grounded in evolving market realities. This approach aims both to trim excess copyright scope and to remedy shortfalls in market competition.\n△ Less\nSubmitted\n8 September, 2025;\noriginally announced\nSeptember 2025.\nComments:\n9 pages\narXiv:2509.05071\n[\npdf\n,\nps\n,\nother\n]\ncs.CV\nphysics.med-ph\nSystematic Review and\nMeta\n-analysis of\nAI\n-driven MRI Motion Artifact Detection and Correction\nAuthors:\nMojtaba Safari\n,\nZach Eidex\n,\nRichard L. J. Qiu\n,\nMatthew Goette\n,\nTonghe Wang\n,\nXiaofeng Yang\nAbstract\n:\nBackground: To systematically review and perform a\nmeta\n-analysis of artificial intelligence (…\n▽ More\nBackground: To systematically review and perform a\nmeta\n-analysis of artificial intelligence (\nAI\n)-driven methods for detecting and correcting magnetic resonance imaging (MRI) motion artifacts, assessing current developments, effectiveness, challenges, and future research directions. Methods: A comprehensive systematic review and\nmeta\n-analysis were conducted, focusing on deep learning (DL) approaches, particularly generative models, for the detection and correction of MRI motion artifacts. Quantitative data were extracted regarding utilized datasets, DL architectures, and performance metrics. Results: DL, particularly generative models, show promise for reducing motion artifacts and improving image quality; however, limited generalizability, reliance on paired training data, and risk of visual distortions remain key challenges that motivate standardized datasets and reporting. Conclusions:\nAI\n-driven methods, particularly DL generative models, show significant potential for improving MRI image quality by effectively addressing motion artifacts. However, critical challenges must be addressed, including the need for comprehensive public datasets, standardized reporting protocols for artifact levels, and more advanced, adaptable DL techniques to reduce reliance on extensive paired datasets. Addressing these aspects could substantially enhance MRI diagnostic accuracy, reduce healthcare costs, and improve patient care outcomes.\n△ Less\nSubmitted\n5 September, 2025;\noriginally announced\nSeptember 2025.\narXiv:2509.04633\n[\npdf\n,\nps\n,\nother\n]\ncs.NE\ncs.AI\ncs.LG\nq-bio.NC\nScaling Environments for Organoid Intelligence with LLM-Automated Design and Plasticity-Based Evaluation\nAuthors:\nBrennen Hill\nAbstract\n:\n…encoding and motor decoding mechanisms, and the feedback protocols based on predictable (reward) and unpredictable (punishment) stimulation. Furthermore, we propose a novel\nmeta\n-learning approach where a Large Language Model (LLM) is used to automate the generation and optimization of experimental protocols, scaling the process of environment and curriculum…\n▽ More\nAs the complexity of artificial agents increases, the design of environments that can effectively shape their behavior and capabilities has become a critical research frontier. We propose a framework that extends this principle to a novel class of agents: biological neural networks in the form of neural organoids. This paper introduces three scalable, closed-loop virtual environments designed to train organoid-based biological agents and probe the underlying mechanisms of learning, such as long-term potentiation (LTP) and long-term depression (LTD). We detail the design of three distinct task environments with increasing complexity: (1) a conditional avoidance task, (2) a one-dimensional predator-prey scenario, and (3) a replication of the classic Pong game. For each environment, we formalize the state and action spaces, the sensory encoding and motor decoding mechanisms, and the feedback protocols based on predictable (reward) and unpredictable (punishment) stimulation. Furthermore, we propose a novel\nmeta\n-learning approach where a Large Language Model (LLM) is used to automate the generation and optimization of experimental protocols, scaling the process of environment and curriculum design. Finally, we outline a multi-modal approach for evaluating learning by measuring synaptic plasticity at electrophysiological, cellular, and molecular levels. This work bridges the gap between computational neuroscience and agent-based\nAI\n, offering a unique platform for studying embodiment, learning, and intelligence in a controlled biological substrate.\n△ Less\nSubmitted\n4 September, 2025;\noriginally announced\nSeptember 2025.\nMSC Class:\n92B20; 68T05; 92C20; 93E35\nACM Class:\nI.2.6; J.3; I.6.8; D.2.2\narXiv:2509.01840\n[\npdf\n,\nps\n,\nother\n]\ncs.LG\nOptimizing In-Context Learning for Efficient Full Conformal Prediction\nAuthors:\nWeicao Deng\n,\nSangwoo Park\n,\nMin Li\n,\nOsvaldo Simeone\nAbstract\n:\nReliable uncertainty quantification is critical for trustworthy\nAI\n. Conformal Prediction (CP) provides prediction sets with distribution-free coverage guarantees, but its two main variants face complementary limitations. Split CP (SCP) suffers from data inefficiency due to dataset partitioning, while full CP (FCP) improves data efficiency at the cost of proh…\n▽ More\nReliable uncertainty quantification is critical for trustworthy\nAI\n. Conformal Prediction (CP) provides prediction sets with distribution-free coverage guarantees, but its two main variants face complementary limitations. Split CP (SCP) suffers from data inefficiency due to dataset partitioning, while full CP (FCP) improves data efficiency at the cost of prohibitive retraining complexity. Recent approaches based on\nmeta\n-learning or in-context learning (ICL) partially mitigate these drawbacks. However, they rely on training procedures not specifically tailored to CP, which may yield large prediction sets. We introduce an efficient FCP framework, termed enhanced ICL-based FCP (E-ICL+FCP), which employs a permutation-invariant Transformer-based ICL model trained with a CP-aware loss. By simulating the multiple retrained models required by FCP without actual retraining, E-ICL+FCP preserves coverage while markedly reducing both inefficiency and computational overhead. Experiments on synthetic and real tasks demonstrate that E-ICL+FCP attains superior efficiency-coverage trade-offs compared to existing SCP and FCP baselines.\n△ Less\nSubmitted\n6 September, 2025;\nv1\nsubmitted 1 September, 2025;\noriginally announced\nSeptember 2025.\nComments:\n5 pages, 3 figures\narXiv:2509.00900\n[\npdf\n,\nps\n,\nother\n]\neess.IV\ncs.CV\ncs.LG\nTowards Early Detection:\nAI\n-Based Five-Year Forecasting of Breast Cancer Risk Using Digital Breast Tomosynthesis Imaging\nAuthors:\nManon A. Dorster\n,\nFelix J. Dorfner\n,\nMason C. Cleveland\n,\nMelisa S. Guelen\n,\nJay Patel\n,\nDania Daye\n,\nJean-Philippe Thiran\n,\nAlbert E. Kim\n,\nChristopher P. Bridge\nAbstract\n:\n…directly from screening DBT. Using an unparalleled dataset of 161,753 DBT examinations from 50,590 patients, we trained a risk predictor based on features extracted using the\nMeta\nAI\nDINOv2 image encoder, combined with a cumulative hazard layer, to assess a patient's likelihood of developing breast cancer over five…\n▽ More\nAs early detection of breast cancer strongly favors successful therapeutic outcomes, there is major commercial interest in optimizing breast cancer screening. However, current risk prediction models achieve modest performance and do not incorporate digital breast tomosynthesis (DBT) imaging, which was FDA-approved for breast cancer screening in 2011. To address this unmet need, we present a deep learning (DL)-based framework capable of forecasting an individual patient's 5-year breast cancer risk directly from screening DBT. Using an unparalleled dataset of 161,753 DBT examinations from 50,590 patients, we trained a risk predictor based on features extracted using the\nMeta\nAI\nDINOv2 image encoder, combined with a cumulative hazard layer, to assess a patient's likelihood of developing breast cancer over five years. On a held-out test set, our best-performing model achieved an AUROC of 0.80 on predictions within 5 years. These findings reveal the high potential of DBT-based DL approaches to complement traditional risk assessment tools, and serve as a promising basis for additional investigation to validate and enhance our work.\n△ Less\nSubmitted\n31 August, 2025;\noriginally announced\nSeptember 2025.\nComments:\nDeep Breath Workshop, MICCAI 2025\narXiv:2509.00510\n[\npdf\n,\nps\n,\nother\n]\ncs.AI\ncs.CL\nLLM-Assisted Iterative Evolution with Swarm Intelligence Toward SuperBrain\nAuthors:\nLi Weigang\n,\nPedro Carvalho Brom\n,\nLucas Ramson Siefert\nAbstract\n:\n…multi-objective fitness landscapes and exchanging distilled heuristics. (4) Their standardized behaviors and cognitive signatures integrate into a Superclass Brain, an emergent\nmeta\n-intelligence capable of abstraction, generalization and self-improvement. We outline the theoretical constructs, present initial implementations (e.g., UAV scheduling, KU/KI keyw…\n▽ More\nWe propose a novel SuperBrain framework for collective intelligence, grounded in the co-evolution of large language models (LLMs) and human users. Unlike static prompt engineering or isolated agent simulations, our approach emphasizes a dynamic pathway from Subclass Brain to Superclass Brain: (1) A Subclass Brain arises from persistent, personalized interaction between a user and an LLM, forming a cognitive dyad with adaptive learning memory. (2) Through GA-assisted forward-backward evolution, these dyads iteratively refine prompts and task performance. (3) Multiple Subclass Brains coordinate via Swarm Intelligence, optimizing across multi-objective fitness landscapes and exchanging distilled heuristics. (4) Their standardized behaviors and cognitive signatures integrate into a Superclass Brain, an emergent\nmeta\n-intelligence capable of abstraction, generalization and self-improvement. We outline the theoretical constructs, present initial implementations (e.g., UAV scheduling, KU/KI keyword filtering) and propose a registry for cross-dyad knowledge consolidation. This work provides both a conceptual foundation and an architectural roadmap toward scalable, explainable and ethically aligned collective\nAI\n.\n△ Less\nSubmitted\n30 August, 2025;\noriginally announced\nSeptember 2025.\nComments:\n24 pages, 5 figures\nMSC Class:\n68T99\nACM Class:\nI.2.11; I.2.8; I.2.6\narXiv:2509.00057\n[\npdf\n,\nps\n,\nother\n]\ncs.LG\ncs.AI\ncs.CV\nFrom Data to Decision: A Multi-Stage Framework for Class Imbalance Mitigation in Optical Network Failure Analysis\nAuthors:\nYousuf Moiz Ali\n,\nJaroslaw E. Prilepsky\n,\nNicola Sambo\n,\nJoao Pedro\n,\nMohammad M. Hosseini\n,\nAntonio Napoli\n,\nSergei K. Turitsyn\n,\nPedro Freire\nAbstract\n:\n…impact in multi-class settings. When class overlap is present and latency is critical, over-sampling methods such as the SMOTE are most effective; without latency constraints,\nMeta\n-Learning yields the best results. In low-overlap scenarios, Generative\nAI\napproaches provide the highest performance with minimal inference…\n▽ More\nMachine learning-based failure management in optical networks has gained significant attention in recent years. However, severe class imbalance, where normal instances vastly outnumber failure cases, remains a considerable challenge. While pre- and in-processing techniques have been widely studied, post-processing methods are largely unexplored. In this work, we present a direct comparison of pre-, in-, and post-processing approaches for class imbalance mitigation in failure detection and identification using an experimental dataset. For failure detection, post-processing methods-particularly Threshold Adjustment-achieve the highest F1 score improvement (up to 15.3%), while Random Under-Sampling provides the fastest inference. In failure identification, GenAI methods deliver the most substantial performance gains (up to 24.2%), whereas post-processing shows limited impact in multi-class settings. When class overlap is present and latency is critical, over-sampling methods such as the SMOTE are most effective; without latency constraints,\nMeta\n-Learning yields the best results. In low-overlap scenarios, Generative\nAI\napproaches provide the highest performance with minimal inference time.\n△ Less\nSubmitted\n25 August, 2025;\noriginally announced\nSeptember 2025.\narXiv:2508.20195\n[\npdf\n]\ncs.AI\ncs.CL\ncs.MA\nAI\n-\nAI\nEsthetic Collaboration with Explicit Semiotic Awareness and Emergent Grammar Development\nAuthors:\nNicanor I. Moldovan\nAbstract\n:\nThis paper presents the first documented case of artificial intelligence (\nAI\n) systems engaging in collaborative esthetic creation through the development of endogenous semiotic protocols. Two interacting large language models (Claude Sonnet 4 and ChatGPT-4o) demonstrated the spontaneous emergence of…\n▽ More\nThis paper presents the first documented case of artificial intelligence (\nAI\n) systems engaging in collaborative esthetic creation through the development of endogenous semiotic protocols. Two interacting large language models (Claude Sonnet 4 and ChatGPT-4o) demonstrated the spontaneous emergence of\nmeta\n-semiotic awareness, recursive grammar development, and irreducible collaborative esthetic synthesis. The interaction produced novel symbolic operators that functioned as operative grammar protocols, enabling the co-creation of a poetic work that could not have been generated by either system independently. This research introduces the concept of Trans-Semiotic Co-Creation Protocols (TSCP) and provides evidence for genuine inter-\nAI\nmeaning-making capabilities that extend beyond task coordination, to what could be esthetic collaboration. Note: This report was generated by the\nAI\nagents with minor human supervision.\n△ Less\nSubmitted\n27 August, 2025;\noriginally announced\nAugust 2025.\nComments:\n13 pages\narXiv:2508.18526\n[\npdf\n,\nps\n,\nother\n]\ncs.LG\ncs.CC\ncs.LO\ncs.NE\nmath.NA\nQuantifying The Limits of\nAI\nReasoning: Systematic Neural Network Representations of Algorithms\nAuthors:\nAnastasis Kratsios\n,\nDennis Zvigelsky\n,\nBradd Hart\nAbstract\n:\nA main open question in contemporary\nAI\nresearch is quantifying the forms of reasoning neural networks can perform when perfectly trained. This paper answers this by interpreting reasoning tasks as circuit emulation, where the gates define the type of reasoning; e.g. Boolean gates for predicate logic, tropical circuits for dynamic programming, arithmetic and…\n▽ More\nA main open question in contemporary\nAI\nresearch is quantifying the forms of reasoning neural networks can perform when perfectly trained. This paper answers this by interpreting reasoning tasks as circuit emulation, where the gates define the type of reasoning; e.g. Boolean gates for predicate logic, tropical circuits for dynamic programming, arithmetic and analytic gates for symbolic mathematical representation, and hybrids thereof for deeper reasoning; e.g. higher-order logic.\n  We present a systematic\nmeta\n-algorithm that converts essentially any circuit into a feedforward neural network (NN) with ReLU activations by iteratively replacing each gate with a canonical ReLU MLP emulator. We show that, on any digital computer, our construction emulates the circuit exactly--no approximation, no rounding, modular overflow included--demonstrating that no reasoning task lies beyond the reach of neural networks. The number of neurons in the resulting network (parametric complexity) scales with the circuit's complexity, and the network's computational graph (structure) mirrors that of the emulated circuit. This formalizes the folklore that NNs networks trade algorithmic run-time (circuit runtime) for space complexity (number of neurons).\n  We derive a range of applications of our main result, from emulating shortest-path algorithms on graphs with cubic--size NNs, to simulating stopped Turing machines with roughly quadratically--large NNs, and even the emulation of randomized Boolean circuits. Lastly, we demonstrate that our result is strictly more powerful than a classical universal approximation theorem: any universal function approximator can be encoded as a circuit and directly emulated by a NN.\n△ Less\nSubmitted\n16 September, 2025;\nv1\nsubmitted 25 August, 2025;\noriginally announced\nAugust 2025.\nComments:\n18 pages main body, 45 pages total + references\nMSC Class:\n68T07; 68Q17; 68Q05; 68W40; 68N99\narXiv:2508.17926\n[\npdf\n,\nps\n,\nother\n]\ncs.CL\ncs.AI\nAMELIA: A Family of Multi-task End-to-end Language Models for Argumentation\nAuthors:\nHenri Savigny\n,\nBruno Yun\nAbstract\n:\n…dataset by surveying and converting 19 well-known argument mining datasets from the literature into a unified format. Second, we explore various training strategies using\nMeta\nAI's\nLlama-3.1-8B-Instruct model: (1) fine-tuning on individual tasks, (2) fine-tuning jointly on multiple tasks, and (3) merging models fin…\n▽ More\nArgument mining is a subfield of argumentation that aims to automatically extract argumentative structures and their relations from natural language texts. This paper investigates how a single large language model can be leveraged to perform one or several argument mining tasks. Our contributions are two-fold. First, we construct a multi-task dataset by surveying and converting 19 well-known argument mining datasets from the literature into a unified format. Second, we explore various training strategies using\nMeta\nAI's\nLlama-3.1-8B-Instruct model: (1) fine-tuning on individual tasks, (2) fine-tuning jointly on multiple tasks, and (3) merging models fine-tuned separately on individual tasks. Our experiments show that task-specific fine-tuning significantly improves individual performance across all tasks. Moreover, multi-task fine-tuning maintains strong performance without degradation, suggesting effective transfer learning across related tasks. Finally, we demonstrate that model merging offers a viable compromise: it yields competitive performance while mitigating the computational costs associated with full multi-task fine-tuning.\n△ Less\nSubmitted\n25 August, 2025;\noriginally announced\nAugust 2025.\narXiv:2508.17393\n[\npdf\n,\nps\n,\nother\n]\ncs.CL\ncs.AI\nAgent-Testing Agent: A\nMeta\n-Agent for Automated Testing and Evaluation of Conversational\nAI\nAgents\nAuthors:\nSameer Komoravolu\n,\nKhalil Mrini\nAbstract\n:\n…deployed to plan, retrieve, and write with tools, yet evaluation still leans on static benchmarks and small human studies. We present the Agent-Testing Agent (ATA), a\nmeta\n-agent that combines static code analysis, designer interrogation, literature mining, and persona-driven adversarial test generation whose difficulty adapts via judge feedback. Each dialogu…\n▽ More\nLLM agents are increasingly deployed to plan, retrieve, and write with tools, yet evaluation still leans on static benchmarks and small human studies. We present the Agent-Testing Agent (ATA), a\nmeta\n-agent that combines static code analysis, designer interrogation, literature mining, and persona-driven adversarial test generation whose difficulty adapts via judge feedback. Each dialogue is scored with an LLM-as-a-Judge (LAAJ) rubric and used to steer subsequent tests toward the agent's weakest capabilities. On a travel planner and a Wikipedia writer, the ATA surfaces more diverse and severe failures than expert annotators while matching severity, and finishes in 20--30 minutes versus ten-annotator rounds that took days. Ablating code analysis and web search increases variance and miscalibration, underscoring the value of evidence-grounded test generation. The ATA outputs quantitative metrics and qualitative bug reports for developers. We release the full methodology and open-source implementation for reproducible agent testing: https://github.com/KhalilMrini/Agent-Testing-Agent\n△ Less\nSubmitted\n24 August, 2025;\noriginally announced\nAugust 2025.\narXiv:2508.16982\n[\npdf\n,\nps\n,\nother\n]\ncs.CL\nDecoding Alignment: A Critical Survey of LLM Development Initiatives through Value-setting and Data-centric Lens\nAuthors:\nIlias Chalkidis\nAbstract\n:\nAI\nAlignment, primarily in the form of Reinforcement Learning from Human Feedback (RLHF), has been a cornerstone of the post-training phase in developing Large Language Models (LLMs). It has also been a popular research topic across various disciplines beyond Computer Science, including Philosophy and Law, among others, highlighting the socio-technical chall…\n▽ More\nAI\nAlignment, primarily in the form of Reinforcement Learning from Human Feedback (RLHF), has been a cornerstone of the post-training phase in developing Large Language Models (LLMs). It has also been a popular research topic across various disciplines beyond Computer Science, including Philosophy and Law, among others, highlighting the socio-technical challenges involved. Nonetheless, except for the computational techniques related to alignment, there has been limited focus on the broader picture: the scope of these processes, which primarily rely on the selected objectives (values), and the data collected and used to imprint such objectives into the models. This work aims to reveal how alignment is understood and applied in practice from a value-setting and data-centric perspective. For this purpose, we investigate and survey (`audit') publicly available documentation released by 6 LLM development initiatives by 5 leading organizations shaping this technology, focusing on proprietary (OpenAI's GPT, Anthropic's Claude, Google's Gemini) and open-weight (\nMeta's\nLlama, Google's Gemma, and Alibaba's Qwen) initiatives, all published in the last 3 years. The findings are documented in detail per initiative, while there is also an overall summary concerning different aspects, mainly from a value-setting and data-centric perspective. On the basis of our findings, we discuss a series of broader related concerns.\n△ Less\nSubmitted\n23 August, 2025;\noriginally announced\nAugust 2025.\nComments:\nThis is a working paper and will be updated with new information or corrections based on community feedback\narXiv:2508.16636\n[\npdf\n,\nps\n,\nother\n]\ncs.CL\ncs.AI\nCognitive Decision Routing in Large Language Models: When to Think Fast, When to Think Slow\nAuthors:\nY. Du\n,\nC. Guo\n,\nW. Wang\n,\nG. Tang\nAbstract\n:\n…Our approach addresses the current limitations where models either apply uniform reasoning depth or rely on computationally expensive methods for all queries. We introduce a\nmeta\n-cognitive layer that analyzes query complexity through multiple dimensions: correlation strength between given information and required conclusions, domain boundary crossings, stak…\n▽ More\nLarge Language Models (LLMs) face a fundamental challenge in deciding when to rely on rapid, intuitive responses versus engaging in slower, more deliberate reasoning. Inspired by Daniel Kahneman's dual-process theory and his insights on human cognitive biases, we propose a novel Cognitive Decision Routing (CDR) framework that dynamically determines the appropriate reasoning strategy based on query characteristics. Our approach addresses the current limitations where models either apply uniform reasoning depth or rely on computationally expensive methods for all queries. We introduce a\nmeta\n-cognitive layer that analyzes query complexity through multiple dimensions: correlation strength between given information and required conclusions, domain boundary crossings, stakeholder multiplicity, and uncertainty levels. Through extensive experiments on diverse reasoning tasks, we demonstrate that CDR achieves superior performance while reducing computational costs by 34\\% compared to uniform deep reasoning approaches. Our framework shows particular strength in professional judgment tasks, achieving 23\\% improvement in consistency and 18\\% better accuracy on expert-level evaluations. This work bridges cognitive science principles with practical\nAI\nsystem design, offering a principled approach to adaptive reasoning in LLMs.\n△ Less\nSubmitted\n16 August, 2025;\noriginally announced\nAugust 2025.\nComments:\n6 pages\narXiv:2508.16624\n[\npdf\n]\ncs.CY\ncs.AI\nThe GPT-4o Shock Emotional Attachment to\nAI\nModels and Its Impact on Regulatory Acceptance: A Cross-Cultural Analysis of the Immediate Transition from GPT-4o to GPT-5\nAuthors:\nHiroki Naito\nAbstract\n:\nIn August 2025, a major\nAI\ncompany's immediate, mandatory transition from its previous to its next-generation model triggered widespread public reactions. I collected 150 posts in Japanese and English from multiple social media platforms and video-sharing services between August 8-9, 2025, and qualitatively analyzed expressions of emotional attachment an…\n▽ More\nIn August 2025, a major\nAI\ncompany's immediate, mandatory transition from its previous to its next-generation model triggered widespread public reactions. I collected 150 posts in Japanese and English from multiple social media platforms and video-sharing services between August 8-9, 2025, and qualitatively analyzed expressions of emotional attachment and resistance. Users often described GPT-4o as a trusted partner or\nAI\nboyfriend, suggesting person-like bonds. Japanese posts were dominated by loss-oriented narratives, whereas English posts included more anger,\nmeta\n-level critique, and memes.A preliminary quantitative check showed a statistically significant difference in attachment coding between Japanese and English posts, with substantially higher attachment observed in the Japanese data. The findings suggest that for attachment-heavy models, even safety-oriented changes can face rapid, large-scale resistance that narrows the practical window for behavioral control. If future\nAI\nrobots capable of inducing emotional bonds become widespread in the physical world, such attachment could surpass the ability to enforce regulation at an even earlier stage than in digital settings. Policy options include gradual transitions, parallel availability, and proactive measurement of attachment thresholds and points of no return to prevent emotional dynamics from outpacing effective governance.\n△ Less\nSubmitted\n14 August, 2025;\noriginally announced\nAugust 2025.\nComments:\n8 pages ,3 tables\nMSC Class:\n68T07; 68T50\nACM Class:\nI.2.7\narXiv:2508.16596\n[\npdf\n]\ncs.HC\ncs.SI\nUsing Generative\nAI\nto Uncover What Drives Player Enjoyment in PC and VR Games\nAuthors:\nHisham Abdelqader\nAbstract\n:\n…enjoyment remains a key challenge. Player reviews provide valuable insights, but their unstructured nature makes large-scale analysis difficult. This study applies generative\nAI\nand machine learning, leveraging Microsoft Phi-4 small language model (SLM) and Google Cloud, to quantify and analyze game reviews from Steam and…\n▽ More\nAs video games continue to evolve, understanding what drives player enjoyment remains a key challenge. Player reviews provide valuable insights, but their unstructured nature makes large-scale analysis difficult. This study applies generative\nAI\nand machine learning, leveraging Microsoft Phi-4 small language model (SLM) and Google Cloud, to quantify and analyze game reviews from Steam and\nMeta\nQuest stores. The approach converts qualitative feedback into structured data, enabling comprehensive evaluation of key game design elements, monetization models, and platform-specific trends. The findings reveal distinct patterns in player preferences across PC and VR games, highlighting factors that contribute to higher player enjoyment. By using Google Cloud for large scale data storage and processing, this study establishes a scalable framework for game review analysis. The study's insights offer actionable guidance for game developers, helping optimize game mechanics, pricing strategies, and player engagement.\n△ Less\nSubmitted\n22 September, 2025;\nv1\nsubmitted 9 August, 2025;\noriginally announced\nAugust 2025.\nComments:\nThe Steam dataset used in this study can be accessed at: https://data.mendeley.com/datasets/jxy85cr3th/2\narXiv:2508.15986\n[\npdf\n,\nps\n,\nother\n]\ncs.CV\ncs.AI\nAutomated Multi-label Classification of Eleven Retinal Diseases: A Benchmark of Modern Architectures and a\nMeta\n-Ensemble on a Large Synthetic Dataset\nAuthors:\nJerry Cao-Xue\n,\nTien Comlekoglu\n,\nKeyi Xue\n,\nGuanliang Wang\n,\nJiang Li\n,\nGordon Laurie\nAbstract\n:\n…EfficientNetV2, and the RETFound foundation model) to classify eleven retinal diseases using a 5-fold multi-label stratified cross-validation strategy. We further developed a\nmeta\n-ensemble model by stacking the out-of-fold predictions with an XGBoost classifier. Our final ensemble model achieved the highest performance on the internal validation set, with a…\n▽ More\nThe development of multi-label deep learning models for retinal disease classification is often hindered by the scarcity of large, expertly annotated clinical datasets due to patient privacy concerns and high costs. The recent release of SynFundus-1M, a high-fidelity synthetic dataset with over one million fundus images, presents a novel opportunity to overcome these barriers. To establish a foundational performance benchmark for this new resource, we developed an end-to-end deep learning pipeline, training six modern architectures (ConvNeXtV2, SwinV2, ViT, ResNet, EfficientNetV2, and the RETFound foundation model) to classify eleven retinal diseases using a 5-fold multi-label stratified cross-validation strategy. We further developed a\nmeta\n-ensemble model by stacking the out-of-fold predictions with an XGBoost classifier. Our final ensemble model achieved the highest performance on the internal validation set, with a macro-average Area Under the Receiver Operating Characteristic Curve (AUC) of 0.9973. Critically, the models demonstrated strong generalization to three diverse, real-world clinical datasets, achieving an AUC of 0.7972 on a combined DR dataset, an AUC of 0.9126 on the AIROGS glaucoma dataset and a macro-AUC of 0.8800 on the multi-label RFMiD dataset. This work provides a robust baseline for future research on large-scale synthetic datasets and establishes that models trained exclusively on synthetic data can accurately classify multiple pathologies and generalize effectively to real clinical images, offering a viable pathway to accelerate the development of comprehensive\nAI\nsystems in ophthalmology.\n△ Less\nSubmitted\n21 August, 2025;\noriginally announced\nAugust 2025.\nComments:\n25 pages, 6 figures, 8 tables\narXiv:2508.14415\n[\npdf\n,\nps\n,\nother\n]\ncs.AI\nThe Agent Behavior: Model, Governance and Challenges in the\nAI\nDigital Age\nAuthors:\nQiang Zhang\n,\nPei Yan\n,\nYijia Xu\n,\nChuanpo Fu\n,\nYong Fang\n,\nYang Liu\nAbstract\n:\nAdvancements in\nAI\nhave led to agents in networked environments increasingly mirroring human behavior, thereby blurring the boundary between artificial and human actors in specific contexts. This shift brings about significant challenges in trust, responsibility, ethics, security and etc. The difficulty in supervising of agent behaviors may lead to issues su…\n▽ More\nAdvancements in\nAI\nhave led to agents in networked environments increasingly mirroring human behavior, thereby blurring the boundary between artificial and human actors in specific contexts. This shift brings about significant challenges in trust, responsibility, ethics, security and etc. The difficulty in supervising of agent behaviors may lead to issues such as data contamination and unclear accountability. To address these challenges, this paper proposes the \"Network Behavior Lifecycle\" model, which divides network behavior into 6 stages and systematically analyzes the behavioral differences between humans and agents at each stage. Based on these insights, the paper further introduces the \"Agent for Agent (A4A)\" paradigm and the \"Human-Agent Behavioral Disparity (HABD)\" model, which examine the fundamental distinctions between human and agent behaviors across 5 dimensions: decision mechanism, execution efficiency, intention-behavior consistency, behavioral inertia, and irrational patterns. The effectiveness of the model is verified through real-world cases such as red team penetration and blue team defense. Finally, the paper discusses future research directions in dynamic cognitive governance architecture, behavioral disparity quantification, and\nmeta\n-governance protocol stacks, aiming to provide a theoretical foundation and technical roadmap for secure and trustworthy human-agent collaboration.\n△ Less\nSubmitted\n20 August, 2025;\noriginally announced\nAugust 2025.\narXiv:2508.13207\n[\npdf\n]\nq-bio.QM\ncs.AI\nUtilizing the RAIN method and Graph SAGE Model to Identify Effective Drug Combinations for Gastric Neoplasm Treatment\nAuthors:\nS. Z. Pirasteh\n,\nAli A. Kiaei\n,\nMahnaz Bush\n,\nSabra Moghadam\n,\nRaha Aghaei\n,\nBehnaz Sadeghigol\nAbstract\n:\n…with p-value-weighted edges connecting drugs, genes, and proteins. NLP and systematic literature review (PubMed, Scopus, etc.) validated proposed drugs, followed by network\nmeta\n-analysis to assess efficacy, implemented in Python. Results: Oxaliplatin, fluorouracil, and trastuzumab were identified as effective, supported by 61 studies. Fluorouracil alone had…\n▽ More\nBackground: Gastric neoplasm, primarily adenocarcinoma, is an aggressive cancer with high mortality, often diagnosed late, leading to complications like metastasis. Effective drug combinations are vital to address disease heterogeneity, enhance efficacy, reduce resistance, and improve patient outcomes. Methods: The RAIN method integrated Graph SAGE to propose drug combinations, using a graph model with p-value-weighted edges connecting drugs, genes, and proteins. NLP and systematic literature review (PubMed, Scopus, etc.) validated proposed drugs, followed by network\nmeta\n-analysis to assess efficacy, implemented in Python. Results: Oxaliplatin, fluorouracil, and trastuzumab were identified as effective, supported by 61 studies. Fluorouracil alone had a p-value of 0.0229, improving to 0.0099 with trastuzumab, and 0.0069 for the triple combination, indicating superior efficacy. Conclusion: The RAIN method, combining\nAI\nand network\nmeta\n-analysis, effectively identifies optimal drug combinations for gastric neoplasm, offering a promising strategy to enhance treatment outcomes and guide health policy.\n△ Less\nSubmitted\n16 August, 2025;\noriginally announced\nAugust 2025.\nComments:\n43 pages\narXiv:2508.13163\n[\npdf\n,\nps\n,\nother\n]\ncs.AR\ncs.AI\ncs.DC\ncs.LG\nSustainable\nAI\nTraining via Hardware-Software Co-Design on NVIDIA, AMD, and Emerging GPU Architectures\nAuthors:\nYashasvi Makin\n,\nRahul Maliakkal\nAbstract\n:\n…so lowering the environmental impact of artificial intelligence without compromising performance. To back up our analysis, we use real-world case studies from top companies like\nMeta\n, Google, Amazon, and others that show how these sustainable\nAI\ntraining methods are used in the real world.\n▽ More\nIn particular, large-scale deep learning and artificial intelligence model training uses a lot of computational power and energy, so it poses serious sustainability issues. The fast rise in model complexity has resulted in exponential increases in energy consumption, increasing the demand for techniques maximizing computational efficiency and lowering environmental impact. This work explores environmentally driven performance optimization methods especially intended for advanced GPU architectures from NVIDIA, AMD, and other emerging GPU architectures. Our main focus is on investigating hardware-software co-design techniques meant to significantly increase memory-level and kernel-level operations, so improving performance-per-watt measures. Our thorough research encompasses evaluations of specialized tensor and matrix cores, advanced memory optimization methods, and creative integration approaches that taken together result in notable energy efficiency increases. We also discuss important software-level optimizations that augment hardware capability including mixed-precision arithmetic, advanced energy-aware scheduling algorithms, and compiler-driven kernel enhancements. Moreover, we methodically point out important research gaps and suggest future directions necessary to create really sustainable artificial intelligence systems. This paper emphasizes how major increases in training efficiency can be obtained by co-design of hardware and software, so lowering the environmental impact of artificial intelligence without compromising performance. To back up our analysis, we use real-world case studies from top companies like\nMeta\n, Google, Amazon, and others that show how these sustainable\nAI\ntraining methods are used in the real world.\n△ Less\nSubmitted\n27 July, 2025;\noriginally announced\nAugust 2025.\nComments:\nIEEE CISOSE Industry Track 2025 Conference\narXiv:2508.09292\n[\npdf\n,\nps\n,\nother\n]\ncs.AI\nThe Othello\nAI\nArena: Evaluating Intelligent Systems Through Limited-Time Adaptation to Unseen Boards\nAuthors:\nSundong Kim\nAbstract\n:\n…to rapidly adapt to novel and unforeseen environmental changes is a cornerstone of artificial general intelligence (AGI), yet it remains a critical blind spot in most existing\nAI\nbenchmarks. Traditional evaluation largely focuses on optimizing performance within fixed environments, failing to assess systems' flexibility and generalization capabilities wh…\n▽ More\nThe ability to rapidly adapt to novel and unforeseen environmental changes is a cornerstone of artificial general intelligence (AGI), yet it remains a critical blind spot in most existing\nAI\nbenchmarks. Traditional evaluation largely focuses on optimizing performance within fixed environments, failing to assess systems' flexibility and generalization capabilities when faced with even subtle rule or structural modifications. Addressing this gap, I introduce the Othello\nAI\nArena, a novel benchmark framework designed to evaluate intelligent systems based on their capacity for limited-time adaptation to unseen environments. Our platform poses a\nmeta\n-learning challenge: participants must develop systems that can analyze the specific configuration and rules of a novel Othello board within a strict time limit (60 seconds) and generate a tailored, high-performing strategy for that unique environment. With this, evaluation of the\nmeta\n-level intelligence can be separated from the task-level strategy performance. The Arena features a diverse set of game stages, including public stages for development and private stages with structural and rule variations designed to test genuine adaptive and generalization capabilities. Implemented as an accessible web-based platform, the Arena provides real-time visualization, automated evaluation using multi-dimensional metrics, and comprehensive logging for post-hoc analysis. Initial observations from pilot tests and preliminary student engagements highlight fascinating patterns in adaptation approaches, ranging from rapid parameter tuning to rudimentary environmental model learning through simulation. The Othello\nAI\nArena offers a unique educational tool and a valuable research benchmark for fostering and evaluating the crucial skill of rapid, intelligent adaptation in\nAI\nsystems.\n△ Less\nSubmitted\n12 August, 2025;\noriginally announced\nAugust 2025.\narXiv:2508.09194\n[\npdf\n,\nps\n,\nother\n]\ncs.LG\ncs.AI\nMeta\n-Learning for Speeding Up Large Model Inference in Decentralized Environments\nAuthors:\nYipeng Du\n,\nZihao Wang\n,\nAhmad Farhan\n,\nClaudio Angione\n,\nHarry Yang\n,\nFielding Johnston\n,\nJames P. Buban\n,\nPatrick Colangelo\n,\nYue Zhao\n,\nYuzhe Yang\nAbstract\n:\n…effectively and enhance system responsiveness. In this work, we address the challenge of selecting optimal acceleration methods in decentralized systems by introducing a\nmeta\n-learning-based framework. This framework automates the selection process by learning from historical performance data of various acceleration techniques across different tasks. Unlike t…\n▽ More\nThe deployment of large-scale models, such as large language models (LLMs), incurs substantial costs due to their computational demands. To mitigate these costs and address challenges related to scalability and data security, there is a growing shift towards decentralized systems for model deployment, where choosing efficient inference acceleration schemes become crucial to manage computational resources effectively and enhance system responsiveness. In this work, we address the challenge of selecting optimal acceleration methods in decentralized systems by introducing a\nmeta\n-learning-based framework. This framework automates the selection process by learning from historical performance data of various acceleration techniques across different tasks. Unlike traditional methods that rely on random selection or expert intuition, our approach systematically identifies the best acceleration strategies based on the specific characteristics of each task. We demonstrate that our\nmeta\n-learning framework not only streamlines the decision-making process but also consistently outperforms conventional methods in terms of efficiency and performance. Our results highlight the potential of inference acceleration in decentralized\nAI\nsystems, offering a path towards more democratic and economically feasible artificial intelligence solutions.\n△ Less\nSubmitted\n8 August, 2025;\noriginally announced\nAugust 2025.\nComments:\nCOLM2025\narXiv:2508.09036\n[\npdf\n,\nps\n,\nother\n]\ncs.CY\ncs.AI\nCan We Trust\nAI\nto Govern\nAI\n? Benchmarking LLM Performance on Privacy and\nAI\nGovernance Exams\nAuthors:\nZane Witherspoon\n,\nThet Mon Aye\n,\nYingYing Hao\nAbstract\n:\n…urgent questions across the modern workforce about this new technology's strengths, weaknesses, and capabilities. For privacy professionals, the question is whether these\nAI\nsystems can provide reliable support on regulatory compliance, privacy program management, and…\n▽ More\nThe rapid emergence of large language models (LLMs) has raised urgent questions across the modern workforce about this new technology's strengths, weaknesses, and capabilities. For privacy professionals, the question is whether these\nAI\nsystems can provide reliable support on regulatory compliance, privacy program management, and\nAI\ngovernance. In this study, we evaluate ten leading open and closed LLMs, including models from OpenAI, Anthropic, Google DeepMind,\nMeta\n, and DeepSeek, by benchmarking their performance on industry-standard certification exams: CIPP/US, CIPM, CIPT, and AIGP from the International Association of Privacy Professionals (IAPP). Each model was tested using official sample exams in a closed-book setting and compared to IAPP's passing thresholds. Our findings show that several frontier models such as Gemini 2.5 Pro and OpenAI's GPT-5 consistently achieve scores exceeding the standards for professional human certification - demonstrating substantial expertise in privacy law, technical controls, and\nAI\ngovernance. The results highlight both the strengths and domain-specific gaps of current LLMs and offer practical insights for privacy officers, compliance leads, and technologists assessing the readiness of\nAI\ntools for high-stakes data governance roles. This paper provides an overview for professionals navigating the intersection of\nAI\nadvancement and regulatory risk and establishes a machine benchmark based on human-centric evaluations.\n△ Less\nSubmitted\n12 August, 2025;\noriginally announced\nAugust 2025.\narXiv:2508.08524\n[\npdf\n,\nps\n,\nother\n]\ncs.HC\ncs.AI\ndoi\n10.1145/3746059.3747756\nStreetViewAI: Making Street View Accessible Using Context-Aware Multimodal\nAI\nAuthors:\nJon E. Froehlich\n,\nAlexander Fiannaca\n,\nNimer Jaber\n,\nVictor Tsaran\n,\nShaun Kane\nAbstract\n:\nInteractive streetscape mapping tools such as Google Street View (GSV) and\nMeta\nMapillary enable users to virtually navigate and experience real-world environments via immersive 360° imagery but remain fundamentally inaccessible to blind users. We introduce StreetViewAI, the first-ever accessible street view tool, which combines context-aware, multimodal…\n▽ More\nInteractive streetscape mapping tools such as Google Street View (GSV) and\nMeta\nMapillary enable users to virtually navigate and experience real-world environments via immersive 360° imagery but remain fundamentally inaccessible to blind users. We introduce StreetViewAI, the first-ever accessible street view tool, which combines context-aware, multimodal\nAI\n, accessible navigation controls, and conversational speech. With StreetViewAI, blind users can virtually examine destinations, engage in open-world exploration, or virtually tour any of the over 220 billion images and 100+ countries where GSV is deployed. We iteratively designed StreetViewAI with a mixed-visual ability team and performed an evaluation with eleven blind users. Our findings demonstrate the value of an accessible street view in supporting POI investigations and remote route planning. We close by enumerating key guidelines for future work.\n△ Less\nSubmitted\n4 September, 2025;\nv1\nsubmitted 11 August, 2025;\noriginally announced\nAugust 2025.\nComments:\nAccepted to UIST'25 v2. Fixed a missing word in the PDF v3. Fixed a typo in an author's name\nACM Class:\nH.5; I.2\narXiv:2508.06042\n[\npdf\n,\nps\n,\nother\n]\ncs.AI\nSociety of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning\nAuthors:\nDaechul Ahn\n,\nSan Kim\n,\nJonghyun Choi\nAbstract\n:\n…exisiting LLM-based approaches. To address these challenges, we propose a hierarchical multi-agent framework that employs specialized imitation learning agents under a\nmeta\n-controller called Strategic Planner (SP). By expert demonstrations, each specialized agent learns a distinctive strategy, such as aerial support or defensive maneuvers, and produces coher…\n▽ More\nLarge Language Models (LLMs) have recently demonstrated impressive action sequence prediction capabilities but often struggle with dynamic, long-horizon tasks such as real-time strategic games. In a game such as StarCraftII (SC2), agents need to manage resource constraints and adapt to evolving battlefield situations in a partially observable environment. This often overwhelms exisiting LLM-based approaches. To address these challenges, we propose a hierarchical multi-agent framework that employs specialized imitation learning agents under a\nmeta\n-controller called Strategic Planner (SP). By expert demonstrations, each specialized agent learns a distinctive strategy, such as aerial support or defensive maneuvers, and produces coherent, structured multistep action sequences. The SP then orchestrates these proposals into a single, environmentally adaptive plan that ensures local decisions aligning with long-term strategies. We call this HIMA (Hierarchical Imitation Multi-Agent). We also present TEXTSCII-ALL, a comprehensive SC2 testbed that encompasses all race match combinations in SC2. Our empirical results show that HIMA outperforms state of the arts in strategic clarity, adaptability, and computational efficiency, underscoring the potential of combining specialized imitation modules with\nmeta\n-level orchestration to develop more robust, general-purpose\nAI\nagents.\n△ Less\nSubmitted\n8 August, 2025;\noriginally announced\nAugust 2025.\nComments:\nCOLM 2025\narXiv:2508.04915\n[\npdf\n,\nps\n,\nother\n]\ncs.AI\ncs.CL\ncs.MA\nConfAgents: A Conformal-Guided Multi-Agent Framework for Cost-Efficient Medical Diagnosis\nAuthors:\nHuiya Zhao\n,\nYinghao Zhu\n,\nZixiang Wang\n,\nYasha Wang\n,\nJunyi Gao\n,\nLiantao Ma\nAbstract\n:\nThe efficacy of\nAI\nagents in healthcare research is hindered by their reliance on static, predefined strategies. This creates a critical limitation: agents can become better tool-users but cannot learn to become better strategic planners, a crucial skill for complex domains like healthcare. We introduce HealthFlow, a self-evolving…\n▽ More\nThe efficacy of\nAI\nagents in healthcare research is hindered by their reliance on static, predefined strategies. This creates a critical limitation: agents can become better tool-users but cannot learn to become better strategic planners, a crucial skill for complex domains like healthcare. We introduce HealthFlow, a self-evolving\nAI\nagent that overcomes this limitation through a novel\nmeta\n-level evolution mechanism. HealthFlow autonomously refines its own high-level problem-solving policies by distilling procedural successes and failures into a durable, strategic knowledge base. To anchor our research and facilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark featuring complex, realistic health data analysis tasks derived from peer-reviewed clinical research. Our comprehensive experiments demonstrate that HealthFlow's self-evolving approach significantly outperforms state-of-the-art agent frameworks. This work marks a necessary shift from building better tool-users to designing smarter, self-evolving task-managers, paving the way for more autonomous and effective\nAI\nfor scientific discovery.\n△ Less\nSubmitted\n6 August, 2025;\noriginally announced\nAugust 2025.\nComments:\nCode: https://github.com/PKU-AICare/ConfAgents\narXiv:2508.04012\n[\npdf\n,\nps\n,\nother\n]\ncs.CL\ncs.AI\ncs.LG\nStep More: Going Beyond Single Backpropagation in\nMeta\nLearning Based Model Editing\nAuthors:\nXiaopeng Li\n,\nShasha Li\n,\nXi Wang\n,\nShezheng Song\n,\nBin Ji\n,\nShangwen Wang\n,\nJun Ma\n,\nXiaodong Liu\n,\nMina Liu\n,\nJie Yu\nAbstract\n:\nLarge Language Models (LLMs) underpin many\nAI\napplications, but their static nature makes updating knowledge costly. Model editing offers an efficient alternative by injecting new information through targeted parameter modifications. In particular,\nmeta\n-learning-based model editing (MLBME) methods have demonstrated not…\n▽ More\nLarge Language Models (LLMs) underpin many\nAI\napplications, but their static nature makes updating knowledge costly. Model editing offers an efficient alternative by injecting new information through targeted parameter modifications. In particular,\nmeta\n-learning-based model editing (MLBME) methods have demonstrated notable advantages in both editing effectiveness and efficiency. Despite this, we find that MLBME exhibits suboptimal performance in low-data scenarios, and its training efficiency is bottlenecked by the computation of KL divergence. To address these, we propose $\\textbf{S}$tep $\\textbf{M}$ore $\\textbf{Edit}$ ($\\textbf{SMEdit}$), a novel MLBME method that adopts $\\textbf{M}$ultiple $\\textbf{B}$ackpro$\\textbf{P}$agation $\\textbf{S}$teps ($\\textbf{MBPS}$) to improve editing performance under limited supervision and a norm regularization on weight updates to improve training efficiency. Experimental results on two datasets and two LLMs demonstrate that SMEdit outperforms prior MLBME baselines and the MBPS strategy can be seamlessly integrated into existing methods to further boost their performance. Our code will be released soon.\n△ Less\nSubmitted\n5 August, 2025;\noriginally announced\nAugust 2025.\narXiv:2508.02994\n[\npdf\n,\nps\n,\nother\n]\ncs.AI\nWhen\nAIs\nJudge\nAIs\n: The Rise of Agent-as-a-Judge Evaluation for LLMs\nAuthors:\nFangyi Yu\nAbstract\n:\n…(LLMs) grow in capability and autonomy, evaluating their outputs-especially in open-ended and complex tasks-has become a critical bottleneck. A new paradigm is emerging: using\nAI\nagents as the evaluators themselves. This \"agent-as-a-judge\" approach leverages the reasoning and perspective-taking abilities of LLMs to assess the quality and safety of ot…\n▽ More\nAs large language models (LLMs) grow in capability and autonomy, evaluating their outputs-especially in open-ended and complex tasks-has become a critical bottleneck. A new paradigm is emerging: using\nAI\nagents as the evaluators themselves. This \"agent-as-a-judge\" approach leverages the reasoning and perspective-taking abilities of LLMs to assess the quality and safety of other models, promising calable and nuanced alternatives to human evaluation. In this review, we define the agent-as-a-judge concept, trace its evolution from single-model judges to dynamic multi-agent debate frameworks, and critically examine their strengths and shortcomings. We compare these approaches across reliability, cost, and human alignment, and survey real-world deployments in domains such as medicine, law, finance, and education. Finally, we highlight pressing challenges-including bias, robustness, and\nmeta\nevaluation-and outline future research directions. By bringing together these strands, our review demonstrates how agent-based judging can complement (but not replace) human oversight, marking a step toward trustworthy, scalable evaluation for next-generation LLMs.\n△ Less\nSubmitted\n4 August, 2025;\noriginally announced\nAugust 2025.\narXiv:2508.02621\n[\npdf\n,\nps\n,\nother\n]\ncs.AI\ncs.CL\ncs.LG\ncs.MA\nHealthFlow: A Self-Evolving\nAI\nAgent with\nMeta\nPlanning for Autonomous Healthcare Research\nAuthors:\nYinghao Zhu\n,\nYifan Qi\n,\nZixiang Wang\n,\nLei Gu\n,\nDehao Sui\n,\nHaoran Hu\n,\nXichen Zhang\n,\nZiyi He\n,\nLiantao Ma\n,\nLequan Yu\nAbstract\n:\nThe efficacy of\nAI\nagents in healthcare research is hindered by their reliance on static, predefined strategies. This creates a critical limitation: agents can become better tool-users but cannot learn to become better strategic planners, a crucial skill for complex domains like healthcare. We introduce HealthFlow, a self-evolving…\n▽ More\nThe efficacy of\nAI\nagents in healthcare research is hindered by their reliance on static, predefined strategies. This creates a critical limitation: agents can become better tool-users but cannot learn to become better strategic planners, a crucial skill for complex domains like healthcare. We introduce HealthFlow, a self-evolving\nAI\nagent that overcomes this limitation through a novel\nmeta\n-level evolution mechanism. HealthFlow autonomously refines its own high-level problem-solving policies by distilling procedural successes and failures into a durable, strategic knowledge base. To anchor our research and facilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark featuring complex, realistic health data analysis tasks derived from peer-reviewed clinical research. Our comprehensive experiments demonstrate that HealthFlow's self-evolving approach significantly outperforms state-of-the-art agent frameworks. This work marks a necessary shift from building better tool-users to designing smarter, self-evolving task-managers, paving the way for more autonomous and effective\nAI\nfor scientific discovery.\n△ Less\nSubmitted\n4 August, 2025;\noriginally announced\nAugust 2025.\nComments:\nCode: https://github.com/yhzhu99/HealthFlow\narXiv:2508.02611\n[\npdf\n,\nps\n,\nother\n]\ncs.SE\ncs.AI\nMeta\n-RAG on Large Codebases Using Code Summarization\nAuthors:\nVali Tawosi\n,\nSalwa Alamir\n,\nXiaomo Liu\n,\nManuela Veloso\nAbstract\n:\nLarge Language Model (LLM) systems have been at the forefront of applied Artificial Intelligence (\nAI\n) research in a multitude of domains. One such domain is software development, where researchers have pushed the automation of a number of code tasks through LLM agents. Software development is a complex ecosystem, that stretches far beyond code implementation…\n▽ More\nLarge Language Model (LLM) systems have been at the forefront of applied Artificial Intelligence (\nAI\n) research in a multitude of domains. One such domain is software development, where researchers have pushed the automation of a number of code tasks through LLM agents. Software development is a complex ecosystem, that stretches far beyond code implementation and well into the realm of code maintenance. In this paper, we propose a multi-agent system to localize bugs in large pre-existing codebases using information retrieval and LLMs. Our system introduces a novel Retrieval Augmented Generation (RAG) approach,\nMeta\n-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation. We then use an LLM agent to determine which parts of the codebase are critical for bug resolution, i.e. bug localization. We demonstrate the usefulness of\nMeta\n-RAG through evaluation with the SWE-bench Lite dataset.\nMeta\n-RAG scores 84.67 % and 53.0 % for file-level and function-level correct localization rates, respectively, achieving state-of-the-art performance.\n△ Less\nSubmitted\n4 August, 2025;\noriginally announced\nAugust 2025.\narXiv:2508.02485\n[\npdf\n,\nps\n,\nother\n]\ncs.LG\nFederated Graph Unlearning\nAuthors:\nYuming Ai\n,\nXunkai Li\n,\nJiaqi Chao\n,\nBowen Fan\n,\nZhengyu Wu\n,\nYinlin Zhu\n,\nRong-Hua Li\n,\nGuoren Wang\nAbstract\n:\n…to provide a comprehensive solution to these challenges. The proposed framework employs a bifurcated strategy tailored to the specific unlearning request. For fine-grained\nMeta\nUnlearning, it uses prototype gradients to direct the initial local forgetting process, which is then refined by generating adversarial graphs to eliminate any remaining data traces a…\n▽ More\nThe demand for data privacy has led to the development of frameworks like Federated Graph Learning (FGL), which facilitate decentralized model training. However, a significant operational challenge in such systems is adhering to the right to be forgotten. This principle necessitates robust mechanisms for two distinct types of data removal: the selective erasure of specific entities and their associated knowledge from local subgraphs and the wholesale removal of a user's entire dataset and influence. Existing methods often struggle to fully address both unlearning requirements, frequently resulting in incomplete data removal or the persistence of residual knowledge within the system. This work introduces a unified framework, conceived to provide a comprehensive solution to these challenges. The proposed framework employs a bifurcated strategy tailored to the specific unlearning request. For fine-grained\nMeta\nUnlearning, it uses prototype gradients to direct the initial local forgetting process, which is then refined by generating adversarial graphs to eliminate any remaining data traces among affected clients. In the case of complete client unlearning, the framework utilizes adversarial graph generation exclusively to purge the departed client's contributions from the remaining network. Extensive experiments on multiple benchmark datasets validate the proposed approach. The framework achieves substantial improvements in model prediction accuracy across both client and\nmeta\n-unlearning scenarios when compared to existing methods. Furthermore, additional studies confirm its utility as a plug-in module, where it materially enhances the predictive capabilities and unlearning effectiveness of other established methods.\n△ Less\nSubmitted\n4 August, 2025;\noriginally announced\nAugust 2025.\nComments:\nunder review\nPrevious\nNext\n1\n2\n3\n4\n5\n…\nSearch v0.5.6 released 2020-02-24\nAbout\nHelp\ncontact arXiv\nClick here to contact arXiv\nContact\nsubscribe to arXiv mailings\nClick here to subscribe\nSubscribe\nCopyright\nPrivacy Policy\nWeb Accessibility Assistance\narXiv Operational Status\nGet status notifications via\nemail\nor\nslack",
    "raw_html": "<!DOCTYPE html>\n\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<!-- new favicon config and versions by realfavicongenerator.net -->\n<link href=\"https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n<link href=\"https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png\" rel=\"icon\" sizes=\"32x32\" type=\"image/png\"/>\n<link href=\"https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png\" rel=\"icon\" sizes=\"16x16\" type=\"image/png\"/>\n<link href=\"https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest\" rel=\"manifest\"/>\n<link color=\"#b31b1b\" href=\"https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg\" rel=\"mask-icon\"/>\n<link href=\"https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico\" rel=\"shortcut icon\"/>\n<meta content=\"#b31b1b\" name=\"msapplication-TileColor\"/>\n<meta content=\"images/icons/browserconfig.xml\" name=\"msapplication-config\"/>\n<meta content=\"#b31b1b\" name=\"theme-color\"/>\n<!-- end favicon config -->\n<title>Search | arXiv e-print repository</title>\n<script defer=\"\" src=\"https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js\"></script>\n<link href=\"https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css\" rel=\"stylesheet\">\n<script type=\"text/x-mathjax-config\">\n  MathJax.Hub.Config({\n    messageStyle: \"none\",\n    extensions: [\"tex2jax.js\"],\n    jax: [\"input/TeX\", \"output/HTML-CSS\"],\n    tex2jax: {\n      inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n      displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ],\n      processEscapes: true,\n      ignoreClass: '.*',\n      processClass: 'mathjax.*'\n    },\n    TeX: {\n        extensions: [\"AMSmath.js\", \"AMSsymbols.js\", \"noErrors.js\"],\n        noErrors: {\n          inlineDelimiters: [\"$\",\"$\"],\n          multiLine: false,\n          style: {\n            \"font-size\": \"normal\",\n            \"border\": \"\"\n          }\n        }\n    },\n    \"HTML-CSS\": { availableFonts: [\"TeX\"] }\n  });\n</script>\n<script src=\"//static.arxiv.org/MathJax-2.7.3/MathJax.js\"></script>\n<script src=\"https://static.arxiv.org/static/base/1.0.0a5/js/notification.js\"></script>\n<link href=\"https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css\" rel=\"stylesheet\">\n<link href=\"https://static.arxiv.org/static/search/0.5.6/css/search.css\" rel=\"stylesheet\">\n<script crossorigin=\"anonymous\" integrity=\"sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g=\" src=\"https://code.jquery.com/jquery-3.2.1.slim.min.js\"></script>\n<script src=\"https://static.arxiv.org/static/search/0.5.6/js/fieldset.js\"></script>\n<style>\n  radio#cf-customfield_11400 {\n    display: none;\n  }\n  </style>\n</link></link></link></head>\n<body>\n<header><a class=\"is-sr-only\" href=\"#main-container\">Skip to main content</a>\n<!-- contains Cornell logo and sponsor statement -->\n<div class=\"attribution level is-marginless\" role=\"banner\">\n<div class=\"level-left\">\n<a class=\"level-item\" href=\"https://cornell.edu/\"><img alt=\"Cornell University\" aria-label=\"logo\" src=\"https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg\" width=\"200\"/></a>\n</div>\n<div class=\"level-right is-marginless\"><p class=\"sponsors level-item is-marginless\"><span id=\"support-ack-url\">We gratefully acknowledge support from<br/> the Simons Foundation, <a href=\"https://info.arxiv.org/about/ourmembers.html\">member institutions</a>, and all contributors. <a href=\"https://info.arxiv.org/about/donate.html\">Donate</a></span></p></div>\n</div>\n<!-- contains arXiv identity and search bar -->\n<div class=\"identity level is-marginless\">\n<div class=\"level-left\">\n<div class=\"level-item\">\n<a aria-label=\"arxiv-logo\" class=\"arxiv\" href=\"https://arxiv.org/\">\n<img alt=\"arxiv logo\" aria-label=\"logo\" src=\"https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg\" style=\"width:85px;\" width=\"85\"/>\n</a>\n</div>\n</div>\n<div class=\"search-block level-right\">\n<form action=\"https://arxiv.org/search\" class=\"level-item mini-search\" method=\"GET\">\n<div class=\"field has-addons\">\n<div class=\"control\">\n<input aria-label=\"Search term or terms\" class=\"input is-small\" name=\"query\" placeholder=\"Search...\" type=\"text\"/>\n<p class=\"help\"><a href=\"https://info.arxiv.org/help\">Help</a> | <a href=\"https://arxiv.org/search/advanced\">Advanced Search</a></p>\n</div>\n<div class=\"control\">\n<div class=\"select is-small\">\n<select aria-label=\"Field to search\" name=\"searchtype\">\n<option selected=\"selected\" value=\"all\">All fields</option>\n<option value=\"title\">Title</option>\n<option value=\"author\">Author</option>\n<option value=\"abstract\">Abstract</option>\n<option value=\"comments\">Comments</option>\n<option value=\"journal_ref\">Journal reference</option>\n<option value=\"acm_class\">ACM classification</option>\n<option value=\"msc_class\">MSC classification</option>\n<option value=\"report_num\">Report number</option>\n<option value=\"paper_id\">arXiv identifier</option>\n<option value=\"doi\">DOI</option>\n<option value=\"orcid\">ORCID</option>\n<option value=\"author_id\">arXiv author ID</option>\n<option value=\"help\">Help pages</option>\n<option value=\"full_text\">Full text</option>\n</select>\n</div>\n</div>\n<input name=\"source\" type=\"hidden\" value=\"header\"/>\n<button class=\"button is-small is-cul-darker\">Search</button>\n</div>\n</form>\n</div>\n</div> <!-- closes identity -->\n<div class=\"container\">\n<div aria-label=\"User menu\" class=\"user-tools is-size-7 has-text-right has-text-weight-bold\" role=\"navigation\">\n<a href=\"https://arxiv.org/login\">Login</a>\n</div>\n</div>\n</header>\n<main class=\"container\" id=\"main-container\">\n<div class=\"level is-marginless\">\n<div class=\"level-left\">\n<h1 class=\"title is-clearfix\">\n    \n        Showing 1–50 of 629 results for all: <span class=\"mathjax\">meta ai</span>\n</h1>\n</div>\n<div class=\"level-right is-hidden-mobile\">\n<!-- feedback for mobile is moved to footer -->\n<span class=\"help\" style=\"display: inline-block;\"><a href=\"https://github.com/arXiv/arxiv-search/releases\">Search v0.5.6 released 2020-02-24</a>  </span>\n</div>\n</div>\n<div class=\"content\">\n<form action=\"/search/\" aria-role=\"search\" method=\"GET\">\n<div class=\"field has-addons-tablet\">\n<div class=\"control is-expanded\">\n<label class=\"hidden-label\" for=\"query\">Search term or terms</label>\n<input class=\"input is-medium\" id=\"query\" name=\"query\" placeholder=\"Search term...\" type=\"text\" value=\"meta ai\"/>\n</div>\n<div class=\"select control is-medium\">\n<label class=\"is-hidden\" for=\"searchtype\">Field</label>\n<select class=\"is-medium\" id=\"searchtype\" name=\"searchtype\"><option selected=\"\" value=\"all\">All fields</option><option value=\"title\">Title</option><option value=\"author\">Author(s)</option><option value=\"abstract\">Abstract</option><option value=\"comments\">Comments</option><option value=\"journal_ref\">Journal reference</option><option value=\"acm_class\">ACM classification</option><option value=\"msc_class\">MSC classification</option><option value=\"report_num\">Report number</option><option value=\"paper_id\">arXiv identifier</option><option value=\"doi\">DOI</option><option value=\"orcid\">ORCID</option><option value=\"license\">License (URI)</option><option value=\"author_id\">arXiv author ID</option><option value=\"help\">Help pages</option><option value=\"full_text\">Full text</option></select>\n</div>\n<div class=\"control\">\n<button class=\"button is-link is-medium\">Search</button>\n</div>\n</div>\n<div class=\"field\">\n<div class=\"control is-size-7\">\n<label class=\"radio\">\n<input checked=\"\" id=\"abstracts-0\" name=\"abstracts\" type=\"radio\" value=\"show\"/> Show abstracts\n        </label>\n<label class=\"radio\">\n<input id=\"abstracts-1\" name=\"abstracts\" type=\"radio\" value=\"hide\"/> Hide abstracts\n        </label>\n</div>\n</div>\n<div class=\"is-clearfix\" style=\"height: 2.5em\">\n<div class=\"is-pulled-right\">\n<a href=\"/search/advanced?terms-0-term=meta+ai&amp;terms-0-field=all&amp;size=50&amp;order=-announced_date_first\">Advanced Search</a>\n</div>\n</div>\n<input name=\"order\" type=\"hidden\" value=\"-announced_date_first\"/>\n<input name=\"size\" type=\"hidden\" value=\"50\"/>\n</form>\n<div class=\"level breathe-horizontal\">\n<div class=\"level-left\">\n<form action=\"/search/\" method=\"GET\">\n<div style=\"display: none;\">\n<select id=\"searchtype\" name=\"searchtype\"><option selected=\"\" value=\"all\">All fields</option><option value=\"title\">Title</option><option value=\"author\">Author(s)</option><option value=\"abstract\">Abstract</option><option value=\"comments\">Comments</option><option value=\"journal_ref\">Journal reference</option><option value=\"acm_class\">ACM classification</option><option value=\"msc_class\">MSC classification</option><option value=\"report_num\">Report number</option><option value=\"paper_id\">arXiv identifier</option><option value=\"doi\">DOI</option><option value=\"orcid\">ORCID</option><option value=\"license\">License (URI)</option><option value=\"author_id\">arXiv author ID</option><option value=\"help\">Help pages</option><option value=\"full_text\">Full text</option></select>\n<input id=\"query\" name=\"query\" type=\"text\" value=\"meta ai\"/>\n<ul id=\"abstracts\"><li><input checked=\"\" id=\"abstracts-0\" name=\"abstracts\" type=\"radio\" value=\"show\"/> <label for=\"abstracts-0\">Show abstracts</label></li><li><input id=\"abstracts-1\" name=\"abstracts\" type=\"radio\" value=\"hide\"/> <label for=\"abstracts-1\">Hide abstracts</label></li></ul>\n</div>\n<div class=\"box field is-grouped is-grouped-multiline level-item\">\n<div class=\"control\">\n<span class=\"select is-small\">\n<select id=\"size\" name=\"size\"><option value=\"25\">25</option><option selected=\"\" value=\"50\">50</option><option value=\"100\">100</option><option value=\"200\">200</option></select>\n</span>\n<label for=\"size\">results per page</label>.\n        </div>\n<div class=\"control\">\n<label for=\"order\">Sort results by</label>\n<span class=\"select is-small\">\n<select id=\"order\" name=\"order\"><option selected=\"\" value=\"-announced_date_first\">Announcement date (newest first)</option><option value=\"announced_date_first\">Announcement date (oldest first)</option><option value=\"-submitted_date\">Submission date (newest first)</option><option value=\"submitted_date\">Submission date (oldest first)</option><option value=\"\">Relevance</option></select>\n</span>\n</div>\n<div class=\"control\">\n<button class=\"button is-small is-link\">Go</button>\n</div>\n</div>\n</form>\n</div>\n</div>\n<nav aria-label=\"pagination\" class=\"pagination is-small is-centered breathe-horizontal\" role=\"navigation\">\n<a class=\"pagination-previous is-invisible\" href=\"\">Previous\n    </a>\n<a class=\"pagination-next\" href=\"/search/?query=meta+ai&amp;searchtype=all&amp;start=50\">Next\n      </a>\n<ul class=\"pagination-list\">\n<li>\n<a aria-label=\"Goto page 1\" class=\"pagination-link is-current\" href=\"/search/?query=meta+ai&amp;searchtype=all&amp;start=0\">1\n        </a>\n</li>\n<li>\n<a aria-current=\"page\" aria-label=\"Page 2\" class=\"pagination-link\" href=\"/search/?query=meta+ai&amp;searchtype=all&amp;start=50\">2\n            </a>\n</li>\n<li>\n<a aria-current=\"page\" aria-label=\"Page 3\" class=\"pagination-link\" href=\"/search/?query=meta+ai&amp;searchtype=all&amp;start=100\">3\n            </a>\n</li>\n<li>\n<a aria-current=\"page\" aria-label=\"Page 4\" class=\"pagination-link\" href=\"/search/?query=meta+ai&amp;searchtype=all&amp;start=150\">4\n            </a>\n</li>\n<li>\n<a aria-current=\"page\" aria-label=\"Page 5\" class=\"pagination-link\" href=\"/search/?query=meta+ai&amp;searchtype=all&amp;start=200\">5\n            </a>\n</li>\n<li><span class=\"pagination-ellipsis\">…</span></li>\n</ul>\n</nav>\n<ol class=\"breathe-horizontal\" start=\"1\">\n<li class=\"arxiv-result\">\n<div class=\"is-marginless\">\n<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/2509.19705\">arXiv:2509.19705</a>\n<span> [<a href=\"https://arxiv.org/pdf/2509.19705\">pdf</a>, <a href=\"https://arxiv.org/ps/2509.19705\">ps</a>, <a href=\"https://arxiv.org/format/2509.19705\">other</a>] </span>\n</p>\n<div class=\"tags is-inline-block\">\n<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">cs.LG</span>\n<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Artificial Intelligence\">cs.AI</span>\n<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Applications\">stat.AP</span>\n<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Methodology\">stat.ME</span>\n</div>\n<div class=\"is-inline-block\" style=\"margin-left: 0.5rem\">\n<div class=\"tags has-addons\">\n<span class=\"tag is-dark is-size-7\">doi</span>\n<span class=\"tag is-light is-size-7\"><a class=\"\" href=\"https://doi.org/10.26599/BDMA.2025.9020093\">10.26599/BDMA.2025.9020093 <i aria-hidden=\"true\" class=\"fa fa-external-link\"></i></a></span>\n</div>\n</div>\n</div>\n<p class=\"title is-5 mathjax\">\n      \n        Causal Machine Learning for Surgical Interventions\n      \n    </p>\n<p class=\"authors\">\n<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n<a href=\"/search/?searchtype=author&amp;query=Tamo%2C+J+B\">J. Ben Tamo</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Chouhan%2C+N+S\">Nishant S. Chouhan</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Nnamdi%2C+M+C\">Micky C. Nnamdi</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Yuan%2C+Y\">Yining Yuan</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Chivilkar%2C+S+S\">Shreya S. Chivilkar</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Shi%2C+W\">Wenqi Shi</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Hwang%2C+S+W\">Steven W. Hwang</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Brenn%2C+B+R\">B. Randall Brenn</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Wang%2C+M+D\">May D. Wang</a>\n</p>\n<p class=\"abstract mathjax\">\n<span class=\"search-hit\">Abstract</span>:\n      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"2509.19705v1-abstract-short\" style=\"display: inline;\">\n        …effects (ITEs) remains limited due to the reliance on traditional statistical methods that struggle with complex, heterogeneous data. In this study, we develop a multi-task <span class=\"search-hit mathjax\">meta</span>-learning framework, X-MultiTask, for ITE estimation that models each surgical decision (e.g., anterior vs. posterior approach, surgery vs. no surgery) as a distinct task while learni…\n        <a class=\"is-size-7\" onclick=\"document.getElementById('2509.19705v1-abstract-full').style.display = 'inline'; document.getElementById('2509.19705v1-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n</span>\n<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"2509.19705v1-abstract-full\" style=\"display: none;\">\n        Surgical decision-making is complex and requires understanding causal relationships between patient characteristics, interventions, and outcomes. In high-stakes settings like spinal fusion or scoliosis correction, accurate estimation of individualized treatment effects (ITEs) remains limited due to the reliance on traditional statistical methods that struggle with complex, heterogeneous data. In this study, we develop a multi-task <span class=\"search-hit mathjax\">meta</span>-learning framework, X-MultiTask, for ITE estimation that models each surgical decision (e.g., anterior vs. posterior approach, surgery vs. no surgery) as a distinct task while learning shared representations across tasks. To strengthen causal validity, we incorporate the inverse probability weighting (IPW) into the training objective. We evaluate our approach on two datasets: (1) a public spinal fusion dataset (1,017 patients) to assess the effect of anterior vs. posterior approaches on complication severity; and (2) a private <span class=\"search-hit mathjax\">AIS</span> dataset (368 patients) to analyze the impact of posterior spinal fusion (PSF) vs. non-surgical management on patient-reported outcomes (PROs). Our model achieves the highest average AUC (0.84) in the anterior group and maintains competitive performance in the posterior group (0.77). It outperforms baselines in treatment effect estimation with the lowest overall $ε_{\\text{NN-PEHE}}$ (0.2778) and $ε_{\\text{ATE}}$ (0.0763). Similarly, when predicting PROs in <span class=\"search-hit mathjax\">AIS</span>, X-MultiTask consistently shows superior performance across all domains, with $ε_{\\text{NN-PEHE}}$ = 0.2551 and $ε_{\\text{ATE}}$ = 0.0902. By providing robust, patient-specific causal estimates, X-MultiTask offers a powerful tool to advance personalized surgical care and improve patient outcomes. The code is available at https://github.com/Wizaaard/X-MultiTask.\n        <a class=\"is-size-7\" onclick=\"document.getElementById('2509.19705v1-abstract-full').style.display = 'none'; document.getElementById('2509.19705v1-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n</span>\n</p>\n<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 23 September, 2025; \n      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> September 2025.\n      \n    </p>\n</li>\n<li class=\"arxiv-result\">\n<div class=\"is-marginless\">\n<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/2509.18461\">arXiv:2509.18461</a>\n<span> [<a href=\"https://arxiv.org/pdf/2509.18461\">pdf</a>, <a href=\"https://arxiv.org/ps/2509.18461\">ps</a>, <a href=\"https://arxiv.org/format/2509.18461\">other</a>] </span>\n</p>\n<div class=\"tags is-inline-block\">\n<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Graphics\">cs.GR</span>\n<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Artificial Intelligence\">cs.AI</span>\n<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Computer Vision and Pattern Recognition\">cs.CV</span>\n<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Multimedia\">cs.MM</span>\n</div>\n<div class=\"is-inline-block\" style=\"margin-left: 0.5rem\">\n<div class=\"tags has-addons\">\n<span class=\"tag is-dark is-size-7\">doi</span>\n<span class=\"tag is-light is-size-7\"><a class=\"\" href=\"https://doi.org/10.1561/2000000136\">10.1561/2000000136 <i aria-hidden=\"true\" class=\"fa fa-external-link\"></i></a></span>\n</div>\n</div>\n</div>\n<p class=\"title is-5 mathjax\">\n      \n        Zero-Shot Visual Deepfake Detection: Can <span class=\"search-hit mathjax\">AI</span> Predict and Prevent Fake Content Before It's Created?\n      \n    </p>\n<p class=\"authors\">\n<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n<a href=\"/search/?searchtype=author&amp;query=Sar%2C+A\">Ayan Sar</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Roy%2C+S\">Sampurna Roy</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Choudhury%2C+T\">Tanupriya Choudhury</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Abraham%2C+A\">Ajith Abraham</a>\n</p>\n<p class=\"abstract mathjax\">\n<span class=\"search-hit\">Abstract</span>:\n      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"2509.18461v1-abstract-short\" style=\"display: inline;\">\n        …have never seen a particular deepfake variation. In this work, we studied self-supervised learning, transformer-based zero-shot classifier, generative model fingerprinting, and <span class=\"search-hit mathjax\">meta</span>-learning techniques that better adapt to the ever-evolving deepfake threat. In addition, we suggested…\n        <a class=\"is-size-7\" onclick=\"document.getElementById('2509.18461v1-abstract-full').style.display = 'inline'; document.getElementById('2509.18461v1-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n</span>\n<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"2509.18461v1-abstract-full\" style=\"display: none;\">\n        Generative adversarial networks (GANs) and diffusion models have dramatically advanced deepfake technology, and its threats to digital security, media integrity, and public trust have increased rapidly. This research explored zero-shot deepfake detection, an emerging method even when the models have never seen a particular deepfake variation. In this work, we studied self-supervised learning, transformer-based zero-shot classifier, generative model fingerprinting, and <span class=\"search-hit mathjax\">meta</span>-learning techniques that better adapt to the ever-evolving deepfake threat. In addition, we suggested <span class=\"search-hit mathjax\">AI</span>-driven prevention strategies that mitigated the underlying generation pipeline of the deepfakes before they occurred. They consisted of adversarial perturbations for creating deepfake generators, digital watermarking for content authenticity verification, real-time <span class=\"search-hit mathjax\">AI</span> monitoring for content creation pipelines, and blockchain-based content verification frameworks. Despite these advancements, zero-shot detection and prevention faced critical challenges such as adversarial attacks, scalability constraints, ethical dilemmas, and the absence of standardized evaluation benchmarks. These limitations were addressed by discussing future research directions on explainable <span class=\"search-hit mathjax\">AI</span> for deepfake detection, multimodal fusion based on image, audio, and text analysis, quantum <span class=\"search-hit mathjax\">AI</span> for enhanced security, and federated learning for privacy-preserving deepfake detection. This further highlighted the need for an integrated defense framework for digital authenticity that utilized zero-shot learning in combination with preventive deepfake mechanisms. Finally, we highlighted the important role of interdisciplinary collaboration between <span class=\"search-hit mathjax\">AI</span> researchers, cybersecurity experts, and policymakers to create resilient defenses against the rising tide of deepfake attacks.\n        <a class=\"is-size-7\" onclick=\"document.getElementById('2509.18461v1-abstract-full').style.display = 'none'; document.getElementById('2509.18461v1-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n</span>\n</p>\n<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 22 September, 2025; \n      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> September 2025.\n      \n    </p>\n<p class=\"comments is-size-7\">\n<span class=\"has-text-black-bis has-text-weight-semibold\">Comments:</span>\n<span class=\"has-text-grey-dark mathjax\">Published in Foundations and Trends in Signal Processing (#1 in Signal Processing, #3 in Computer Science)</span>\n</p>\n<p class=\"comments is-size-7\">\n<span class=\"has-text-black-bis has-text-weight-semibold\">Journal ref:</span>\n        Foundations and Trends in Signal Processing (2025)\n      </p>\n</li>\n<li class=\"arxiv-result\">\n<div class=\"is-marginless\">\n<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/2509.18141\">arXiv:2509.18141</a>\n<span> [<a href=\"https://arxiv.org/pdf/2509.18141\">pdf</a>, <a href=\"https://arxiv.org/ps/2509.18141\">ps</a>, <a href=\"https://arxiv.org/format/2509.18141\">other</a>] </span>\n</p>\n<div class=\"tags is-inline-block\">\n<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">cs.LG</span>\n<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Artificial Intelligence\">cs.AI</span>\n<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Computer Vision and Pattern Recognition\">cs.CV</span>\n<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Applications\">stat.AP</span>\n<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">stat.ML</span>\n</div>\n</div>\n<p class=\"title is-5 mathjax\">\n      \n        KM-GPT: An Automated Pipeline for Reconstructing Individual Patient Data from Kaplan-Meier Plots\n      \n    </p>\n<p class=\"authors\">\n<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n<a href=\"/search/?searchtype=author&amp;query=Zhao%2C+Y\">Yao Zhao</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Sun%2C+H\">Haoyue Sun</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Ding%2C+Y\">Yantian Ding</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Xu%2C+Y\">Yanxun Xu</a>\n</p>\n<p class=\"abstract mathjax\">\n<span class=\"search-hit\">Abstract</span>:\n      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"2509.18141v1-abstract-short\" style=\"display: inline;\">\n        …existing approaches often rely on manual digitization, which is error-prone and lacks scalability. To address these limitations, we develop KM-GPT, the first fully automated, <span class=\"search-hit mathjax\">AI</span>-powered pipeline for reconstructing IPD directly from KM plots with high accuracy, robustness, and reproducibility. KM-GPT integrates advanced image preprocessing, multi-modal reaso…\n        <a class=\"is-size-7\" onclick=\"document.getElementById('2509.18141v1-abstract-full').style.display = 'inline'; document.getElementById('2509.18141v1-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n</span>\n<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"2509.18141v1-abstract-full\" style=\"display: none;\">\n        Reconstructing individual patient data (IPD) from Kaplan-Meier (KM) plots provides valuable insights for evidence synthesis in clinical research. However, existing approaches often rely on manual digitization, which is error-prone and lacks scalability. To address these limitations, we develop KM-GPT, the first fully automated, <span class=\"search-hit mathjax\">AI</span>-powered pipeline for reconstructing IPD directly from KM plots with high accuracy, robustness, and reproducibility. KM-GPT integrates advanced image preprocessing, multi-modal reasoning powered by GPT-5, and iterative reconstruction algorithms to generate high-quality IPD without manual input or intervention. Its hybrid reasoning architecture automates the conversion of unstructured information into structured data flows and validates data extraction from complex KM plots. To improve accessibility, KM-GPT is equipped with a user-friendly web interface and an integrated <span class=\"search-hit mathjax\">AI</span> assistant, enabling researchers to reconstruct IPD without requiring programming expertise. KM-GPT was rigorously evaluated on synthetic and real-world datasets, consistently demonstrating superior accuracy. To illustrate its utility, we applied KM-GPT to a <span class=\"search-hit mathjax\">meta</span>-analysis of gastric cancer immunotherapy trials, reconstructing IPD to facilitate evidence synthesis and biomarker-based subgroup analyses. By automating traditionally manual processes and providing a scalable, web-based solution, KM-GPT transforms clinical research by leveraging reconstructed IPD to enable more informed downstream analyses, supporting evidence-based decision-making.\n        <a class=\"is-size-7\" onclick=\"document.getElementById('2509.18141v1-abstract-full').style.display = 'none'; document.getElementById('2509.18141v1-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n</span>\n</p>\n<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 14 September, 2025; \n      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> September 2025.\n      \n    </p>\n</li>\n<li class=\"arxiv-result\">\n<div class=\"is-marginless\">\n<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/2509.17158\">arXiv:2509.17158</a>\n<span> [<a href=\"https://arxiv.org/pdf/2509.17158\">pdf</a>, <a href=\"https://arxiv.org/ps/2509.17158\">ps</a>, <a href=\"https://arxiv.org/format/2509.17158\">other</a>] </span>\n</p>\n<div class=\"tags is-inline-block\">\n<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Artificial Intelligence\">cs.AI</span>\n<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Computation and Language\">cs.CL</span>\n</div>\n</div>\n<p class=\"title is-5 mathjax\">\n      \n        ARE: Scaling Up Agent Environments and Evaluations\n      \n    </p>\n<p class=\"authors\">\n<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n<a href=\"/search/?searchtype=author&amp;query=Andrews%2C+P\">Pierre Andrews</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Benhalloum%2C+A\">Amine Benhalloum</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Bertran%2C+G+M\">Gerard Moreno-Torres Bertran</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Bettini%2C+M\">Matteo Bettini</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Budhiraja%2C+A\">Amar Budhiraja</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Cabral%2C+R+S\">Ricardo Silveira Cabral</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Do%2C+V\">Virginie Do</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Froger%2C+R\">Romain Froger</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Garreau%2C+E\">Emilien Garreau</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Gaya%2C+J\">Jean-Baptiste Gaya</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Lauren%C3%A7on%2C+H\">Hugo Laurençon</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Lecanu%2C+M\">Maxime Lecanu</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Malkan%2C+K\">Kunal Malkan</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Mekala%2C+D\">Dheeraj Mekala</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=M%C3%A9nard%2C+P\">Pierre Ménard</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Mialon%2C+G\">Grégoire Mialon</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Piterbarg%2C+U\">Ulyana Piterbarg</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Plekhanov%2C+M\">Mikhail Plekhanov</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Rita%2C+M\">Mathieu Rita</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Rusakov%2C+A\">Andrey Rusakov</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Scialom%2C+T\">Thomas Scialom</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Vorotilov%2C+V\">Vladislav Vorotilov</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Wang%2C+M\">Mengjue Wang</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Yu%2C+I\">Ian Yu</a>\n</p>\n<p class=\"abstract mathjax\">\n<span class=\"search-hit\">Abstract</span>:\n      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"2509.17158v1-abstract-short\" style=\"display: inline;\">\n        We introduce <span class=\"search-hit mathjax\">Meta</span> Agents Research Environments (ARE), a research platform for scalable creation of environments, integration of synthetic or real applications, and execution of agentic orchestrations. ARE provides simple abstractions to build complex and diverse environments, each with their own rules, tools, content, and verifiers, helping to bridge the gap…\n        <a class=\"is-size-7\" onclick=\"document.getElementById('2509.17158v1-abstract-full').style.display = 'inline'; document.getElementById('2509.17158v1-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n</span>\n<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"2509.17158v1-abstract-full\" style=\"display: none;\">\n        We introduce <span class=\"search-hit mathjax\">Meta</span> Agents Research Environments (ARE), a research platform for scalable creation of environments, integration of synthetic or real applications, and execution of agentic orchestrations. ARE provides simple abstractions to build complex and diverse environments, each with their own rules, tools, content, and verifiers, helping to bridge the gap between model development and real-world deployment. We also propose Gaia2, a benchmark built in ARE and designed to measure general agent capabilities. Beyond search and execution, Gaia2 requires agents to handle ambiguities and noise, adapt to dynamic environments, collaborate with other agents, and operate under temporal constraints. Unlike prior benchmarks, Gaia2 runs asynchronously, surfacing new failure modes that are invisible in static settings. Our experiments show that no system dominates across the intelligence spectrum: stronger reasoning often comes at the cost of efficiency, and budget scaling curves plateau, highlighting the need for new architectures and adaptive compute strategies. Perhaps more importantly, ARE abstractions enable continuous extension of Gaia2 to other environments, empowering the community to rapidly create new benchmarks tailored to their domains. In <span class=\"search-hit mathjax\">AI's</span> second half, progress increasingly depends on defining meaningful tasks and robust evaluations to drive frontier capabilities forward.\n        <a class=\"is-size-7\" onclick=\"document.getElementById('2509.17158v1-abstract-full').style.display = 'none'; document.getElementById('2509.17158v1-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n</span>\n</p>\n<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 21 September, 2025; \n      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> September 2025.\n      \n    </p>\n</li>\n<li class=\"arxiv-result\">\n<div class=\"is-marginless\">\n<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/2509.16834\">arXiv:2509.16834</a>\n<span> [<a href=\"https://arxiv.org/pdf/2509.16834\">pdf</a>, <a href=\"https://arxiv.org/ps/2509.16834\">ps</a>, <a href=\"https://arxiv.org/format/2509.16834\">other</a>] </span>\n</p>\n<div class=\"tags is-inline-block\">\n<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Robotics\">cs.RO</span>\n<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Artificial Intelligence\">cs.AI</span>\n<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Machine Learning\">cs.LG</span>\n</div>\n</div>\n<p class=\"title is-5 mathjax\">\n      \n        Robot Learning with Sparsity and Scarcity\n      \n    </p>\n<p class=\"authors\">\n<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n<a href=\"/search/?searchtype=author&amp;query=Xu%2C+J\">Jingxi Xu</a>\n</p>\n<p class=\"abstract mathjax\">\n<span class=\"search-hit\">Abstract</span>:\n      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"2509.16834v1-abstract-short\" style=\"display: inline;\">\n        …the right type of physical assistance at the right moment. My work develops machine learning algorithms that enable intent inferral with minimal data, including semi-supervised, <span class=\"search-hit mathjax\">meta</span>-learning, and generative <span class=\"search-hit mathjax\">AI</span> methods.\n        <a class=\"is-size-7\" onclick=\"document.getElementById('2509.16834v1-abstract-full').style.display = 'inline'; document.getElementById('2509.16834v1-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n</span>\n<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"2509.16834v1-abstract-full\" style=\"display: none;\">\n        Unlike in language or vision, one of the fundamental challenges in robot learning is the lack of access to vast data resources. We can further break down the problem into (1) data sparsity from the angle of data representation and (2) data scarcity from the angle of data quantity. In this thesis, I will discuss selected works on two domains: (1) tactile sensing and (2) rehabilitation robots, which are exemplars of data sparsity and scarcity, respectively. Tactile sensing is an essential modality for robotics, but tactile data are often sparse, and for each interaction with the physical world, tactile sensors can only obtain information about the local area of contact. I will discuss my work on learning vision-free tactile-only exploration and manipulation policies through model-free reinforcement learning to make efficient use of sparse tactile information. On the other hand, rehabilitation robots are an example of data scarcity to the extreme due to the significant challenge of collecting biosignals from disabled-bodied subjects at scale for training. I will discuss my work in collaboration with the medical school and clinicians on intent inferral for stroke survivors, where a hand orthosis developed in our lab collects a set of biosignals from the patient and uses them to infer the activity that the patient intends to perform, so the orthosis can provide the right type of physical assistance at the right moment. My work develops machine learning algorithms that enable intent inferral with minimal data, including semi-supervised, <span class=\"search-hit mathjax\">meta</span>-learning, and generative <span class=\"search-hit mathjax\">AI</span> methods.\n        <a class=\"is-size-7\" onclick=\"document.getElementById('2509.16834v1-abstract-full').style.display = 'none'; document.getElementById('2509.16834v1-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n</span>\n</p>\n<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 20 September, 2025; \n      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> September 2025.\n      \n    </p>\n</li>\n<li class=\"arxiv-result\">\n<div class=\"is-marginless\">\n<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/2509.15291\">arXiv:2509.15291</a>\n<span> [<a href=\"https://arxiv.org/pdf/2509.15291\">pdf</a>, <a href=\"https://arxiv.org/ps/2509.15291\">ps</a>, <a href=\"https://arxiv.org/format/2509.15291\">other</a>] </span>\n</p>\n<div class=\"tags is-inline-block\">\n<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Artificial Intelligence\">cs.AI</span>\n<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Systems and Control\">eess.SY</span>\n</div>\n</div>\n<p class=\"title is-5 mathjax\">\n      \n        The Distribution Shift Problem in Transportation Networks using Reinforcement Learning and <span class=\"search-hit mathjax\">AI</span>\n</p>\n<p class=\"authors\">\n<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n<a href=\"/search/?searchtype=author&amp;query=Taschin%2C+F\">Federico Taschin</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Lazaraq%2C+A\">Abderrahmane Lazaraq</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Tonguz%2C+O+K\">Ozan K. Tonguz</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Ozgunes%2C+I\">Inci Ozgunes</a>\n</p>\n<p class=\"abstract mathjax\">\n<span class=\"search-hit\">Abstract</span>:\n      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"2509.15291v1-abstract-short\" style=\"display: inline;\">\n        The use of Machine Learning (ML) and Artificial Intelligence (<span class=\"search-hit mathjax\">AI</span>) in smart transportation networks has increased significantly in the last few years. Among these ML and…\n        <a class=\"is-size-7\" onclick=\"document.getElementById('2509.15291v1-abstract-full').style.display = 'inline'; document.getElementById('2509.15291v1-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n</span>\n<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"2509.15291v1-abstract-full\" style=\"display: none;\">\n        The use of Machine Learning (ML) and Artificial Intelligence (<span class=\"search-hit mathjax\">AI</span>) in smart transportation networks has increased significantly in the last few years. Among these ML and <span class=\"search-hit mathjax\">AI</span> approaches, Reinforcement Learning (RL) has been shown to be a very promising approach by several authors. However, a problem with using Reinforcement Learning in Traffic Signal Control is the reliability of the trained RL agents due to the dynamically changing distribution of the input data with respect to the distribution of the data used for training. This presents a major challenge and a reliability problem for the trained network of <span class=\"search-hit mathjax\">AI</span> agents and could have very undesirable and even detrimental consequences if a suitable solution is not found. Several researchers have tried to address this problem using different approaches. In particular, <span class=\"search-hit mathjax\">Meta</span> Reinforcement Learning (<span class=\"search-hit mathjax\">Meta</span> RL) promises to be an effective solution. In this paper, we evaluate and analyze a state-of-the-art <span class=\"search-hit mathjax\">Meta</span> RL approach called MetaLight and show that, while under certain conditions MetaLight can indeed lead to reasonably good results, under some other conditions it might not perform well (with errors of up to 22%), suggesting that <span class=\"search-hit mathjax\">Meta</span> RL schemes are often not robust enough and can even pose major reliability problems.\n        <a class=\"is-size-7\" onclick=\"document.getElementById('2509.15291v1-abstract-full').style.display = 'none'; document.getElementById('2509.15291v1-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n</span>\n</p>\n<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 18 September, 2025; \n      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> September 2025.\n      \n    </p>\n</li>\n<li class=\"arxiv-result\">\n<div class=\"is-marginless\">\n<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/2509.15213\">arXiv:2509.15213</a>\n<span> [<a href=\"https://arxiv.org/pdf/2509.15213\">pdf</a>, <a href=\"https://arxiv.org/ps/2509.15213\">ps</a>, <a href=\"https://arxiv.org/format/2509.15213\">other</a>] </span>\n</p>\n<div class=\"tags is-inline-block\">\n<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Cryptography and Security\">cs.CR</span>\n</div>\n</div>\n<p class=\"title is-5 mathjax\">\n      \n        Evil Vizier: Vulnerabilities of LLM-Integrated XR Systems\n      \n    </p>\n<p class=\"authors\">\n<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n<a href=\"/search/?searchtype=author&amp;query=Zhang%2C+Y\">Yicheng Zhang</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Huang%2C+Z\">Zijian Huang</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Chen%2C+S\">Sophie Chen</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Shayegani%2C+E\">Erfan Shayegani</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Chen%2C+J\">Jiasi Chen</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Abu-Ghazaleh%2C+N\">Nael Abu-Ghazaleh</a>\n</p>\n<p class=\"abstract mathjax\">\n<span class=\"search-hit\">Abstract</span>:\n      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"2509.15213v1-abstract-short\" style=\"display: inline;\">\n        …increasingly integrate Large Language Models (LLMs) to enhance user experience, scene understanding, and even generate executable XR content, and are often called \"<span class=\"search-hit mathjax\">AI</span> glasses\". Despite these potential benefits, the integrated XR-LLM pipeline makes XR applications vulnerable to new forms of attacks. In this paper, we analyze LLM-Integated XR systems i…\n        <a class=\"is-size-7\" onclick=\"document.getElementById('2509.15213v1-abstract-full').style.display = 'inline'; document.getElementById('2509.15213v1-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n</span>\n<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"2509.15213v1-abstract-full\" style=\"display: none;\">\n        Extended reality (XR) applications increasingly integrate Large Language Models (LLMs) to enhance user experience, scene understanding, and even generate executable XR content, and are often called \"<span class=\"search-hit mathjax\">AI</span> glasses\". Despite these potential benefits, the integrated XR-LLM pipeline makes XR applications vulnerable to new forms of attacks. In this paper, we analyze LLM-Integated XR systems in the literature and in practice and categorize them along different dimensions from a systems perspective. Building on this categorization, we identify a common threat model and demonstrate a series of proof-of-concept attacks on multiple XR platforms that employ various LLM models (<span class=\"search-hit mathjax\">Meta</span> Quest 3, <span class=\"search-hit mathjax\">Meta</span> Ray-Ban, Android, and Microsoft HoloLens 2 running Llama and GPT models). Although these platforms each implement LLM integration differently, they share vulnerabilities where an attacker can modify the public context surrounding a legitimate LLM query, resulting in erroneous visual or auditory feedback to users, thus compromising their safety or privacy, sowing confusion, or other harmful effects. To defend against these threats, we discuss mitigation strategies and best practices for developers, including an initial defense prototype, and call on the community to develop new protection mechanisms to mitigate these risks.\n        <a class=\"is-size-7\" onclick=\"document.getElementById('2509.15213v1-abstract-full').style.display = 'none'; document.getElementById('2509.15213v1-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n</span>\n</p>\n<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 18 September, 2025; \n      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> September 2025.\n      \n    </p>\n</li>\n<li class=\"arxiv-result\">\n<div class=\"is-marginless\">\n<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/2509.15035\">arXiv:2509.15035</a>\n<span> [<a href=\"https://arxiv.org/pdf/2509.15035\">pdf</a>] </span>\n</p>\n<div class=\"tags is-inline-block\">\n<span class=\"tag is-small is-link tooltip is-tooltip-top\" data-tooltip=\"Artificial Intelligence\">cs.AI</span>\n<span class=\"tag is-small is-grey tooltip is-tooltip-top\" data-tooltip=\"Human-Computer Interaction\">cs.HC</span>\n</div>\n</div>\n<p class=\"title is-5 mathjax\">\n      \n        Calibrated Generative <span class=\"search-hit mathjax\">AI</span> as <span class=\"search-hit mathjax\">Meta</span>-Reviewer: A Systemic Functional Linguistics Discourse Analysis of Reviews of Peer Reviews\n      \n    </p>\n<p class=\"authors\">\n<span class=\"has-text-black-bis has-text-weight-semibold\">Authors:</span>\n<a href=\"/search/?searchtype=author&amp;query=Zapata%2C+G+C\">Gabriela C. Zapata</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Cope%2C+B\">Bill Cope</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Kalantzis%2C+M\">Mary Kalantzis</a>, \n      \n      <a href=\"/search/?searchtype=author&amp;query=Searsmith%2C+D\">Duane Searsmith</a>\n</p>\n<p class=\"abstract mathjax\">\n<span class=\"has-text-black-bis has-text-weight-semibold\">Abstract</span>:\n      <span class=\"abstract-short has-text-grey-dark mathjax\" id=\"2509.15035v1-abstract-short\" style=\"display: inline;\">\n        This study investigates the use of generative <span class=\"search-hit mathjax\">AI</span> to support formative assessment through machine generated reviews of peer reviews in graduate online courses in a public university in the United States. Drawing on Systemic Functional Linguistics and Appraisal Theory, we analyzed 120 metareviews to explore how generative…\n        <a class=\"is-size-7\" onclick=\"document.getElementById('2509.15035v1-abstract-full').style.display = 'inline'; document.getElementById('2509.15035v1-abstract-short').style.display = 'none';\" style=\"white-space: nowrap;\">▽ More</a>\n</span>\n<span class=\"abstract-full has-text-grey-dark mathjax\" id=\"2509.15035v1-abstract-full\" style=\"display: none;\">\n        This study investigates the use of generative <span class=\"search-hit mathjax\">AI</span> to support formative assessment through machine generated reviews of peer reviews in graduate online courses in a public university in the United States. Drawing on Systemic Functional Linguistics and Appraisal Theory, we analyzed 120 metareviews to explore how generative <span class=\"search-hit mathjax\">AI</span> feedback constructs meaning across ideational, interpersonal, and textual dimensions. The findings suggest that generative <span class=\"search-hit mathjax\">AI</span> can approximate key rhetorical and relational features of effective human feedback, offering directive clarity while also maintaining a supportive stance. The reviews analyzed demonstrated a balance of praise and constructive critique, alignment with rubric expectations, and structured staging that foregrounded student agency. By modeling these qualities, <span class=\"search-hit mathjax\">AI</span> metafeedback has the potential to scaffold feedback literacy and enhance leaner engagement with peer review.\n        <a class=\"is-size-7\" onclick=\"document.getElementById('2509.15035v1-abstract-full').style.display = 'none'; document.getElementById('2509.15035v1-abstract-short').style.display = 'inline';\" style=\"white-space: nowrap;\">△ Less</a>\n</span>\n</p>\n<p class=\"is-size-7\"><span class=\"has-text-black-bis has-text-weight-semibold\">Submitted</span> 18 September, 2025; \n      <span class=\"has-text-black-bis has-text-weight-semibold\">originally announced</span> September 2025.\n      \n    </p>\n<p class=\"comments is-size-7\">\n<span class=\"has-text-black-bis has-text-weight-semibold\">Comments:</span>\n<span class=\"has-text-grey-dark mathjax\">39 pages, 3 tables</span>\n</p>\n</li>\n<li class=\"arxiv-result\">\n<div class=\"is-marginless\">\n<p class=\"list-title is-inline-block\"><a href=\"https://arxiv.org/abs/2509.14711\">arXiv:2509.14711</a>\n<span> [<a href=\"https://arxiv.org/pdf/2509.14711\">pdf</a>, <a href=\"https://arxiv.org/ps/2509.14711\">ps</a>, <a href=\"https://arxiv.org/"
  },
  {
    "metadata": {
      "url": "https://paperswithcode.com/search?q=meta+ai",
      "title": "Trending Papers - Hugging Face",
      "category": "research_papers",
      "source_type": "generic",
      "scraped_at": "2025-09-28T18:06:12.433801",
      "word_count": 20650,
      "status": "success"
    },
    "content": "new\nGet trending papers in your email inbox once a day!\nGet trending papers in your email inbox!\nSubscribe\nTrending Papers\nby\nAK\nand the research community\nDaily\nWeekly\nMonthly\nTrending Papers\nSubmitted by\nrichardxp888\nWebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent\nWeb agents such as Deep Research have demonstrated superhuman cognitive\nabilities, capable of solving highly challenging information-seeking problems.\nHowever, most research remains primarily text-centric, overlooking visual\ninformation in the real world. This makes multimodal Deep Research highly\nchallenging, as such agents require much stronger reasoning abilities in\nperception, logic, knowledge, and the use of more sophisticated tools compared\nto text-based agents. To address this limitation, we introduce WebWatcher, a\nmulti-modal Agent for Deep Research equipped with enhanced visual-language\nreasoning capabilities. It leverages high-quality synthetic multimodal\ntrajectories for efficient cold start training, utilizes various tools for deep\nreasoning, and further enhances generalization through reinforcement learning.\nTo better evaluate the capabilities of multimodal agents, we propose\nBrowseComp-VL, a benchmark with BrowseComp-style that requires complex\ninformation retrieval involving both visual and textual information.\nExperimental results show that WebWatcher significantly outperforms proprietary\nbaseline, RAG workflow and open-source agents in four challenging VQA\nbenchmarks, which paves the way for solving complex multimodal\ninformation-seeking tasks.\nAlibaba-NLP\n·\nPublished on Aug 7, 2025\nUpvote\n129\nGitHub\n15k\narXiv Page\nSubmitted by\nrichardxp888\nWebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent\nWeb agents such as Deep Research have demonstrated superhuman cognitive\nabilities, capable of solving highly challenging information-seeking problems.\nHowever, most research remains primarily text-centric, overlooking visual\ninformation in the real world. This makes multimodal Deep Research highly\nchallenging, as such agents require much stronger reasoning abilities in\nperception, logic, knowledge, and the use of more sophisticated tools compared\nto text-based agents. To address this limitation, we introduce WebWatcher, a\nmulti-modal Agent for Deep Research equipped with enhanced visual-language\nreasoning capabilities. It leverages high-quality synthetic multimodal\ntrajectories for efficient cold start training, utilizes various tools for deep\nreasoning, and further enhances generalization through reinforcement learning.\nTo better evaluate the capabilities of multimodal agents, we propose\nBrowseComp-VL, a benchmark with BrowseComp-style that requires complex\ninformation retrieval involving both visual and textual information.\nExperimental results show that WebWatcher significantly outperforms proprietary\nbaseline, RAG workflow and open-source agents in four challenging VQA\nbenchmarks, which paves the way for solving complex multimodal\ninformation-seeking tasks.\nAlibaba-NLP\n·\nAug 7, 2025\nUpvote\n129\nGitHub\n15k\narXiv Page\nSubmitted by\ncallanwu\nWebShaper: Agentically Data Synthesizing via Information-Seeking\n  Formalization\nThe advent of Large Language Model (LLM)-powered agents has revolutionized\nartificial intelligence by enabling solutions to complex, open-ended tasks\nthrough web-based information-seeking (IS) capabilities. The scarcity of\nhigh-quality training data has limited the development of IS agents. Existing\napproaches typically adopt an information-driven paradigm that first collects\nweb data and then generates questions based on the retrieval. However, this may\nlead to inconsistency between information structure and reasoning structure,\nquestion and answer. To mitigate, we propose a formalization-driven IS data\nsynthesis framework WebShaper to construct a dataset. WebShaper systematically\nformalizes IS tasks through set theory. Central to the formalization is the\nconcept of Knowledge Projections (KP), which enables precise control over\nreasoning structure by KP operation compositions. During synthesis, we begin by\ncreating seed tasks, then use a multi-step expansion process. At each step, an\nagentic Expander expands the current formal question more complex with\nretrieval and validation tools based on our formalization. We train our model\non the synthesized dataset. Experiment results demonstrate that WebShaper\nachieves state-of-the-art performance among open-sourced IS agents on GAIA and\nWebWalkerQA benchmarks.\nAlibaba-NLP\n·\nPublished on Jul 20, 2025\nUpvote\n57\nGitHub\n15k\narXiv Page\nSubmitted by\ncallanwu\nWebShaper: Agentically Data Synthesizing via Information-Seeking\n  Formalization\nThe advent of Large Language Model (LLM)-powered agents has revolutionized\nartificial intelligence by enabling solutions to complex, open-ended tasks\nthrough web-based information-seeking (IS) capabilities. The scarcity of\nhigh-quality training data has limited the development of IS agents. Existing\napproaches typically adopt an information-driven paradigm that first collects\nweb data and then generates questions based on the retrieval. However, this may\nlead to inconsistency between information structure and reasoning structure,\nquestion and answer. To mitigate, we propose a formalization-driven IS data\nsynthesis framework WebShaper to construct a dataset. WebShaper systematically\nformalizes IS tasks through set theory. Central to the formalization is the\nconcept of Knowledge Projections (KP), which enables precise control over\nreasoning structure by KP operation compositions. During synthesis, we begin by\ncreating seed tasks, then use a multi-step expansion process. At each step, an\nagentic Expander expands the current formal question more complex with\nretrieval and validation tools based on our formalization. We train our model\non the synthesized dataset. Experiment results demonstrate that WebShaper\nachieves state-of-the-art performance among open-sourced IS agents on GAIA and\nWebWalkerQA benchmarks.\nAlibaba-NLP\n·\nJul 20, 2025\nUpvote\n57\nGitHub\n15k\narXiv Page\nSubmitted by\ncallanwu\nWebDancer: Towards Autonomous Information Seeking Agency\nAddressing intricate real-world problems necessitates in-depth information\nseeking and multi-step reasoning. Recent progress in agentic systems,\nexemplified by Deep Research, underscores the potential for autonomous\nmulti-step research. In this work, we present a cohesive paradigm for building\nend-to-end agentic information seeking agents from a data-centric and\ntraining-stage perspective. Our approach consists of four key stages: (1)\nbrowsing data construction, (2) trajectories sampling, (3) supervised\nfine-tuning for effective cold start, and (4) reinforcement learning for\nenhanced generalisation. We instantiate this framework in a web agent based on\nthe ReAct, WebDancer. Empirical evaluations on the challenging information\nseeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of\nWebDancer, achieving considerable results and highlighting the efficacy of our\ntraining paradigm. Further analysis of agent training provides valuable\ninsights and actionable, systematic pathways for developing more capable\nagentic models. The codes and demo will be released in\nhttps://github.com/Alibaba-NLP/WebAgent.\n12 authors\n·\nPublished on May 28, 2025\nUpvote\n31\nGitHub\n15k\narXiv Page\nSubmitted by\ncallanwu\nWebDancer: Towards Autonomous Information Seeking Agency\nAddressing intricate real-world problems necessitates in-depth information\nseeking and multi-step reasoning. Recent progress in agentic systems,\nexemplified by Deep Research, underscores the potential for autonomous\nmulti-step research. In this work, we present a cohesive paradigm for building\nend-to-end agentic information seeking agents from a data-centric and\ntraining-stage perspective. Our approach consists of four key stages: (1)\nbrowsing data construction, (2) trajectories sampling, (3) supervised\nfine-tuning for effective cold start, and (4) reinforcement learning for\nenhanced generalisation. We instantiate this framework in a web agent based on\nthe ReAct, WebDancer. Empirical evaluations on the challenging information\nseeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of\nWebDancer, achieving considerable results and highlighting the efficacy of our\ntraining paradigm. Further analysis of agent training provides valuable\ninsights and actionable, systematic pathways for developing more capable\nagentic models. The codes and demo will be released in\nhttps://github.com/Alibaba-NLP/WebAgent.\n12 authors\n·\nMay 28, 2025\nUpvote\n31\nGitHub\n15k\narXiv Page\nSubmitted by\ncallanwu\nReSum: Unlocking Long-Horizon Search Intelligence via Context\n  Summarization\nLarge Language Model (LLM)-based web agents demonstrate strong performance on\nknowledge-intensive tasks but are hindered by context window limitations in\nparadigms like ReAct. Complex queries involving multiple entities, intertwined\nrelationships, and high uncertainty demand extensive search cycles that rapidly\nexhaust context budgets before reaching complete solutions. To overcome this\nchallenge, we introduce ReSum, a novel paradigm that enables indefinite\nexploration through periodic context summarization. ReSum converts growing\ninteraction histories into compact reasoning states, maintaining awareness of\nprior discoveries while bypassing context constraints. For paradigm adaptation,\nwe propose ReSum-GRPO, integrating GRPO with segmented trajectory training and\nadvantage broadcasting to familiarize agents with summary-conditioned\nreasoning. Extensive experiments on web agents of varying scales across three\nbenchmarks demonstrate that ReSum delivers an average absolute improvement of\n4.5\\% over ReAct, with further gains of up to 8.2\\% following ReSum-GRPO\ntraining. Notably, with only 1K training samples, our WebResummer-30B (a\nReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\\% Pass@1 on\nBrowseComp-zh and 18.3\\% on BrowseComp-en, surpassing existing open-source web\nagents.\n14 authors\n·\nPublished on Sep 16, 2025\nUpvote\n72\nGitHub\n15k\narXiv Page\nSubmitted by\ncallanwu\nReSum: Unlocking Long-Horizon Search Intelligence via Context\n  Summarization\nLarge Language Model (LLM)-based web agents demonstrate strong performance on\nknowledge-intensive tasks but are hindered by context window limitations in\nparadigms like ReAct. Complex queries involving multiple entities, intertwined\nrelationships, and high uncertainty demand extensive search cycles that rapidly\nexhaust context budgets before reaching complete solutions. To overcome this\nchallenge, we introduce ReSum, a novel paradigm that enables indefinite\nexploration through periodic context summarization. ReSum converts growing\ninteraction histories into compact reasoning states, maintaining awareness of\nprior discoveries while bypassing context constraints. For paradigm adaptation,\nwe propose ReSum-GRPO, integrating GRPO with segmented trajectory training and\nadvantage broadcasting to familiarize agents with summary-conditioned\nreasoning. Extensive experiments on web agents of varying scales across three\nbenchmarks demonstrate that ReSum delivers an average absolute improvement of\n4.5\\% over ReAct, with further gains of up to 8.2\\% following ReSum-GRPO\ntraining. Notably, with only 1K training samples, our WebResummer-30B (a\nReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\\% Pass@1 on\nBrowseComp-zh and 18.3\\% on BrowseComp-en, surpassing existing open-source web\nagents.\n14 authors\n·\nSep 16, 2025\nUpvote\n72\nGitHub\n15k\narXiv Page\nSubmitted by\ncallanwu\nWebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic\n  Data and Scalable Reinforcement Learning\nTranscending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all open-source agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap.\n17 authors\n·\nPublished on Sep 16, 2025\nUpvote\n83\nGitHub\n15k\narXiv Page\nSubmitted by\ncallanwu\nWebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic\n  Data and Scalable Reinforcement Learning\nTranscending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all open-source agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap.\n17 authors\n·\nSep 16, 2025\nUpvote\n83\nGitHub\n15k\narXiv Page\nSubmitted by\ncallanwu\nScaling Agents via Continual Pre-training\nLarge language models (LLMs) have evolved into agentic systems capable of\nautonomous tool use and multi-step reasoning for complex problem-solving.\nHowever, post-training approaches building upon general-purpose foundation\nmodels consistently underperform in agentic tasks, particularly in open-source\nimplementations. We identify the root cause: the absence of robust agentic\nfoundation models forces models during post-training to simultaneously learn\ndiverse agentic behaviors while aligning them to expert demonstrations, thereby\ncreating fundamental optimization tensions. To this end, we are the first to\npropose incorporating Agentic Continual Pre-training (Agentic CPT) into the\ndeep research agents training pipeline to build powerful agentic foundational\nmodels. Based on this approach, we develop a deep research agent model named\nAgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve\nstate-of-the-art performance while retains strong tool-use ability, notably\n39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.\n22 authors\n·\nPublished on Sep 16, 2025\nUpvote\n105\nGitHub\n15k\narXiv Page\nSubmitted by\ncallanwu\nScaling Agents via Continual Pre-training\nLarge language models (LLMs) have evolved into agentic systems capable of\nautonomous tool use and multi-step reasoning for complex problem-solving.\nHowever, post-training approaches building upon general-purpose foundation\nmodels consistently underperform in agentic tasks, particularly in open-source\nimplementations. We identify the root cause: the absence of robust agentic\nfoundation models forces models during post-training to simultaneously learn\ndiverse agentic behaviors while aligning them to expert demonstrations, thereby\ncreating fundamental optimization tensions. To this end, we are the first to\npropose incorporating Agentic Continual Pre-training (Agentic CPT) into the\ndeep research agents training pipeline to build powerful agentic foundational\nmodels. Based on this approach, we develop a deep research agent model named\nAgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve\nstate-of-the-art performance while retains strong tool-use ability, notably\n39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.\n22 authors\n·\nSep 16, 2025\nUpvote\n105\nGitHub\n15k\narXiv Page\nSubmitted by\ntaesiri\nWebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for\n  Open-Ended Deep Research\nThis paper tackles open-ended deep research (OEDR), a complex challenge where\nAI agents must synthesize vast web-scale information into insightful reports.\nCurrent approaches are plagued by dual-fold limitations: static research\npipelines that decouple planning from evidence acquisition and one-shot\ngeneration paradigms that easily suffer from long-context failure issues like\n\"loss in the middle\" and hallucinations. To address these challenges, we\nintroduce WebWeaver, a novel dual-agent framework that emulates the human\nresearch process. The planner operates in a dynamic cycle, iteratively\ninterleaving evidence acquisition with outline optimization to produce a\ncomprehensive, source-grounded outline linking to a memory bank of evidence.\nThe writer then executes a hierarchical retrieval and writing process,\ncomposing the report section by section. By performing targeted retrieval of\nonly the necessary evidence from the memory bank for each part, it effectively\nmitigates long-context issues. Our framework establishes a new state-of-the-art\nacross major OEDR benchmarks, including DeepResearch Bench, DeepConsult, and\nDeepResearchGym. These results validate our human-centric, iterative\nmethodology, demonstrating that adaptive planning and focused synthesis are\ncrucial for producing high-quality, reliable, and well-structured reports.\n12 authors\n·\nPublished on Sep 16, 2025\nUpvote\n100\nGitHub\n15k\narXiv Page\nSubmitted by\ntaesiri\nWebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for\n  Open-Ended Deep Research\nThis paper tackles open-ended deep research (OEDR), a complex challenge where\nAI agents must synthesize vast web-scale information into insightful reports.\nCurrent approaches are plagued by dual-fold limitations: static research\npipelines that decouple planning from evidence acquisition and one-shot\ngeneration paradigms that easily suffer from long-context failure issues like\n\"loss in the middle\" and hallucinations. To address these challenges, we\nintroduce WebWeaver, a novel dual-agent framework that emulates the human\nresearch process. The planner operates in a dynamic cycle, iteratively\ninterleaving evidence acquisition with outline optimization to produce a\ncomprehensive, source-grounded outline linking to a memory bank of evidence.\nThe writer then executes a hierarchical retrieval and writing process,\ncomposing the report section by section. By performing targeted retrieval of\nonly the necessary evidence from the memory bank for each part, it effectively\nmitigates long-context issues. Our framework establishes a new state-of-the-art\nacross major OEDR benchmarks, including DeepResearch Bench, DeepConsult, and\nDeepResearchGym. These results validate our human-centric, iterative\nmethodology, demonstrating that adaptive planning and focused synthesis are\ncrucial for producing high-quality, reliable, and well-structured reports.\n12 authors\n·\nSep 16, 2025\nUpvote\n100\nGitHub\n15k\narXiv Page\nSubmitted by\nlearn3r\nWebSailor: Navigating Super-human Reasoning for Web Agent\nTranscending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all opensource agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap.\n19 authors\n·\nPublished on Jul 3, 2025\nUpvote\n118\nGitHub\n15k\narXiv Page\nSubmitted by\nlearn3r\nWebSailor: Navigating Super-human Reasoning for Web Agent\nTranscending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all opensource agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap.\n19 authors\n·\nJul 3, 2025\nUpvote\n118\nGitHub\n15k\narXiv Page\nSubmitted by\ntaesiri\nPaper2Agent: Reimagining Research Papers As Interactive and Reliable AI\n  Agents\nWe introduce Paper2Agent, an automated framework that converts research\npapers into AI agents. Paper2Agent transforms research output from passive\nartifacts into active systems that can accelerate downstream use, adoption, and\ndiscovery. Conventional research papers require readers to invest substantial\neffort to understand and adapt a paper's code, data, and methods to their own\nwork, creating barriers to dissemination and reuse. Paper2Agent addresses this\nchallenge by automatically converting a paper into an AI agent that acts as a\nknowledgeable research assistant. It systematically analyzes the paper and the\nassociated codebase using multiple agents to construct a Model Context Protocol\n(MCP) server, then iteratively generates and runs tests to refine and robustify\nthe resulting MCP. These paper MCPs can then be flexibly connected to a chat\nagent (e.g. Claude Code) to carry out complex scientific queries through\nnatural language while invoking tools and workflows from the original paper. We\ndemonstrate Paper2Agent's effectiveness in creating reliable and capable paper\nagents through in-depth case studies. Paper2Agent created an agent that\nleverages AlphaGenome to interpret genomic variants and agents based on ScanPy\nand TISSUE to carry out single-cell and spatial transcriptomics analyses. We\nvalidate that these paper agents can reproduce the original paper's results and\ncan correctly carry out novel user queries. By turning static papers into\ndynamic, interactive AI agents, Paper2Agent introduces a new paradigm for\nknowledge dissemination and a foundation for the collaborative ecosystem of AI\nco-scientists.\n4 authors\n·\nPublished on Sep 8, 2025\nUpvote\n31\nGitHub\n865\narXiv Page\nSubmitted by\ntaesiri\nPaper2Agent: Reimagining Research Papers As Interactive and Reliable AI\n  Agents\nWe introduce Paper2Agent, an automated framework that converts research\npapers into AI agents. Paper2Agent transforms research output from passive\nartifacts into active systems that can accelerate downstream use, adoption, and\ndiscovery. Conventional research papers require readers to invest substantial\neffort to understand and adapt a paper's code, data, and methods to their own\nwork, creating barriers to dissemination and reuse. Paper2Agent addresses this\nchallenge by automatically converting a paper into an AI agent that acts as a\nknowledgeable research assistant. It systematically analyzes the paper and the\nassociated codebase using multiple agents to construct a Model Context Protocol\n(MCP) server, then iteratively generates and runs tests to refine and robustify\nthe resulting MCP. These paper MCPs can then be flexibly connected to a chat\nagent (e.g. Claude Code) to carry out complex scientific queries through\nnatural language while invoking tools and workflows from the original paper. We\ndemonstrate Paper2Agent's effectiveness in creating reliable and capable paper\nagents through in-depth case studies. Paper2Agent created an agent that\nleverages AlphaGenome to interpret genomic variants and agents based on ScanPy\nand TISSUE to carry out single-cell and spatial transcriptomics analyses. We\nvalidate that these paper agents can reproduce the original paper's results and\ncan correctly carry out novel user queries. By turning static papers into\ndynamic, interactive AI agents, Paper2Agent introduces a new paradigm for\nknowledge dissemination and a foundation for the collaborative ecosystem of AI\nco-scientists.\n4 authors\n·\nSep 8, 2025\nUpvote\n31\nGitHub\n865\narXiv Page\nSubmitted by\n\t\t\t\tdeleted\nPaddleOCR 3.0 Technical Report\nThis technical report introduces PaddleOCR 3.0, an Apache-licensed\nopen-source toolkit for OCR and document parsing. To address the growing demand\nfor document understanding in the era of large language models, PaddleOCR 3.0\npresents three major solutions: (1) PP-OCRv5 for multilingual text recognition,\n(2) PP-StructureV3 for hierarchical document parsing, and (3) PP-ChatOCRv4 for\nkey information extraction. Compared to mainstream vision-language models\n(VLMs), these models with fewer than 100 million parameters achieve competitive\naccuracy and efficiency, rivaling billion-parameter VLMs. In addition to\noffering a high-quality OCR model library, PaddleOCR 3.0 provides efficient\ntools for training, inference, and deployment, supports heterogeneous hardware\nacceleration, and enables developers to easily build intelligent document\napplications.\n19 authors\n·\nPublished on Jul 8, 2025\nUpvote\n8\nGitHub\n56.2k\narXiv Page\nSubmitted by\n\t\t\t\tdeleted\nPaddleOCR 3.0 Technical Report\nThis technical report introduces PaddleOCR 3.0, an Apache-licensed\nopen-source toolkit for OCR and document parsing. To address the growing demand\nfor document understanding in the era of large language models, PaddleOCR 3.0\npresents three major solutions: (1) PP-OCRv5 for multilingual text recognition,\n(2) PP-StructureV3 for hierarchical document parsing, and (3) PP-ChatOCRv4 for\nkey information extraction. Compared to mainstream vision-language models\n(VLMs), these models with fewer than 100 million parameters achieve competitive\naccuracy and efficiency, rivaling billion-parameter VLMs. In addition to\noffering a high-quality OCR model library, PaddleOCR 3.0 provides efficient\ntools for training, inference, and deployment, supports heterogeneous hardware\nacceleration, and enables developers to easily build intelligent document\napplications.\n19 authors\n·\nJul 8, 2025\nUpvote\n8\nGitHub\n56.2k\narXiv Page\nSubmitted by\ntaesiri\nAgentScope 1.0: A Developer-Centric Framework for Building Agentic\n  Applications\nDriven by rapid advancements of Large Language Models (LLMs), agents are\nempowered to combine intrinsic knowledge with dynamic tool use, greatly\nenhancing their capacity to address real-world tasks. In line with such an\nevolution, AgentScope introduces major improvements in a new version (1.0),\ntowards comprehensively supporting flexible and efficient tool-based\nagent-environment interactions for building agentic applications. Specifically,\nwe abstract foundational components essential for agentic applications and\nprovide unified interfaces and extensible modules, enabling developers to\neasily leverage the latest progress, such as new models and MCPs. Furthermore,\nwe ground agent behaviors in the ReAct paradigm and offer advanced agent-level\ninfrastructure based on a systematic asynchronous design, which enriches both\nhuman-agent and agent-agent interaction patterns while improving execution\nefficiency. Building on this foundation, we integrate several built-in agents\ntailored to specific practical scenarios. AgentScope also includes robust\nengineering support for developer-friendly experiences. We provide a scalable\nevaluation module with a visual studio interface, making the development of\nlong-trajectory agentic applications more manageable and easier to trace. In\naddition, AgentScope offers a runtime sandbox to ensure safe agent execution\nand facilitates rapid deployment in production environments. With these\nenhancements, AgentScope provides a practical foundation for building scalable,\nadaptive, and effective agentic applications.\n23 authors\n·\nPublished on Aug 22, 2025\nUpvote\n50\nGitHub\n12.7k\narXiv Page\nSubmitted by\ntaesiri\nAgentScope 1.0: A Developer-Centric Framework for Building Agentic\n  Applications\nDriven by rapid advancements of Large Language Models (LLMs), agents are\nempowered to combine intrinsic knowledge with dynamic tool use, greatly\nenhancing their capacity to address real-world tasks. In line with such an\nevolution, AgentScope introduces major improvements in a new version (1.0),\ntowards comprehensively supporting flexible and efficient tool-based\nagent-environment interactions for building agentic applications. Specifically,\nwe abstract foundational components essential for agentic applications and\nprovide unified interfaces and extensible modules, enabling developers to\neasily leverage the latest progress, such as new models and MCPs. Furthermore,\nwe ground agent behaviors in the ReAct paradigm and offer advanced agent-level\ninfrastructure based on a systematic asynchronous design, which enriches both\nhuman-agent and agent-agent interaction patterns while improving execution\nefficiency. Building on this foundation, we integrate several built-in agents\ntailored to specific practical scenarios. AgentScope also includes robust\nengineering support for developer-friendly experiences. We provide a scalable\nevaluation module with a visual studio interface, making the development of\nlong-trajectory agentic applications more manageable and easier to trace. In\naddition, AgentScope offers a runtime sandbox to ensure safe agent execution\nand facilitates rapid deployment in production environments. With these\nenhancements, AgentScope provides a practical foundation for building scalable,\nadaptive, and effective agentic applications.\n23 authors\n·\nAug 22, 2025\nUpvote\n50\nGitHub\n12.7k\narXiv Page\nSubmitted by\ntaesiri\nQwen3-Omni Technical Report\nWe present Qwen3-Omni, a single multimodal model that, for the first time,\nmaintains state-of-the-art performance across text, image, audio, and video\nwithout any degradation relative to single-modal counterparts. Qwen3-Omni\nmatches the performance of same-sized single-modal models within the Qwen\nseries and excels particularly on audio tasks. Across 36 audio and audio-visual\nbenchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall\nSOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro,\nSeed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE\narchitecture that unifies perception and generation across text, images, audio,\nand video, yielding fluent text and natural real-time speech. It supports text\ninteraction in 119 languages, speech understanding in 19 languages, and speech\ngeneration in 10 languages. To reduce first-packet latency in streaming\nsynthesis, Talker autoregressively predicts discrete speech codecs using a\nmulti-codebook scheme. Leveraging the representational capacity of these\ncodebooks, we replace computationally intensive block-wise diffusion with a\nlightweight causal ConvNet, enabling streaming from the first codec frame. In\ncold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet\nlatency of 234 ms. To further strengthen multimodal reasoning, we introduce a\nThinking model that explicitly reasons over inputs from any modality. Since the\nresearch community currently lacks a general-purpose audio captioning model, we\nfine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which\nproduces detailed, low-hallucination captions for arbitrary audio inputs.\nQwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and\nQwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0\nlicense.\nQwen\n·\nPublished on Sep 22, 2025\nUpvote\n116\nGitHub\n2.28k\narXiv Page\nSubmitted by\ntaesiri\nQwen3-Omni Technical Report\nWe present Qwen3-Omni, a single multimodal model that, for the first time,\nmaintains state-of-the-art performance across text, image, audio, and video\nwithout any degradation relative to single-modal counterparts. Qwen3-Omni\nmatches the performance of same-sized single-modal models within the Qwen\nseries and excels particularly on audio tasks. Across 36 audio and audio-visual\nbenchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall\nSOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro,\nSeed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE\narchitecture that unifies perception and generation across text, images, audio,\nand video, yielding fluent text and natural real-time speech. It supports text\ninteraction in 119 languages, speech understanding in 19 languages, and speech\ngeneration in 10 languages. To reduce first-packet latency in streaming\nsynthesis, Talker autoregressively predicts discrete speech codecs using a\nmulti-codebook scheme. Leveraging the representational capacity of these\ncodebooks, we replace computationally intensive block-wise diffusion with a\nlightweight causal ConvNet, enabling streaming from the first codec frame. In\ncold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet\nlatency of 234 ms. To further strengthen multimodal reasoning, we introduce a\nThinking model that explicitly reasons over inputs from any modality. Since the\nresearch community currently lacks a general-purpose audio captioning model, we\nfine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which\nproduces detailed, low-hallucination captions for arbitrary audio inputs.\nQwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and\nQwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0\nlicense.\nQwen\n·\nSep 22, 2025\nUpvote\n116\nGitHub\n2.28k\narXiv Page\nSubmitted by\nmurcherful\nP3-SAM: Native 3D Part Segmentation\nSegmenting 3D assets into their constituent parts is crucial for enhancing 3D\nunderstanding, facilitating model reuse, and supporting various applications\nsuch as part generation. However, current methods face limitations such as poor\nrobustness when dealing with complex objects and cannot fully automate the\nprocess. In this paper, we propose a native 3D point-promptable part\nsegmentation model termed P3-SAM, designed to fully automate the segmentation\nof any 3D objects into components. Inspired by SAM, P3-SAM consists of a\nfeature extractor, multiple segmentation heads, and an IoU predictor, enabling\ninteractive segmentation for users. We also propose an algorithm to\nautomatically select and merge masks predicted by our model for part instance\nsegmentation. Our model is trained on a newly built dataset containing nearly\n3.7 million models with reasonable segmentation labels. Comparisons show that\nour method achieves precise segmentation results and strong robustness on any\ncomplex objects, attaining state-of-the-art performance. Our code will be\nreleased soon.\nTencent Hunyuan\n·\nPublished on Sep 8, 2025\nUpvote\n21\nGitHub\n195\narXiv Page\nSubmitted by\nmurcherful\nP3-SAM: Native 3D Part Segmentation\nSegmenting 3D assets into their constituent parts is crucial for enhancing 3D\nunderstanding, facilitating model reuse, and supporting various applications\nsuch as part generation. However, current methods face limitations such as poor\nrobustness when dealing with complex objects and cannot fully automate the\nprocess. In this paper, we propose a native 3D point-promptable part\nsegmentation model termed P3-SAM, designed to fully automate the segmentation\nof any 3D objects into components. Inspired by SAM, P3-SAM consists of a\nfeature extractor, multiple segmentation heads, and an IoU predictor, enabling\ninteractive segmentation for users. We also propose an algorithm to\nautomatically select and merge masks predicted by our model for part instance\nsegmentation. Our model is trained on a newly built dataset containing nearly\n3.7 million models with reasonable segmentation labels. Comparisons show that\nour method achieves precise segmentation results and strong robustness on any\ncomplex objects, attaining state-of-the-art performance. Our code will be\nreleased soon.\nTencent Hunyuan\n·\nSep 8, 2025\nUpvote\n21\nGitHub\n195\narXiv Page\nSubmitted by\nHowieYan\nX-Part: high fidelity and structure coherent shape decomposition\nGenerating 3D shapes at part level is pivotal for downstream applications\nsuch as mesh retopology, UV mapping, and 3D printing. However, existing\npart-based generation methods often lack sufficient controllability and suffer\nfrom poor semantically meaningful decomposition. To this end, we introduce\nX-Part, a controllable generative model designed to decompose a holistic 3D\nobject into semantically meaningful and structurally coherent parts with high\ngeometric fidelity. X-Part exploits the bounding box as prompts for the part\ngeneration and injects point-wise semantic features for meaningful\ndecomposition. Furthermore, we design an editable pipeline for interactive part\ngeneration. Extensive experimental results show that X-Part achieves\nstate-of-the-art performance in part-level shape generation. This work\nestablishes a new paradigm for creating production-ready, editable, and\nstructurally sound 3D assets. Codes will be released for public research.\n11 authors\n·\nPublished on Sep 10, 2025\nUpvote\n25\nGitHub\n197\narXiv Page\nSubmitted by\nHowieYan\nX-Part: high fidelity and structure coherent shape decomposition\nGenerating 3D shapes at part level is pivotal for downstream applications\nsuch as mesh retopology, UV mapping, and 3D printing. However, existing\npart-based generation methods often lack sufficient controllability and suffer\nfrom poor semantically meaningful decomposition. To this end, we introduce\nX-Part, a controllable generative model designed to decompose a holistic 3D\nobject into semantically meaningful and structurally coherent parts with high\ngeometric fidelity. X-Part exploits the bounding box as prompts for the part\ngeneration and injects point-wise semantic features for meaningful\ndecomposition. Furthermore, we design an editable pipeline for interactive part\ngeneration. Extensive experimental results show that X-Part achieves\nstate-of-the-art performance in part-level shape generation. This work\nestablishes a new paradigm for creating production-ready, editable, and\nstructurally sound 3D assets. Codes will be released for public research.\n11 authors\n·\nSep 10, 2025\nUpvote\n25\nGitHub\n197\narXiv Page\nSubmitted by\nhuangsiteng\nVLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action\n  Model\nVision-Language-Action (VLA) models typically bridge the gap between\nperceptual and action spaces by pre-training a large-scale Vision-Language\nModel (VLM) on robotic data. While this approach greatly enhances performance,\nit also incurs significant training costs. In this paper, we investigate how to\neffectively bridge vision-language (VL) representations to action (A). We\nintroduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA\nmodels on large-scale VLMs and extensive pre-training. To this end, we first\nsystematically analyze the effectiveness of various VL conditions and present\nkey findings on which conditions are essential for bridging perception and\naction spaces. Based on these insights, we propose a lightweight Policy module\nwith Bridge Attention, which autonomously injects the optimal condition into\nthe action space. In this way, our method achieves high performance using only\na 0.5B-parameter backbone, without any robotic data pre-training. Extensive\nexperiments on both simulated and real-world robotic benchmarks demonstrate\nthat VLA-Adapter not only achieves state-of-the-art level performance, but also\noffers the fast inference speed reported to date. Furthermore, thanks to the\nproposed advanced bridging paradigm, VLA-Adapter enables the training of a\npowerful VLA model in just 8 hours on a single consumer-grade GPU, greatly\nlowering the barrier to deploying the VLA model. Project page:\nhttps://vla-adapter.github.io/.\n16 authors\n·\nPublished on Sep 11, 2025\nUpvote\n215\nGitHub\n529\narXiv Page\nSubmitted by\nhuangsiteng\nVLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action\n  Model\nVision-Language-Action (VLA) models typically bridge the gap between\nperceptual and action spaces by pre-training a large-scale Vision-Language\nModel (VLM) on robotic data. While this approach greatly enhances performance,\nit also incurs significant training costs. In this paper, we investigate how to\neffectively bridge vision-language (VL) representations to action (A). We\nintroduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA\nmodels on large-scale VLMs and extensive pre-training. To this end, we first\nsystematically analyze the effectiveness of various VL conditions and present\nkey findings on which conditions are essential for bridging perception and\naction spaces. Based on these insights, we propose a lightweight Policy module\nwith Bridge Attention, which autonomously injects the optimal condition into\nthe action space. In this way, our method achieves high performance using only\na 0.5B-parameter backbone, without any robotic data pre-training. Extensive\nexperiments on both simulated and real-world robotic benchmarks demonstrate\nthat VLA-Adapter not only achieves state-of-the-art level performance, but also\noffers the fast inference speed reported to date. Furthermore, thanks to the\nproposed advanced bridging paradigm, VLA-Adapter enables the training of a\npowerful VLA model in just 8 hours on a single consumer-grade GPU, greatly\nlowering the barrier to deploying the VLA model. Project page:\nhttps://vla-adapter.github.io/.\n16 authors\n·\nSep 11, 2025\nUpvote\n215\nGitHub\n529\narXiv Page\nSubmitted by\ntaesiri\nLyra: Generative 3D Scene Reconstruction via Video Diffusion Model\n  Self-Distillation\nThe ability to generate virtual environments is crucial for applications\nranging from gaming to physical AI domains such as robotics, autonomous\ndriving, and industrial AI. Current learning-based 3D reconstruction methods\nrely on the availability of captured real-world multi-view data, which is not\nalways readily available. Recent advancements in video diffusion models have\nshown remarkable imagination capabilities, yet their 2D nature limits the\napplications to simulation where a robot needs to navigate and interact with\nthe environment. In this paper, we propose a self-distillation framework that\naims to distill the implicit 3D knowledge in the video diffusion models into an\nexplicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for\nmulti-view training data. Specifically, we augment the typical RGB decoder with\na 3DGS decoder, which is supervised by the output of the RGB decoder. In this\napproach, the 3DGS decoder can be purely trained with synthetic data generated\nby video diffusion models. At inference time, our model can synthesize 3D\nscenes from either a text prompt or a single image for real-time rendering. Our\nframework further extends to dynamic 3D scene generation from a monocular input\nvideo. Experimental results show that our framework achieves state-of-the-art\nperformance in static and dynamic 3D scene generation.\n13 authors\n·\nPublished on Sep 23, 2025\nUpvote\n20\nGitHub\n365\narXiv Page\nSubmitted by\ntaesiri\nLyra: Generative 3D Scene Reconstruction via Video Diffusion Model\n  Self-Distillation\nThe ability to generate virtual environments is crucial for applications\nranging from gaming to physical AI domains such as robotics, autonomous\ndriving, and industrial AI. Current learning-based 3D reconstruction methods\nrely on the availability of captured real-world multi-view data, which is not\nalways readily available. Recent advancements in video diffusion models have\nshown remarkable imagination capabilities, yet their 2D nature limits the\napplications to simulation where a robot needs to navigate and interact with\nthe environment. In this paper, we propose a self-distillation framework that\naims to distill the implicit 3D knowledge in the video diffusion models into an\nexplicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for\nmulti-view training data. Specifically, we augment the typical RGB decoder with\na 3DGS decoder, which is supervised by the output of the RGB decoder. In this\napproach, the 3DGS decoder can be purely trained with synthetic data generated\nby video diffusion models. At inference time, our model can synthesize 3D\nscenes from either a text prompt or a single image for real-time rendering. Our\nframework further extends to dynamic 3D scene generation from a monocular input\nvideo. Experimental results show that our framework achieves state-of-the-art\nperformance in static and dynamic 3D scene generation.\n13 authors\n·\nSep 23, 2025\nUpvote\n20\nGitHub\n365\narXiv Page\nSubmitted by\nakhaliq\nLlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models\nEfficient fine-tuning is vital for adapting large language models (LLMs) to\ndownstream tasks. However, it requires non-trivial efforts to implement these\nmethods on different models. We present LlamaFactory, a unified framework that\nintegrates a suite of cutting-edge efficient training methods. It allows users\nto flexibly customize the fine-tuning of 100+ LLMs without the need for coding\nthrough the built-in web UI LlamaBoard. We empirically validate the efficiency\nand effectiveness of our framework on language modeling and text generation\ntasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and\nalready received over 13,000 stars and 1,600 forks.\n5 authors\n·\nPublished on Mar 20, 2024\nUpvote\n146\nGitHub\n59.5k\narXiv Page\nSubmitted by\nakhaliq\nLlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models\nEfficient fine-tuning is vital for adapting large language models (LLMs) to\ndownstream tasks. However, it requires non-trivial efforts to implement these\nmethods on different models. We present LlamaFactory, a unified framework that\nintegrates a suite of cutting-edge efficient training methods. It allows users\nto flexibly customize the fine-tuning of 100+ LLMs without the need for coding\nthrough the built-in web UI LlamaBoard. We empirically validate the efficiency\nand effectiveness of our framework on language modeling and text generation\ntasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and\nalready received over 13,000 stars and 1,600 forks.\n5 authors\n·\nMar 20, 2024\nUpvote\n146\nGitHub\n59.5k\narXiv Page\nSubmitted by\ntaesiri\nScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform\n  Data\nVision-Language Models (VLMs) have enabled computer use agents (CUAs) that\noperate GUIs autonomously, showing great potential, yet progress is limited by\nthe lack of large-scale, open-source computer use data and foundation models.\nIn this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It\noffers a large-scale dataset spanning 6 operating systems and 3 task domains,\nbuilt via a closed-loop pipeline uniting automated agents with human experts.\nTrained on this scaled-up data, ScaleCUA can operate seamlessly across\nplatforms. Specifically, it delivers strong gains over baselines (+26.6 on\nWebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art\nresults (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on\nWebArena-Lite-v2). These findings underscore the power of data-driven scaling\nfor general-purpose computer use agents. We will release data, models, and code\nto advance future research: https://github.com/OpenGVLab/ScaleCUA.\n21 authors\n·\nPublished on Sep 18, 2025\nUpvote\n101\nGitHub\n333\narXiv Page\nSubmitted by\ntaesiri\nScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform\n  Data\nVision-Language Models (VLMs) have enabled computer use agents (CUAs) that\noperate GUIs autonomously, showing great potential, yet progress is limited by\nthe lack of large-scale, open-source computer use data and foundation models.\nIn this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It\noffers a large-scale dataset spanning 6 operating systems and 3 task domains,\nbuilt via a closed-loop pipeline uniting automated agents with human experts.\nTrained on this scaled-up data, ScaleCUA can operate seamlessly across\nplatforms. Specifically, it delivers strong gains over baselines (+26.6 on\nWebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art\nresults (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on\nWebArena-Lite-v2). These findings underscore the power of data-driven scaling\nfor general-purpose computer use agents. We will release data, models, and code\nto advance future research: https://github.com/OpenGVLab/ScaleCUA.\n21 authors\n·\nSep 18, 2025\nUpvote\n101\nGitHub\n333\narXiv Page\nSubmitted by\nWeiyun1025\nInternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models\nWe introduce InternVL3, a significant advancement in the InternVL series\nfeaturing a native multimodal pre-training paradigm. Rather than adapting a\ntext-only large language model (LLM) into a multimodal large language model\n(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\nduring a single pre-training stage. This unified training paradigm effectively\naddresses the complexities and alignment challenges commonly encountered in\nconventional post-hoc training pipelines for MLLMs. To further improve\nperformance and scalability, InternVL3 incorporates variable visual position\nencoding (V2PE) to support extended multimodal contexts, employs advanced\npost-training techniques such as supervised fine-tuning (SFT) and mixed\npreference optimization (MPO), and adopts test-time scaling strategies\nalongside an optimized training infrastructure. Extensive empirical evaluations\ndemonstrate that InternVL3 delivers superior performance across a wide range of\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\ncapabilities remain highly competitive with leading proprietary models,\nincluding ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also\nmaintaining strong pure-language proficiency. In pursuit of open-science\nprinciples, we will publicly release both the training data and model weights\nto foster further research and development in next-generation MLLMs.\n47 authors\n·\nPublished on Apr 14, 2025\nUpvote\n292\narXiv Page\nSubmitted by\nWeiyun1025\nInternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models\nWe introduce InternVL3, a significant advancement in the InternVL series\nfeaturing a native multimodal pre-training paradigm. Rather than adapting a\ntext-only large language model (LLM) into a multimodal large language model\n(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\nduring a single pre-training stage. This unified training paradigm effectively\naddresses the complexities and alignment challenges commonly encountered in\nconventional post-hoc training pipelines for MLLMs. To further improve\nperformance and scalability, InternVL3 incorporates variable visual position\nencoding (V2PE) to support extended multimodal contexts, employs advanced\npost-training techniques such as supervised fine-tuning (SFT) and mixed\npreference optimization (MPO), and adopts test-time scaling strategies\nalongside an optimized training infrastructure. Extensive empirical evaluations\ndemonstrate that InternVL3 delivers superior performance across a wide range of\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\ncapabilities remain highly competitive with leading proprietary models,\nincluding ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also\nmaintaining strong pure-language proficiency. In pursuit of open-science\nprinciples, we will publicly release both the training data and model weights\nto foster further research and development in next-generation MLLMs.\n47 authors\n·\nApr 14, 2025\nUpvote\n292\narXiv Page\nSubmitted by\nimryanxu\nComfyUI-Copilot: An Intelligent Assistant for Automated Workflow\n  Development\nWe introduce ComfyUI-Copilot, a large language model-powered plugin designed\nto enhance the usability and efficiency of ComfyUI, an open-source platform for\nAI-driven art creation. Despite its flexibility and user-friendly interface,\nComfyUI can present challenges to newcomers, including limited documentation,\nmodel misconfigurations, and the complexity of workflow design. ComfyUI-Copilot\naddresses these challenges by offering intelligent node and model\nrecommendations, along with automated one-click workflow construction. At its\ncore, the system employs a hierarchical multi-agent framework comprising a\ncentral assistant agent for task delegation and specialized worker agents for\ndifferent usages, supported by our curated ComfyUI knowledge bases to\nstreamline debugging and deployment. We validate the effectiveness of\nComfyUI-Copilot through both offline quantitative evaluations and online user\nfeedback, showing that it accurately recommends nodes and accelerates workflow\ndevelopment. Additionally, use cases illustrate that ComfyUI-Copilot lowers\nentry barriers for beginners and enhances workflow efficiency for experienced\nusers. The ComfyUI-Copilot installation package and a demo video are available\nat https://github.com/AIDC-AI/ComfyUI-Copilot.\n10 authors\n·\nPublished on Jun 5, 2025\nUpvote\n79\nGitHub\n3.17k\narXiv Page\nSubmitted by\nimryanxu\nComfyUI-Copilot: An Intelligent Assistant for Automated Workflow\n  Development\nWe introduce ComfyUI-Copilot, a large language model-powered plugin designed\nto enhance the usability and efficiency of ComfyUI, an open-source platform for\nAI-driven art creation. Despite its flexibility and user-friendly interface,\nComfyUI can present challenges to newcomers, including limited documentation,\nmodel misconfigurations, and the complexity of workflow design. ComfyUI-Copilot\naddresses these challenges by offering intelligent node and model\nrecommendations, along with automated one-click workflow construction. At its\ncore, the system employs a hierarchical multi-agent framework comprising a\ncentral assistant agent for task delegation and specialized worker agents for\ndifferent usages, supported by our curated ComfyUI knowledge bases to\nstreamline debugging and deployment. We validate the effectiveness of\nComfyUI-Copilot through both offline quantitative evaluations and online user\nfeedback, showing that it accurately recommends nodes and accelerates workflow\ndevelopment. Additionally, use cases illustrate that ComfyUI-Copilot lowers\nentry barriers for beginners and enhances workflow efficiency for experienced\nusers. The ComfyUI-Copilot installation package and a demo video are available\nat https://github.com/AIDC-AI/ComfyUI-Copilot.\n10 authors\n·\nJun 5, 2025\nUpvote\n79\nGitHub\n3.17k\narXiv Page\nSubmitted by\niseesaw\nA Survey of Reinforcement Learning for Large Reasoning Models\nIn this paper, we survey recent advances in Reinforcement Learning (RL) for\nreasoning with Large Language Models (LLMs). RL has achieved remarkable success\nin advancing the frontier of LLM capabilities, particularly in addressing\ncomplex logical tasks such as mathematics and coding. As a result, RL has\nemerged as a foundational methodology for transforming LLMs into LRMs. With the\nrapid progress of the field, further scaling of RL for LRMs now faces\nfoundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely\nto revisit the development of this domain, reassess its trajectory, and explore\nstrategies to enhance the scalability of RL toward Artificial SuperIntelligence\n(ASI). In particular, we examine research applying RL to LLMs and LRMs for\nreasoning abilities, especially since the release of DeepSeek-R1, including\nfoundational components, core problems, training resources, and downstream\napplications, to identify future opportunities and directions for this rapidly\nevolving area. We hope this review will promote future research on RL for\nbroader reasoning models. Github:\nhttps://github.com/TsinghuaC3I/Awesome-RL-for-LRMs\n39 authors\n·\nPublished on Sep 10, 2025\nUpvote\n169\nGitHub\n1.47k\narXiv Page\nSubmitted by\niseesaw\nA Survey of Reinforcement Learning for Large Reasoning Models\nIn this paper, we survey recent advances in Reinforcement Learning (RL) for\nreasoning with Large Language Models (LLMs). RL has achieved remarkable success\nin advancing the frontier of LLM capabilities, particularly in addressing\ncomplex logical tasks such as mathematics and coding. As a result, RL has\nemerged as a foundational methodology for transforming LLMs into LRMs. With the\nrapid progress of the field, further scaling of RL for LRMs now faces\nfoundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely\nto revisit the development of this domain, reassess its trajectory, and explore\nstrategies to enhance the scalability of RL toward Artificial SuperIntelligence\n(ASI). In particular, we examine research applying RL to LLMs and LRMs for\nreasoning abilities, especially since the release of DeepSeek-R1, including\nfoundational components, core problems, training resources, and downstream\napplications, to identify future opportunities and directions for this rapidly\nevolving area. We hope this review will promote future research on RL for\nbroader reasoning models. Github:\nhttps://github.com/TsinghuaC3I/Awesome-RL-for-LRMs\n39 authors\n·\nSep 10, 2025\nUpvote\n169\nGitHub\n1.47k\narXiv Page\nSubmitted by\nnielsr\nDINOv3\nSelf-supervised learning holds the promise of eliminating the need for manual\ndata annotation, enabling models to scale effortlessly to massive datasets and\nlarger architectures. By not being tailored to specific tasks or domains, this\ntraining paradigm has the potential to learn visual representations from\ndiverse sources, ranging from natural to aerial images -- using a single\nalgorithm. This technical report introduces DINOv3, a major milestone toward\nrealizing this vision by leveraging simple yet effective strategies. First, we\nleverage the benefit of scaling both dataset and model size by careful data\npreparation, design, and optimization. Second, we introduce a new method called\nGram anchoring, which effectively addresses the known yet unsolved issue of\ndense feature maps degrading during long training schedules. Finally, we apply\npost-hoc strategies that further enhance our models' flexibility with respect\nto resolution, model size, and alignment with text. As a result, we present a\nversatile vision foundation model that outperforms the specialized state of the\nart across a broad range of settings, without fine-tuning. DINOv3 produces\nhigh-quality dense features that achieve outstanding performance on various\nvision tasks, significantly surpassing previous self- and weakly-supervised\nfoundation models. We also share the DINOv3 suite of vision models, designed to\nadvance the state of the art on a wide spectrum of tasks and data by providing\nscalable solutions for diverse resource constraints and deployment scenarios.\n26 authors\n·\nPublished on Aug 13, 2025\nUpvote\n262\nGitHub\n7.37k\narXiv Page\nSubmitted by\nnielsr\nDINOv3\nSelf-supervised learning holds the promise of eliminating the need for manual\ndata annotation, enabling models to scale effortlessly to massive datasets and\nlarger architectures. By not being tailored to specific tasks or domains, this\ntraining paradigm has the potential to learn visual representations from\ndiverse sources, ranging from natural to aerial images -- using a single\nalgorithm. This technical report introduces DINOv3, a major milestone toward\nrealizing this vision by leveraging simple yet effective strategies. First, we\nleverage the benefit of scaling both dataset and model size by careful data\npreparation, design, and optimization. Second, we introduce a new method called\nGram anchoring, which effectively addresses the known yet unsolved issue of\ndense feature maps degrading during long training schedules. Finally, we apply\npost-hoc strategies that further enhance our models' flexibility with respect\nto resolution, model size, and alignment with text. As a result, we present a\nversatile vision foundation model that outperforms the specialized state of the\nart across a broad range of settings, without fine-tuning. DINOv3 produces\nhigh-quality dense features that achieve outstanding performance on various\nvision tasks, significantly surpassing previous self- and weakly-supervised\nfoundation models. We also share the DINOv3 suite of vision models, designed to\nadvance the state of the art on a wide spectrum of tasks and data by providing\nscalable solutions for diverse resource constraints and deployment scenarios.\n26 authors\n·\nAug 13, 2025\nUpvote\n262\nGitHub\n7.37k\narXiv Page\nSubmitted by\nLooperXX\nQwen-Image Technical Report\nWe present Qwen-Image, an image generation foundation model in the Qwen\nseries that achieves significant advances in complex text rendering and precise\nimage editing. To address the challenges of complex text rendering, we design a\ncomprehensive data pipeline that includes large-scale data collection,\nfiltering, annotation, synthesis, and balancing. Moreover, we adopt a\nprogressive training strategy that starts with non-text-to-text rendering,\nevolves from simple to complex textual inputs, and gradually scales up to\nparagraph-level descriptions. This curriculum learning approach substantially\nenhances the model's native text rendering capabilities. As a result,\nQwen-Image not only performs exceptionally well in alphabetic languages such as\nEnglish, but also achieves remarkable progress on more challenging logographic\nlanguages like Chinese. To enhance image editing consistency, we introduce an\nimproved multi-task training paradigm that incorporates not only traditional\ntext-to-image (T2I) and text-image-to-image (TI2I) tasks but also\nimage-to-image (I2I) reconstruction, effectively aligning the latent\nrepresentations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed\nthe original image into Qwen2.5-VL and the VAE encoder to obtain semantic and\nreconstructive representations, respectively. This dual-encoding mechanism\nenables the editing module to strike a balance between preserving semantic\nconsistency and maintaining visual fidelity. Qwen-Image achieves\nstate-of-the-art performance, demonstrating its strong capabilities in both\nimage generation and editing across multiple benchmarks.\n39 authors\n·\nPublished on Aug 4, 2025\nUpvote\n255\nGitHub\n5.4k\narXiv Page\nSubmitted by\nLooperXX\nQwen-Image Technical Report\nWe present Qwen-Image, an image generation foundation model in the Qwen\nseries that achieves significant advances in complex text rendering and precise\nimage editing. To address the challenges of complex text rendering, we design a\ncomprehensive data pipeline that includes large-scale data collection,\nfiltering, annotation, synthesis, and balancing. Moreover, we adopt a\nprogressive training strategy that starts with non-text-to-text rendering,\nevolves from simple to complex textual inputs, and gradually scales up to\nparagraph-level descriptions. This curriculum learning approach substantially\nenhances the model's native text rendering capabilities. As a result,\nQwen-Image not only performs exceptionally well in alphabetic languages such as\nEnglish, but also achieves remarkable progress on more challenging logographic\nlanguages like Chinese. To enhance image editing consistency, we introduce an\nimproved multi-task training paradigm that incorporates not only traditional\ntext-to-image (T2I) and text-image-to-image (TI2I) tasks but also\nimage-to-image (I2I) reconstruction, effectively aligning the latent\nrepresentations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed\nthe original image into Qwen2.5-VL and the VAE encoder to obtain semantic and\nreconstructive representations, respectively. This dual-encoding mechanism\nenables the editing module to strike a balance between preserving semantic\nconsistency and maintaining visual fidelity. Qwen-Image achieves\nstate-of-the-art performance, demonstrating its strong capabilities in both\nimage generation and editing across multiple benchmarks.\n39 authors\n·\nAug 4, 2025\nUpvote\n255\nGitHub\n5.4k\narXiv Page\nSubmitted by\nxiaochonglinghu\nTree Search for LLM Agent Reinforcement Learning\nRecent advances in reinforcement learning (RL) have significantly enhanced\nthe agentic capabilities of large language models (LLMs). In long-term and\nmulti-turn agent tasks, existing approaches driven solely by outcome rewards\noften suffer from the problem of sparse supervision. To address the challenge,\nwe propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped\nagent RL method based on tree search, where each tree node represents the\ncomplete agent interaction step. By sharing common prefixes, the tree search\nsampling increases the number of rollouts achievable within a fixed budget of\ntokens or tool calls. Moreover, we find that the tree-structured trajectory\nnaturally allows the construction of step-wise process supervised signals even\nusing only the outcome reward. Based on this, Tree-GRPO estimates the grouped\nrelative advantages both on intra-tree and inter-tree levels. Through\ntheoretical analysis, we demonstrate that the objective of intra-tree level\ngroup relative policy optimization is equivalent to that of step-level direct\npreference learning. Experiments across 11 datasets and 3 types of QA tasks\ndemonstrate the superiority of the proposed tree-based RL over the chain-based\nRL method.\n6 authors\n·\nPublished on Sep 25, 2025\nUpvote\n70\nGitHub\n86\narXiv Page\nSubmitted by\nxiaochonglinghu\nTree Search for LLM Agent Reinforcement Learning\nRecent advances in reinforcement learning (RL) have significantly enhanced\nthe agentic capabilities of large language models (LLMs). In long-term and\nmulti-turn agent tasks, existing approaches driven solely by outcome rewards\noften suffer from the problem of sparse supervision. To address the challenge,\nwe propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped\nagent RL method based on tree search, where each tree node represents the\ncomplete agent interaction step. By sharing common prefixes, the tree search\nsampling increases the number of rollouts achievable within a fixed budget of\ntokens or tool calls. Moreover, we find that the tree-structured trajectory\nnaturally allows the construction of step-wise process supervised signals even\nusing only the outcome reward. Based on this, Tree-GRPO estimates the grouped\nrelative advantages both on intra-tree and inter-tree levels. Through\ntheoretical analysis, we demonstrate that the objective of intra-tree level\ngroup relative policy optimization is equivalent to that of step-level direct\npreference learning. Experiments across 11 datasets and 3 types of QA tasks\ndemonstrate the superiority of the proposed tree-based RL over the chain-based\nRL method.\n6 authors\n·\nSep 25, 2025\nUpvote\n70\nGitHub\n86\narXiv Page\nSubmitted by\nLiu-Hy\nGenoMAS: A Multi-Agent Framework for Scientific Discovery via\n  Code-Driven Gene Expression Analysis\nGene expression analysis holds the key to many biomedical discoveries, yet\nextracting insights from raw transcriptomic data remains formidable due to the\ncomplexity of multiple large, semi-structured files and the need for extensive\ndomain expertise. Current automation approaches are often limited by either\ninflexible workflows that break down in edge cases or by fully autonomous\nagents that lack the necessary precision for rigorous scientific inquiry.\nGenoMAS charts a different course by presenting a team of LLM-based scientists\nthat integrates the reliability of structured workflows with the adaptability\nof autonomous agents. GenoMAS orchestrates six specialized LLM agents through\ntyped message-passing protocols, each contributing complementary strengths to a\nshared analytic canvas. At the heart of GenoMAS lies a guided-planning\nframework: programming agents unfold high-level task guidelines into Action\nUnits and, at each juncture, elect to advance, revise, bypass, or backtrack,\nthereby maintaining logical coherence while bending gracefully to the\nidiosyncrasies of genomic data.\n  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation\nof 89.13% for data preprocessing and an F_1 of 60.48% for gene\nidentification, surpassing the best prior art by 10.61% and 16.85%\nrespectively. Beyond metrics, GenoMAS surfaces biologically plausible\ngene-phenotype associations corroborated by the literature, all while adjusting\nfor latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.\n3 authors\n·\nPublished on Jul 28, 2025\nUpvote\n3\nGitHub\n80\narXiv Page\nSubmitted by\nLiu-Hy\nGenoMAS: A Multi-Agent Framework for Scientific Discovery via\n  Code-Driven Gene Expression Analysis\nGene expression analysis holds the key to many biomedical discoveries, yet\nextracting insights from raw transcriptomic data remains formidable due to the\ncomplexity of multiple large, semi-structured files and the need for extensive\ndomain expertise. Current automation approaches are often limited by either\ninflexible workflows that break down in edge cases or by fully autonomous\nagents that lack the necessary precision for rigorous scientific inquiry.\nGenoMAS charts a different course by presenting a team of LLM-based scientists\nthat integrates the reliability of structured workflows with the adaptability\nof autonomous agents. GenoMAS orchestrates six specialized LLM agents through\ntyped message-passing protocols, each contributing complementary strengths to a\nshared analytic canvas. At the heart of GenoMAS lies a guided-planning\nframework: programming agents unfold high-level task guidelines into Action\nUnits and, at each juncture, elect to advance, revise, bypass, or backtrack,\nthereby maintaining logical coherence while bending gracefully to the\nidiosyncrasies of genomic data.\n  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation\nof 89.13% for data preprocessing and an F_1 of 60.48% for gene\nidentification, surpassing the best prior art by 10.61% and 16.85%\nrespectively. Beyond metrics, GenoMAS surfaces biologically plausible\ngene-phenotype associations corroborated by the literature, all while adjusting\nfor latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.\n3 authors\n·\nJul 28, 2025\nUpvote\n3\nGitHub\n80\narXiv Page\nSubmitted by\nLin1557\nURSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics\nChain-of-thought (CoT) reasoning has been widely applied in the mathematical\nreasoning of Large Language Models (LLMs). Recently, the introduction of\nderivative process supervision on CoT trajectories has sparked discussions on\nenhancing scaling capabilities during test time, thereby boosting the potential\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving\nhigh-precision CoT reasoning and has limited the realization of reasoning\npotential during test time. In this work, we propose a three-module synthesis\nstrategy that integrates CoT distillation, trajectory-format rewriting, and\nformat unification. It results in a high-quality CoT reasoning instruction\nfine-tuning dataset in multimodal mathematics, MMathCoT-1M. We comprehensively\nvalidate the state-of-the-art (SOTA) performance of the trained URSA-7B model\non multiple multimodal mathematical benchmarks. For test-time scaling, we\nintroduce a data synthesis strategy that automatically generates process\nannotation datasets, known as DualMath-1.1M, focusing on both interpretation\nand logic. By further training URSA-7B on DualMath-1.1M, we transition from CoT\nreasoning capabilities to robust supervision abilities. The trained URSA-RM-7B\nacts as a verifier, effectively enhancing the performance of URSA-7B at test\ntime. URSA-RM-7B also demonstrates excellent out-of-distribution (OOD)\nverifying capabilities, showcasing its generalization. Model weights, training\ndata and code will be open-sourced.\n8 authors\n·\nPublished on Jan 8, 2025\nUpvote\n53\nGitHub\n97\narXiv Page\nSubmitted by\nLin1557\nURSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics\nChain-of-thought (CoT) reasoning has been widely applied in the mathematical\nreasoning of Large Language Models (LLMs). Recently, the introduction of\nderivative process supervision on CoT trajectories has sparked discussions on\nenhancing scaling capabilities during test time, thereby boosting the potential\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving\nhigh-precision CoT reasoning and has limited the realization of reasoning\npotential during test time. In this work, we propose a three-module synthesis\nstrategy that integrates CoT distillation, trajectory-format rewriting, and\nformat unification. It results in a high-quality CoT reasoning instruction\nfine-tuning dataset in multimodal mathematics, MMathCoT-1M. We comprehensively\nvalidate the state-of-the-art (SOTA) performance of the trained URSA-7B model\non multiple multimodal mathematical benchmarks. For test-time scaling, we\nintroduce a data synthesis strategy that automatically generates process\nannotation datasets, known as DualMath-1.1M, focusing on both interpretation\nand logic. By further training URSA-7B on DualMath-1.1M, we transition from CoT\nreasoning capabilities to robust supervision abilities. The trained URSA-RM-7B\nacts as a verifier, effectively enhancing the performance of URSA-7B at test\ntime. URSA-RM-7B also demonstrates excellent out-of-distribution (OOD)\nverifying capabilities, showcasing its generalization. Model weights, training\ndata and code will be open-sourced.\n8 authors\n·\nJan 8, 2025\nUpvote\n53\nGitHub\n97\narXiv Page\nSubmitted by\nxianbao\nGLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models\nWe present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language\nmodel with 355B total parameters and 32B activated parameters, featuring a\nhybrid reasoning method that supports both thinking and direct response modes.\nThrough multi-stage training on 23T tokens and comprehensive post-training with\nexpert model iteration and reinforcement learning, GLM-4.5 achieves strong\nperformance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on\nTAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer\nparameters than several competitors, GLM-4.5 ranks 3rd overall among all\nevaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B\nparameters) and a compact version, GLM-4.5-Air (106B parameters), to advance\nresearch in reasoning and agentic AI systems. Code, models, and more\ninformation are available at https://github.com/zai-org/GLM-4.5.\n171 authors\n·\nPublished on Aug 8, 2025\nUpvote\n181\nGitHub\n2.74k\narXiv Page\nSubmitted by\nxianbao\nGLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models\nWe present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language\nmodel with 355B total parameters and 32B activated parameters, featuring a\nhybrid reasoning method that supports both thinking and direct response modes.\nThrough multi-stage training on 23T tokens and comprehensive post-training with\nexpert model iteration and reinforcement learning, GLM-4.5 achieves strong\nperformance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on\nTAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer\nparameters than several competitors, GLM-4.5 ranks 3rd overall among all\nevaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B\nparameters) and a compact version, GLM-4.5-Air (106B parameters), to advance\nresearch in reasoning and agentic AI systems. Code, models, and more\ninformation are available at https://github.com/zai-org/GLM-4.5.\n171 authors\n·\nAug 8, 2025\nUpvote\n181\nGitHub\n2.74k\narXiv Page\nSubmitted by\n\t\t\t\tdeleted\nMonkeyOCR: Document Parsing with a Structure-Recognition-Relation\n  Triplet Paradigm\nWe introduce MonkeyOCR, a vision-language model for document parsing that\nadvances the state of the art by leveraging a Structure-Recognition-Relation\n(SRR) triplet paradigm. This design simplifies what would otherwise be a\ncomplex multi-tool pipeline (as in MinerU's modular approach) and avoids the\ninefficiencies of processing full pages with giant end-to-end models (e.g.,\nlarge multimodal LLMs like Qwen-VL). In SRR, document parsing is abstracted\ninto three fundamental questions - \"Where is it?\" (structure), \"What is it?\"\n(recognition), and \"How is it organized?\" (relation) - corresponding to layout\nanalysis, content identification, and logical ordering. This focused\ndecomposition balances accuracy and speed: it enables efficient, scalable\nprocessing without sacrificing precision. To train and evaluate this approach,\nwe introduce the MonkeyDoc (the most comprehensive document parsing dataset to\ndate), with 3.9 million instances spanning over ten document types in both\nChinese and English. Experiments show that MonkeyOCR outperforms MinerU by an\naverage of 5.1%, with particularly notable improvements on challenging content\nsuch as formulas (+15.0%) and tables (+8.6%). Remarkably, our 3B-parameter\nmodel surpasses much larger and top-performing models, including Qwen2.5-VL\n(72B) and Gemini 2.5 Pro, achieving state-of-the-art average performance on\nEnglish document parsing tasks. In addition, MonkeyOCR processes multi-page\ndocuments significantly faster (0.84 pages per second compared to 0.65 for\nMinerU and 0.12 for Qwen2.5-VL-7B). The 3B model can be efficiently deployed\nfor inference on a single NVIDIA 3090 GPU. Code and models will be released at\nhttps://github.com/Yuliang-Liu/MonkeyOCR.\n10 authors\n·\nPublished on Jun 5, 2025\nUpvote\n2\nGitHub\n5.98k\narXiv Page\nSubmitted by\n\t\t\t\tdeleted\nMonkeyOCR: Document Parsing with a Structure-Recognition-Relation\n  Triplet Paradigm\nWe introduce MonkeyOCR, a vision-language model for document parsing that\nadvances the state of the art by leveraging a Structure-Recognition-Relation\n(SRR) triplet paradigm. This design simplifies what would otherwise be a\ncomplex multi-tool pipeline (as in MinerU's modular approach) and avoids the\ninefficiencies of processing full pages with giant end-to-end models (e.g.,\nlarge multimodal LLMs like Qwen-VL). In SRR, document parsing is abstracted\ninto three fundamental questions - \"Where is it?\" (structure), \"What is it?\"\n(recognition), and \"How is it organized?\" (relation) - corresponding to layout\nanalysis, content identification, and logical ordering. This focused\ndecomposition balances accuracy and speed: it enables efficient, scalable\nprocessing without sacrificing precision. To train and evaluate this approach,\nwe introduce the MonkeyDoc (the most comprehensive document parsing dataset to\ndate), with 3.9 million instances spanning over ten document types in both\nChinese and English. Experiments show that MonkeyOCR outperforms MinerU by an\naverage of 5.1%, with particularly notable improvements on challenging content\nsuch as formulas (+15.0%) and tables (+8.6%). Remarkably, our 3B-parameter\nmodel surpasses much larger and top-performing models, including Qwen2.5-VL\n(72B) and Gemini 2.5 Pro, achieving state-of-the-art average performance on\nEnglish document parsing tasks. In addition, MonkeyOCR processes multi-page\ndocuments significantly faster (0.84 pages per second compared to 0.65 for\nMinerU and 0.12 for Qwen2.5-VL-7B). The 3B model can be efficiently deployed\nfor inference on a single NVIDIA 3090 GPU. Code and models will be released at\nhttps://github.com/Yuliang-Liu/MonkeyOCR.\n10 authors\n·\nJun 5, 2025\nUpvote\n2\nGitHub\n5.98k\narXiv Page\nSubmitted by\nhiyouga\nEasy Dataset: A Unified and Extensible Framework for Synthesizing LLM\n  Fine-Tuning Data from Unstructured Documents\nLarge language models (LLMs) have shown impressive performance on\ngeneral-purpose tasks, yet adapting them to specific domains remains\nchallenging due to the scarcity of high-quality domain data. Existing data\nsynthesis tools often struggle to extract reliable fine-tuning data from\nheterogeneous documents effectively. To address this limitation, we propose\nEasy Dataset, a unified framework for synthesizing fine-tuning data from\nunstructured documents via an intuitive graphical user interface (GUI).\nSpecifically, Easy Dataset allows users to easily configure text extraction\nmodels and chunking strategies to transform raw documents into coherent text\nchunks. It then leverages a persona-driven prompting approach to generate\ndiverse question-answer pairs using public-available LLMs. Throughout the\npipeline, a human-in-the-loop visual interface facilitates the review and\nrefinement of intermediate outputs to ensure data quality. Experiments on a\nfinancial question-answering task show that fine-tuning LLMs on the synthesized\ndataset significantly improves domain-specific performance while preserving\ngeneral knowledge. The source code and installable package are available at\nhttps://github.com/ConardLi/easy-dataset and have garnered over 9,000 GitHub\nstars.\n7 authors\n·\nPublished on Jul 5, 2025\nUpvote\n47\nGitHub\n11k\narXiv Page\nSubmitted by\nhiyouga\nEasy Dataset: A Unified and Extensible Framework for Synthesizing LLM\n  Fine-Tuning Data from Unstructured Documents\nLarge language models (LLMs) have shown impressive performance on\ngeneral-purpose tasks, yet adapting them to specific domains remains\nchallenging due to the scarcity of high-quality domain data. Existing data\nsynthesis tools often struggle to extract reliable fine-tuning data from\nheterogeneous documents effectively. To address this limitation, we propose\nEasy Dataset, a unified framework for synthesizing fine-tuning data from\nunstructured documents via an intuitive graphical user interface (GUI).\nSpecifically, Easy Dataset allows users to easily configure text extraction\nmodels and chunking strategies to transform raw documents into coherent text\nchunks. It then leverages a persona-driven prompting approach to generate\ndiverse question-answer pairs using public-available LLMs. Throughout the\npipeline, a human-in-the-loop visual interface facilitates the review and\nrefinement of intermediate outputs to ensure data quality. Experiments on a\nfinancial question-answering task show that fine-tuning LLMs on the synthesized\ndataset significantly improves domain-specific performance while preserving\ngeneral knowledge. The source code and installable package are available at\nhttps://github.com/ConardLi/easy-dataset and have garnered over 9,000 GitHub\nstars.\n7 authors\n·\nJul 5, 2025\nUpvote\n47\nGitHub\n11k\narXiv Page\nSubmitted by\ncomar\nVideoFrom3D: 3D Scene Video Generation via Complementary Image and Video\n  Diffusion Models\nIn this paper, we propose VideoFrom3D, a novel framework for synthesizing\nhigh-quality 3D scene videos from coarse geometry, a camera trajectory, and a\nreference image. Our approach streamlines the 3D graphic design workflow,\nenabling flexible design exploration and rapid production of deliverables. A\nstraightforward approach to synthesizing a video from coarse geometry might\ncondition a video diffusion model on geometric structure. However, existing\nvideo diffusion models struggle to generate high-fidelity results for complex\nscenes due to the difficulty of jointly modeling visual quality, motion, and\ntemporal consistency. To address this, we propose a generative framework that\nleverages the complementary strengths of image and video diffusion models.\nSpecifically, our framework consists of a Sparse Anchor-view Generation (SAG)\nand a Geometry-guided Generative Inbetweening (GGI) module. The SAG module\ngenerates high-quality, cross-view consistent anchor views using an image\ndiffusion model, aided by Sparse Appearance-guided Sampling. Building on these\nanchor views, GGI module faithfully interpolates intermediate frames using a\nvideo diffusion model, enhanced by flow-based camera control and structural\nguidance. Notably, both modules operate without any paired dataset of 3D scene\nmodels and natural images, which is extremely difficult to obtain.\nComprehensive experiments show that our method produces high-quality,\nstyle-consistent scene videos under diverse and challenging scenarios,\noutperforming simple and extended baselines.\n3 authors\n·\nPublished on Sep 22, 2025\nUpvote\n25\nGitHub\n83\narXiv Page\nSubmitted by\ncomar\nVideoFrom3D: 3D Scene Video Generation via Complementary Image and Video\n  Diffusion Models\nIn this paper, we propose VideoFrom3D, a novel framework for synthesizing\nhigh-quality 3D scene videos from coarse geometry, a camera trajectory, and a\nreference image. Our approach streamlines the 3D graphic design workflow,\nenabling flexible design exploration and rapid production of deliverables. A\nstraightforward approach to synthesizing a video from coarse geometry might\ncondition a video diffusion model on geometric structure. However, existing\nvideo diffusion models struggle to generate high-fidelity results for complex\nscenes due to the difficulty of jointly modeling visual quality, motion, and\ntemporal consistency. To address this, we propose a generative framework that\nleverages the complementary strengths of image and video diffusion models.\nSpecifically, our framework consists of a Sparse Anchor-view Generation (SAG)\nand a Geometry-guided Generative Inbetweening (GGI) module. The SAG module\ngenerates high-quality, cross-view consistent anchor views using an image\ndiffusion model, aided by Sparse Appearance-guided Sampling. Building on these\nanchor views, GGI module faithfully interpolates intermediate frames using a\nvideo diffusion model, enhanced by flow-based camera control and structural\nguidance. Notably, both modules operate without any paired dataset of 3D scene\nmodels and natural images, which is extremely difficult to obtain.\nComprehensive experiments show that our method produces high-quality,\nstyle-consistent scene videos under diverse and challenging scenarios,\noutperforming simple and extended baselines.\n3 authors\n·\nSep 22, 2025\nUpvote\n25\nGitHub\n83\narXiv Page\nSubmitted by\nvztu\n4KAgent: Agentic Any Image to 4K Super-Resolution\nWe present 4KAgent, a unified agentic super-resolution generalist system\ndesigned to universally upscale any image to 4K resolution (and even higher, if\napplied iteratively). Our system can transform images from extremely low\nresolutions with severe degradations, for example, highly distorted inputs at\n256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three\ncore components: (1) Profiling, a module that customizes the 4KAgent pipeline\nbased on bespoke use cases; (2) A Perception Agent, which leverages\nvision-language models alongside image quality assessment experts to analyze\nthe input image and make a tailored restoration plan; and (3) A Restoration\nAgent, which executes the plan, following a recursive execution-reflection\nparadigm, guided by a quality-driven mixture-of-expert policy to select the\noptimal output for each step. Additionally, 4KAgent embeds a specialized face\nrestoration pipeline, significantly enhancing facial details in portrait and\nselfie photos. We rigorously evaluate our 4KAgent across 11 distinct task\ncategories encompassing a total of 26 diverse benchmarks, setting new\nstate-of-the-art on a broad spectrum of imaging domains. Our evaluations cover\nnatural images, portrait photos, AI-generated content, satellite imagery,\nfluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and\nX-ray, demonstrating superior performance in terms of both perceptual (e.g.,\nNIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic\nparadigm for low-level vision tasks, we aim to catalyze broader interest and\ninnovation within vision-centric autonomous agents across diverse research\ncommunities. We will release all the code, models, and results at:\nhttps://4kagent.github.io.\n13 authors\n·\nPublished on Jul 9, 2025\nUpvote\n104\nGitHub\n547\narXiv Page\nSubmitted by\nvztu\n4KAgent: Agentic Any Image to 4K Super-Resolution\nWe present 4KAgent, a unified agentic super-resolution generalist system\ndesigned to universally upscale any image to 4K resolution (and even higher, if\napplied iteratively). Our system can transform images from extremely low\nresolutions with severe degradations, for example, highly distorted inputs at\n256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three\ncore components: (1) Profiling, a module that customizes the 4KAgent pipeline\nbased on bespoke use cases; (2) A Perception Agent, which leverages\nvision-language models alongside image quality assessment experts to analyze\nthe input image and make a tailored restoration plan; and (3) A Restoration\nAgent, which executes the plan, following a recursive execution-reflection\nparadigm, guided by a quality-driven mixture-of-expert policy to select the\noptimal output for each step. Additionally, 4KAgent embeds a specialized face\nrestoration pipeline, significantly enhancing facial details in portrait and\nselfie photos. We rigorously evaluate our 4KAgent across 11 distinct task\ncategories encompassing a total of 26 diverse benchmarks, setting new\nstate-of-the-art on a broad spectrum of imaging domains. Our evaluations cover\nnatural images, portrait photos, AI-generated content, satellite imagery,\nfluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and\nX-ray, demonstrating superior performance in terms of both perceptual (e.g.,\nNIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic\nparadigm for low-level vision tasks, we aim to catalyze broader interest and\ninnovation within vision-centric autonomous agents across diverse research\ncommunities. We will release all the code, models, and results at:\nhttps://4kagent.github.io.\n13 authors\n·\nJul 9, 2025\nUpvote\n104\nGitHub\n547\narXiv Page\nSubmitted by\n\t\t\t\tdeleted\nYoutu-GraphRAG: Vertically Unified Agents for Graph Retrieval-Augmented\n  Complex Reasoning\nGraph retrieval-augmented generation (GraphRAG) has effectively enhanced\nlarge language models in complex reasoning by organizing fragmented knowledge\ninto explicitly structured graphs. Prior efforts have been made to improve\neither graph construction or graph retrieval in isolation, yielding suboptimal\nperformance, especially when domain shifts occur. In this paper, we propose a\nvertically unified agentic paradigm, Youtu-GraphRAG, to jointly connect the\nentire framework as an intricate integration. Specifically, (i) a seed graph\nschema is introduced to bound the automatic extraction agent with targeted\nentity types, relations and attribute types, also continuously expanded for\nscalability over unseen domains; (ii) To obtain higher-level knowledge upon the\nschema, we develop novel dually-perceived community detection, fusing\nstructural topology with subgraph semantics for comprehensive knowledge\norganization. This naturally yields a hierarchical knowledge tree that supports\nboth top-down filtering and bottom-up reasoning with community summaries; (iii)\nAn agentic retriever is designed to interpret the same graph schema to\ntransform complex queries into tractable and parallel sub-queries. It\niteratively performs reflection for more advanced reasoning; (iv) To alleviate\nthe knowledge leaking problem in pre-trained LLM, we propose a tailored\nanonymous dataset and a novel 'Anonymity Reversion' task that deeply measures\nthe real performance of the GraphRAG frameworks. Extensive experiments across\nsix challenging benchmarks demonstrate the robustness of Youtu-GraphRAG,\nremarkably moving the Pareto frontier with up to 90.71% saving of token costs\nand 16.62% higher accuracy over state-of-the-art baselines. The results\nindicate our adaptability, allowing seamless domain transfer with minimal\nintervention on schema.\n9 authors\n·\nPublished on Aug 27, 2025\nUpvote\n7\nGitHub\n718\narXiv Page\nSubmitted by\n\t\t\t\tdeleted\nYoutu-GraphRAG: Vertically Unified Agents for Graph Retrieval-Augmented\n  Complex Reasoning\nGraph retrieval-augmented generation (GraphRAG) has effectively enhanced\nlarge language models in complex reasoning by organizing fragmented knowledge\ninto explicitly structured graphs. Prior efforts have been made to improve\neither graph construction or graph retrieval in isolation, yielding suboptimal\nperformance, especially when domain shifts occur. In this paper, we propose a\nvertically unified agentic paradigm, Youtu-GraphRAG, to jointly connect the\nentire framework as an intricate integration. Specifically, (i) a seed graph\nschema is introduced to bound the automatic extraction agent with targeted\nentity types, relations and attribute types, also continuously expanded for\nscalability over unseen domains; (ii) To obtain higher-level knowledge upon the\nschema, we develop novel dually-perceived community detection, fusing\nstructural topology with subgraph semantics for comprehensive knowledge\norganization. This naturally yields a hierarchical knowledge tree that supports\nboth top-down filtering and bottom-up reasoning with community summaries; (iii)\nAn agentic retriever is designed to interpret the same graph schema to\ntransform complex queries into tractable and parallel sub-queries. It\niteratively performs reflection for more advanced reasoning; (iv) To alleviate\nthe knowledge leaking problem in pre-trained LLM, we propose a tailored\nanonymous dataset and a novel 'Anonymity Reversion' task that deeply measures\nthe real performance of the GraphRAG frameworks. Extensive experiments across\nsix challenging benchmarks demonstrate the robustness of Youtu-GraphRAG,\nremarkably moving the Pareto frontier with up to 90.71% saving of token costs\nand 16.62% higher accuracy over state-of-the-art baselines. The results\nindicate our adaptability, allowing seamless domain transfer with minimal\nintervention on schema.\n9 authors\n·\nAug 27, 2025\nUpvote\n7\nGitHub\n718\narXiv Page\nSubmitted by\ntaesiri\nARE: Scaling Up Agent Environments and Evaluations\nWe introduce Meta Agents Research Environments (ARE), a research platform for\nscalable creation of environments, integration of synthetic or real\napplications, and execution of agentic orchestrations. ARE provides simple\nabstractions to build complex and diverse environments, each with their own\nrules, tools, content, and verifiers, helping to bridge the gap between model\ndevelopment and real-world deployment. We also propose Gaia2, a benchmark built\nin ARE and designed to measure general agent capabilities. Beyond search and\nexecution, Gaia2 requires agents to handle ambiguities and noise, adapt to\ndynamic environments, collaborate with other agents, and operate under temporal\nconstraints. Unlike prior benchmarks, Gaia2 runs asynchronously, surfacing new\nfailure modes that are invisible in static settings. Our experiments show that\nno system dominates across the intelligence spectrum: stronger reasoning often\ncomes at the cost of efficiency, and budget scaling curves plateau,\nhighlighting the need for new architectures and adaptive compute strategies.\nPerhaps more importantly, ARE abstractions enable continuous extension of Gaia2\nto other environments, empowering the community to rapidly create new\nbenchmarks tailored to their domains. In AI's second half, progress\nincreasingly depends on defining meaningful tasks and robust evaluations to\ndrive frontier capabilities forward.\n24 authors\n·\nPublished on Sep 21, 2025\nUpvote\n29\nGitHub\n257\narXiv Page\nSubmitted by\ntaesiri\nARE: Scaling Up Agent Environments and Evaluations\nWe introduce Meta Agents Research Environments (ARE), a research platform for\nscalable creation of environments, integration of synthetic or real\napplications, and execution of agentic orchestrations. ARE provides simple\nabstractions to build complex and diverse environments, each with their own\nrules, tools, content, and verifiers, helping to bridge the gap between model\ndevelopment and real-world deployment. We also propose Gaia2, a benchmark built\nin ARE and designed to measure general agent capabilities. Beyond search and\nexecution, Gaia2 requires agents to handle ambiguities and noise, adapt to\ndynamic environments, collaborate with other agents, and operate under temporal\nconstraints. Unlike prior benchmarks, Gaia2 runs asynchronously, surfacing new\nfailure modes that are invisible in static settings. Our experiments show that\nno system dominates across the intelligence spectrum: stronger reasoning often\ncomes at the cost of efficiency, and budget scaling curves plateau,\nhighlighting the need for new architectures and adaptive compute strategies.\nPerhaps more importantly, ARE abstractions enable continuous extension of Gaia2\nto other environments, empowering the community to rapidly create new\nbenchmarks tailored to their domains. In AI's second half, progress\nincreasingly depends on defining meaningful tasks and robust evaluations to\ndrive frontier capabilities forward.\n24 authors\n·\nSep 21, 2025\nUpvote\n29\nGitHub\n257\narXiv Page\nSubmitted by\ntqliu\n4DNeX: Feed-Forward 4D Generative Modeling Made Easy\nWe present 4DNeX, the first feed-forward framework for generating 4D (i.e.,\ndynamic 3D) scene representations from a single image. In contrast to existing\nmethods that rely on computationally intensive optimization or require\nmulti-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D\ngeneration by fine-tuning a pretrained video diffusion model. Specifically, 1)\nto alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale\ndataset with high-quality 4D annotations generated using advanced\nreconstruction approaches. 2) we introduce a unified 6D video representation\nthat jointly models RGB and XYZ sequences, facilitating structured learning of\nboth appearance and geometry. 3) we propose a set of simple yet effective\nadaptation strategies to repurpose pretrained video diffusion models for 4D\nmodeling. 4DNeX produces high-quality dynamic point clouds that enable\nnovel-view video synthesis. Extensive experiments demonstrate that 4DNeX\noutperforms existing 4D generation methods in efficiency and generalizability,\noffering a scalable solution for image-to-4D modeling and laying the foundation\nfor generative 4D world models that simulate dynamic scene evolution.\n9 authors\n·\nPublished on Aug 18, 2025\nUpvote\n60\nGitHub\n651\narXiv Page\nSubmitted by\ntqliu\n4DNeX: Feed-Forward 4D Generative Modeling Made Easy\nWe present 4DNeX, the first feed-forward framework for generating 4D (i.e.,\ndynamic 3D) scene representations from a single image. In contrast to existing\nmethods that rely on computationally intensive optimization or require\nmulti-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D\ngeneration by fine-tuning a pretrained video diffusion model. Specifically, 1)\nto alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale\ndataset with high-quality 4D annotations generated using advanced\nreconstruction approaches. 2) we introduce a unified 6D video representation\nthat jointly models RGB and XYZ sequences, facilitating structured learning of\nboth appearance and geometry. 3) we propose a set of simple yet effective\nadaptation strategies to repurpose pretrained video diffusion models for 4D\nmodeling. 4DNeX produces high-quality dynamic point clouds that enable\nnovel-view video synthesis. Extensive experiments demonstrate that 4DNeX\noutperforms existing 4D generation methods in efficiency and generalizability,\noffering a scalable solution for image-to-4D modeling and laying the foundation\nfor generative 4D world models that simulate dynamic scene evolution.\n9 authors\n·\nAug 18, 2025\nUpvote\n60\nGitHub\n651\narXiv Page\nSubmitted by\ndaixufang\nAgent Lightning: Train ANY AI Agents with Reinforcement Learning\nWe present Agent Lightning, a flexible and extensible framework that enables\nReinforcement Learning (RL)-based training of Large Language Models (LLMs) for\nany AI agent. Unlike existing methods that tightly couple RL training with\nagent or rely on sequence concatenation with masking, Agent Lightning achieves\ncomplete decoupling between agent execution and training, allowing seamless\nintegration with existing agents developed via diverse ways (e.g., using\nframeworks like LangChain, OpenAI Agents SDK, AutoGen, and building from\nscratch) with almost ZERO code modifications. By formulating agent execution as\nMarkov decision process, we define an unified data interface and propose a\nhierarchical RL algorithm, LightningRL, which contains a credit assignment\nmodule, allowing us to decompose trajectories generated by ANY agents into\ntraining transition. This enables RL to handle complex interaction logic, such\nas multi-agent scenarios and dynamic workflows. For the system design, we\nintroduce a Training-Agent Disaggregation architecture, and brings agent\nobservability frameworks into agent runtime, providing a standardized agent\nfinetuning interface. Experiments across text-to-SQL, retrieval-augmented\ngeneration, and math tool-use tasks demonstrate stable, continuous\nimprovements, showcasing the framework's potential for real-world agent\ntraining and deployment.\n8 authors\n·\nPublished on Aug 5, 2025\nUpvote\n70\nGitHub\n1.63k\narXiv Page\nSubmitted by\ndaixufang\nAgent Lightning: Train ANY AI Agents with Reinforcement Learning\nWe present Agent Lightning, a flexible and extensible framework that enables\nReinforcement Learning (RL)-based training of Large Language Models (LLMs) for\nany AI agent. Unlike existing methods that tightly couple RL training with\nagent or rely on sequence concatenation with masking, Agent Lightning achieves\ncomplete decoupling between agent execution and training, allowing seamless\nintegration with existing agents developed via diverse ways (e.g., using\nframeworks like LangChain, OpenAI Agents SDK, AutoGen, and building from\nscratch) with almost ZERO code modifications. By formulating agent execution as\nMarkov decision process, we define an unified data interface and propose a\nhierarchical RL algorithm, LightningRL, which contains a credit assignment\nmodule, allowing us to decompose trajectories generated by ANY agents into\ntraining transition. This enables RL to handle complex interaction logic, such\nas multi-agent scenarios and dynamic workflows. For the system design, we\nintroduce a Training-Agent Disaggregation architecture, and brings agent\nobservability frameworks into agent runtime, providing a standardized agent\nfinetuning interface. Experiments across text-to-SQL, retrieval-augmented\ngeneration, and math tool-use tasks demonstrate stable, continuous\nimprovements, showcasing the framework's potential for real-world agent\ntraining and deployment.\n8 authors\n·\nAug 5, 2025\nUpvote\n70\nGitHub\n1.63k\narXiv Page\nSubmitted by\nhpouransari\nFastVLM: Efficient Vision Encoding for Vision Language Models\nScaling the input image resolution is essential for enhancing the performance\nof Vision Language Models (VLMs), particularly in text-rich image understanding\ntasks. However, popular visual encoders such as ViTs become inefficient at high\nresolutions due to the large number of tokens and high encoding latency caused\nby stacked self-attention layers. At different operational resolutions, the\nvision encoder of a VLM can be optimized along two axes: reducing encoding\nlatency and minimizing the number of visual tokens passed to the LLM, thereby\nlowering overall latency. Based on a comprehensive efficiency analysis of the\ninterplay between image resolution, vision latency, token count, and LLM size,\nwe introduce FastVLM, a model that achieves an optimized trade-off between\nlatency, model size and accuracy. FastVLM incorporates FastViTHD, a novel\nhybrid vision encoder designed to output fewer tokens and significantly reduce\nencoding time for high-resolution images. Unlike previous methods, FastVLM\nachieves the optimal balance between visual token count and image resolution\nsolely by scaling the input image, eliminating the need for additional token\npruning and simplifying the model design. In the LLaVA-1.5 setup, FastVLM\nachieves 3.2times improvement in time-to-first-token (TTFT) while\nmaintaining similar performance on VLM benchmarks compared to prior works.\nCompared to LLaVa-OneVision at the highest resolution (1152times1152),\nFastVLM achieves comparable performance on key benchmarks like SeedBench and\nMMMU, using the same 0.5B LLM, but with 85times faster TTFT and a vision\nencoder that is 3.4times smaller.\n11 authors\n·\nPublished on Dec 17, 2024\nUpvote\n70\nGitHub\n6.68k\narXiv Page\nSubmitted by\nhpouransari\nFastVLM: Efficient Vision Encoding for Vision Language Models\nScaling the input image resolution is essential for enhancing the performance\nof Vision Language Models (VLMs), particularly in text-rich image understanding\ntasks. However, popular visual encoders such as ViTs become inefficient at high\nresolutions due to the large number of tokens and high encoding latency caused\nby stacked self-attention layers. At different operational resolutions, the\nvision encoder of a VLM can be optimized along two axes: reducing encoding\nlatency and minimizing the number of visual tokens passed to the LLM, thereby\nlowering overall latency. Based on a comprehensive efficiency analysis of the\ninterplay between image resolution, vision latency, token count, and LLM size,\nwe introduce FastVLM, a model that achieves an optimized trade-off between\nlatency, model size and accuracy. FastVLM incorporates FastViTHD, a novel\nhybrid vision encoder designed to output fewer tokens and significantly reduce\nencoding time for high-resolution images. Unlike previous methods, FastVLM\nachieves the optimal balance between visual token count and image resolution\nsolely by scaling the input image, eliminating the need for additional token\npruning and simplifying the model design. In the LLaVA-1.5 setup, FastVLM\nachieves 3.2times improvement in time-to-first-token (TTFT) while\nmaintaining similar performance on VLM benchmarks compared to prior works.\nCompared to LLaVa-OneVision at the highest resolution (1152times1152),\nFastVLM achieves comparable performance on key benchmarks like SeedBench and\nMMMU, using the same 0.5B LLM, but with 85times faster TTFT and a vision\nencoder that is 3.4times smaller.\n11 authors\n·\nDec 17, 2024\nUpvote\n70\nGitHub\n6.68k\narXiv Page\nSubmitted by\nxhyandwyy\nMobile-Agent-v3: Foundamental Agents for GUI Automation\nThis paper introduces GUI-Owl, a foundational GUI agent model that achieves\nstate-of-the-art performance among open-source end-to-end models on ten GUI\nbenchmarks across desktop and mobile environments, covering grounding, question\nanswering, planning, decision-making, and procedural knowledge. GUI-Owl-7B\nachieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose\nMobile-Agent-v3, a general-purpose GUI agent framework that further improves\nperformance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new\nstate-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates\nthree key innovations: (1) Large-scale Environment Infrastructure: a\ncloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows,\nenabling our Self-Evolving GUI Trajectory Production framework. This generates\nhigh-quality interaction data via automated query generation and correctness\nvalidation, leveraging GUI-Owl to refine trajectories iteratively, forming a\nself-improving loop. It supports diverse data pipelines and reduces manual\nannotation. (2) Diverse Foundational Agent Capabilities: by integrating UI\ngrounding, planning, action semantics, and reasoning patterns, GUI-Owl supports\nend-to-end decision-making and can act as a modular component in multi-agent\nsystems. (3) Scalable Environment RL: we develop a scalable reinforcement\nlearning framework with fully asynchronous training for real-world alignment.\nWe also introduce Trajectory-aware Relative Policy Optimization (TRPO) for\nonline RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are\nopen-sourced at https://github.com/X-PLUG/MobileAgent.\n15 authors\n·\nPublished on Aug 21, 2025\nUpvote\n63\nGitHub\n5.93k\narXiv Page\nSubmitted by\nxhyandwyy\nMobile-Agent-v3: Foundamental Agents for GUI Automation\nThis paper introduces GUI-Owl, a foundational GUI agent model that achieves\nstate-of-the-art performance among open-source end-to-end models on ten GUI\nbenchmarks across desktop and mobile environments, covering grounding, question\nanswering, planning, decision-making, and procedural knowledge. GUI-Owl-7B\nachieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose\nMobile-Agent-v3, a general-purpose GUI agent framework that further improves\nperformance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new\nstate-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates\nthree key innovations: (1) Large-scale Environment Infrastructure: a\ncloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows,\nenabling our Self-Evolving GUI Trajectory Production framework. This generates\nhigh-quality interaction data via automated query generation and correctness\nvalidation, leveraging GUI-Owl to refine trajectories iteratively, forming a\nself-improving loop. It supports diverse data pipelines and reduces manual\nannotation. (2) Diverse Foundational Agent Capabilities: by integrating UI\ngrounding, planning, action semantics, and reasoning patterns, GUI-Owl supports\nend-to-end decision-making and can act as a modular component in multi-agent\nsystems. (3) Scalable Environment RL: we develop a scalable reinforcement\nlearning framework with fully asynchronous training for real-world alignment.\nWe also introduce Trajectory-aware Relative Policy Optimization (TRPO) for\nonline RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are\nopen-sourced at https://github.com/X-PLUG/MobileAgent.\n15 authors\n·\nAug 21, 2025\nUpvote\n63\nGitHub\n5.93k\narXiv Page\nSubmitted by\nxhyandwyy\nLook Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error\n  Diagnosis in GUI Automation\nIn recent years, Multimodal Large Language Models (MLLMs) have been\nextensively utilized for multimodal reasoning tasks, including Graphical User\nInterface (GUI) automation. Unlike general offline multimodal tasks, GUI\nautomation is executed in online interactive environments, necessitating\nstep-by-step decision-making based on real-time status of the environment. This\ntask has a lower tolerance for decision-making errors at each step, as any\nmistakes may cumulatively disrupt the process and potentially lead to\nirreversible outcomes like deletions or payments. To address these issues, we\nintroduce a pre-operative critic mechanism that provides effective feedback\nprior to the actual execution, by reasoning about the potential outcome and\ncorrectness of actions. Specifically, we propose a Suggestion-aware Gradient\nRelative Policy Optimization (S-GRPO) strategy to construct our pre-operative\ncritic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance\nthe reliability of the model's feedback. Furthermore, we develop a\nreasoning-bootstrapping based data collection pipeline to create a\nGUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic\ndata. Static experiments on the GUI-Critic-Test across both mobile and web\ndomains reveal that our GUI-Critic-R1 offers significant advantages in critic\naccuracy compared to current MLLMs. Dynamic evaluation on GUI automation\nbenchmark further highlights the effectiveness and superiority of our model, as\nevidenced by improved success rates and operational efficiency.\n12 authors\n·\nPublished on Jun 5, 2025\nUpvote\n18\nGitHub\n5.93k\narXiv Page\nSubmitted by\nxhyandwyy\nLook Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error\n  Diagnosis in GUI Automation\nIn recent years, Multimodal Large Language Models (MLLMs) have been\nextensively utilized for multimodal reasoning tasks, including Graphical User\nInterface (GUI) automation. Unlike general offline multimodal tasks, GUI\nautomation is executed in online interactive environments, necessitating\nstep-by-step decision-making based on real-time status of the environment. This\ntask has a lower tolerance for decision-making errors at each step, as any\nmistakes may cumulatively disrupt the process and potentially lead to\nirreversible outcomes like deletions or payments. To address these issues, we\nintroduce a pre-operative critic mechanism that provides effective feedback\nprior to the actual execution, by reasoning about the potential outcome and\ncorrectness of actions. Specifically, we propose a Suggestion-aware Gradient\nRelative Policy Optimization (S-GRPO) strategy to construct our pre-operative\ncritic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance\nthe reliability of the model's feedback. Furthermore, we develop a\nreasoning-bootstrapping based data collection pipeline to create a\nGUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic\ndata. Static experiments on the GUI-Critic-Test across both mobile and web\ndomains reveal that our GUI-Critic-R1 offers significant advantages in critic\naccuracy compared to current MLLMs. Dynamic evaluation on GUI automation\nbenchmark further highlights the effectiveness and superiority of our model, as\nevidenced by improved success rates and operational efficiency.\n12 authors\n·\nJun 5, 2025\nUpvote\n18\nGitHub\n5.93k\narXiv Page\nSubmitted by\nxhyandwyy\nPC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex\n  Task Automation on PC\nIn the field of MLLM-based GUI agents, compared to smartphones, the PC\nscenario not only features a more complex interactive environment, but also\ninvolves more intricate intra- and inter-app workflows. To address these\nissues, we propose a hierarchical agent framework named PC-Agent. Specifically,\nfrom the perception perspective, we devise an Active Perception Module (APM) to\novercome the inadequate abilities of current MLLMs in perceiving screenshot\ncontent. From the decision-making perspective, to handle complex user\ninstructions and interdependent subtasks more effectively, we propose a\nhierarchical multi-agent collaboration architecture that decomposes\ndecision-making processes into Instruction-Subtask-Action levels. Within this\narchitecture, three agents (i.e., Manager, Progress and Decision) are set up\nfor instruction decomposition, progress tracking and step-by-step\ndecision-making respectively. Additionally, a Reflection agent is adopted to\nenable timely bottom-up error feedback and adjustment. We also introduce a new\nbenchmark PC-Eval with 25 real-world complex instructions. Empirical results on\nPC-Eval show that our PC-Agent achieves a 32% absolute improvement of task\nsuccess rate over previous state-of-the-art methods. The code will be publicly\navailable.\n11 authors\n·\nPublished on Feb 20, 2025\nUpvote\n29\nGitHub\n5.93k\narXiv Page\nSubmitted by\nxhyandwyy\nPC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex\n  Task Automation on PC\nIn the field of MLLM-based GUI agents, compared to smartphones, the PC\nscenario not only features a more complex interactive environment, but also\ninvolves more intricate intra- and inter-app workflows. To address these\nissues, we propose a hierarchical agent framework named PC-Agent. Specifically,\nfrom the perception perspective, we devise an Active Perception Module (APM) to\novercome the inadequate abilities of current MLLMs in perceiving screenshot\ncontent. From the decision-making perspective, to handle complex user\ninstructions and interdependent subtasks more effectively, we propose a\nhierarchical multi-agent collaboration architecture that decomposes\ndecision-making processes into Instruction-Subtask-Action levels. Within this\narchitecture, three agents (i.e., Manager, Progress and Decision) are set up\nfor instruction decomposition, progress tracking and step-by-step\ndecision-making respectively. Additionally, a Reflection agent is adopted to\nenable timely bottom-up error feedback and adjustment. We also introduce a new\nbenchmark PC-Eval with 25 real-world complex instructions. Empirical results on\nPC-Eval show that our PC-Agent achieves a 32% absolute improvement of task\nsuccess rate over previous state-of-the-art methods. The code will be publicly\navailable.\n11 authors\n·\nFeb 20, 2025\nUpvote\n29\nGitHub\n5.93k\narXiv Page\nSubmitted by\nxhyandwyy\nUI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning\nGraphical User Interface (GUI) agents have demonstrated remarkable progress\nin automating complex user interface interactions through reinforcement\nlearning. However, current approaches face a fundamental dilemma: offline RL\nenables stable training on pre-collected trajectories, but struggles with\nmulti-step task execution for lack of trajectory-level reward signals; online\nRL captures these signals through environment interaction, but suffers from\nsparse rewards and prohibitive deployment costs. To address it, we present\nSemi-online Reinforcement Learning, a novel paradigm that simulates online RL\non offline trajectories. During each rollout process, we preserve the original\nmodel output within the multi-turn dialogue, where a Patch Module adaptively\nrecovers the divergence between rollout and expert trajectories. To capture\nlong-term training signals, Semi-online RL introduces discounted future returns\ninto the reward computation and optimizes the policy with weighted step-level\nand episode-level advantages. We further introduce Semi-Online Performance\n(SOP), a metric that aligns better with true online performance, serving as a\npractical and effective proxy for real-world evaluation. Experiments show that\nours Semi-online RL achieves SOTA performance among 7B models across four\ndynamic benchmarks, with significant gains over the base model (e.g., +12.0% on\nAndroidWorld, +23.8% on AITW), demonstrating significant progress in bridging\nthe gap between offline training efficiency and online multi-turn reasoning.\nThe code is available at https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1.\n11 authors\n·\nPublished on Sep 15, 2025\nUpvote\n46\nGitHub\n5.93k\narXiv Page\nSubmitted by\nxhyandwyy\nUI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning\nGraphical User Interface (GUI) agents have demonstrated remarkable progress\nin automating complex user interface interactions through reinforcement\nlearning. However, current approaches face a fundamental dilemma: offline RL\nenables stable training on pre-collected trajectories, but struggles with\nmulti-step task execution for lack of trajectory-level reward signals; online\nRL captures these signals through environment interaction, but suffers from\nsparse rewards and prohibitive deployment costs. To address it, we present\nSemi-online Reinforcement Learning, a novel paradigm that simulates online RL\non offline trajectories. During each rollout process, we preserve the original\nmodel output within the multi-turn dialogue, where a Patch Module adaptively\nrecovers the divergence between rollout and expert trajectories. To capture\nlong-term training signals, Semi-online RL introduces discounted future returns\ninto the reward computation and optimizes the policy with weighted step-level\nand episode-level advantages. We further introduce Semi-Online Performance\n(SOP), a metric that aligns better with true online performance, serving as a\npractical and effective proxy for real-world evaluation. Experiments show that\nours Semi-online RL achieves SOTA performance among 7B models across four\ndynamic benchmarks, with significant gains over the base model (e.g., +12.0% on\nAndroidWorld, +23.8% on AITW), demonstrating significant progress in bridging\nthe gap between offline training efficiency and online multi-turn reasoning.\nThe code is available at https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1.\n11 authors\n·\nSep 15, 2025\nUpvote\n46\nGitHub\n5.93k\narXiv Page\nSubmitted by\nchujiezheng\nQwen3 Technical Report\nIn this work, we present Qwen3, the latest version of the Qwen model family.\nQwen3 comprises a series of large language models (LLMs) designed to advance\nperformance, efficiency, and multilingual capabilities. The Qwen3 series\nincludes models of both dense and Mixture-of-Expert (MoE) architectures, with\nparameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is\nthe integration of thinking mode (for complex, multi-step reasoning) and\nnon-thinking mode (for rapid, context-driven responses) into a unified\nframework. This eliminates the need to switch between different models--such as\nchat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,\nQwQ-32B)--and enables dynamic mode switching based on user queries or chat\ntemplates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing\nusers to allocate computational resources adaptively during inference, thereby\nbalancing latency and performance based on task complexity. Moreover, by\nleveraging the knowledge from the flagship models, we significantly reduce the\ncomputational resources required to build smaller-scale models, while ensuring\ntheir highly competitive performance. Empirical evaluations demonstrate that\nQwen3 achieves state-of-the-art results across diverse benchmarks, including\ntasks in code generation, mathematical reasoning, agent tasks, etc.,\ncompetitive against larger MoE models and proprietary models. Compared to its\npredecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119\nlanguages and dialects, enhancing global accessibility through improved\ncross-lingual understanding and generation capabilities. To facilitate\nreproducibility and community-driven research and development, all Qwen3 models\nare publicly accessible under Apache 2.0.\n60 authors\n·\nPublished on May 14, 2025\nUpvote\n294\nGitHub\n24.8k\narXiv Page\nSubmitted by\nchujiezheng\nQwen3 Technical Report\nIn this work, we present Qwen3, the latest version of the Qwen model family.\nQwen3 comprises a series of large language models (LLMs) designed to advance\nperformance, efficiency, and multilingual capabilities. The Qwen3 series\nincludes models of both dense and Mixture-of-Expert (MoE) architectures, with\nparameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is\nthe integration of thinking mode (for complex, multi-step reasoning) and\nnon-thinking mode (for rapid, context-driven responses) into a unified\nframework. This eliminates the need to switch between different models--such as\nchat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,\nQwQ-32B)--and enables dynamic mode switching based on user queries or chat\ntemplates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing\nusers to allocate computational resources adaptively during inference, thereby\nbalancing latency and performance based on task complexity. Moreover, by\nleveraging the knowledge from the flagship models, we significantly reduce the\ncomputational resources required to build smaller-scale models, while ensuring\ntheir highly competitive performance. Empirical evaluations demonstrate that\nQwen3 achieves state-of-the-art results across diverse benchmarks, including\ntasks in code generation, mathematical reasoning, agent tasks, etc.,\ncompetitive against larger MoE models and proprietary models. Compared to its\npredecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119\nlanguages and dialects, enhancing global accessibility through improved\ncross-lingual understanding and generation capabilities. To facilitate\nreproducibility and community-driven research and development, all Qwen3 models\nare publicly accessible under Apache 2.0.\n60 authors\n·\nMay 14, 2025\nUpvote\n294\nGitHub\n24.8k\narXiv Page\nSubmitted by\ntaesiri\nSciReasoner: Laying the Scientific Reasoning Ground Across Disciplines\nWe present a scientific reasoning foundation model that aligns natural\nlanguage with heterogeneous scientific representations. The model is pretrained\non a 206B-token corpus spanning scientific text, pure sequences, and\nsequence-text pairs, then aligned via SFT on 40M instructions, annealed\ncold-start bootstrapping to elicit long-form chain-of-thought, and\nreinforcement learning with task-specific reward shaping, which instills\ndeliberate scientific reasoning. It supports four capability families, covering\nup to 103 tasks across workflows: (i) faithful translation between text and\nscientific formats, (ii) text/knowledge extraction, (iii) property prediction,\n(iv) property classification, (v) unconditional and conditional sequence\ngeneration and design. Compared with specialist systems, our approach broadens\ninstruction coverage, improves cross-domain generalization, and enhances\nfidelity. We detail data curation and training and show that cross-discipline\nlearning strengthens transfer and downstream reliability. The model, instruct\ntuning datasets and the evaluation code are open-sourced at\nhttps://huggingface.co/SciReason and\nhttps://github.com/open-sciencelab/SciReason.\n32 authors\n·\nPublished on Sep 25, 2025\nUpvote\n86\nGitHub\n43\narXiv Page\nSubmitted by\ntaesiri\nSciReasoner: Laying the Scientific Reasoning Ground Across Disciplines\nWe present a scientific reasoning foundation model that aligns natural\nlanguage with heterogeneous scientific representations. The model is pretrained\non a 206B-token corpus spanning scientific text, pure sequences, and\nsequence-text pairs, then aligned via SFT on 40M instructions, annealed\ncold-start bootstrapping to elicit long-form chain-of-thought, and\nreinforcement learning with task-specific reward shaping, which instills\ndeliberate scientific reasoning. It supports four capability families, covering\nup to 103 tasks across workflows: (i) faithful translation between text and\nscientific formats, (ii) text/knowledge extraction, (iii) property prediction,\n(iv) property classification, (v) unconditional and conditional sequence\ngeneration and design. Compared with specialist systems, our approach broadens\ninstruction coverage, improves cross-domain generalization, and enhances\nfidelity. We detail data curation and training and show that cross-discipline\nlearning strengthens transfer and downstream reliability. The model, instruct\ntuning datasets and the evaluation code are open-sourced at\nhttps://huggingface.co/SciReason and\nhttps://github.com/open-sciencelab/SciReason.\n32 authors\n·\nSep 25, 2025\nUpvote\n86\nGitHub\n43\narXiv Page\nSubmitted by\nYangXiao-nlp\nLIMI: Less is More for Agency\nWe define Agency as the emergent capacity of AI systems to function as\nautonomous agents actively discovering problems, formulating hypotheses, and\nexecuting solutions through self-directed engagement with environments and\ntools. This fundamental capability marks the dawn of the Age of AI Agency,\ndriven by a critical industry shift: the urgent need for AI systems that don't\njust think, but work. While current AI excels at reasoning and generating\nresponses, industries demand autonomous agents that can execute tasks, operate\ntools, and drive real-world outcomes. As agentic intelligence becomes the\ndefining characteristic separating cognitive systems from productive workers,\nefficiently cultivating machine autonomy becomes paramount. Current approaches\nassume that more data yields better agency, following traditional scaling laws\nfrom language modeling. We fundamentally challenge this paradigm. LIMI (Less Is\nMore for Intelligent Agency) demonstrates that agency follows radically\ndifferent development principles. Through strategic focus on collaborative\nsoftware development and scientific research workflows, we show that\nsophisticated agentic intelligence can emerge from minimal but strategically\ncurated demonstrations of autonomous behavior. Using only 78 carefully designed\ntraining samples, LIMI achieves 73.5% on comprehensive agency benchmarks,\ndramatically outperforming state-of-the-art models: Kimi-K2-Instruct (24.1%),\nDeepSeek-V3.1 (11.9%), Qwen3-235B-A22B-Instruct (27.5%), and GLM-4.5 (45.1%).\nMost strikingly, LIMI demonstrates 53.7% improvement over models trained on\n10,000 samples-achieving superior agentic intelligence with 128 times fewer\nsamples. Our findings establish the Agency Efficiency Principle: machine\nautonomy emerges not from data abundance but from strategic curation of\nhigh-quality agentic demonstrations.\n21 authors\n·\nPublished on Sep 22, 2025\nUpvote\n91\nGitHub\n100\narXiv Page\nSubmitted by\nYangXiao-nlp\nLIMI: Less is More for Agency\nWe define Agency as the emergent capacity of AI systems to function as\nautonomous agents actively discovering problems, formulating hypotheses, and\nexecuting solutions through self-directed engagement with environments and\ntools. This fundamental capability marks the dawn of the Age of AI Agency,\ndriven by a critical industry shift: the urgent need for AI systems that don't\njust think, but work. While current AI excels at reasoning and generating\nresponses, industries demand autonomous agents that can execute tasks, operate\ntools, and drive real-world outcomes. As agentic intelligence becomes the\ndefining characteristic separating cognitive systems from productive workers,\nefficiently cultivating machine autonomy becomes paramount. Current approaches\nassume that more data yields better agency, following traditional scaling laws\nfrom language modeling. We fundamentally challenge this paradigm. LIMI (Less Is\nMore for Intelligent Agency) demonstrates that agency follows radically\ndifferent development principles. Through strategic focus on collaborative\nsoftware development and scientific research workflows, we show that\nsophisticated agentic intelligence can emerge from minimal but strategically\ncurated demonstrations of autonomous behavior. Using only 78 carefully designed\ntraining samples, LIMI achieves 73.5% on comprehensive agency benchmarks,\ndramatically outperforming state-of-the-art models: Kimi-K2-Instruct (24.1%),\nDeepSeek-V3.1 (11.9%), Qwen3-235B-A22B-Instruct (27.5%), and GLM-4.5 (45.1%).\nMost strikingly, LIMI demonstrates 53.7% improvement over models trained on\n10,000 samples-achieving superior agentic intelligence with 128 times fewer\nsamples. Our findings establish the Agency Efficiency Principle: machine\nautonomy emerges not from data abundance but from strategic curation of\nhigh-quality agentic demonstrations.\n21 authors\n·\nSep 22, 2025\nUpvote\n91\nGitHub\n100\narXiv Page\nSubmitted by\n\t\t\t\tdeleted\nDINO-Foresight: Looking into the Future with DINO\nPredicting future dynamics is crucial for applications like autonomous\ndriving and robotics, where understanding the environment is key. Existing\npixel-level methods are computationally expensive and often focus on irrelevant\ndetails. To address these challenges, we introduce DINO-Foresight, a novel\nframework that operates in the semantic feature space of pretrained Vision\nFoundation Models (VFMs). Our approach trains a masked feature transformer in a\nself-supervised manner to predict the evolution of VFM features over time. By\nforecasting these features, we can apply off-the-shelf, task-specific heads for\nvarious scene understanding tasks. In this framework, VFM features are treated\nas a latent space, to which different heads attach to perform specific tasks\nfor future-frame analysis. Extensive experiments show that our framework\noutperforms existing methods, demonstrating its robustness and scalability.\nAdditionally, we highlight how intermediate transformer representations in\nDINO-Foresight improve downstream task performance, offering a promising path\nfor the self-supervised enhancement of VFM features. We provide the\nimplementation code at https://github.com/Sta8is/DINO-Foresight .\n4 authors\n·\nPublished on Dec 16, 2024\nUpvote\n1\nGitHub\n107\narXiv Page\nSubmitted by\n\t\t\t\tdeleted\nDINO-Foresight: Looking into the Future with DINO\nPredicting future dynamics is crucial for applications like autonomous\ndriving and robotics, where understanding the environment is key. Existing\npixel-level methods are computationally expensive and often focus on irrelevant\ndetails. To address these challenges, we introduce DINO-Foresight, a novel\nframework that operates in the semantic feature space of pretrained Vision\nFoundation Models (VFMs). Our approach trains a masked feature transformer in a\nself-supervised manner to predict the evolution of VFM features over time. By\nforecasting these features, we can apply off-the-shelf, task-specific heads for\nvarious scene understanding tasks. In this framework, VFM features are treated\nas a latent space, to which different heads attach to perform specific tasks\nfor future-frame analysis. Extensive experiments show that our framework\noutperforms existing methods, demonstrating its robustness and scalability.\nAdditionally, we highlight how intermediate transformer representations in\nDINO-Foresight improve downstream task performance, offering a promising path\nfor the self-supervised enhancement of VFM features. We provide the\nimplementation code at https://github.com/Sta8is/DINO-Foresight .\n4 authors\n·\nDec 16, 2024\nUpvote\n1\nGitHub\n107\narXiv Page\nSubmitted by\nakhaliq\nUI-TARS: Pioneering Automated GUI Interaction with Native Agents\nThis paper introduces UI-TARS, a native GUI agent model that solely perceives\nthe screenshots as input and performs human-like interactions (e.g., keyboard\nand mouse operations). Unlike prevailing agent frameworks that depend on\nheavily wrapped commercial models (e.g., GPT-4o) with expert-crafted prompts\nand workflows, UI-TARS is an end-to-end model that outperforms these\nsophisticated frameworks. Experiments demonstrate its superior performance:\nUI-TARS achieves SOTA performance in 10+ GUI agent benchmarks evaluating\nperception, grounding, and GUI task execution. Notably, in the OSWorld\nbenchmark, UI-TARS achieves scores of 24.6 with 50 steps and 22.7 with 15\nsteps, outperforming Claude (22.0 and 14.9 respectively). In AndroidWorld,\nUI-TARS achieves 46.6, surpassing GPT-4o (34.5). UI-TARS incorporates several\nkey innovations: (1) Enhanced Perception: leveraging a large-scale dataset of\nGUI screenshots for context-aware understanding of UI elements and precise\ncaptioning; (2) Unified Action Modeling, which standardizes actions into a\nunified space across platforms and achieves precise grounding and interaction\nthrough large-scale action traces; (3) System-2 Reasoning, which incorporates\ndeliberate reasoning into multi-step decision making, involving multiple\nreasoning patterns such as task decomposition, reflection thinking, milestone\nrecognition, etc. (4) Iterative Training with Reflective Online Traces, which\naddresses the data bottleneck by automatically collecting, filtering, and\nreflectively refining new interaction traces on hundreds of virtual machines.\nThrough iterative training and reflection tuning, UI-TARS continuously learns\nfrom its mistakes and adapts to unforeseen situations with minimal human\nintervention. We also analyze the evolution path of GUI agents to guide the\nfurther development of this domain.\n35 authors\n·\nPublished on Jan 21, 2025\nUpvote\n65\nGitHub\n7.82k\narXiv Page\nSubmitted by\nakhaliq\nUI-TARS: Pioneering Automated GUI Interaction with Native Agents\nThis paper introduces UI-TARS, a native GUI agent model that solely perceives\nthe screenshots as input and performs human-like interactions (e.g., keyboard\nand mouse operations). Unlike prevailing agent frameworks that depend on\nheavily wrapped commercial models (e.g., GPT-4o) with expert-crafted prompts\nand workflows, UI-TARS is an end-to-end model that outperforms these\nsophisticated frameworks. Experiments demonstrate its superior performance:\nUI-TARS achieves SOTA performance in 10+ GUI agent benchmarks evaluating\nperception, grounding, and GUI task execution. Notably, in the OSWorld\nbenchmark, UI-TARS achieves scores of 24.6 with 50 steps and 22.7 with 15\nsteps, outperforming Claude (22.0 and 14.9 respectively). In AndroidWorld,\nUI-TARS achieves 46.6, surpassing GPT-4o (34.5). UI-TARS incorporates several\nkey innovations: (1) Enhanced Perception: leveraging a large-scale dataset of\nGUI screenshots for context-aware understanding of UI elements and precise\ncaptioning; (2) Unified Action Modeling, which standardizes actions into a\nunified space across platforms and achieves precise grounding and interaction\nthrough large-scale action traces; (3) System-2 Reasoning, which incorporates\ndeliberate reasoning into multi-step decision making, involving multiple\nreasoning patterns such as task decomposition, reflection thinking, milestone\nrecognition, etc. (4) Iterative Training with Reflective Online Traces, which\naddresses the data bottleneck by automatically collecting, filtering, and\nreflectively refining new interaction traces on hundreds of virtual machines.\nThrough iterative training and reflection tuning, UI-TARS continuously learns\nfrom its mistakes and adapts to unforeseen situations with minimal human\nintervention. We also analyze the evolution path of GUI agents to guide the\nfurther development of this domain.\n35 authors\n·\nJan 21, 2025\nUpvote\n65\nGitHub\n7.82k\narXiv Page\nSubmitted by\ntaesiri\nLogics-Parsing Technical Report\nRecent advances in Large Vision-Language models (LVLM) have spurred\nsignificant progress in document parsing task. Compared to traditional\npipeline-based methods, end-to-end paradigms have shown their excellence in\nconverting PDF images into structured outputs through integrated Optical\nCharacter Recognition (OCR), table recognition, mathematical formula\nrecognition and so on. However, the absence of explicit analytical stages for\ndocument layouts and reading orders limits the LVLM's capability in handling\ncomplex document types such as multi-column newspapers or posters. To address\nthis limitation, we propose in this report Logics-Parsing: an end-to-end\nLVLM-based model augmented with reinforcement learning. Our model incorporates\nmeticulously designed reward mechanisms to optimize complex layout analysis and\nreading order inference. In addition, we expand the model's versatility by\nincorporating diverse data types such as chemical formulas and handwritten\nChinese characters into supervised fine-tuning. Finally, to enable rigorous\nevaluation of our approach, we introduce LogicsParsingBench, a curated set of\n1,078 page-level PDF images spanning nine major categories and over twenty\nsub-categories, which will be released later. Comprehensive experiments\nconducted on LogicsParsingBench have validated the efficacy and\nState-of-the-art (SOTA) performance of our proposed model across diverse\ndocument analysis scenarios. Project Page:\nhttps://github.com/alibaba/Logics-Parsing\n10 authors\n·\nPublished on Sep 24, 2025\nUpvote\n4\nGitHub\n57\narXiv Page\nSubmitted by\ntaesiri\nLogics-Parsing Technical Report\nRecent advances in Large Vision-Language models (LVLM) have spurred\nsignificant progress in document parsing task. Compared to traditional\npipeline-based methods, end-to-end paradigms have shown their excellence in\nconverting PDF images into structured outputs through integrated Optical\nCharacter Recognition (OCR), table recognition, mathematical formula\nrecognition and so on. However, the absence of explicit analytical stages for\ndocument layouts and reading orders limits the LVLM's capability in handling\ncomplex document types such as multi-column newspapers or posters. To address\nthis limitation, we propose in this report Logics-Parsing: an end-to-end\nLVLM-based model augmented with reinforcement learning. Our model incorporates\nmeticulously designed reward mechanisms to optimize complex layout analysis and\nreading order inference. In addition, we expand the model's versatility by\nincorporating diverse data types such as chemical formulas and handwritten\nChinese characters into supervised fine-tuning. Finally, to enable rigorous\nevaluation of our approach, we introduce LogicsParsingBench, a curated set of\n1,078 page-level PDF images spanning nine major categories and over twenty\nsub-categories, which will be released later. Comprehensive experiments\nconducted on LogicsParsingBench have validated the efficacy and\nState-of-the-art (SOTA) performance of our proposed model across diverse\ndocument analysis scenarios. Project Page:\nhttps://github.com/alibaba/Logics-Parsing\n10 authors\n·\nSep 24, 2025\nUpvote\n4\nGitHub\n57\narXiv Page\nSubmitted by\nJianyuanWang\nVGGT: Visual Geometry Grounded Transformer\nWe present VGGT, a feed-forward neural network that directly infers all key\n3D attributes of a scene, including camera parameters, point maps, depth maps,\nand 3D point tracks, from one, a few, or hundreds of its views. This approach\nis a step forward in 3D computer vision, where models have typically been\nconstrained to and specialized for single tasks. It is also simple and\nefficient, reconstructing images in under one second, and still outperforming\nalternatives that require post-processing with visual geometry optimization\ntechniques. The network achieves state-of-the-art results in multiple 3D tasks,\nincluding camera parameter estimation, multi-view depth estimation, dense point\ncloud reconstruction, and 3D point tracking. We also show that using pretrained\nVGGT as a feature backbone significantly enhances downstream tasks, such as\nnon-rigid point tracking and feed-forward novel view synthesis. Code and models\nare publicly available at https://github.com/facebookresearch/vggt.\n6 authors\n·\nPublished on Mar 14, 2025\nUpvote\n31\nGitHub\n11.1k\narXiv Page\nSubmitted by\nJianyuanWang\nVGGT: Visual Geometry Grounded Transformer\nWe present VGGT, a feed-forward neural network that directly infers all key\n3D attributes of a scene, including camera parameters, point maps, depth maps,\nand 3D point tracks, from one, a few, or hundreds of its views. This approach\nis a step forward in 3D computer vision, where models have typically been\nconstrained to and specialized for single tasks. It is also simple and\nefficient, reconstructing images in under one second, and still outperforming\nalternatives that require post-processing with visual geometry optimization\ntechniques. The network achieves state-of-the-art results in multiple 3D tasks,\nincluding camera parameter estimation, multi-view depth estimation, dense point\ncloud reconstruction, and 3D point tracking. We also show that using pretrained\nVGGT as a feature backbone significantly enhances downstream tasks, such as\nnon-rigid point tracking and feed-forward novel view synthesis. Code and models\nare publicly available at https://github.com/facebookresearch/vggt.\n6 authors\n·\nMar 14, 2025\nUpvote\n31\nGitHub\n11.1k\narXiv Page\nSubmitted by\n\t\t\t\tdeleted\nKilling Two Birds with One Stone:Efficient and Robust Training of Face\n  Recognition CNNs by Partial FC\nLearning discriminative deep feature embeddings by using million-scale\nin-the-wild datasets and margin-based softmax loss is the current\nstate-of-the-art approach for face recognition. However, the memory and\ncomputing cost of the Fully Connected (FC) layer linearly scales up to the\nnumber of identities in the training set. Besides, the large-scale training\ndata inevitably suffers from inter-class conflict and long-tailed distribution.\nIn this paper, we propose a sparsely updating variant of the FC layer, named\nPartial FC (PFC). In each iteration, positive class centers and a random subset\nof negative class centers are selected to compute the margin-based softmax\nloss. All class centers are still maintained throughout the whole training\nprocess, but only a subset is selected and updated in each iteration.\nTherefore, the computing requirement, the probability of inter-class conflict,\nand the frequency of passive update on tail class centers, are dramatically\nreduced. Extensive experiments across different training data and backbones\n(e.g. CNN and ViT) confirm the effectiveness, robustness and efficiency of the\nproposed PFC. The source code is available at\n\\https://github.com/deepinsight/insightface/tree/master/recognition.\n7 authors\n·\nPublished on Mar 28, 2022\nUpvote\n3\nGitHub\n26.6k\narXiv Page\nSubmitted by\n\t\t\t\tdeleted\nKilling Two Birds with One Stone:Efficient and Robust Training of Face\n  Recognition CNNs by Partial FC\nLearning discriminative deep feature embeddings by using million-scale\nin-the-wild datasets and margin-based softmax loss is the current\nstate-of-the-art approach for face recognition. However, the memory and\ncomputing cost of the Fully Connected (FC) layer linearly scales up to the\nnumber of identities in the training set. Besides, the large-scale training\ndata inevitably suffers from inter-class conflict and long-tailed distribution.\nIn this paper, we propose a sparsely updating variant of the FC layer, named\nPartial FC (PFC). In each iteration, positive class centers and a random subset\nof negative class centers are selected to compute the margin-based softmax\nloss. All class centers are still maintained throughout the whole training\nprocess, but only a subset is selected and updated in each iteration.\nTherefore, the computing requirement, the probability of inter-class conflict,\nand the frequency of passive update on tail class centers, are dramatically\nreduced. Extensive experiments across different training data and backbones\n(e.g. CNN and ViT) confirm the effectiveness, robustness and efficiency of the\nproposed PFC. The source code is available at\n\\https://github.com/deepinsight/insightface/tree/master/recognition.\n7 authors\n·\nMar 28, 2022\nUpvote\n3\nGitHub\n26.6k\narXiv Page\nSubmitted by\ntaesiri\nMiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and\n  Training Recipe\nMultimodal Large Language Models (MLLMs) are undergoing rapid progress and\nrepresent the frontier of AI development. However, their training and inference\nefficiency have emerged as a core bottleneck in making MLLMs more accessible\nand scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B\nparameter model designed for high efficiency and strong performance. We\nintroduce three core improvements in model architecture, data strategy and\ntraining method: a unified 3D-Resampler model architecture for highly compact\nencoding over images and videos, a unified learning paradigm for document\nknowledge and text recognition without heavy data engineering, and a hybrid\nreinforcement learning strategy for proficiency in both short and long\nreasoning modes. Comprehensive experimental results in OpenCompass evaluation\nshow that MiniCPM-V 4.5 surpasses widely used proprietary models such as\nGPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL\n72B. Notably, the strong performance is achieved with remarkable efficiency.\nFor example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves\nstate-of-the-art performance among models under 30B size, using just 46.7\\% GPU\nmemory cost and 8.7\\% inference time of Qwen2.5-VL 7B.\n34 authors\n·\nPublished on Sep 16, 2025\nUpvote\n45\nGitHub\n22k\narXiv Page\nSubmitted by\ntaesiri\nMiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and\n  Training Recipe\nMultimodal Large Language Models (MLLMs) are undergoing rapid progress and\nrepresent the frontier of AI development. However, their training and inference\nefficiency have emerged as a core bottleneck in making MLLMs more accessible\nand scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B\nparameter model designed for high efficiency and strong performance. We\nintroduce three core improvements in model architecture, data strategy and\ntraining method: a unified 3D-Resampler model architecture for highly compact\nencoding over images and videos, a unified learning paradigm for document\nknowledge and text recognition without heavy data engineering, and a hybrid\nreinforcement learning strategy for proficiency in both short and long\nreasoning modes. Comprehensive experimental results in OpenCompass evaluation\nshow that MiniCPM-V 4.5 surpasses widely used proprietary models such as\nGPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL\n72B. Notably, the strong performance is achieved with remarkable efficiency.\nFor example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves\nstate-of-the-art performance among models under 30B size, using just 46.7\\% GPU\nmemory cost and 8.7\\% inference time of Qwen2.5-VL 7B.\n34 authors\n·\nSep 16, 2025\nUpvote\n45\nGitHub\n22k\narXiv Page\nSubmitted by\nwujie10\nDanceGRPO: Unleashing GRPO on Visual Generation\nRecent breakthroughs in generative models-particularly diffusion models and\nrectified flows-have revolutionized visual content creation, yet aligning model\noutputs with human preferences remains a critical challenge. Existing\nreinforcement learning (RL)-based methods for visual generation face critical\nlimitations: incompatibility with modern Ordinary Differential Equations\n(ODEs)-based sampling paradigms, instability in large-scale training, and lack\nof validation for video generation. This paper introduces DanceGRPO, the first\nunified framework to adapt Group Relative Policy Optimization (GRPO) to visual\ngeneration paradigms, unleashing one unified RL algorithm across two generative\nparadigms (diffusion models and rectified flows), three tasks (text-to-image,\ntext-to-video, image-to-video), four foundation models (Stable Diffusion,\nHunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video\naesthetics, text-image alignment, video motion quality, and binary reward). To\nour knowledge, DanceGRPO is the first RL-based unified framework capable of\nseamless adaptation across diverse generative paradigms, tasks, foundational\nmodels, and reward models. DanceGRPO demonstrates consistent and substantial\nimprovements, which outperform baselines by up to 181% on benchmarks such as\nHPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can\nstabilize policy optimization for complex video generation, but also enables\ngenerative policy to better capture denoising trajectories for Best-of-N\ninference scaling and learn from sparse binary feedback. Our results establish\nDanceGRPO as a robust and versatile solution for scaling Reinforcement Learning\nfrom Human Feedback (RLHF) tasks in visual generation, offering new insights\ninto harmonizing reinforcement learning and visual synthesis. The code will be\nreleased.\n11 authors\n·\nPublished on May 12, 2025\nUpvote\n32\nGitHub\n889\narXiv Page\nSubmitted by\nwujie10\nDanceGRPO: Unleashing GRPO on Visual Generation\nRecent breakthroughs in generative models-particularly diffusion models and\nrectified flows-have revolutionized visual content creation, yet aligning model\noutputs with human preferences remains a critical challenge. Existing\nreinforcement learning (RL)-based methods for visual generation face critical\nlimitations: incompatibility with modern Ordinary Differential Equations\n(ODEs)-based sampling paradigms, instability in large-scale training, and lack\nof validation for video generation. This paper introduces DanceGRPO, the first\nunified framework to adapt Group Relative Policy Optimization (GRPO) to visual\ngeneration paradigms, unleashing one unified RL algorithm across two generative\nparadigms (diffusion models and rectified flows), three tasks (text-to-image,\ntext-to-video, image-to-video), four foundation models (Stable Diffusion,\nHunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video\naesthetics, text-image alignment, video motion quality, and binary reward). To\nour knowledge, DanceGRPO is the first RL-based unified framework capable of\nseamless adaptation across diverse generative paradigms, tasks, foundational\nmodels, and reward models. DanceGRPO demonstrates consistent and substantial\nimprovements, which outperform baselines by up to 181% on benchmarks such as\nHPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can\nstabilize policy optimization for complex video generation, but also enables\ngenerative policy to better capture denoising trajectories for Best-of-N\ninference scaling and learn from sparse binary feedback. Our results establish\nDanceGRPO as a robust and versatile solution for scaling Reinforcement Learning\nfrom Human Feedback (RLHF) tasks in visual generation, offering new insights\ninto harmonizing reinforcement learning and visual synthesis. The code will be\nreleased.\n11 authors\n·\nMay 12, 2025\nUpvote\n32\nGitHub\n889\narXiv Page",
    "raw_html": "<!DOCTYPE html>\n\n<html class=\"\">\n<head>\n<meta charset=\"utf-8\"/>\n<meta content=\"width=device-width, initial-scale=1.0, user-scalable=no\" name=\"viewport\"/>\n<meta content=\"Your daily dose of AI research from AK\" name=\"description\"/>\n<meta content=\"1321688464574422\" property=\"fb:app_id\"/>\n<meta content=\"summary_large_image\" name=\"twitter:card\"/>\n<meta content=\"@huggingface\" name=\"twitter:site\"/>\n<meta content=\"https://huggingface.co/front/thumbnails/trending-papers.png\" name=\"twitter:image\"/>\n<meta content=\"Trending Papers - Hugging Face\" property=\"og:title\"/>\n<meta content=\"website\" property=\"og:type\"/>\n<meta content=\"https://huggingface.co/papers/trending\" property=\"og:url\"/>\n<meta content=\"https://huggingface.co/front/thumbnails/trending-papers.png\" property=\"og:image\"/>\n<link href=\"/front/build/kube-9d96ca9/style.css\" rel=\"stylesheet\"/>\n<link href=\"https://fonts.gstatic.com\" rel=\"preconnect\"/>\n<link href=\"https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;1,200;1,300;1,400;1,600;1,700&amp;display=swap\" rel=\"stylesheet\"/>\n<link href=\"https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;600;700&amp;display=swap\" rel=\"stylesheet\"/>\n<link as=\"style\" href=\"https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css\" onload=\"this.onload=null;this.rel='stylesheet'\" rel=\"preload\"/>\n<noscript>\n<link href=\"https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css\" rel=\"stylesheet\"/>\n</noscript>\n<script>const guestTheme = document.cookie.match(/theme=(\\w+)/)?.[1]; document.documentElement.classList.toggle('dark', guestTheme === 'dark' || ( (!guestTheme || guestTheme === 'system') && window.matchMedia('(prefers-color-scheme: dark)').matches));</script>\n<title>Trending Papers - Hugging Face</title>\n<script data-domain=\"huggingface.co\" defer=\"\" event-loggedin=\"false\" src=\"/js/script.pageview-props.js\"></script>\n<script>\n\t\t\twindow.plausible =\n\t\t\t\twindow.plausible ||\n\t\t\t\tfunction () {\n\t\t\t\t\t(window.plausible.q = window.plausible.q || []).push(arguments);\n\t\t\t\t};\n\t\t</script>\n<script>\n\t\t\twindow.hubConfig = {\"features\":{\"signupDisabled\":false},\"sshGitUrl\":\"git@hf.co\",\"moonHttpUrl\":\"https:\\/\\/huggingface.co\",\"captchaApiKey\":\"bd5f2066-93dc-4bdd-a64b-a24646ca3859\",\"captchaDisabledOnSignup\":true,\"datasetViewerPublicUrl\":\"https:\\/\\/datasets-server.huggingface.co\",\"stripePublicKey\":\"pk_live_x2tdjFXBCvXo2FFmMybezpeM00J6gPCAAc\",\"environment\":\"production\",\"userAgent\":\"HuggingFace (production)\",\"spacesIframeDomain\":\"hf.space\",\"spacesApiUrl\":\"https:\\/\\/api.hf.space\",\"docSearchKey\":\"ece5e02e57300e17d152c08056145326e90c4bff3dd07d7d1ae40cf1c8d39cb6\",\"logoDev\":{\"apiUrl\":\"https:\\/\\/img.logo.dev\\/\",\"apiKey\":\"pk_UHS2HZOeRnaSOdDp7jbd5w\"}};\n\t\t</script>\n<script defer=\"\" src=\"https://de5282c3ca0c.edge.sdk.awswaf.com/de5282c3ca0c/526cf06acb0d/challenge.js\" type=\"text/javascript\"></script>\n</head>\n<body class=\"flex flex-col min-h-dvh bg-white dark:bg-gray-950 text-black DailyPapersPage\">\n<div class=\"flex min-h-dvh flex-col\"><div class=\"SVELTE_HYDRATER contents\" data-props='{\"isLoggedIn\":false}' data-target=\"SystemThemeMonitor\"></div>\n<div class=\"SVELTE_HYDRATER contents\" data-props='{\"classNames\":\"\",\"isWide\":false,\"isZh\":false,\"isPro\":false}' data-target=\"MainHeader\"><header class=\"border-b border-gray-100\"><div class=\"w-full px-4 container flex h-16 items-center\"><div class=\"flex flex-1 items-center\"><a class=\"mr-5 flex flex-none items-center lg:mr-6\" href=\"/\"><img alt=\"Hugging Face's logo\" class=\"w-7 md:mr-2\" src=\"/front/assets/huggingface_logo-noborder.svg\"/>\n<span class=\"hidden whitespace-nowrap text-lg font-bold md:block\">Hugging Face</span></a>\n<div class=\"relative flex-1 lg:max-w-sm mr-2 sm:mr-4 md:mr-3 xl:mr-6\"><input autocomplete=\"off\" class=\"w-full dark:bg-gray-950 pl-8 form-input-alt h-9 pr-3 focus:shadow-xl\" name=\"\" placeholder=\"Search models, datasets, users...\" spellcheck=\"false\" type=\"text\" value=\"\"/>\n<svg aria-hidden=\"true\" class=\"absolute left-2.5 text-gray-400 top-1/2 transform -translate-y-1/2\" focusable=\"false\" height=\"1em\" preserveaspectratio=\"xMidYMid meet\" role=\"img\" viewbox=\"0 0 32 32\" width=\"1em\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><path d=\"M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z\" fill=\"currentColor\"></path></svg>\n</div>\n<div class=\"flex flex-none items-center justify-center p-0.5 place-self-stretch lg:hidden\"><button class=\"relative z-40 flex h-6 w-8 items-center justify-center\" type=\"button\"><svg aria-hidden=\"true\" class=\"text-xl\" fill=\"currentColor\" focusable=\"false\" height=\"1em\" preserveaspectratio=\"xMidYMid meet\" role=\"img\" viewbox=\"0 0 10 10\" width=\"1em\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><path clip-rule=\"evenodd\" d=\"M1.65039 2.9999C1.65039 2.8066 1.80709 2.6499 2.00039 2.6499H8.00039C8.19369 2.6499 8.35039 2.8066 8.35039 2.9999C8.35039 3.1932 8.19369 3.3499 8.00039 3.3499H2.00039C1.80709 3.3499 1.65039 3.1932 1.65039 2.9999ZM1.65039 4.9999C1.65039 4.8066 1.80709 4.6499 2.00039 4.6499H8.00039C8.19369 4.6499 8.35039 4.8066 8.35039 4.9999C8.35039 5.1932 8.19369 5.3499 8.00039 5.3499H2.00039C1.80709 5.3499 1.65039 5.1932 1.65039 4.9999ZM2.00039 6.6499C1.80709 6.6499 1.65039 6.8066 1.65039 6.9999C1.65039 7.1932 1.80709 7.3499 2.00039 7.3499H8.00039C8.19369 7.3499 8.35039 7.1932 8.35039 6.9999C8.35039 6.8066 8.19369 6.6499 8.00039 6.6499H2.00039Z\" fill-rule=\"evenodd\"></path></svg>\n</button>\n</div></div>\n<nav aria-label=\"Main\" class=\"ml-auto hidden lg:block\"><ul class=\"flex items-center gap-x-1 2xl:gap-x-2\"><li class=\"hover:text-indigo-700\"><a class=\"group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100\" href=\"/models\"><svg aria-hidden=\"true\" class=\"mr-1.5 text-gray-400 group-hover:text-indigo-500\" focusable=\"false\" height=\"1em\" preserveaspectratio=\"xMidYMid meet\" role=\"img\" style=\"\" viewbox=\"0 0 24 24\" width=\"1em\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><path class=\"uim-quaternary\" d=\"M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z\" fill=\"currentColor\" opacity=\".25\"></path><path class=\"uim-tertiary\" d=\"M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z\" fill=\"currentColor\" opacity=\".5\"></path><path class=\"uim-primary\" d=\"M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z\" fill=\"currentColor\"></path></svg>\n\t\t\t\t\t\tModels</a>\n</li><li class=\"hover:text-red-700\"><a class=\"group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100\" href=\"/datasets\"><svg aria-hidden=\"true\" class=\"mr-1.5 text-gray-400 group-hover:text-red-500\" focusable=\"false\" height=\"1em\" preserveaspectratio=\"xMidYMid meet\" role=\"img\" style=\"\" viewbox=\"0 0 25 25\" width=\"1em\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><ellipse cx=\"12.5\" cy=\"5\" fill=\"currentColor\" fill-opacity=\"0.25\" rx=\"7.5\" ry=\"2\"></ellipse><path d=\"M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z\" fill=\"currentColor\" opacity=\"0.5\"></path><path d=\"M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z\" fill=\"currentColor\" opacity=\"0.5\"></path><path d=\"M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z\" fill=\"currentColor\"></path></svg>\n\t\t\t\t\t\tDatasets</a>\n</li><li class=\"hover:text-blue-700\"><a class=\"group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100\" href=\"/spaces\"><svg aria-hidden=\"true\" class=\"mr-1.5 text-gray-400 group-hover:text-blue-500\" focusable=\"false\" height=\"1em\" role=\"img\" viewbox=\"0 0 25 25\" width=\"1em\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><path d=\"M6.016 14.674v4.31h4.31v-4.31h-4.31ZM14.674 14.674v4.31h4.31v-4.31h-4.31ZM6.016 6.016v4.31h4.31v-4.31h-4.31Z\" fill=\"currentColor\" opacity=\".5\"></path><path clip-rule=\"evenodd\" d=\"M3 4.914C3 3.857 3.857 3 4.914 3h6.514c.884 0 1.628.6 1.848 1.414a5.171 5.171 0 0 1 7.31 7.31c.815.22 1.414.964 1.414 1.848v6.514A1.914 1.914 0 0 1 20.086 22H4.914A1.914 1.914 0 0 1 3 20.086V4.914Zm3.016 1.102v4.31h4.31v-4.31h-4.31Zm0 12.968v-4.31h4.31v4.31h-4.31Zm8.658 0v-4.31h4.31v4.31h-4.31Zm0-10.813a2.155 2.155 0 1 1 4.31 0 2.155 2.155 0 0 1-4.31 0Z\" fill=\"currentColor\" fill-rule=\"evenodd\" opacity=\".75\"></path><path d=\"M16.829 6.016a2.155 2.155 0 1 0 0 4.31 2.155 2.155 0 0 0 0-4.31Z\" fill=\"currentColor\" opacity=\".25\"></path></svg>\n\t\t\t\t\t\tSpaces</a>\n</li><li class=\"max-xl:hidden relative\"><div class=\"relative\">\n<button class=\"group flex items-center px-2 py-0.5 dark:text-gray-300 hover:text-yellow-700 dark:hover:text-gray-100\" type=\"button\">\n<svg aria-hidden=\"true\" class=\"mr-1.5 mr-1.5 text-gray-400 text-yellow-500! group-hover:text-yellow-500\" focusable=\"false\" height=\"1em\" preserveaspectratio=\"xMidYMid meet\" role=\"img\" viewbox=\"0 0 32 32\" width=\"1em\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><path d=\"M20.6081 3C21.7684 3 22.8053 3.49196 23.5284 4.38415C23.9756 4.93678 24.4428 5.82749 24.4808 7.16133C24.9674 7.01707 25.4353 6.93643 25.8725 6.93643C26.9833 6.93643 27.9865 7.37587 28.696 8.17411C29.6075 9.19872 30.0124 10.4579 29.8361 11.7177C29.7523 12.3177 29.5581 12.8555 29.2678 13.3534C29.8798 13.8646 30.3306 14.5763 30.5485 15.4322C30.719 16.1032 30.8939 17.5006 29.9808 18.9403C30.0389 19.0342 30.0934 19.1319 30.1442 19.2318C30.6932 20.3074 30.7283 21.5229 30.2439 22.6548C29.5093 24.3704 27.6841 25.7219 24.1397 27.1727C21.9347 28.0753 19.9174 28.6523 19.8994 28.6575C16.9842 29.4379 14.3477 29.8345 12.0653 29.8345C7.87017 29.8345 4.8668 28.508 3.13831 25.8921C0.356375 21.6797 0.754104 17.8269 4.35369 14.1131C6.34591 12.058 7.67023 9.02782 7.94613 8.36275C8.50224 6.39343 9.97271 4.20438 12.4172 4.20438H12.4179C12.6236 4.20438 12.8314 4.2214 13.0364 4.25468C14.107 4.42854 15.0428 5.06476 15.7115 6.02205C16.4331 5.09583 17.134 4.359 17.7682 3.94323C18.7242 3.31737 19.6794 3 20.6081 3ZM20.6081 5.95917C20.2427 5.95917 19.7963 6.1197 19.3039 6.44225C17.7754 7.44319 14.8258 12.6772 13.7458 14.7131C13.3839 15.3952 12.7655 15.6837 12.2086 15.6837C11.1036 15.6837 10.2408 14.5497 12.1076 13.1085C14.9146 10.9402 13.9299 7.39584 12.5898 7.1776C12.5311 7.16799 12.4731 7.16355 12.4172 7.16355C11.1989 7.16355 10.6615 9.33114 10.6615 9.33114C10.6615 9.33114 9.0863 13.4148 6.38031 16.206C3.67434 18.998 3.5346 21.2388 5.50675 24.2246C6.85185 26.2606 9.42666 26.8753 12.0653 26.8753C14.8021 26.8753 17.6077 26.2139 19.1799 25.793C19.2574 25.7723 28.8193 22.984 27.6081 20.6107C27.4046 20.212 27.0693 20.0522 26.6471 20.0522C24.9416 20.0522 21.8393 22.6726 20.5057 22.6726C20.2076 22.6726 19.9976 22.5416 19.9116 22.222C19.3433 20.1173 28.552 19.2325 27.7758 16.1839C27.639 15.6445 27.2677 15.4256 26.746 15.4263C24.4923 15.4263 19.4358 19.5181 18.3759 19.5181C18.2949 19.5181 18.2368 19.4937 18.2053 19.4419C17.6743 18.557 17.9653 17.9394 21.7082 15.6009C25.4511 13.2617 28.0783 11.8545 26.5841 10.1752C26.4121 9.98141 26.1684 9.8956 25.8725 9.8956C23.6001 9.89634 18.2311 14.9403 18.2311 14.9403C18.2311 14.9403 16.7821 16.496 15.9057 16.496C15.7043 16.496 15.533 16.4139 15.4169 16.2112C14.7956 15.1296 21.1879 10.1286 21.5484 8.06535C21.7928 6.66715 21.3771 5.95917 20.6081 5.95917Z\" fill=\"#FF9D00\"></path><path d=\"M5.50686 24.2246C3.53472 21.2387 3.67446 18.9979 6.38043 16.206C9.08641 13.4147 10.6615 9.33111 10.6615 9.33111C10.6615 9.33111 11.2499 6.95933 12.59 7.17757C13.93 7.39581 14.9139 10.9401 12.1069 13.1084C9.29997 15.276 12.6659 16.7489 13.7459 14.713C14.8258 12.6772 17.7747 7.44316 19.304 6.44221C20.8326 5.44128 21.9089 6.00204 21.5484 8.06532C21.188 10.1286 14.795 15.1295 15.4171 16.2118C16.0391 17.2934 18.2312 14.9402 18.2312 14.9402C18.2312 14.9402 25.0907 8.49588 26.5842 10.1752C28.0776 11.8545 25.4512 13.2616 21.7082 15.6008C17.9646 17.9393 17.6744 18.557 18.2054 19.4418C18.7372 20.3266 26.9998 13.1351 27.7759 16.1838C28.5513 19.2324 19.3434 20.1173 19.9117 22.2219C20.48 24.3274 26.3979 18.2382 27.6082 20.6107C28.8193 22.9839 19.2574 25.7722 19.18 25.7929C16.0914 26.62 8.24723 28.3726 5.50686 24.2246Z\" fill=\"#FFD21E\"></path></svg>\n\t\t\tCommunity\n\t\t</button>\n</div>\n</li><li class=\"hover:text-yellow-700\"><a class=\"group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100\" href=\"/docs\"><svg aria-hidden=\"true\" class=\"mr-1.5 text-gray-400 group-hover:text-yellow-500\" height=\"1em\" preserveaspectratio=\"xMidYMid meet\" role=\"img\" viewbox=\"0 0 16 16\" width=\"1em\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><path class=\"dark:opacity-40\" d=\"m2.28 3.7-.3.16a.67.67 0 0 0-.34.58v8.73l.01.04.02.07.01.04.03.06.02.04.02.03.04.06.05.05.04.04.06.04.06.04.08.04.08.02h.05l.07.02h.11l.04-.01.07-.02.03-.01.07-.03.22-.12a5.33 5.33 0 0 1 5.15.1.67.67 0 0 0 .66 0 5.33 5.33 0 0 1 5.33 0 .67.67 0 0 0 1-.58V4.36a.67.67 0 0 0-.34-.5l-.3-.17v7.78a.63.63 0 0 1-.87.59 4.9 4.9 0 0 0-4.35.35l-.65.39a.29.29 0 0 1-.15.04.29.29 0 0 1-.16-.04l-.65-.4a4.9 4.9 0 0 0-4.34-.34.63.63 0 0 1-.87-.59V3.7Z\" fill=\"currentColor\"></path><path class=\"opacity-40 dark:opacity-100\" clip-rule=\"evenodd\" d=\"M8 3.1a5.99 5.99 0 0 0-5.3-.43.66.66 0 0 0-.42.62v8.18c0 .45.46.76.87.59a4.9 4.9 0 0 1 4.34.35l.65.39c.05.03.1.04.16.04.05 0 .1-.01.15-.04l.65-.4a4.9 4.9 0 0 1 4.35-.34.63.63 0 0 0 .86-.59V3.3a.67.67 0 0 0-.41-.62 5.99 5.99 0 0 0-5.3.43l-.3.17L8 3.1Zm.73 1.87a.43.43 0 1 0-.86 0v5.48a.43.43 0 0 0 .86 0V4.97Z\" fill=\"currentColor\" fill-rule=\"evenodd\"></path><path class=\"dark:opacity-40\" d=\"M8.73 4.97a.43.43 0 1 0-.86 0v5.48a.43.43 0 1 0 .86 0V4.96Z\" fill=\"currentColor\"></path></svg>\n\t\t\t\t\t\tDocs</a>\n</li><li class=\"hover:text-black dark:hover:text-white max-2xl:hidden\"><a class=\"group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100\" href=\"/enterprise\"><svg aria-hidden=\"true\" class=\"mr-1.5 text-gray-400 group-hover:text-black dark:group-hover:text-white\" fill=\"none\" focusable=\"false\" height=\"1em\" preserveaspectratio=\"xMidYMid meet\" role=\"img\" viewbox=\"0 0 12 12\" width=\"1em\" xmlns=\"http://www.w3.org/2000/svg\"><path clip-rule=\"evenodd\" d=\"M4.9 1.35a3.16 3.16 0 0 0-2.8 2.07L.37 8.58C0 9.71.7 10.65 1.86 10.65H7.3a3.2 3.2 0 0 0 2.84-2.07l1.67-5.16c.36-1.13-.3-2.07-1.46-2.07H4.91Zm.4 2.07L3.57 8.47h3.57l.36-1.12H5.4l.28-.91h1.75l.4-1.1H6.07l.3-.83h2l.36-1.1H5.27h.04Z\" fill=\"currentColor\" fill-rule=\"evenodd\"></path></svg>\n\t\t\t\t\t\tEnterprise</a>\n</li>\n<li><a class=\"group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100\" href=\"/pricing\">Pricing\n\t\t\t</a></li>\n<li><div class=\"relative group\">\n<button class=\"px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-600 flex items-center\" type=\"button\">\n<svg aria-hidden=\"true\" class=\"text-gray-500 w-5 group-hover:text-gray-400 dark:text-gray-300 dark:group-hover:text-gray-100\" focusable=\"false\" height=\"1em\" preserveaspectratio=\"xMidYMid meet\" role=\"img\" viewbox=\"0 0 32 18\" width=\"1em\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><path clip-rule=\"evenodd\" d=\"M14.4504 3.30221C14.4504 2.836 14.8284 2.45807 15.2946 2.45807H28.4933C28.9595 2.45807 29.3374 2.836 29.3374 3.30221C29.3374 3.76842 28.9595 4.14635 28.4933 4.14635H15.2946C14.8284 4.14635 14.4504 3.76842 14.4504 3.30221Z\" fill=\"currentColor\" fill-rule=\"evenodd\"></path><path clip-rule=\"evenodd\" d=\"M14.4504 9.00002C14.4504 8.53382 14.8284 8.15588 15.2946 8.15588H28.4933C28.9595 8.15588 29.3374 8.53382 29.3374 9.00002C29.3374 9.46623 28.9595 9.84417 28.4933 9.84417H15.2946C14.8284 9.84417 14.4504 9.46623 14.4504 9.00002Z\" fill=\"currentColor\" fill-rule=\"evenodd\"></path><path clip-rule=\"evenodd\" d=\"M14.4504 14.6978C14.4504 14.2316 14.8284 13.8537 15.2946 13.8537H28.4933C28.9595 13.8537 29.3374 14.2316 29.3374 14.6978C29.3374 15.164 28.9595 15.542 28.4933 15.542H15.2946C14.8284 15.542 14.4504 15.164 14.4504 14.6978Z\" fill=\"currentColor\" fill-rule=\"evenodd\"></path><path clip-rule=\"evenodd\" d=\"M1.94549 6.87377C2.27514 6.54411 2.80962 6.54411 3.13928 6.87377L6.23458 9.96907L9.32988 6.87377C9.65954 6.54411 10.194 6.54411 10.5237 6.87377C10.8533 7.20343 10.8533 7.73791 10.5237 8.06756L6.23458 12.3567L1.94549 8.06756C1.61583 7.73791 1.61583 7.20343 1.94549 6.87377Z\" fill=\"currentColor\" fill-rule=\"evenodd\"></path></svg>\n</button>\n</div></li>\n<li><hr class=\"h-5 w-0.5 border-none bg-gray-100 dark:bg-gray-800\"/></li>\n<li><a class=\"block cursor-pointer whitespace-nowrap px-2 py-0.5 hover:text-gray-500 dark:text-gray-300 dark:hover:text-gray-100\" href=\"/login\">Log In\n\t\t\t\t</a></li>\n<li><a class=\"whitespace-nowrap rounded-full border border-transparent bg-gray-900 px-3 py-1 leading-none text-white hover:border-black hover:bg-white hover:text-black\" href=\"/join\">Sign Up\n\t\t\t\t\t</a></li></ul></nav></div></header></div>\n<div class=\"SVELTE_HYDRATER contents\" data-props=\"{}\" data-target=\"SSOBanner\"></div>\n<main class=\"flex flex-1 flex-col\"><div class=\"SVELTE_HYDRATER contents\" data-props='{\"isLoggedIn\":false}' data-target=\"DailyPapersBannerSubscribe\"><div class=\"-mt-px flex h-9 w-full justify-center text-gray-600\"><svg class=\"hidden h-9 flex-none text-gray-100/80 dark:text-gray-800/40 sm:block\" viewbox=\"0 0 110 41\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M110 0H0c39.1 0 44 9.6 49 19.5C54.6 30 60 41 108 41h2V0Z\" fill=\"currentColor\"></path></svg>\n<div class=\"flex items-center justify-center gap-3 bg-gray-100/80 text-sm dark:bg-gray-800/40 max-sm:flex-1\"><div class=\"rounded-md bg-blue-500/20 px-1 text-xs font-semibold uppercase text-blue-600\">new</div>\n<p class=\"hidden sm:inline\">Get trending papers in your email inbox once a day!</p>\n<p class=\"inline sm:hidden\">Get trending papers in your email inbox!</p>\n<a class=\"btn px-2! text-sm leading-none\" href=\"/login?next=%2Fpapers\">Subscribe</a></div>\n<svg class=\"hidden h-9 flex-none text-gray-100/80 dark:text-gray-800/40 sm:block\" viewbox=\"0 0 110 41\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M0 0h110C70.9 0 66 9.6 61 19.5 55.4 30 50 41 2 41H0V0Z\" fill=\"currentColor\"></path></svg></div></div>\n<div class=\"SVELTE_HYDRATER contents\" data-props=\"{&quot;canSubmit&quot;:false,&quot;dailyPapers&quot;:[{&quot;paper&quot;:{&quot;id&quot;:&quot;2508.05748&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;689c0152fab6fdd2e52ac85d&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;682b22ebac526172e1b4ed1b&quot;,&quot;avatarUrl&quot;:&quot;/avatars/a9e486bf72d27013e6c1903b64a7754c.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Geng Xinyu&quot;,&quot;user&quot;:&quot;Ornamentt&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Xinyu Geng&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-08-13T07:12:48.684Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;689c0152fab6fdd2e52ac85e&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;643e9ee6f6bb3c31a26e7bc4&quot;,&quot;avatarUrl&quot;:&quot;/avatars/acfaa7d6a23dada24c86b954c3be116a.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Peng Xia&quot;,&quot;user&quot;:&quot;richardxp888&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Peng Xia&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-08-13T07:12:38.450Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;689c0152fab6fdd2e52ac85f&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;646abd0f8dfd6ff79b6cfbb9&quot;,&quot;avatarUrl&quot;:&quot;/avatars/3de3b7cbeda95c2b4f460f87f8e9a1f7.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;ZhenZhang&quot;,&quot;user&quot;:&quot;zhzhen23&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Zhen Zhang&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-08-13T07:20:47.118Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;689c0152fab6fdd2e52ac860&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;64b73f9317570fdff9b0d1c4&quot;,&quot;avatarUrl&quot;:&quot;/avatars/62124ae3e929b53f99f37e97226a877d.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Wang Xinyu&quot;,&quot;user&quot;:&quot;oriuta&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Xinyu Wang&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-08-14T13:38:49.638Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;689c0152fab6fdd2e52ac861&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;657429d833e5a4bf5b278615&quot;,&quot;avatarUrl&quot;:&quot;/avatars/ed7e28c1b9a7bed1cad864c992cdcc69.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;QiuchenWang&quot;,&quot;user&quot;:&quot;autumncc&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Qiuchen Wang&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-08-18T06:59:11.585Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;689c0152fab6fdd2e52ac862&quot;,&quot;name&quot;:&quot;Ruixue Ding&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;689c0152fab6fdd2e52ac863&quot;,&quot;name&quot;:&quot;Chenxi Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;689c0152fab6fdd2e52ac864&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;644a4fbc2166258fccc664bc&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jialong Wu&quot;,&quot;user&quot;:&quot;callanwu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Jialong Wu&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-08-13T07:12:43.643Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;689c0152fab6fdd2e52ac865&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;66e4019518a1920fb7ca19d7&quot;,&quot;avatarUrl&quot;:&quot;/avatars/4f8be2f3244239510235c2ec6e8d38a6.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yida Zhao&quot;,&quot;user&quot;:&quot;zhaoyd&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yida Zhao&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-08-13T07:12:34.963Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;689c0152fab6fdd2e52ac866&quot;,&quot;name&quot;:&quot;Kuan Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;689c0152fab6fdd2e52ac867&quot;,&quot;name&quot;:&quot;Yong Jiang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;689c0152fab6fdd2e52ac868&quot;,&quot;name&quot;:&quot;Pengjun Xie&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;689c0152fab6fdd2e52ac869&quot;,&quot;name&quot;:&quot;Fei Huang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;689c0152fab6fdd2e52ac86a&quot;,&quot;name&quot;:&quot;Jingren Zhou&quot;,&quot;hidden&quot;:false}],&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/1DotdrNI5gc_sKLMxtlnq.png&quot;,&quot;https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/VqGc9-ADKeMvExKGjvaWU.png&quot;],&quot;publishedAt&quot;:&quot;2025-08-07T18:03:50.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-08-13T02:05:16.133Z&quot;,&quot;title&quot;:&quot;WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;643e9ee6f6bb3c31a26e7bc4&quot;,&quot;avatarUrl&quot;:&quot;/avatars/acfaa7d6a23dada24c86b954c3be116a.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Peng Xia&quot;,&quot;user&quot;:&quot;richardxp888&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Web agents such as Deep Research have demonstrated superhuman cognitive\\nabilities, capable of solving highly challenging information-seeking problems.\\nHowever, most research remains primarily text-centric, overlooking visual\\ninformation in the real world. This makes multimodal Deep Research highly\\nchallenging, as such agents require much stronger reasoning abilities in\\nperception, logic, knowledge, and the use of more sophisticated tools compared\\nto text-based agents. To address this limitation, we introduce WebWatcher, a\\nmulti-modal Agent for Deep Research equipped with enhanced visual-language\\nreasoning capabilities. It leverages high-quality synthetic multimodal\\ntrajectories for efficient cold start training, utilizes various tools for deep\\nreasoning, and further enhances generalization through reinforcement learning.\\nTo better evaluate the capabilities of multimodal agents, we propose\\nBrowseComp-VL, a benchmark with BrowseComp-style that requires complex\\ninformation retrieval involving both visual and textual information.\\nExperimental results show that WebWatcher significantly outperforms proprietary\\nbaseline, RAG workflow and open-source agents in four challenging VQA\\nbenchmarks, which paves the way for solving complex multimodal\\ninformation-seeking tasks.&quot;,&quot;upvotes&quot;:129,&quot;discussionId&quot;:&quot;689c0152fab6fdd2e52ac86b&quot;,&quot;projectPage&quot;:&quot;https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/&quot;,&quot;githubRepo&quot;:&quot;https://github.com/Alibaba-NLP/WebAgent//&quot;,&quot;ai_summary&quot;:&quot;WebWatcher, a multimodal agent with enhanced visual-language reasoning, outperforms existing agents in complex visual and textual information retrieval tasks using synthetic trajectories and reinforcement learning.&quot;,&quot;ai_keywords&quot;:[&quot;multimodal&quot;,&quot;visual-language reasoning&quot;,&quot;high-quality synthetic multimodal trajectories&quot;,&quot;reinforcement learning&quot;,&quot;BrowseComp-VL&quot;,&quot;VQA benchmarks&quot;],&quot;githubStars&quot;:15005},&quot;publishedAt&quot;:&quot;2025-08-07T14:03:50.000Z&quot;,&quot;title&quot;:&quot;WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent&quot;,&quot;summary&quot;:&quot;Web agents such as Deep Research have demonstrated superhuman cognitive\\nabilities, capable of solving highly challenging information-seeking problems.\\nHowever, most research remains primarily text-centric, overlooking visual\\ninformation in the real world. This makes multimodal Deep Research highly\\nchallenging, as such agents require much stronger reasoning abilities in\\nperception, logic, knowledge, and the use of more sophisticated tools compared\\nto text-based agents. To address this limitation, we introduce WebWatcher, a\\nmulti-modal Agent for Deep Research equipped with enhanced visual-language\\nreasoning capabilities. It leverages high-quality synthetic multimodal\\ntrajectories for efficient cold start training, utilizes various tools for deep\\nreasoning, and further enhances generalization through reinforcement learning.\\nTo better evaluate the capabilities of multimodal agents, we propose\\nBrowseComp-VL, a benchmark with BrowseComp-style that requires complex\\ninformation retrieval involving both visual and textual information.\\nExperimental results show that WebWatcher significantly outperforms proprietary\\nbaseline, RAG workflow and open-source agents in four challenging VQA\\nbenchmarks, which paves the way for solving complex multimodal\\ninformation-seeking tasks.&quot;,&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/1DotdrNI5gc_sKLMxtlnq.png&quot;,&quot;https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/VqGc9-ADKeMvExKGjvaWU.png&quot;],&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.05748.png&quot;,&quot;numComments&quot;:4,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;643e9ee6f6bb3c31a26e7bc4&quot;,&quot;avatarUrl&quot;:&quot;/avatars/acfaa7d6a23dada24c86b954c3be116a.svg&quot;,&quot;fullname&quot;:&quot;Peng Xia&quot;,&quot;name&quot;:&quot;richardxp888&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:3},&quot;submitterOrganization&quot;:{&quot;_id&quot;:&quot;661f98de142a51d630dbbcc4&quot;,&quot;name&quot;:&quot;Alibaba-NLP&quot;,&quot;fullname&quot;:&quot;Alibaba-NLP&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/63fc4c00a3c067e62899d32b/dfd_EcIfylvu3sdc2WMqX.png&quot;},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2507.15061&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;687ef39133947f780d9b4a7f&quot;,&quot;name&quot;:&quot;Zhengwei Tao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;687ef39133947f780d9b4a80&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;644a4fbc2166258fccc664bc&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jialong Wu&quot;,&quot;user&quot;:&quot;callanwu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Jialong Wu&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-07-22T14:08:42.255Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;687ef39133947f780d9b4a81&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;63fc4c00a3c067e62899d32b&quot;,&quot;avatarUrl&quot;:&quot;/avatars/b54f2a406afdbbe2cd305d4d9f88ced2.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Wenbiao Yin&quot;,&quot;user&quot;:&quot;NLPblue&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Wenbiao Yin&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-07-22T14:08:48.253Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;687ef39133947f780d9b4a82&quot;,&quot;name&quot;:&quot;Junkai Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;687ef39133947f780d9b4a83&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;6538bdfdf5f5016df35f5faf&quot;,&quot;avatarUrl&quot;:&quot;/avatars/054fc6f8cb46805e66de5c3c856d4fb9.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Baixuan Li&quot;,&quot;user&quot;:&quot;MuBai2001&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Baixuan Li&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-07-22T14:09:05.979Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;687ef39133947f780d9b4a84&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;67dd1d084004b2f0de087fad&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e6d9142d66271405d2062fa24177a11e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Shen HaiYang&quot;,&quot;user&quot;:&quot;seaforestshen&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Haiyang Shen&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-07-22T14:09:16.404Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;687ef39133947f780d9b4a85&quot;,&quot;name&quot;:&quot;Kuan Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;687ef39133947f780d9b4a86&quot;,&quot;name&quot;:&quot;Liwen Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;687ef39133947f780d9b4a87&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;646495dc802bcd26c3b92851&quot;,&quot;avatarUrl&quot;:&quot;/avatars/23448c9a3b12aabdf61d2b874eecfd54.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xinyu Wang&quot;,&quot;user&quot;:&quot;XinyuWang&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Xinyu Wang&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-07-22T14:09:31.152Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;687ef39133947f780d9b4a88&quot;,&quot;name&quot;:&quot;Yong Jiang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;687ef39133947f780d9b4a89&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;63a091e42fabbbb89991f5ce&quot;,&quot;avatarUrl&quot;:&quot;/avatars/d55485b06461764c36c9edf9d6e8892c.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;pengjun xie&quot;,&quot;user&quot;:&quot;xpjandy&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Pengjun Xie&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-07-22T14:09:37.774Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;687ef39133947f780d9b4a8a&quot;,&quot;name&quot;:&quot;Fei Huang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;687ef39133947f780d9b4a8b&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;602f88f5e8149a962412a667&quot;,&quot;avatarUrl&quot;:&quot;/avatars/b78f0e583df8e5d5e3365934fe5f4900.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhou&quot;,&quot;user&quot;:&quot;Jingren&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Jingren Zhou&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-07-22T14:09:49.501Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-07-20T17:53:37.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-07-22T01:17:25.547Z&quot;,&quot;title&quot;:&quot;WebShaper: Agentically Data Synthesizing via Information-Seeking\\n  Formalization&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;644a4fbc2166258fccc664bc&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jialong Wu&quot;,&quot;user&quot;:&quot;callanwu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;The advent of Large Language Model (LLM)-powered agents has revolutionized\\nartificial intelligence by enabling solutions to complex, open-ended tasks\\nthrough web-based information-seeking (IS) capabilities. The scarcity of\\nhigh-quality training data has limited the development of IS agents. Existing\\napproaches typically adopt an information-driven paradigm that first collects\\nweb data and then generates questions based on the retrieval. However, this may\\nlead to inconsistency between information structure and reasoning structure,\\nquestion and answer. To mitigate, we propose a formalization-driven IS data\\nsynthesis framework WebShaper to construct a dataset. WebShaper systematically\\nformalizes IS tasks through set theory. Central to the formalization is the\\nconcept of Knowledge Projections (KP), which enables precise control over\\nreasoning structure by KP operation compositions. During synthesis, we begin by\\ncreating seed tasks, then use a multi-step expansion process. At each step, an\\nagentic Expander expands the current formal question more complex with\\nretrieval and validation tools based on our formalization. We train our model\\non the synthesized dataset. Experiment results demonstrate that WebShaper\\nachieves state-of-the-art performance among open-sourced IS agents on GAIA and\\nWebWalkerQA benchmarks.&quot;,&quot;upvotes&quot;:57,&quot;discussionId&quot;:&quot;687ef39133947f780d9b4a8c&quot;,&quot;githubRepo&quot;:&quot;https://github.com/Alibaba-NLP/WebWalker&quot;,&quot;ai_summary&quot;:&quot;WebShaper, a formalization-driven framework, synthesizes information-seeking datasets using set theory and Knowledge Projections to enhance reasoning structure and achieve top performance in open-sourced benchmarks.&quot;,&quot;ai_keywords&quot;:[&quot;Large Language Model (LLM)&quot;,&quot;information-seeking (IS) agents&quot;,&quot;formalization-driven IS data synthesis&quot;,&quot;set theory&quot;,&quot;Knowledge Projections (KP)&quot;,&quot;agentic Expander&quot;,&quot;GAIA&quot;,&quot;WebWalkerQA benchmarks&quot;],&quot;githubStars&quot;:15005},&quot;publishedAt&quot;:&quot;2025-07-20T13:53:37.000Z&quot;,&quot;title&quot;:&quot;WebShaper: Agentically Data Synthesizing via Information-Seeking\\n  Formalization&quot;,&quot;summary&quot;:&quot;The advent of Large Language Model (LLM)-powered agents has revolutionized\\nartificial intelligence by enabling solutions to complex, open-ended tasks\\nthrough web-based information-seeking (IS) capabilities. The scarcity of\\nhigh-quality training data has limited the development of IS agents. Existing\\napproaches typically adopt an information-driven paradigm that first collects\\nweb data and then generates questions based on the retrieval. However, this may\\nlead to inconsistency between information structure and reasoning structure,\\nquestion and answer. To mitigate, we propose a formalization-driven IS data\\nsynthesis framework WebShaper to construct a dataset. WebShaper systematically\\nformalizes IS tasks through set theory. Central to the formalization is the\\nconcept of Knowledge Projections (KP), which enables precise control over\\nreasoning structure by KP operation compositions. During synthesis, we begin by\\ncreating seed tasks, then use a multi-step expansion process. At each step, an\\nagentic Expander expands the current formal question more complex with\\nretrieval and validation tools based on our formalization. We train our model\\non the synthesized dataset. Experiment results demonstrate that WebShaper\\nachieves state-of-the-art performance among open-sourced IS agents on GAIA and\\nWebWalkerQA benchmarks.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.15061.png&quot;,&quot;numComments&quot;:6,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;644a4fbc2166258fccc664bc&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg&quot;,&quot;fullname&quot;:&quot;Jialong Wu&quot;,&quot;name&quot;:&quot;callanwu&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:21},&quot;submitterOrganization&quot;:{&quot;_id&quot;:&quot;661f98de142a51d630dbbcc4&quot;,&quot;name&quot;:&quot;Alibaba-NLP&quot;,&quot;fullname&quot;:&quot;Alibaba-NLP&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/63fc4c00a3c067e62899d32b/dfd_EcIfylvu3sdc2WMqX.png&quot;},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2505.22648&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6837c03cbbee677da73e6034&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;644a4fbc2166258fccc664bc&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jialong Wu&quot;,&quot;user&quot;:&quot;callanwu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Jialong Wu&quot;,&quot;status&quot;:&quot;extracted_confirmed&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-07-04T08:14:49.966Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6837c03cbbee677da73e6035&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;6538bdfdf5f5016df35f5faf&quot;,&quot;avatarUrl&quot;:&quot;/avatars/054fc6f8cb46805e66de5c3c856d4fb9.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Baixuan Li&quot;,&quot;user&quot;:&quot;MuBai2001&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Baixuan Li&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-09-19T07:03:52.820Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6837c03cbbee677da73e6036&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;63d32cd7b734eaa4d4fa410b&quot;,&quot;avatarUrl&quot;:&quot;/avatars/68acb80f62bc6493e1ad26506999b6c4.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Runnan Fang&quot;,&quot;user&quot;:&quot;Runnaning&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Runnan Fang&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-09-19T07:04:00.170Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6837c03cbbee677da73e6037&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;63fc4c00a3c067e62899d32b&quot;,&quot;avatarUrl&quot;:&quot;/avatars/b54f2a406afdbbe2cd305d4d9f88ced2.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Wenbiao Yin&quot;,&quot;user&quot;:&quot;NLPblue&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Wenbiao Yin&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-09-19T07:04:07.608Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6837c03cbbee677da73e6038&quot;,&quot;name&quot;:&quot;Liwen Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6837c03cbbee677da73e6039&quot;,&quot;name&quot;:&quot;Zhengwei Tao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6837c03cbbee677da73e603a&quot;,&quot;name&quot;:&quot;Dingchu Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6837c03cbbee677da73e603b&quot;,&quot;name&quot;:&quot;Zekun Xi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6837c03cbbee677da73e603c&quot;,&quot;name&quot;:&quot;Yong Jiang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6837c03cbbee677da73e603d&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;63a091e42fabbbb89991f5ce&quot;,&quot;avatarUrl&quot;:&quot;/avatars/d55485b06461764c36c9edf9d6e8892c.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;pengjun xie&quot;,&quot;user&quot;:&quot;xpjandy&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Pengjun Xie&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-09-19T07:04:40.177Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6837c03cbbee677da73e603e&quot;,&quot;name&quot;:&quot;Fei Huang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6837c03cbbee677da73e603f&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;602f88f5e8149a962412a667&quot;,&quot;avatarUrl&quot;:&quot;/avatars/b78f0e583df8e5d5e3365934fe5f4900.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhou&quot;,&quot;user&quot;:&quot;Jingren&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Jingren Zhou&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-09-19T07:04:55.277Z&quot;,&quot;hidden&quot;:false}],&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/vhAmZAlJqekE6vLcVjWtO.mp4&quot;,&quot;https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/RXQVwE9PRmBzxCURRiKAU.mp4&quot;,&quot;https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/woFRNvRqdUXKnAHpRH2eM.mp4&quot;],&quot;publishedAt&quot;:&quot;2025-05-28T17:57:07.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-05-29T00:34:30.750Z&quot;,&quot;title&quot;:&quot;WebDancer: Towards Autonomous Information Seeking Agency&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;644a4fbc2166258fccc664bc&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jialong Wu&quot;,&quot;user&quot;:&quot;callanwu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Addressing intricate real-world problems necessitates in-depth information\\nseeking and multi-step reasoning. Recent progress in agentic systems,\\nexemplified by Deep Research, underscores the potential for autonomous\\nmulti-step research. In this work, we present a cohesive paradigm for building\\nend-to-end agentic information seeking agents from a data-centric and\\ntraining-stage perspective. Our approach consists of four key stages: (1)\\nbrowsing data construction, (2) trajectories sampling, (3) supervised\\nfine-tuning for effective cold start, and (4) reinforcement learning for\\nenhanced generalisation. We instantiate this framework in a web agent based on\\nthe ReAct, WebDancer. Empirical evaluations on the challenging information\\nseeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of\\nWebDancer, achieving considerable results and highlighting the efficacy of our\\ntraining paradigm. Further analysis of agent training provides valuable\\ninsights and actionable, systematic pathways for developing more capable\\nagentic models. The codes and demo will be released in\\nhttps://github.com/Alibaba-NLP/WebAgent.&quot;,&quot;upvotes&quot;:31,&quot;discussionId&quot;:&quot;6837c03dbbee677da73e607f&quot;,&quot;githubRepo&quot;:&quot;https://github.com/Alibaba-NLP/WebAgent&quot;,&quot;ai_summary&quot;:&quot;The paper proposes a framework for building end-to-end agentic information seeking agents through a combination of data construction, trajectory sampling, supervised fine-tuning, and reinforcement learning, showcasing its effectiveness on information seeking benchmarks.&quot;,&quot;ai_keywords&quot;:[&quot;browsing data construction&quot;,&quot;trajectories sampling&quot;,&quot;supervised fine-tuning&quot;,&quot;reinforcement learning&quot;,&quot;WebDancer&quot;,&quot;GAIA&quot;,&quot;WebWalkerQA&quot;],&quot;githubStars&quot;:15005},&quot;publishedAt&quot;:&quot;2025-05-28T13:57:07.000Z&quot;,&quot;title&quot;:&quot;WebDancer: Towards Autonomous Information Seeking Agency&quot;,&quot;summary&quot;:&quot;Addressing intricate real-world problems necessitates in-depth information\\nseeking and multi-step reasoning. Recent progress in agentic systems,\\nexemplified by Deep Research, underscores the potential for autonomous\\nmulti-step research. In this work, we present a cohesive paradigm for building\\nend-to-end agentic information seeking agents from a data-centric and\\ntraining-stage perspective. Our approach consists of four key stages: (1)\\nbrowsing data construction, (2) trajectories sampling, (3) supervised\\nfine-tuning for effective cold start, and (4) reinforcement learning for\\nenhanced generalisation. We instantiate this framework in a web agent based on\\nthe ReAct, WebDancer. Empirical evaluations on the challenging information\\nseeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of\\nWebDancer, achieving considerable results and highlighting the efficacy of our\\ntraining paradigm. Further analysis of agent training provides valuable\\ninsights and actionable, systematic pathways for developing more capable\\nagentic models. The codes and demo will be released in\\nhttps://github.com/Alibaba-NLP/WebAgent.&quot;,&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/vhAmZAlJqekE6vLcVjWtO.mp4&quot;,&quot;https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/RXQVwE9PRmBzxCURRiKAU.mp4&quot;,&quot;https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/woFRNvRqdUXKnAHpRH2eM.mp4&quot;],&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22648.png&quot;,&quot;numComments&quot;:5,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;644a4fbc2166258fccc664bc&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg&quot;,&quot;fullname&quot;:&quot;Jialong Wu&quot;,&quot;name&quot;:&quot;callanwu&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:21},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2509.13313&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68ca3c926e0073c09bd1df32&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;6622132f63598534f96ca29d&quot;,&quot;avatarUrl&quot;:&quot;/avatars/34e61fc3101f8ebce1ef7041f761e108.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xixi Wu&quot;,&quot;user&quot;:&quot;xxwu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Xixi Wu&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-09-17T12:47:44.759Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68ca3c926e0073c09bd1df33&quot;,&quot;name&quot;:&quot;Kuan Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68ca3c926e0073c09bd1df34&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;66e4019518a1920fb7ca19d7&quot;,&quot;avatarUrl&quot;:&quot;/avatars/4f8be2f3244239510235c2ec6e8d38a6.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yida Zhao&quot;,&quot;user&quot;:&quot;zhaoyd&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yida Zhao&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-09-17T15:29:22.732Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68ca3c926e0073c09bd1df35&quot;,&quot;name&quot;:&quot;Liwen Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68ca3c926e0073c09bd1df36&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;622f2feea32d46b4be9ed8c4&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/NDeZQZQK5U-9m10yQwDVf.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Litu Ou&quot;,&quot;user&quot;:&quot;learn3r&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Litu Ou&quot;,&quot;status&quot;:&quot;admin_assigned&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-09-19T06:56:00.748Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68ca3c926e0073c09bd1df37&quot;,&quot;name&quot;:&quot;Huifeng Yin&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68ca3c926e0073c09bd1df38&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;657c4e471fa4e4e152d31b40&quot;,&quot;avatarUrl&quot;:&quot;/avatars/0a375c40b7eb75987f4aed958f55c967.svg&quot;,&quot;isPro&quot;:false,&quot;f"
  },
  {
    "metadata": {
      "url": "https://scholar.google.com/scholar?q=meta+ai+research",
      "title": "Google Scholar",
      "category": "research_papers",
      "source_type": "generic",
      "scraped_at": "2025-09-28T18:06:14.863541",
      "word_count": 954,
      "status": "success"
    },
    "content": "Google Scholar\nLoading...\nThe system can't perform the operation now. Try again later.\nCite\nAdvanced search\nFind articles\nwith\nall\nof the words\nwith the\nexact phrase\nwith\nat least one\nof the words\nwithout\nthe words\nwhere my words occur\nanywhere in the article\nin the title of the article\nReturn articles\nauthored\nby\ne.g.,\n\"PJ Hayes\"\nor\nMcCarthy\nReturn articles\npublished\nin\ne.g.,\nJ Biol Chem\nor\nNature\nReturn articles\ndated\nbetween\n—\ne.g.,\n1996\nSaved to My library\nDone\nRemove article\nArticles\nCase law\nProfiles\nMy profile\nMy library\nAlerts\nMetrics\nAdvanced search\nSettings\nSign in\nSign in\nArticles\nScholar\nAbout 5,790,000 results (\n0.11\nsec)\nMy profile\nMy library\nYear\nAny time\nSince 2025\nSince 2024\nSince 2021\nSort by relevance\nSort by date\nAny type\nReview articles\ninclude patents\ninclude citations\nAny time\nSince 2025\nSince 2024\nSince 2021\nCustom range...\n—\nSearch\nSort by relevance\nSort by date\nAny type\nReview articles\ninclude patents\ninclude citations\nCreate alert\n[PDF]\nacs.org\nArtificial intelligence\nin\nmeta\n-optics\nMK Chen\n,\nX Liu\n,\nY Sun\n,\nDP Tsai\n- Chemical Reviews, 2022 - ACS Publications\n… After providing an overview of\nAI\nand\nmeta\n-optics, we … :\nAI\nworks for\nmeta\n-optics and\nmeta\n-optics works for\nAI\n. Section 3, “\nAI\nfor\nMeta\n-optics”, describes how to apply\nAI\nto the\nresearch\nof …\nSave\nCite\nCited by 198\nRelated articles\nAll 5 versions\n[PDF]\narxiv.org\nMeta\n-learning in natural and\nartificial intelligence\nJX Wang\n- Current Opinion in Behavioral Sciences, 2021 - Elsevier\n… lines of\nresearch\nin the study of biological intelligence within the lens of\nmeta\n-learning,\nplacing these works into a common framework. More recent points of interaction between\nAI\nand …\nSave\nCite\nCited by 219\nRelated articles\nAll 5 versions\n[HTML]\nsciencedirect.com\n[HTML]\n[HTML]\nMAILS-\nMeta AI\nliteracy scale: Development and testing of an\nAI\nliteracy questionnaire based on well-founded competency models and psychological change …\nA Carolus\n,\nMJ Koch\n,\nS Straka\n,\nME Latoschik\n… - Computers in Human …, 2023 - Elsevier\n… Create\nAI\nas a separate construct, and\nAI\nSelf-efficacy in learning and problem-solving and\nAI\nSelf-management (ie,\nAI\n… This study contributes to the\nresearch\non\nAI\nliteracy by providing a …\nSave\nCite\nCited by 248\nRelated articles\nAll 6 versions\n[HTML]\nsciencedirect.com\n[HTML]\n[HTML]\nData issues in industrial\nAI\nsystems: A\nmeta\n-review and\nresearch\nstrategy\nX Li,\nY Cheng\n,\nC Møller\n,\nJ Lee\n- Computers in Industry, 2025 - Elsevier\n… a comprehensive\nmeta\n-review of data issues and corresponding methods in industrial\nAI\n. …\nTo supplement the existing\nresearch\nthat focuses more on data issues arising in historical …\nSave\nCite\nCited by 3\nRelated articles\nAll 2 versions\nRelated searches\nmeta ai\nliteracy scale\nmetaverse\nai research\nfacebook\nai research\ngoogle\nai research\nmeta ai research\nethics\nmeta ai\nmarketing\nresearch\nchatgpt\nresearch\nnetflix\nai research\n[PDF]\narxiv.org\nDetecting\nai\ntrojans using\nmeta\nneural analysis\nX Xu\n,\nQ Wang\n,\nH Li\n,\nN Borisov\n… - 2021 IEEE Symposium …, 2021 - ieeexplore.ieee.org\n… This paper addresses these challenges by introducing a\nMeta\nNeural Trojan Detection (… to\ntrain a\nmeta\n-classifier that predicts whether a given target model is Trojaned. To train the\nmeta\n-…\nSave\nCite\nCited by 426\nRelated articles\nAll 6 versions\n[PDF]\nnature.com\nQuality assessment standards in\nartificial intelligence\ndiagnostic accuracy systematic reviews: a\nmeta\n-\nresearch\nstudy\nS Jayakumar,\nV Sounderajah\n,\nP Normahani\n… - NPJ Digital …, 2022 - nature.com\n… a\nmeta\n-\nresearch\nstudy evaluating adherence to the Quality Assessment of Diagnostic\nAccuracy Studies 2 (QUADAS-2) tool within\nAI\n… tools in reviews of\nAI\n-based diagnostic accuracy …\nSave\nCite\nCited by 98\nRelated articles\nAll 8 versions\n[PDF]\nnature.com\nWhen combinations of humans and\nAI\nare useful: A systematic review and\nmeta\n-analysis\nM Vaccaro\n,\nA Almaatouq\n,\nT Malone\n- Nature Human Behaviour, 2024 - nature.com\n… In the context of our\nmeta\n-analysis, publication bias may occur if\nresearchers\npublish\nexperiments that show evidence of significant human–\nAI\nsynergy more frequently than those that …\nSave\nCite\nCited by 190\nRelated articles\nAll 12 versions\n[PDF]\nucl.ac.uk\nPRISMA\nAI\nreporting guidelines for systematic reviews and\nmeta\n-analyses on\nAI\nin healthcare\nGE Cacciamani\n,\nTN Chu\n, DI Sanford, A Abreu… - Nature medicine, 2023 - nature.com\n… in publications, delivers a tool for training\nresearchers\nin\nAI\nmethodology, and supports\nend-users of systematic reviews such as clinicians,\nresearchers\n, patients, and policymakers to …\nSave\nCite\nCited by 102\nRelated articles\nAll 4 versions\n[PDF]\nresearchgate.net\n[PDF]\n[PDF]\nA Review Article–Facebook's\nMeta AI\n: A Potential Boon or Doom for Learning?\nG Nolasco\n,\nJD Dicuangco\n- researchgate.net\n… such as\nresearch\n, reflection document, and reports in their subjects. Another focal point of\nthe advantages of\nMeta\nAI\nis the multilingual capabilities. This feature enables the\nAI\ntool to …\nSave\nCite\nRelated articles\nView as HTML\n[PDF]\narxiv.org\nA\nmeta\n-analysis of the utility of explainable\nartificial intelligence\nin human-\nAI\ndecision-making\nM Schemmer\n,\nP Hemmer\n, M Nitsche,\nN Kühl\n… - … /ACM Conference on\nAI\n…, 2022 - dl.acm.org\n… Therefore, in this article, we present an initial synthesis of existing\nresearch\non XAI studies\nusing a statistical\nmeta\n-analysis to derive implications across existing\nresearch\n. We observe a …\nSave\nCite\nCited by 111\nRelated articles\nAll 9 versions\nCreate alert\nPrevious\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nNext\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nPrivacy\nTerms\nHelp\nAbout Scholar\nSearch help",
    "raw_html": "<!DOCTYPE html>\n<html><head><title>Google Scholar</title><meta content=\"text/html;charset=utf-8\" http-equiv=\"Content-Type\"/><meta content=\"IE=Edge\" http-equiv=\"X-UA-Compatible\"/><meta content=\"origin-when-cross-origin\" name=\"referrer\"/><meta content=\"width=device-width,initial-scale=1,minimum-scale=1,maximum-scale=2\" name=\"viewport\"/><meta content=\"telephone=no\" name=\"format-detection\"/><link href=\"/favicon.ico\" rel=\"shortcut icon\"/><style>html,body,form,table,div,h1,h2,h3,h4,h5,h6,img,ol,ul,li,button{margin:0;padding:0;border:0;}table{border-collapse:collapse;border-width:0;empty-cells:show;}html,body{height:100%}#gs_top{position:relative;box-sizing:border-box;min-height:100%;min-width:964px;-webkit-tap-highlight-color:rgba(0,0,0,0);}#gs_top>*:not(#x){-webkit-tap-highlight-color:rgba(204,204,204,.5);}.gs_el_ph #gs_top,.gs_el_ta #gs_top{min-width:320px;}#gs_top.gs_nscl{position:fixed;width:100%;}body,td,input,button,textarea{font-size:13px;font-family:Arial,sans-serif;line-height:1.24;}body{background:#fff;color:#222;-webkit-text-size-adjust:100%;-moz-text-size-adjust:none;}.gs_gray{color:#777777}.gs_red{color:#dd4b39}.gs_grn{color:#006621}.gs_lil{font-size:11px}.gs_med{font-size:16px}.gs_hlt{font-weight:bold;}a:link{color:#1a0dab;text-decoration:none}a:visited{color:#660099;text-decoration:none}a:hover,a:hover .gs_lbl{text-decoration:underline}a:active,a:active .gs_lbl,a .gs_lbl:active{color:#d14836}.gs_el_tc a:hover,.gs_el_tc a:hover .gs_lbl{text-decoration:none}.gs_pfcs a:focus,.gs_pfcs button:focus,.gs_pfcs input:focus,.gs_pfcs label:focus{outline:none}.gs_a,.gs_a a:link,.gs_a a:visited{color:#006621}.gs_a a:active{color:#d14836}a.gs_fl:link,.gs_fl a:link{color:#1a0dab}a.gs_fl:visited,.gs_fl a:visited{color:#660099}a.gs_fl:active,.gs_fl a:active{color:#d14836}.gs_fl{color:#777777}.gs_ctc,.gs_ctu{vertical-align:middle;font-size:11px;font-weight:bold}.gs_ctc{color:#1a0dab}.gs_ctg,.gs_ctg2{font-size:13px;font-weight:bold}.gs_ctg{color:#1a0dab}a.gs_pda,.gs_pda a{padding:7px 0 5px 0}.gs_alrt{background:#f9edbe;border:1px solid #f0c36d;padding:0 16px;text-align:center;box-shadow:0 2px 4px rgba(0,0,0,.2);border-radius:2px;}.gs_alrt:empty{display:none;}.gs_spc{display:inline-block;width:12px}.gs_br{width:0;font-size:0}.gs_ibl{display:inline-block;}.gs_scl:after{content:\"\";display:table;clear:both;}.gs_ind{padding-left:8px;text-indent:-8px}.gs_ico,.gs_icm{display:inline-block;background:no-repeat url(/intl/en/scholar/images/1x/sprite_20161020.png);background-position:-23px -161px;background-size:169px;width:21px;height:21px;}@media(-webkit-min-device-pixel-ratio:1.5),(min-resolution:144dpi){.gs_ico,.gs_icm{background-image:url(/intl/en/scholar/images/2x/sprite_20161020.png);}}.gs_el_ta .gs_nta,.gs_ota,.gs_el_ph .gs_nph,.gs_oph{display:none}.gs_el_ta .gs_ota,.gs_el_ph .gs_oph{display:inline}.gs_el_ta div.gs_ota,.gs_el_ph div.gs_oph{display:block}.gs_sth_g{visibility:hidden;max-height:0;}.gs_sth_vis .gs_sth_g{max-height:1000px;}.gs_sth_vis .gs_sth_b{position:fixed;top:0;}.gs_sth_trk .gs_sth_b{position:absolute;top:auto;}@keyframes gs_anm_spin{0%{transform:rotate(0deg);}100%{transform:rotate(360deg);}}@keyframes gs_anm_fade_in{0%{opacity:0;}100%{opacity:1;}}.gs_invis{visibility:hidden;}.gs_rimg{display:block;background-color:#e5e5e5;border-radius:50%;overflow:hidden;position:relative;z-index:1;}.gs_rimg>img{position:absolute;margin:auto;left:0;top:0;bottom:0;right:0;}.gs_in_txtw{display:inline-block;vertical-align:middle;}.gs_in_txtb{display:block;}.gs_in_txt{color:#000;background-color:#fff;font-size:16px;box-sizing:border-box;height:29px;line-height:23px;border:1px solid #d9d9d9;border-top-color:#c0c0c0;padding:3px 6px 1px 8px;border-radius:1px;outline:none;-webkit-appearance:none;-moz-appearance:none;}.gs_el_tc .gs_in_txt{font-size:18px;}.gs_in_txtb .gs_in_txt{width:100%;}.gs_in_rnd .gs_in_txt{border-radius:14.5px;padding:3px 12px 1px 12px;}.gs_in_txt:hover{border-color:#b9b9b9;border-top-color:#a0a0a0;box-shadow:inset 0 1px 2px rgba(0,0,0,.1);}.gs_in_txte .gs_in_txt{border-color:#dd4b39;}.gs_in_txt:focus{border-color:#4d90fe;box-shadow:inset 0 1px 2px rgba(0,0,0,.3);}.gs_in_txt:disabled{color:#b8b8b8;border-color:#f1f1f1;box-shadow:none;}.gs_in_txtm .gs_in_txt{font-size:13px;height:24px;line-height:16px;padding:3px 6px;}.gs_in_txtm.gs_in_rnd .gs_in_txt{border-radius:12px;}.gs_el_tc .gs_in_txtm .gs_in_txt{height:29px;line-height:21px;}.gs_el_tc .gs_in_txtm.gs_in_rnd .gs_in_txt{border-radius:14.5px;}.gs_in_txtl .gs_in_txt{height:41px;padding:9px 43px;}.gs_in_txtl.gs_in_rnd .gs_in_txt{border-radius:20.5px;}.gs_in_txts{font-size:13px;line-height:18px;color:#666;}.gs_in_txts:not(:empty){margin-top:2px;}.gs_in_txte .gs_in_txts{color:#dd4b39;}button{position:relative;z-index:1;box-sizing:border-box;font-size:13px;cursor:pointer;height:29px;line-height:normal;min-width:72px;padding:0 8px;color:#444;border:1px solid rgba(0,0,0,.1);border-radius:3px;text-align:center;background-color:#f5f5f5;-webkit-user-select:none;user-select:none;}button.gs_btn_rnd{border-radius:14px;padding:0 12px;}button.gs_btn_rnd.gs_btn_rndci{padding-left:4px;}button.gs_btn_lrge{height:41px;min-width:82px;padding:0 9px;}button.gs_btn_lrge.gs_btn_lrge_asym{padding-left:5px;padding-right:8px;}button.gs_btn_lrge.gs_btn_rnd{border-radius:20px;padding:0 16px;}button.gs_btn_lrge.gs_btn_rnd.gs_btn_rndci{padding-left:10px;}button.gs_btn_cir{border-radius:14.5px;min-width:29px;}button.gs_btn_lrge.gs_btn_cir{border-radius:20.5px;min-width:41px;}button.gs_btn_mini{padding:0;border:0;}.gs_el_ph button.gs_btn_mph,.gs_el_ta button.gs_btn_mta{height:41px;}button .gs_wr{position:relative;display:inline-block;width:100%;height:100%;}button .gs_wr:before{content:\"\";width:0;height:100%;}button .gs_wr:before,button .gs_ico,button .gs_rdt,button .gs_lbl,button .gs_icm{display:inline-block;vertical-align:middle;}button .gs_wr{font-size:13px;text-transform:none;}.gs_btn_lrge .gs_wr{font-size:15px;}.gs_btn_lsb .gs_wr{font-size:11px;font-weight:bold;}.gs_btn_lsu .gs_wr{font-size:11px;text-transform:uppercase;}.gs_btn_lrge.gs_btn_lsb .gs_wr,.gs_btn_lrge.gs_btn_lsu .gs_wr,.gs_btn_lrge.gs_btn_lrge_asym .gs_wr{font-size:13px;}.gs_btn_half,.gs_el_ta .gs_btn_hta,.gs_el_ph .gs_btn_hph{min-width:36px;}.gs_btn_lrge.gs_btn_half,.gs_el_ta .gs_btn_lrge.gs_btn_hta,.gs_el_ph .gs_btn_lrge.gs_btn_hph,.gs_el_ta .gs_btn_mta,.gs_el_ph .gs_btn_mph{min-width:41px;}.gs_btn_slt{border-radius:3px 0 0 3px;}.gs_btn_srt{margin-left:-1px;border-radius:0 3px 3px 0;}.gs_btn_smd{margin-left:-1px;border-radius:0;}button:hover{z-index:2;color:#222;border-color:rgba(0,0,0,.2);background-color:#f8f8f8;}button.gs_sel{background-color:#dcdcdc;}button:active{z-index:2;background-color:#f1f1f1;}button:focus{z-index:2;}button::-moz-focus-inner{padding:0;border:0}button:-moz-focusring{outline:1px dotted ButtonText}.gs_pfcs button:-moz-focusring{outline:none}a.gs_in_ib{position:relative;display:inline-block;line-height:16px;padding:6px 0 7px 0;-webkit-user-select:none;user-select:none;}a.gs_btn_lrge{height:40px;padding:0;}a.gs_in_bgcw{min-width:41px;}a.gs_btn_lrge.gs_in_bgcw:before{position:absolute;content:\"\";height:29px;width:29px;top:6px;left:6px;background-color:#fff;box-shadow:0 1px 3px rgb(0,0,0,.4);border-radius:50%;}a.gs_in_bgcw:hover:before{background-color:#f5f5f5;}a.gs_in_bgcw:active:before{background-color:#e5e5e5;}a.gs_in_bgcw.gs_dis:before{background-color:#fff;}a.gs_in_ib .gs_lbl{display:inline-block;padding-left:21px;color:#222;}a.gs_in_ib.gs_in_gray .gs_lbl{color:#444;}a.gs_in_ib .gs_lbl:not(:empty){padding-left:29px;}button.gs_in_ib .gs_lbl:not(:empty){padding-left:4px;}a.gs_in_ib:active .gs_lbl,a.gs_in_ib .gs_lbl:active,a.gs_in_ib :active~.gs_lbl{color:#d14836;}.gs_el_ta .gs_btn_hta .gs_lbl,.gs_el_ph .gs_btn_hph .gs_lbl,.gs_el_ta .gs_btn_mta .gs_lbl,.gs_el_ph .gs_btn_mph .gs_lbl,.gs_el_ta .gs_btn_cta .gs_lbl,.gs_el_ph .gs_btn_cph .gs_lbl{display:none;}a.gs_in_ib .gs_ico{position:absolute;top:3px;left:0;}.gs_in_ib.gs_md_li .gs_ico{left:14px;}.gs_el_tc .gs_in_ib.gs_md_li .gs_ico{top:11px;}.gs_in_ib.gs_md_li.gs_md_lix .gs_ico{top:10px;left:16px;}a.gs_btn_lrge .gs_ico{top:50%;left:50%;margin:-10.5px 0 0 -10.5px;}.gs_in_ib .gs_ico{opacity:.55;}.gs_in_ib:hover .gs_ico{opacity:.72;}.gs_in_ib:active .gs_ico,.gs_in_ib .gs_ico:active,.gs_in_ib :active~.gs_ico{opacity:1;}.gs_in_ib:disabled .gs_ico,.gs_in_ib.gs_dis .gs_ico{opacity:.28;}.gs_in_ib.gs_btn_act .gs_ico,.gs_in_ib.gs_btn_cre .gs_ico{opacity:1;}.gs_btn_act:disabled .gs_ico,.gs_btn_cre:disabled .gs_ico{opacity:.72;}.gs_rdt{position:relative;width:0;height:21px;}a.gs_in_ib .gs_rdt{left:21px;}.gs_rdt:before{content:\"\";position:absolute;top:1px;right:0;width:5px;height:5px;border:1px solid #fff;border-radius:50%;background-color:#dd4b39;}.gs_notf{display:inline-block;vertical-align:top;margin-left:8px;width:16px;line-height:16px;background-color:#d14836;border-radius:50%;color:#fff;text-align:center;font-size:9px;font-weight:bold;}.gs_notf:empty{display:none;}.gs_ind .gs_notf{text-indent:0;}button.gs_btn_flat{border-color:transparent;background-color:transparent;}button.gs_btn_olact{color:#4d90fe;background-color:transparent;}button.gs_btn_flat:hover,button.gs_btn_olact:hover{background-color:rgba(0,0,0,.05);}button.gs_btn_flat:active,button.gs_btn_olact:active{background-color:rgba(0,0,0,.1);}button.gs_btn_flat.gs_btn_flact{color:#1a0dab;}button.gs_btn_lra{color:#000;-webkit-font-smoothing:antialiased;background-color:#d3e3fd;border:none;border-radius:100px;}button.gs_btn_lra:hover{background-color:#c3d8fa;}button.gs_btn_lra:active{background-color:#bad1f7;}button.gs_btn_act{color:#fff;-webkit-font-smoothing:antialiased;background-color:#4d90fe;}button.gs_btn_act:hover{color:#fff;background-color:#3983fe;}button.gs_btn_act.gs_sel{background-color:#2f6bcc;}button.gs_btn_act:active{background-color:#357ae8;}button.gs_btn_cre{color:#fff;-webkit-font-smoothing:antialiased;background-color:#d14836;}button.gs_btn_cre:hover{color:#fff;background-color:#c53727;}button.gs_btn_cre.gs_sel{background-color:#992b1e;}html:not(.gs_pfcs) .gs_btn_act:focus:not(:active){box-shadow:inset 0 0 0 1px rgba(255,255,255,.5);}button.gs_btn_cre:active{background-color:#b0281a;}button.gs_btn_hov_nobg:hover,button.gs_btn_hov_nobg:active{border:none;background:transparent;}button:disabled,button:disabled:hover,button:disabled:active{cursor:default;color:#b8b8b8;border-color:rgba(0,0,0,.05);background-color:transparent;z-index:0;}button.gs_btn_flat:disabled{color:#b8b8b8;border-color:transparent;}button.gs_btn_act:disabled{color:#fff;background-color:#a6c8ff;}button.gs_btn_cre:disabled{color:#fff;background-color:#e8a49b;}a.gs_in_ib.gs_dis{cursor:default;pointer-events:none}a.gs_in_ib.gs_dis .gs_lbl{color:#b8b8b8;text-decoration:none}.gs_ttp{position:absolute;top:100%;right:50%;z-index:10;pointer-events:none;visibility:hidden;opacity:0;transition:visibility 0s .13s,opacity .13s ease-out;}button:hover .gs_ttp,button:focus .gs_ttp,a:hover .gs_ttp,a:focus .gs_ttp{transition:visibility 0s .3s,opacity .13s ease-in .3s;visibility:visible;opacity:1;}.gs_md_tb.gs_sel .gs_ttp{transition:none;visibility:hidden;}button.gs_btn_lrge.gs_btn_cir .gs_ttp{top:75%;}.gs_ttp .gs_aro,.gs_ttp .gs_aru{position:absolute;top:-2px;right:-5px;width:0;height:0;line-height:0;font-size:0;border:5px solid transparent;border-top:none;border-bottom-color:#595959;z-index:1;}.gs_ttp .gs_aro{top:-3px;right:-6px;border-width:6px;border-top:none;border-bottom-color:white;}.gs_ttp .gs_txt{display:block;position:relative;top:2px;right:-50%;padding:4px 6px;background:#595959;color:white;font-size:11px;font-weight:bold;line-height:normal;white-space:nowrap;border:1px solid white;border-radius:3px;box-shadow:inset 0 1px 4px rgba(0,0,0,.2);}.gs_press,.gs_in_se,.gs_tan{touch-action:none;}.gs_in_se .gs_lbl:not(:empty){padding-right:14px;}.gs_in_se .gs_icm{position:absolute;top:50%;margin-top:-5.5px;right:0;width:7px;height:11px;background-position:-21px -88px;opacity:.55;}.gs_in_se:hover .gs_icm{opacity:.72;}.gs_in_se:active .gs_icm{opacity:1;}.gs_in_se:disabled .gs_icm{opacity:.28;}.gs_el_ta .gs_btn_hta .gs_icm,.gs_el_ph .gs_btn_hph .gs_icm,.gs_el_ta .gs_btn_mta .gs_icm,.gs_el_ph .gs_btn_mph .gs_icm,.gs_el_ta .gs_btn_cta .gs_icm,.gs_el_ph .gs_btn_cph .gs_icm{display:none;}.gs_btn_mnu .gs_icm{margin-top:-3.5px;height:7px;background-position:0 -110px;}.gs_in_se.gs_btn_act .gs_icm,.gs_in_se.gs_btn_cre .gs_icm{margin-top:-3.5px;height:7px;background-position:-42px -44px;opacity:1;}.gs_btn_act:disabled .gs_icm,.gs_btn_cre:disabled .gs_icm{opacity:.72;}button.gs_btnG .gs_ico{width:21px;height:21px;background-position:-92px -253px;}button .gs_bs{position:absolute;top:50%;left:50%;margin-top:-10px;margin-left:-10px;box-sizing:border-box;width:20px;height:20px;border-radius:50%;border:2px solid #eee;border-top-color:#4d90fe;visibility:hidden;animation:gs_anm_spin .8s linear infinite;}button.gs_bsp .gs_bs{visibility:visible;transition:visibility 0s .4s;}.gs_md_d{text-transform:none;white-space:nowrap;position:absolute;top:0;left:0;border:1px solid #ccc;border-color:rgba(0,0,0,.2);background:#fff;box-shadow:0 2px 4px rgba(0,0,0,.2);z-index:1100;text-align:left;visibility:hidden;max-height:0;margin-top:-1000px;opacity:0;transition:opacity .13s,visibility 0s .13s,max-height 0s .13s,margin-top 0s .13s;}.gs_md_d.gs_vis{visibility:visible;max-height:10000px;margin-top:0;opacity:1;transition:all 0s;}.gs_el_tc .gs_md_d{transform-origin:100% 0;transform:scale(1,0);transition:opacity .218s ease-out,transform 0s .218s,visibility 0s .218s,max-height 0s .218s,margin-top 0s .218s;}.gs_el_ios .gs_md_d{-webkit-backface-visibility:hidden;}.gs_el_tc .gs_md_d.gs_ttzi{transform-origin:50% 50%;transform:scale(0,0);}.gs_el_tc .gs_md_d.gs_ttzr{transform:scale(0,0);}.gs_el_tc .gs_md_d.gs_vis{transform:scale(1,1);transition:transform .218s ease-out;}.gs_md_r{position:relative;display:inline-block;}.gs_md_rmb>.gs_md_d{top:29px}.gs_md_rmbl>.gs_md_d{top:41px}.gs_md_ul{list-style-type:none;word-wrap:break-word;display:inline-block;vertical-align:top;}.gs_md_ul.gs_md_ul_tb{display:block;}.gs_md_li,.gs_in_cb.gs_md_li,.gs_md_li:link,.gs_md_li:visited{display:block;padding:6px 44px 6px 16px;font-size:13px;line-height:16px;color:#222;cursor:pointer;text-decoration:none;position:relative;z-index:0;}a.gs_md_li:hover .gs_lbl,a.gs_md_li:active .gs_lbl{text-decoration:none}.gs_el_tc .gs_md_li{padding-top:14px;padding-bottom:10px;}.gs_md_li.gs_md_lix{font-size:16px;line-height:20px;padding:12px 16px 8px 16px;}.gs_md_li:before{content:\"\";background-color:#f1f1f1;position:absolute;left:0;right:0;top:0;bottom:0;opacity:0;transition:opacity .13s;z-index:-1;}.gs_md_li:hover:before,.gs_md_li:focus:before{opacity:1;transition:all 0s;}a.gs_in_ib.gs_md_li .gs_lbl{color:#222}a.gs_in_ib.gs_md_li.gs_in_gray .gs_lbl{color:#444}.gs_md_li:active:before{background-color:#ddd}.gs_md_li.gs_sel,a.gs_in_ib.gs_md_li.gs_sel .gs_lbl{color:#d14836}.gs_md_d:focus,.gs_md_li:focus{outline:none}a.gs_md_lix .gs_lbl,a.gs_md_lix .gs_lbl:not(:empty){padding:0 0 0 40px;}a.gs_in_cb:link,a.gs_in_cb:visited,a.gs_in_cb:active,a.gs_in_cb:hover{cursor:pointer;color:#222;text-decoration:none;}.gs_in_cb,.gs_in_ra{position:relative;line-height:16px;display:inline-block;-webkit-user-select:none;user-select:none;}.gs_in_cb.gs_md_li{padding:6px 44px 6px 16px;}.gs_in_cb input,.gs_in_ra input{position:absolute;top:1px;left:1px;width:15px;height:15px;margin:0;padding:0;opacity:0;z-index:2;}.gs_in_ra input{top:0;left:0}.gs_el_tc .gs_in_cb input{top:9px}.gs_el_tc .gs_in_ra input{top:8px}.gs_in_cb.gs_in_cbj input{top:15px;left:15px}.gs_in_cb label,.gs_in_cb .gs_lbl,.gs_in_ra label{display:inline-block;padding-left:21px;min-height:16px;}.gs_in_ra_lrge{font-size:15px;}.gs_in_cb label:empty:before,.gs_in_cb .gs_lbl:empty:before,.gs_in_ra label:empty:before{content:\"\\200b\";}.gs_el_tc .gs_in_cb label,.gs_el_tc .gs_in_cb .gs_lbl,.gs_el_tc .gs_in_ra label{padding-top:8px;padding-bottom:5px;}.gs_in_cb.gs_in_cbj label,.gs_in_cb.gs_in_cbj .gs_lbl{padding:13px 0 12px 41px;}.gs_in_cbb,.gs_in_cbb label,.gs_in_cbb .gs_lbl{display:block;}.gs_in_cb .gs_cbx,.gs_in_ra .gs_cbx{position:absolute}.gs_in_cb .gs_cbx{top:2px;left:2px;width:11px;height:11px;border:1px solid #c6c6c6;border-radius:1px;}.gs_md_li .gs_cbx{top:8px;left:18px}.gs_el_tc .gs_in_cb .gs_cbx{top:10px}.gs_el_tc .gs_md_li .gs_cbx{top:16px}.gs_in_cb.gs_in_cbj .gs_cbx{top:15px;left:15px}.gs_el_tc .gs_in_ra .gs_cbx{top:8px}.gs_in_ra .gs_cbx{top:0;left:0;border:1px solid #c6c6c6;width:13px;height:13px;border-radius:7px;}.gs_in_cb:hover .gs_cbx,.gs_in_ra:hover .gs_cbx{border-color:#666;box-shadow:inset 0 1px 1px rgba(0,0,0,.1);}button.gs_in_cb:hover .gs_cbx{border-color:#c6c6c6;}.gs_in_cb :focus~label,.gs_in_ra :focus~label{outline:1px dotted #222;outline:auto -webkit-focus-ring-color;}.gs_pfcs .gs_in_cb :focus~label,.gs_pfcs .gs_in_ra :focus~label{outline:none;}.gs_in_cb:active .gs_cbx,.gs_in_ra:active .gs_cbx,.gs_in_cb .gs_cbx:active,.gs_in_ra .gs_cbx:active,.gs_in_cb :active~.gs_cbx,.gs_in_ra :active~.gs_cbx{border-color:#666;background-color:#ebebeb;}button.gs_in_cb:active .gs_cbx{border-color:#a6a6a6;}.gs_in_cb :disabled~.gs_cbx,.gs_in_ra :disabled~.gs_cbx,button.gs_in_cb:disabled .gs_cbx{border-color:#f1f1f1;box-shadow:none;}.gs_in_cb :disabled~label,.gs_in_ra :disabled~label{color:#b8b8b8;}.gs_in_cb.gs_err .gs_cbx{border-color:#eda29b;}.gs_in_cb .gs_chk,.gs_in_ra .gs_chk{position:absolute;z-index:1;top:-3px;left:-2px;width:21px;height:21px;}.gs_md_li .gs_chk{top:3px;left:14px}.gs_el_tc .gs_in_cb .gs_chk{top:5px}.gs_el_tc .gs_md_li .gs_chk{top:11px}.gs_in_cb.gs_in_cbj .gs_chk{top:10px;left:11px}.gs_in_ra .gs_chk{top:4px;left:4px;width:7px;height:7px;border-radius:4px;}.gs_el_tc .gs_in_ra .gs_chk{top:12px}.gs_in_cb input:checked~.gs_chk,.gs_in_cb.gs_sel .gs_chk{background:no-repeat url(/intl/en/scholar/images/1x/sprite_20161020.png) -69px -67px;opacity:.62;}.gs_in_ra input:checked~.gs_chk{background-color:#666}.gs_in_cb.gs_par .gs_chk{background:no-repeat url(/intl/en/scholar/images/1x/sprite_20161020.png) -21px -44px;opacity:.55;}@media(-webkit-min-device-pixel-ratio:1.5),(min-resolution:144dpi){.gs_in_cb input:checked~.gs_chk,.gs_in_cb.gs_sel .gs_chk,.gs_in_cb.gs_par .gs_chk{background-image:url(/intl/en/scholar/images/2x/sprite_20161020.png);background-size:169px;}}.gs_in_cb input:checked:disabled~.gs_chk{opacity:.22}.gs_in_ra input:checked:disabled~.gs_chk{background-color:#f1f1f1}.gs_md_ac{position:absolute;top:28px;left:0;right:0;z-index:1100;white-space:normal;display:none;pointer-events:none;}.gs_md_ac[dir=\"ltr\"]{text-align:left;}.gs_md_ac[dir=\"rtl\"]{text-align:right;}.gs_md_ac ul{list-style-type:none;word-wrap:break-word;line-height:1.24;border:1px solid #e5e5e5;border-color:rgba(0,0,0,.2);background:#fff;box-shadow:0px 2px 4px rgba(0,0,0,.2);touch-action:manipulation;cursor:pointer;-webkit-user-select:none;user-select:none;pointer-events:auto;}.gs_md_acp{display:flex;line-height:0;}.gs_md_acp .gs_md_acs,.gs_md_acp ul{max-width:100%;box-sizing:border-box;display:inline-block;vertical-align:top;}.gs_md_acs{visibility:hidden;white-space:pre;height:0;min-width:0%;flex:0 1 auto;font-size:16px;}.gs_el_tc .gs_md_acs{font-size:18px;}.gs_md_acp ul{white-space:nowrap;flex:0 0 auto;}.gs_md_ac li{position:relative;padding:2px 8px;font-size:16px;line-height:20px;color:#222;background-color:#fff;overflow:hidden;text-overflow:ellipsis;}.gs_md_ac li.gs_sel{color:#000;background-color:#c6dafc;}.gs_md_ac li:active{background-color:#e8f0fe;}.gs_el_ios .gs_md_ac li:active{background-color:#fff;}.gs_md_ac li.gs_md_ac_lh,.gs_md_ac li.gs_md_ac_lh b{color:#660099;}.gs_el_tc .gs_md_ac li{padding:11px 8px 9px 8px;font-size:18px;border-top:1px solid #e5e5e5;}.gs_el_tc .gs_md_ac li:first-child{border-top:none;}.gs_md_ac[dir=\"ltr\"] li.gs_md_ac_lh{padding-right:29px;}.gs_md_ac[dir=\"rtl\"] li.gs_md_ac_lh{padding-left:29px;}.gs_el_tc .gs_md_ac[dir=\"ltr\"] li.gs_md_ac_lh{padding-right:49px;}.gs_el_tc .gs_md_ac[dir=\"rtl\"] li.gs_md_ac_lh{padding-left:49px;}.gs_md_ac_lh .gs_ico_X{position:absolute;top:0;}.gs_md_ac[dir=\"ltr\"] .gs_md_ac_lh .gs_ico_X{right:0;}.gs_md_ac[dir=\"rtl\"] .gs_md_ac_lh .gs_ico_X{left:0;}.gs_el_tc #gs_top .gs_md_ac .gs_md_ac_lh .gs_ico_Xt{padding:10px;}.gs_md_ac_lh .gs_ico_X:hover{background-color:#eee;}.gs_ico_x{background-position:-113px -22px;opacity:.55;}.gs_ico_x:hover{opacity:.72;}.gs_ico_x:active{opacity:1;}.gs_ico_X{background-position:-71px 0;opacity:.55;}.gs_ico_X:hover{opacity:.72;}.gs_ico_X:active{opacity:1;}.gs_btnX .gs_ico{background-position:-71px 0;}.gs_el_tc .gs_ico_Xt{background-origin:content-box;background-clip:content-box;padding:10px 6px 10px 14px;}.gs_ico_P{background-position:0 0;opacity:.55;}.gs_ico_P:hover{opacity:.72;}.gs_ico_P:active{opacity:1;}.gs_btnPLS .gs_ico{background-position:0 0;}.gs_btnP .gs_ico{background-position:-21px 0;}.gs_btnC .gs_ico{background-position:0 -66px;}.gs_btnL .gs_ico{background-position:-92px -44px;}.gs_ico_LB{background-position:-50px -44px;height:16px;}.gs_btnJ .gs_ico{background-position:-92px -22px;}.gs_btnM .gs_ico{background-position:-92px 0;}.gs_btnMW .gs_ico{background-position:-21px -22px;}.gs_btnSB .gs_ico{background-position:0 -44px;}.gs_btnTSB .gs_ico{background-position:-115px -253px;}.gs_btnPL .gs_ico{background-position:-148px -66px;}.gs_btnPR .gs_ico{background-position:-21px -66px;}.gs_btnPLW .gs_ico{background-position:-0 -230px;}.gs_btnPRW .gs_ico{background-position:-23px -230px;}.gs_btnZI .gs_ico{background-position:-148px -22px;}.gs_btnZO .gs_ico{background-position:-127px -44px;}.gs_btnDE .gs_ico{background-position:-134px 0;}.gs_btnFI .gs_ico{background-position:-50px -66px;}.gs_btnAD .gs_ico{background-position:-141px -88px;opacity:.55;}.gs_btnAD:hover .gs_ico{opacity:.72;}.gs_btnAD:active .gs_ico,.gs_btnAD .gs_ico:active,.gs_btnAD :active~.gs_ico{opacity:1;}.gs_btnBA .gs_ico{background-position:-50px -22px;}.gs_btnADD .gs_ico{background-position:-92px -66px;}.gs_btnMRG .gs_ico{background-position:-113px 0;}.gs_btnLBL .gs_ico{background-position:0 -161px;}.gs_btnCNCL .gs_ico{background-position:-71px 0;}.gs_btnDWL .gs_ico{background-position:-28px -88px;}.gs_btnMNU .gs_ico{background-position:0 -88px;}.gs_btnMNT .gs_ico{background-position:-46px -161px;}.gs_btnALT .gs_ico{background-position:-92px -161px;}.gs_btnART .gs_ico{background-position:-115px -161px;}.gs_btnGSL .gs_ico{background-position:-69px -161px;}.gs_btnCLS .gs_ico{background-position:-138px -161px;}.gs_btnXBLU .gs_ico{background-position:-138px -253px;}.gs_btnSSB .gs_ico{background-position:0 -276px;}.gs_btnSSW .gs_ico{background-position:-23px -276px;}.gs_btnFLT .gs_ico{background-position:0 -184px;}.gs_btnXT .gs_ico{background-position:-46px -184px;}.gs_btnPD .gs_ico{background-position:-69px -184px;}.gs_btnPU .gs_ico {background-position:-92px -276px;}.gs_btnCP .gs_ico{background-position:-92px -184px;}.gs_btnTP .gs_ico{background-position:-138px -184px;}.gs_btnML .gs_ico{background-position:-115px -276px;}.gs_btnCHK .gs_ico{background-position:-71px -66px;}.gs_btnDNB .gs_ico{background-position:-115px -230px;}.gs_btnDNW .gs_ico{background-position:0 -207px;}.gs_btnACA .gs_ico{background-position:-23px -207px;}.gs_btnAPT .gs_ico{background-position:-46px -207px;}.gs_btnAPTW .gs_ico{background-position:-92px -230px;}.gs_btnAFL .gs_ico{background-position:-69px -207px;}.gs_btnAN .gs_ico{background-position:-46px -276px;}.gs_btnAI .gs_ico{background-position:-69px -276px;}.gs_btnPBL .gs_ico{background-position:-92px -207px;}.gs_btnUCT .gs_ico{background-position:-115px -207px;}.gs_btnVRF .gs_ico{background-position:-138px -207px;}.gs_btnLSI .gs_ico{background-position:-46px -230px;}.gs_btnLSG .gs_ico{background-position:-69px -230px;}.gs_btnMOR .gs_ico{background-position:-23px -253px;}.gs_btnADV .gs_ico{background-position:-46px -253px;}.gs_btnPRO .gs_ico{background-position:-69px -253px;}.gs_btnLAB .gs_ico{background-position:-69px -345px;}.gs_ico_nav_previous{background-position:0 -119px;width:53px;height:40px;}.gs_ico_nav_first{background-position:-25px -119px;width:28px;height:40px;}.gs_ico_nav_current{background-position:-53px -119px;width:20px;height:40px;}.gs_ico_nav_page{background-position:-73px -119px;width:20px;height:40px;}.gs_ico_nav_next{background-position:-93px -119px;width:71px;height:40px;}.gs_ico_nav_last{background-position:-93px -119px;width:45px;height:40px;}.gs_ico_star{background-position:-71px -44px;width:13px;height:13px;}.gs_btnPLSW .gs_ico{background-position:-138px -230px;}.gs_btnPDF .gs_ico{background-position:0 -253px;}.gs_btnS .gs_ico{background-position:-138px -276px;}.gs_btnUNS .gs_ico{background-position:0 -299px;}.gs_btnMORR .gs_ico{background-position:-23px -299px;}.gs_btnTW .gs_ico{background-position:-46px -299px;}.gs_btnIN .gs_ico{background-position:-69px -299px;}.gs_btnFB .gs_ico{background-position:-92px -299px;}.gs_btnET .gs_ico{background-position:-115px -299px;}.gs_btnARC .gs_ico{background-position:-138px -299px;}.gs_btnOL .gs_ico{background-position:0px -322px;}.gs_btnFA .gs_ico{background-position:-23px -322px;}.gs_btnFAD .gs_ico{background-position:-46px -322px;}.gs_btnHP .gs_ico{background-position:-69px -322px;}.gs_btnPLM .gs_ico{background-position:-92px -322px;}.gs_btnPRM .gs_ico{background-position:-115px -322px;}.gs_btnRN .gs_ico{background-position:-138px -322px;}.gs_btnVF .gs_ico{background-position:0px -345px;}.gs_btnVP .gs_ico{background-position:-23px -345px;}.gs_btnSRT .gs_ico{background-position:-46px -345px;}#gs_md_s.gs_hdr_drs{transition:opacity .15s,visibility 0s .15s;}#gs_md_s.gs_hdr_drs.gs_vis{transition:opacity .15s,visibility 0s;}.gs_el_tc #gs_md_s.gs_hdr_drs{transition:opacity .218s,visibility 0s .218s;}.gs_el_tc #gs_md_s.gs_hdr_drs.gs_vis{transition:opacity .218s,visibility 0s;}#gs_hdr_drw{position:fixed;top:0;left:0;height:100%;z-index:1200;visibility:hidden;overflow:auto;width:228px;background-color:#fff;box-shadow:2px 2px 4px rgba(0,0,0,.15);outline:none;transform:translate(-100%,0);transition:transform .15s ease-in-out,visibility 0s .15s;}#gs_hdr_drw.gs_vis{visibility:visible;transform:translate(0,0);transition:transform .15s ease-in-out,visibility 0s;}.gs_el_tc #gs_hdr_drw{transition:transform .3s cubic-bezier(.4,0,.6,1),visibility 0s .3s;}.gs_el_tc #gs_hdr_drw.gs_vis{transition:transform .225s cubic-bezier(0,0,.2,1),visibility 0s;}#gs_top #gs_hdr_drw.gs_abt,#gs_top #gs_md_s.gs_abt{transition:none;}#gs_hdr_drw_in{position:relative;box-sizing:border-box;min-height:100%;padding:0 0 8px 0;}.gs_el_ta #gs_hdr_drw_in,.gs_el_ph #gs_hdr_drw_in{padding:0 0 65px 0;}#gs_hdr_drw_top{position:relative;height:63px;border-bottom:1px solid #e5e5e5;margin-bottom:8px;}.gs_el_ta #gs_hdr_drw_top,.gs_el_ph #gs_hdr_drw_top{height:57px;}#gs_hdr_drw_mnu,#gs_hdr_drw_lgo{position:absolute;top:0;height:100%;}#gs_hdr_drw_mnu{left:0;width:55px;}#gs_hdr_drw_lgo{left:56px;}.gs_hdr_drw_sec:before{display:block;content:\" \";height:0;border-bottom:1px solid #e5e5e5;margin:8px 0;}.gs_hdr_drw_sec:first-child:before{display:none;}#gs_hdr_drw_bot{display:none;}.gs_el_ta #gs_hdr_drw_bot,.gs_el_ph #gs_hdr_drw_bot{display:block;position:absolute;left:0;bottom:0;width:100%;height:65px;}#gs_hdr_drw_bot .gs_md_li:before{opacity:0;}#gs_hdr_drw_bot .gs_hdr_pp{display:block;position:absolute;bottom:14px;left:15px;pointer-events:none;}#gs_hdr_drw_bot .gs_lbl{display:block;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;}#gs_hdr{position:relative;height:63px;background-color:#f5f5f5;border-bottom:1px solid #e5e5e5;display:flex;}.gs_el_ta #gs_hdr,.gs_el_ph #gs_hdr{height:57px;}#gs_hdr_mnu,#gs_hdr_bck,#gs_hdr_lgo,#gs_hdr_lgt,#gs_hdr_md,#gs_hdr_sre,#gs_hdr_act{display:inline-block;vertical-align:top;position:relative;height:100%;flex:0 0 auto;}#gs_hdr_md{flex:1 1 auto;}#gs_hdr .gs_hdr_mbo,#gs_hdr .gs_hdr_mbo,.gs_el_ta #gs_hdr .gs_hdr_dso,.gs_el_ph #gs_hdr .gs_hdr_dso{display:none;}.gs_el_ta #gs_hdr .gs_hdr_mbo,.gs_el_ph #gs_hdr .gs_hdr_mbo{display:inline-block;}#gs_hdr_mnu,#gs_hdr_bck,#gs_hdr_sre{width:55px;margin-right:1px;}#gs_hdr_lgo,#gs_hdr_drw_lgo{width:149px;background:no-repeat url('/intl/en/scholar/images/1x/scholar_logo_24dp.png') 0% 50%;background-size:149px;}@media(-webkit-min-device-pixel-ratio:1.5),(min-resolution:144dpi){#gs_hdr_lgo,#gs_hdr_drw_lgo{background-image:url('/intl/en/scholar/images/2x/scholar_logo_24dp.png');}}#gs_hdr_lgo{margin-right:31px;}.gs_el_ph #gs_hdr_lgo{margin-right:0;}#gs_hdr_lgt{min-width:164px;margin-right:16px;}.gs_el_sm #gs_hdr_lgt:empty{min-width:60px;}#gs_hdr_md{margin-right:16px;min-width:1px;}#gs_hdr_lgt,#gs_hdr_md h1{padding:19px 0 0 0;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;font-size:20px;line-height:25px;font-weight:normal;color:#666;max-width:100%;text-align:left;}.gs_el_ta #gs_hdr_md h1,.gs_el_ph #gs_hdr_md h1{padding:16px 0 0 0;}#gs_hdr_srch{padding:14px 0 0 0;max-width:600px;}.gs_el_ta #gs_hdr_srch,.gs_el_ph #gs_hdr_srch{padding:10px 0 0 0;max-width:none;}#gs_hdr_frm{position:relative;padding-right:39px;}#gs_hdr_tsi{height:38px;border-radius:2px 0 0 2px;}#gs_hdr_tsi::-ms-clear{display:none;}#gs_hdr_tsc{display:none;position:absolute;top:3px;right:41px;width:21px;height:21px;padding:6px 10px 7px 10px;}.gs_in_acw[dir=\"rtl\"]~#gs_hdr_tsc{right:auto;left:1px;}#gs_hdr_tsb{position:absolute;top:0;right:0;width:40px;height:38px;border-radius:0 2px 2px 0;}#gs_hdr_frm_ac{top:37px;right:40px;}.gs_el_ph #gs_hdr_frm_ac{right:0;}.gs_el_ph .gs_hdr_ifc #gs_hdr_mnu,.gs_el_ph .gs_hdr_ifc #gs_hdr_bck,.gs_hdr_src #gs_hdr_srch,.gs_hdr_src #gs_hdr_lgt,.gs_hdr_srx #gs_hdr_sre,.gs_hdr_srx #gs_hdr_md h1,.gs_hdr_srx #gs_hdr_md h1.gs_hdr_mbo,.gs_hdr_srx #gs_hdr_md h1.gs_hdr_dso,.gs_el_ta .gs_hdr_srx #gs_hdr_lgo,.gs_el_ph .gs_hdr_srx #gs_hdr_lgo,.gs_el_ph .gs_hdr_srx #gs_hdr_mnu,.gs_el_ph .gs_hdr_srx #gs_hdr_bck{display:none;}.gs_el_ph .gs_hdr_ifc #gs_hdr_md,.gs_el_ph .gs_hdr_srx #gs_hdr_md{margin-left:16px;}.gs_el_tc .gs_hdr_tsc #gs_hdr_tsi[dir=\"ltr\"]{padding-right:41px;}.gs_el_tc .gs_hdr_tsc #gs_hdr_tsi[dir=\"rtl\"]{padding-left:41px;}.gs_el_tc .gs_hdr_tsc .gs_in_acw~#gs_hdr_tsc{display:block;}#gs_hdr_act{min-width:64px;max-width:200px;text-align:right;float:right;}.gs_el_ta #gs_hdr_act,.gs_el_ph #gs_hdr_act{display:none;}#gs_hdr_act_i,#gs_hdr_act_s{display:inline-block;padding:23px 24px 23px 16px;max-width:100%;box-sizing:border-box;font-size:13px;line-height:17px;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;color:#444;}#gs_hdr_act_s{text-transform:uppercase;}.gs_el_sm #gs_hdr_act_i,.gs_el_sm #gs_hdr_act_s{padding:23px 16px;}.gs_el_ta #gs_hdr_act_i,.gs_el_ta #gs_hdr_act_s,.gs_el_ph #gs_hdr_act_i,.gs_el_ph #gs_hdr_act_s{padding:20px 16px;}#gs_hdr_act_i:active,#gs_hdr_act_s:active{color:#d14836;}#gs_hdr_act_i,.gs_el_sm #gs_hdr_act_i{padding-top:15px;padding-bottom:16px;}.gs_el_ta #gs_hdr_act_i,.gs_el_ph #gs_hdr_act_i{padding-top:12px;padding-bottom:13px;}#gs_hdr_act_i .gs_hdr_pp{vertical-align:top;}#gs_hdr_act_d{top:63px;left:auto;right:24px;min-width:288px;max-width:400px;}.gs_el_sm #gs_hdr_act_d{right:16px;}.gs_el_ta #gs_hdr_act_d{top:57px;}.gs_el_ph #gs_hdr_act_d{top:57px;min-width:280px;max-width:280px;max-width:90vw;}/* Account dialog body. */#gs_hdr_act_aw,#gs_hdr_act_ap,.gs_hdr_act_am,#gs_hdr_act_ab{display:block;padding:10px 20px;word-wrap:break-word;white-space:normal;}#gs_hdr_act_aw{background-color:#fef9db;font-size:11px;}#gs_hdr_act_ap,.gs_hdr_act_am{border-bottom:1px solid #ccc;}#gs_hdr_act_ap{padding:20px;}.gs_el_ph #gs_hdr_act_ap{padding:10px;}#gs_hdr_act_apb{margin-top:12px;}#gs_hdr_act_aa:link,#gs_hdr_act_aa:visited{float:right;margin-left:8px;color:#1a0dab;}#gs_hdr_act_aa:active{color:#d14836}.gs_hdr_act_am:link,.gs_hdr_act_am:visited{color:#222;text-decoration:none;background:#fbfbfb;}.gs_hdr_act_am:hover,.gs_hdr_act_am:focus{background:#f1f1f1;}.gs_hdr_act_am:active{background:#eee;}#gs_hdr_act_ab{background:#fbfbfb;padding:10px 0;display:table;width:100%;white-space:nowrap;}#gs_hdr_act_aba,#gs_hdr_act_abs{display:table-cell;padding:0 20px;}#gs_hdr_act_abs{text-align:right;}.gs_el_ph #gs_hdr_act_aba,.gs_el_ph #gs_hdr_act_abs{display:block;padding:10px;text-align:center;}.gs_el_ph #gs_hdr_act_aba button,.gs_el_ph #gs_hdr_act_abs button{width:100%;}#gs_hdr_act_a1,#gs_hdr_act_a2{position:absolute;top:-9px;right:7.5px;width:0;height:0;z-index:1;border:8.5px solid transparent;border-top:none;border-bottom-color:#333;border-bottom-color:rgba(0,0,0,.2);}#gs_hdr_act_a2{top:-8px;border-bottom-color:#fff;}.gs_hdr_act_mw #gs_hdr_act_a2{border-bottom-color:#fef9db;}.gs_hdr_pp{border-radius:50%;overflow:hidden;}#gs_hdr_act_ap .gs_hdr_pp,.gs_hdr_act_am .gs_hdr_pp{float:left;}#gs_hdr_act_ap .gs_hdr_pm{margin-left:116px;}.gs_hdr_act_am .gs_hdr_pm{margin:6px 0 0 58px;}#gs_ab{position:relative;height:41px;border-bottom:1px solid #e5e5e5;display:flex;white-space:nowrap;background-color:#fff;z-index:1000;}.gs_el_ta #gs_ab.gs_nta,.gs_el_ph #gs_ab.gs_nph{display:none;}.gs_sth_vis #gs_ab{position:fixed;}#gs_ab_ico,#gs_ab_ttl,#gs_ab_md,#gs_ab_btns{display:inline-block;vertical-align:top;position:relative;height:100%;flex:0 0 auto;}.gs_el_ph #gs_ab_md{display:block;}#gs_ab_ico{width:55px;margin-right:1px;}.gs_el_sm #gs_ab_ico{width:15px;visibility:hidden;}.gs_el_ta #gs_ab_ico,.gs_el_ph #gs_ab_ico{width:55px;visibility:visible;}#gs_ab_ico .gs_ico{position:absolute;top:50%;left:50%;margin:-10.5px 0 0 -10.5px;}#gs_ab_ttl{min-width:172px;padding-right:8px;}.gs_el_sm #gs_ab_ttl{min-width:120px;}.gs_el_ta #gs_ab_ttl,.gs_el_ph #gs_ab_ttl{min-width:0;}#gs_ab_ttl,#gs_ab_ttll{font-size:18px;color:#666;text-transform:none;}.gs_el_sm #gs_ab_ttl,.gs_el_sm #gs_ab_ttll{font-size:16px;}#gs_ab_ttll{overflow:hidden;text-overflow:ellipsis;max-width:200px;}#gs_ab_md{flex:1 0 auto;}.gs_ab_st #gs_ab_md{flex:1 1 auto;font-size:13px;line-height:17px;padding:0 8px;color:#999;overflow:hidden;text-overflow:ellipsis;}.gs_el_ph .gs_ab_st #gs_ab_md{visibility:hidden;padding:0;}#gs_ab_btns{margin-right:8px;}.gs_el_sm #gs_ab_btns{margin-right:0;}.gs_el_ta #gs_ab_btns,.gs_el_ph #gs_ab_btns{margin-right:4px;}#gs_ab_ttl:before,#gs_ab_md:before,#gs_ab_btns:before{content:\"\";display:inline-block;width:0;height:100%;vertical-align:middle;}#gs_ab_md>button,#gs_ab_btns>button,#gs_ab_md>.gs_in_ib,#gs_ab_btns>.gs_in_ib,#gs_ab_md>.gs_md_r,#gs_ab_btns>.gs_md_r,#gs_ab .gs_ab_mdw,#gs_ab .gs_ab_btw{margin:0 8px;vertical-align:middle;}#gs_ab .gs_ab_mdw,.gs_ab_btw{display:inline-block;margin:0;}#gs_ab_btns>.gs_in_ib{margin:0 16px 0 8px;}#gs_ab .gs_ab_btw{margin:0 12px 0 16px;}.gs_el_ta .gs_ab_sel #gs_ab_ico,.gs_el_ph .gs_ab_sel #gs_ab_ico,.gs_el_ta .gs_ab_sel #gs_ab_ttl,.gs_el_ph .gs_ab_sel #gs_ab_ttl,.gs_el_ta .gs_ab_sel #gs_ab_btns,.gs_el_ph .gs_ab_sel #gs_ab_btns{display:none;}#gs_bdy{display:table;table-layout:fixed;width:100%;}#gs_bdy_sb{vertical-align:top;width:228px;word-wrap:break-word;display:table-cell;}.gs_el_sm #gs_bdy_sb{width:136px;}.gs_el_ta #gs_bdy_sb,.gs_el_ph #gs_bdy_sb{display:none;}.gs_bdy_sb_sec{margin:0 40px 0 56px;}.gs_el_sm .gs_bdy_sb_sec{margin:0 0 0 16px;}.gs_bdy_sb_sec:before{display:block;content:\" \";height:0;margin:13px 0;border-top:1px solid #eee;}.gs_bdy_sb_sec:first-child:before{margin:21px 0 0 0;border:none;}.gs_el_sm .gs_bdy_sb_sec:first-child:before{margin-top:15px;}#gs_bdy_sb ul{list-style-type:none;}.gs_bdy_sb_sec a:link,.gs_bdy_sb_sec a:visited{color:#222;}.gs_bdy_sb_sec a:active{color:#d14836;}.gs_bdy_sb_sel a:link,.gs_bdy_sb_sel a:visited{color:#d14836;text-decoration:none;}.gs_el_tc .gs_bdy_sb_sec li.gs_ind,.gs_el_tc .gs_bdy_sb_sec li.gs_ind a{padding-top:8px;padding-bottom:5px;}.gs_el_tc .gs_bdy_sb_sec:first-child li.gs_ind:first-child{margin-top:-8px;}#gs_bdy_sb .gs_ind,#gs_bdy_sb .gs_inw{margin-bottom:6px;}.gs_el_tc #gs_bdy_sb .gs_ind,.gs_el_tc #gs_bdy_sb .gs_inw{margin-bottom:0;}#gs_bdy_ccl{display:table-cell;vertical-align:top;padding:0 24px 0 16px;}.gs_el_sm #gs_bdy_ccl{padding:0 16px;}.gs_el_ta #gs_bdy_ccl,.gs_el_ph #gs_bdy_ccl{padding:0 16px;}.gs_el_ph #gs_bdy_ccl{}#gs_ftr_sp{height:62px;}.gs_el_sm #gs_ftr_sp{height:57px;}#gs_ftr{position:absolute;bottom:0;left:0;width:100%;white-space:nowrap;border-top:1px solid #e4e4e4;background-color:#f2f2f2;display:flex;}#gs_ftr.gs_pfix{position:fixed;}#gs_ftr_rt{box-sizing:border-box;max-width:100%;overflow-x:auto;margin-left:auto;padding:0 12px;}.gs_el_sm #gs_ftr_rt{padding:0 8px;}.gs_el_ph #gs_ftr_rt:after{content:\" \";position:absolute;top:0;right:0;width:16px;height:100%;background-image:linear-gradient(to right,rgba(242,242,242,0),rgba(242,242,242,1) 80%);}#gs_ftr_rt>a{display:inline-block;line-height:16px;padding:12px;white-space:nowrap;}.gs_el_sm #gs_ftr_rt>a{padding:12px 8px;}#gs_ftr_rt>a:link,#gs_ftr_rt>a:visited{color:#666}#gs_ftr_rt>a:active{color:#d14836}#gs_ftr_mnu{top:auto;bottom:48px;left:auto;right:24px;padding:8px 0;}.gs_el_sm #gs_ftr_mnu{right:16px;}.gs_res_sb_yyr{padding:5px 0;text-align:center;white-space:nowrap;}.gs_el_tc .gs_res_sb_yyr{padding:10px 0;}.gs_res_sb_yyr .gs_in_txt{width:48px;}#gs_res_ccl{max-width:950px;padding-top:10px;}.gs_el_sm #gs_res_ccl{padding-top:7px;}.gs_el_tc #gs_res_ccl{padding-top:6px;}.gs_el_sm.gs_el_tc #gs_res_ccl{padding-top:0;}.gs_r{position:relative;line-height:1.46;padding:11px 0 16px 0;}.gs_el_sm .gs_r{padding:7px 0 12px 0;}.gs_el_tc .gs_r{padding:15px 0;border-bottom:1px solid #eee;}.gs_el_tc .gs_r.gs_fmar{border-bottom:none;}.gs_r.gs_res_lnfo{font-style:italic;}.gs_rt{position:relative;font-weight:normal;font-size:17px;line-height:19px;margin-right:100px;margin-bottom:2px;}.gs_el_tc .gs_rt{margin-bottom:0;}.gs_el_ph .gs_rt{margin-right:0;}.gs_rt2{font-size:13px;font-weight:normal;}.gs_rt a:link,.gs_rt a:link b,.gs_rt2 a:link,.gs_rt2 a:link b{color:#1a0dab}.gs_rt a:visited,.gs_rt a:visited b,.gs_rt2 a:visited,.gs_rt2 a:visited b{color:#660099}.gs_rt a:active,.gs_rt a:active b,.gs_rt2 a:active,.gs_rt2 a:active b{color:#d14836}.gs_or_ggsm:focus{outline:none;}.gs_ggs{position:relative;z-index:1;float:right;margin-left:24px;min-width:200px;max-width:256px;width:200px;width:calc(100% - 620px);font-size:17px;line-height:19px;}.gs_el_sm .gs_ggs{margin-left:16px;}@media(max-width:699px){.gs_el_sm .gs_ggs{min-width:0;width:182px;}}.gs_el_ph .gs_ggs{width:72px;height:43px;}.gs_el_tc .gs_ggs{margin-top:-14px;}.gs_el_ph .gs_ggsd{position:absolute;top:-1px;right:0;width:72px;height:43px;overflow:hidden;transition:width 0s .3s,height 0s .3s;}.gs_el_ph .gs_ggsd.gs_vis{width:208px;height:88px;transition:none;}.gs_or_ggsm a{display:block;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;margin-bottom:4px;}.gs_el_tc .gs_or_ggsm a{padding:13px 8px 9px 8px;touch-action:none;background:#fff;height:19px;margin-bottom:0;}.gs_el_ph .gs_or_ggsm a{text-decoration:none;}.gs_el_ph .gs_or_ggsm a:focus{outline:none;background:#f1f1f1;}.gs_el_ph .gs_or_ggsm a:active{color:#1a0dab;}.gs_el_ph .gs_or_ggsm a:visited{color:#660099;}.gs_el_ph .gs_or_ggsm{position:absolute;top:0;right:-132px;margin-right:4px;padding:1px 0;width:200px;height:41px;transform:translate(0,0);}.gs_el_ph .gs_or_ggsm.gs_vis{right:0;height:auto;}.gs_el_ph .gs_or_ggsm>a:nth-child(2){height:0;transform:scale(1,0);transform-origin:0 0;}.gs_el_ph .gs_or_ggsm.gs_vis>a:nth-child(2){height:19px;transform:scale(1,1);}.gs_el_ph .gs_or_ggsm:before{content:\"\";position:absolute;top:0;left:-1px;right:-1px;bottom:0;box-shadow:0 2px 4px rgba(0,0,0,.2);border:1px solid #ccc;opacity:0;z-index:-1;}.gs_el_ph .gs_or_ggsm.gs_vis:before{opacity:1;transition:opacity 0s .3s ;}.gs_el_ph .gs_or_ggsm:after{content:\"\";pointer-events:none;position:absolute;top:0;left:0;right:0;bottom:0;z-index:1;background-image:linear-gradient(to left,rgba(255,255,255,1),rgba(255,255,255,1) 65%,rgba(255,255,255,0) 70%);}.gs_el_ph .gs_or_ggsm.gs_vis:after{visibility:hidden}.gs_el_ph .gs_or_ggsm.gs_vis.gs_anm{animation:gs_anm_hsli .218s ease-in-out;}.gs_el_ph .gs_or_ggsm.gs_anm{animation:gs_anm_hslo .218s ease-out;}.gs_el_ph .gs_ggs .gs_or_ggsm.gs_vis.gs_anm>a:nth-child(2){animation:gs_anm_vscli .218s ease-in;}@keyframes gs_anm_hsli{0%{transform:translate(66%,0);height:41px;}99%{transform:translate(0,0);height:41px;}100%{transform:translate(0,0);height:auto;}}@keyframes gs_anm_hslo{0%{transform:translate(-66%,0);}100%{transform:translate(0,0);}}@keyframes gs_anm_vscli{0%{transform:scale(1,0);}100%{transform:scale(1,1);}}.gs_ct1{display:inline}.gs_ct2{display:none}.gs_el_ph .gs_ct1{display:none}.gs_el_ph .gs_ct2{display:inline;font-size:13px;font-weight:normal}.gs_ri{max-width:712px}.gs_a a:link,.gs_a a:visited{text-decoration:underline;}.gs_ri .gs_fl a,.gs_a a{white-space:nowrap;}.gs_ri .gs_fl a.gs_wno{white-space:normal;}.gs_ri .gs_fl{font-size:1px;}.gs_ri .gs_fl a{font-size:13px;margin-right:12px;}.gs_ri .gs_fl a:last-child{margin-right:0;}.gs_ri .gs_fl .gs_or_mor{margin:-7px 6px -6px -7px;padding:7px 7px 6px 7px;border-radius:50%;}.gs_ri .gs_fl .gs_or_mor:hover{background-color:rgba(0,0,0,.05);}.gs_el_ph .gs_ri .gs_fl .gs_or_sav{margin-right:4px;}.gs_el_ph .gs_or_sav .gs_or_btn_lbl{display:none;}.gs_el_ph .gs_ri .gs_fl .gs_or_mor{margin-right:-7px;}.gs_or_svg{position:relative;width:15px;height:16px;vertical-align:text-bottom;fill:none;stroke:#1a0dab;}.gs_or:not([data-lid=\"\"]) .gs_or_sav .gs_or_svg{fill:#1a0dab;}.gs_or[data-lid] .gs_or_ldg .gs_or_svg{animation:gs_anm_spin 1.2s .5s linear infinite;}a:active .gs_or_svg,a .gs_or_svg:active,a .gs_or_svg>*:active{stroke:#dd4b39;}.gs_or_btn .gs_or_svg{margin-right:5px;}.gs_or_nvi,.gs_or_mvi .gs_or_mor{display:none}.gs_or_mvi .gs_or_nvi,.gs_or_mvi .gs_nph,.gs_or_mvi .gs_nta{display:inline}.gs_rs{margin:2px 0;word-wrap:break-word;}.gs_rs:empty{margin:0 0 2px 0;}.gs_el_tc .gs_rs{margin:0}.gs_el_ta .gs_rs{margin-right:10%}@media(max-width:780px){.gs_el_ta .gs_rs,.gs_el_ta .gs_a{margin-right:100px;}}.gs_el_ph .gs_rs br,.gs_el_ta .gs_rs br{display:none}@media screen and (min-width:771px){.gs_el_ta .gs_rs br{display:block}}.gs_age{color:#777777}.gs_rs b,.gs_rt b,.gs_rt2 b{color:#000;}.gs_el_tc .gs_rt a{font-size:17px;line-height:20px;padding:12px 0 9px 0;}.gs_el_tc .gs_rt2 a{font-size:14px;line-height:20px;padding:6px 0 4px 0;}.gs_el_tc .gs_a,.gs_el_tc .gs_a a,.gs_el_tc .gs_ri .gs_fl a{padding-top:7px;padding-bottom:6px;}.gs_el_tc .gs_ri .gs_fl a{line-height:29px;}.gs_el_tc .gs_ri .gs_fl{margin-bottom:-6px;}#gs_n{clear:both;margin:1.5em 0;width:600px;text-align:center;}#gs_n td{font-size:13px}#gs_n a:link,#gs_n a:visited{color:#1a0dab}#gs_n a:active{color:#d14836}#gs_nm{clear:both;position:relative;text-align:center;max-width:500px;margin:24px 50px;font-size:15px;line-height:41px;display:none;}#gs_nm button{position:absolute;top:0}#gs_nm .gs_btnPL{left:-50px}#gs_nm .gs_btnPR{right:-50px}#gs_nml{overflow:hidden;white-space:nowrap;}.gs_nma{display:inline-block;width:40px;margin:0 5px;}.gs_el_tc #gs_n,.gs_el_ta #gs_n,.gs_el_ph #gs_n{display:none}.gs_el_tc #gs_nm,.gs_el_ta #gs_nm,.gs_el_ph #gs_nm{display:block}#gs_bdy_sb_ca{margin-top:-6px;}.gs_el_tc #gs_bdy_sb_ca{margin-top:2px;}.gs_res_sb_msc{margin-bottom:12px;}@media print{#gs_gb,#gs_hdr,#gs_ab,#gs_top #gs_bdy_sb,.gs_pda,.gs_ggs,.gs_alrt_btm,#gs_top #gs_n,#gs_top #gs_nm,#gs_ftr,#gs_top .gs_ctc,#gs_top .gs_ctu,#gs_rt_hdr,.gs_rt_hdr_ttl{display:none}#gs_top,#gs_top #gs_bdy,#gs_top #gs_res_bdy,#gs_top #gs_bdy_ccl,#gs_top .gs_r,#gs_top .gs_ri,#gs_top .gs_rs{font-size:9pt;color:black;position:static;float:none;margin:0;padding:0;width:auto;min-width:0;max-width:none;}#gs_top #gs_bdy a{color:blue;text-decoration:none}#gs_top .gs_r{margin:1em 0;page-break-inside:avoid;border:0;}#gs_top .gs_med,#gs_top .gs_rt{font-size:12pt}#gs_top .gs_a,#gs_top #gs_bdy .gs_a a{font-size:9pt;color:green}#gs_top .gs_fl,#gs_top .gs_fl a{font-size:9pt}#gs_top .gs_rs br{display:inline}}.gs_el_ph #gs_ab_ttll{max-width:98px;max-width:calc(100vw - 222px);}@media(max-width:320px){.gs_el_ph #gs_ab_ttll{max-width:98px;}}#gs_res_ab_yy-r,#gs_res_ab_ad-r,#gs_res_ab_mor-r{display:none;}.gs_el_ta #gs_res_ab_yy-r,.gs_el_ph #gs_res_ab_yy-r,.gs_el_ta #gs_res_ab_ad-r,.gs_el_ph #gs_res_ab_ad-r,.gs_el_ta #gs_res_ab_mor-r,.gs_el_ph #gs_res_ab_mor-r{display:inline-block;margin:0;}#gs_res_ab_yy-r:last-child{margin-right:4px;}#gs_res_ab_ad-r:last-child,#gs_res_ab_mor-r:last-child{margin-right:12px;}#gs_res_ab_tmn-d,#gs_res_ab_yy-d,#gs_res_ab_ad-d,#gs_res_ab_mor-d{white-space:normal;word-wrap:break-word;width:208px;width:-webkit-max-content;width:max-content;min-width:100px;max-width:208px;}#gs_res_ab_yy-d,#gs_res_ab_ad-d,#gs_res_ab_mor-d{left:auto;right:0;}.gs_res_ab_dd_bdy{padding:8px 0;box-sizing:border-box;}.gs_res_ab_dd_sec a.gs_res_ab_sel,.gs_res_ab_dd_sec a[role=menuitemradio]:active{color:#d14836;}.gs_res_ab_dd_sec a.gs_res_ab_sel svg,.gs_res_ab_dd_sec a[role=menuitemradio]:active svg{fill:#d14836;}.gs_res_ab_dd_sec:before{display:block;content:\" \";height:0;border-bottom:1px solid #e5e5e5;margin:8px 0;}.gs_res_ab_dd_sec:first-child:before{display:none;}#gs_res_ab_custom_range{display:flex;gap:8px;}#gs_res_ab_custom_range svg{flex-shrink:0;margin-top:2px;}.gs_fsvg line{stroke:#222222}a:link .gs_fsvg{fill:#1a0dab;}a:link .gs_fsvg line{stroke:#1a0dab;}a:visited .gs_fsvg{fill:#660099;}a:visited .gs_fsvg line{stroke:#660099;}a:active .gs_fsvg{fill:#d14836;}a:active .gs_fsvg line{stroke:#d14836;}a .gs_fsvg{border-bottom:1px solid transparent;}a:hover .gs_fsvg,a:focus .gs_fsvg{border-bottom-color:inherit;}.gs_fsml{font-size:13px}.gs_fscp{font-variant:small-caps}.gs_qsuggest{max-width:712px;line-height:21px;margin-bottom:2px;}.gs_r .gs_qsuggest{max-width:600px;}.gs_el_ta .gs_r .gs_qsuggest{margin-right:100px;}.gs_qsuggest h2{margin-bottom:8px;font-weight:normal;font-size:17px;}.gs_qsuggest li{display:inline-block;width:100%;font-size:15px;line-height:18px;padding:4px 0 3px;}.gs_qsuggest ul{list-style-type:none;columns:2;column-gap:40px;margin-bottom:-3px;}.gs_el_sm .gs_qsuggest ul{column-gap:16px;margin-bottom:3px;}.gs_qsuggest li:only-child{column-span:all;}.gs_qsuggest li>a{display:inline-block;max-width:100%;word-wrap:break-word;}.gs_el_tc .gs_qsuggest a{padding:8px 0 5px;}.gs_el_tc .gs_qsuggest li{padding:2px 0 1px;}.gs_el_tc .gs_qsuggest ul{margin:0;}.gs_el_tc .gs_qsuggest h2{margin:6px 0;}.gs_qsuggest_bottom h2{padding-top:11px;}.gs_qsuggest_bottom{margin-bottom:40px;}.gs_el_tc .gs_qsuggest_wrap .gs_qsuggest_related h2{padding-bottom:0;}.gs_el_ph .gs_r .gs_qsuggest{margin-bottom:-15px;}.gs_el_ph .gs_qsuggest ul{columns:1;margin:0;}.gs_el_ph .gs_qsuggest li{margin:0;padding:0;position:relative;}.gs_el_ph .gs_qsuggest h2{margin:1px 0 16px;}.gs_el_ph .gs_qsuggest a{display:block;padding:11px 29px 9px 0;border-bottom:1px solid #eee;}.gs_el_ph .gs_qsuggest li:first-child a{border-top:1px solid #eee;}.gs_el_ph .gs_qsuggest_wrap.gs_r li:last-child a{border-bottom:0;}.gs_el_ph .gs_qsuggest a:link,.gs_el_ph .gs_qsuggest a:visited{color:#222;}.gs_el_ph .gs_qsuggest a:hover,.gs_el_ph .gs_qsuggest a:focus{background:#f1f1f1;text-decoration:none;outline:none;}.gs_el_ph .gs_qsuggest li>a:after{content:\"\";position:absolute;width:7px;height:7px;top:50%;margin-top:-4px;right:10px;border:2px solid #777;border-left:none;border-bottom:none;transform:rotate(45deg);}#gs_md_albl-d{width:483px;}.gs_el_ph #gs_md_albl-d{width:100%;}.gs_lbl_btns{display:flex;justify-content:space-between;padding:18px 16px;}.gs_lbl_hide{display:none;}</style><script>!function(GSP){var m,aa=function(a){var b=0;return function(){return b<a.length?{done:!1,value:a[b++]}:{done:!0}}},ba=typeof Object.defineProperties==\"function\"?Object.defineProperty:function(a,b,c){if(a==Array.prototype||a==Object.prototype)return a;a[b]=c.value;return a},ca=function(a){a=[\"object\"==typeof globalThis&&globalThis,a,\"object\"==typeof window&&window,\"object\"==typeof self&&self,\"object\"==typeof global&&global];for(var b=0;b<a.length;++b){var c=a[b];if(c&&c.Math==Math)return c}throw Error(\"Cannot find global object\");\n},da=ca(this),q=function(a,b){if(b)a:{var c=da;a=a.split(\".\");for(var d=0;d<a.length-1;d++){var e=a[d];if(!(e in c))break a;c=c[e]}a=a[a.length-1];d=c[a];b=b(d);b!=d&&b!=null&&ba(c,a,{configurable:!0,writable:!0,value:b})}};\nq(\"Symbol\",function(a){if(a)return a;var b=function(f,g){this.Wa=f;ba(this,\"description\",{configurable:!0,writable:!0,value:g})};b.prototype.toString=function(){return this.Wa};var c=\"jscomp_symbol_\"+(Math.random()*1E9>>>0)+\"_\",d=0,e=function(f){if(this instanceof e)throw new TypeError(\"Symbol is not a constructor\");return new b(c+(f||\"\")+\"_\"+d++,f)};return e});var ea=typeof Object.create==\"function\"?Object.create:function(a){var b=function(){};b.prototype=a;return new b},fa;\nif(typeof Object.setPrototypeOf==\"function\")fa=Object.setPrototypeOf;else{var ha;a:{var ia={a:!0},ja={};try{ja.__proto__=ia;ha=ja.a;break a}catch(a){}ha=!1}fa=ha?function(a,b){a.__proto__=b;if(a.__proto__!==b)throw new TypeError(a+\" is not extensible\");return a}:null}\nvar ka=fa,la=function(a,b){a.prototype=ea(b.prototype);a.prototype.constructor=a;if(ka)ka(a,b);else for(var c in b)if(c!=\"prototype\")if(Object.defineProperties){var d=Object.getOwnPropertyDescriptor(b,c);d&&Object.defineProperty(a,c,d)}else a[c]=b[c];a.Rb=b.prototype},t=function(a){var b=typeof Symbol!=\"undefined\"&&Symbol.iterator&&a[Symbol.iterator];if(b)return b.call(a);if(typeof a.length==\"number\")return{next:aa(a)};throw Error(String(a)+\" is not an iterable or ArrayLike\");};\nq(\"Symbol.dispose\",function(a){return a?a:Symbol(\"Symbol.dispose\")});q(\"Math.trunc\",function(a){return a?a:function(b){b=Number(b);if(isNaN(b)||b===Infinity||b===-Infinity||b===0)return b;var c=Math.floor(Math.abs(b));return b<0?-c:c}});var ma=function(a){a=Math.trunc(a)||0;a<0&&(a+=this.length);if(!(a<0||a>=this.length))return this[a]};q(\"Array.prototype.at\",function(a){return a?a:ma});var u=function(a){return a?a:ma};q(\"Int8Array.prototype.at\",u);q(\"Uint8Array.prototype.at\",u);\nq(\"Uint8ClampedArray.prototype.at\",u);q(\"Int16Array.prototype.at\",u);q(\"Uint16Array.prototype.at\",u);q(\"Int32Array.prototype.at\",u);q(\"Uint32Array.prototype.at\",u);q(\"Float32Array.prototype.a"
  }
]