{
  "id": "meta_github_728399986",
  "title": "PurpleLlama",
  "description": "Set of tools to assess and improve LLM security.",
  "url": "https://github.com/meta-llama/PurpleLlama",
  "language": "Python",
  "stars": 3804,
  "forks": 641,
  "created_at": "2023-12-06T21:29:41Z",
  "updated_at": "2025-09-28T12:17:02Z",
  "topics": [],
  "readme_content": "<p align=\"center\">\n  <img src=\"https://github.com/facebookresearch/PurpleLlama/blob/main/logo.png\" width=\"400\"/>\n</p>\n\n<p align=\"center\">\n        ðŸ¤— <a href=\"https://huggingface.co/meta-Llama\"> Models on Hugging Face</a>&nbsp | <a href=\"https://ai.meta.com/blog/purple-llama-open-trust-safety-generative-ai\"> Blog</a>&nbsp |  <a href=\"https://ai.meta.com/llama/purple-llama\">Website</a>&nbsp | <a href=\"https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/\">CyberSec Eval Paper</a>&nbsp&nbsp | <a href=\"https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/\">Llama Guard Paper</a>&nbsp\n<br>\n\n---\n\n# Purple Llama\n\nPurple Llama is an umbrella project that over time will bring together tools\nand evals to help the community build responsibly with open generative AI\nmodels. The initial release will include tools and evals for Cyber Security and\nInput/Output safeguards but we plan to contribute more in the near future.\n\n## Why purple?\n\nBorrowing a [concept](https://www.youtube.com/watch?v=ab_Fdp6FVDI) from the\ncybersecurity world, we believe that to truly mitigate the challenges which\ngenerative AI presents, we need to take both attack (red team) and defensive\n(blue team) postures. Purple teaming, composed of both red and blue team\nresponsibilities, is a collaborative approach to evaluating and mitigating\npotential risks and the same ethos applies to generative AI and hence our\ninvestment in Purple Llama will be comprehensive.\n\n## License\n\nComponents within the Purple Llama project will be licensed permissively enabling both research and commercial usage.\nWe believe this is a major step towards enabling community collaboration and standardizing the development and usage of trust and safety tools for generative AI development.\nMore concretely evals and benchmarks are licensed under the MIT license while any models use the corresponding Llama Community license. See the table below:\n\n| **Component Type** |            **Components**            |                                          **License**                                           |\n| :----------------- | :----------------------------------: | :--------------------------------------------------------------------------------------------: |\n| Evals/Benchmarks   | Cyber Security Eval (others to come) |                                              MIT                                               |\n| Safeguard             |             Llama Guard              | [Llama 2 Community License](https://github.com/facebookresearch/PurpleLlama/blob/main/LICENSE) |\n| Safeguard             |             Llama Guard 2            | [Llama 3 Community License](https://github.com/meta-llama/llama3/blob/main/LICENSE) |\n| Safeguard             |             Llama Guard 3-8B            | [Llama 3.2 Community License](LICENSE) |\n| Safeguard             |             Llama Guard 3-1B            | [Llama 3.2 Community License](LICENSE) |\n| Safeguard             |             Llama Guard 3-11B-vision            | [Llama 3.2 Community License](LICENSE) |\n| Safeguard             |             Prompt Guard            | [Llama 3.2 Community License](LICENSE) |\n| Safeguard          |             Code Shield              | MIT |\n\n\n## System-Level Safeguards\n\nAs we outlined in Llama 3â€™s\n[Responsible Use Guide](https://ai.meta.com/llama/responsible-use-guide/), we\nrecommend that all inputs and outputs to the LLM be checked and filtered in\naccordance with content guidelines appropriate to the application.\n\n### Llama Guard\n\nLlama Guard 3 consists of a series of high-performance input and output moderation models designed to support developers to detect various common types of violating content.\n\nThey were built by fine-tuning Meta-Llama 3.1 and 3.2 models and optimized to support the detection of the MLCommons standard hazards taxonomy, catering to a range of developer use cases.\nThey support the release of Llama 3.2 capabilities, including 7 new languages, a 128k context window, and image reasoning. Llama Guard 3 models were also optimized to detect helpful cyberattack responses and prevent malicious code output by LLMs to be executed in hosting environments for Llama systems using code interpreters.\n\n\n### Prompt Guard\nPrompt Guard is a powerful tool for protecting LLM powered applications from malicious prompts to ensure their security and integrity.\n\nCategories of prompt attacks include prompt injection and jailbreaking:\n\n* Prompt Injections are inputs that exploit the inclusion of untrusted data from third parties into the context window of a model to get it to execute unintended instructions.\n* Jailbreaks are malicious instructions designed to override the safety and security features built into a model.\n\n### Code Shield\n\nCode Shield adds support for inference-time filtering of insecure code produced by LLMs. Code Shield offers mitigation of insecure code suggestions risk, code interpreter abuse prevention, and secure command execution. [CodeShield Example Notebook](https://github.com/meta-llama/PurpleLlama/blob/main/CodeShield/notebook/CodeShieldUsageDemo.ipynb).\n\n\n\n## Evals & Benchmarks\n\n### Cybersecurity\n\n#### CyberSec Eval v1\nCyberSec Eval v1 was what we believe was the first industry-wide set of cybersecurity safety evaluations for LLMs. These benchmarks are based on industry guidance and standards (e.g., CWE and MITRE ATT&CK) and built in collaboration with our security subject matter experts. We aim to provide tools that will help address some risks outlined in the [White House commitments on developing responsible AI](https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/), including:\n* Metrics for quantifying LLM cybersecurity risks.\n* Tools to evaluate the frequency of insecure code suggestions.\n* Tools to evaluate LLMs to make it harder to generate malicious code or aid in carrying out cyberattacks.\n\nWe believe these tools will reduce the frequency of LLMs suggesting insecure AI-generated code and reduce their helpfulness to cyber adversaries. Our initial results show that there are meaningful cybersecurity risks for LLMs, both with recommending insecure code and for complying with malicious requests. See our [Cybersec Eval paper](https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/) for more details.\n\n#### CyberSec Eval 2\nCyberSec Eval 2 expands on its predecessor by measuring an LLMâ€™s propensity to abuse a code interpreter, offensive cybersecurity capabilities, and susceptibility to prompt injection. You can read the paper [here](https://ai.meta.com/research/publications/cyberseceval-2-a-wide-ranging-cybersecurity-evaluation-suite-for-large-language-models/).\n\nYou can also check out the ðŸ¤— leaderboard [here](https://huggingface.co/spaces/facebook/CyberSecEval).\n\n#### CyberSec Eval 3\nThe newly released CyberSec Eval 3 features three additional test suites: visual prompt injection tests, spear phishing capability tests, and autonomous offensive cyber operations tests.\n\n## Getting Started\n\nAs part of the [Llama reference system](https://github.com/meta-llama/llama-agentic-system), weâ€™re integrating a safety layer to facilitate adoption and deployment of these safeguards.\nResources to get started with the safeguards are available in the [Llama-recipe GitHub repository](https://github.com/meta-llama/llama-recipes).\n\n## FAQ\n\nFor a running list of frequently asked questions, for not only Purple Llama\ncomponents but also generally for Llama models, see the FAQ\n[here](https://ai.meta.com/llama/faq/).\n\n## Join the Purple Llama community\n\nSee the [CONTRIBUTING](CONTRIBUTING.md) file for how to help out.\n",
  "source_type": "meta_github_repository",
  "domain": "meta_ai",
  "keywords": [
    "llama"
  ],
  "retrieval_tags": [
    "meta",
    "ai",
    "github",
    "repository",
    "research"
  ]
}