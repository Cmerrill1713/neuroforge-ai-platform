{
  "id": "meta_github_78594383",
  "title": "ResNeXt",
  "description": "Implementation of a classification framework from the paper Aggregated Residual Transformations for Deep Neural Networks",
  "url": "https://github.com/facebookresearch/ResNeXt",
  "language": "Lua",
  "stars": 1923,
  "forks": 292,
  "created_at": "2017-01-11T02:20:25Z",
  "updated_at": "2025-09-16T05:47:44Z",
  "topics": [],
  "readme_content": "# ResNeXt: Aggregated Residual Transformations for Deep Neural Networks\n\nBy [Saining Xie](http://vcl.ucsd.edu/~sxie), [Ross Girshick](http://www.rossgirshick.info/), [Piotr Dollár](https://pdollar.github.io/), [Zhuowen Tu](http://pages.ucsd.edu/~ztu/), [Kaiming He](http://kaiminghe.com)\n\nUC San Diego, Facebook AI Research\n\n### Table of Contents\n0. [Introduction](#introduction)\n0. [Citation](#citation)\n0. [Requirements and Dependencies](#requirements-and-dependencies)\n0. [Training](#training)\n0. [ImageNet Pretrained Models](#imagenet-pretrained-models)\n0. [Third-party re-implementations](#third-party-re-implementations)\n\n#### News\n* Congrats to the ILSVRC 2017 classification challenge winner [WMW](http://image-net.org/challenges/LSVRC/2017/results).\nResNeXt is the foundation of their new SENet architecture (a **ResNeXt-152 (64 x 4d)** with the Squeeze-and-Excitation module)!\n* Check out Figure 6 in the new [Memory-Efficient Implementation of DenseNets](https://arxiv.org/pdf/1707.06990.pdf) paper for a comparision between ResNeXts and DenseNets. <sub>（*DenseNet cosine is DenseNet trained with cosine learning rate schedule.*）</sub>\n<p align=\"center\">\n<img src=\"http://vcl.ucsd.edu/resnext/resnextvsdensenet.png\" width=\"480\">\n</p>\n\n\n### Introduction\n\nThis repository contains a [Torch](http://torch.ch) implementation for the [ResNeXt](https://arxiv.org/abs/1611.05431) algorithm for image classification. The code is based on [fb.resnet.torch](https://github.com/facebook/fb.resnet.torch).\n\n[ResNeXt](https://arxiv.org/abs/1611.05431) is a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call “cardinality” (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width.\n\n\n![teaser](http://vcl.ucsd.edu/resnext/teaser.png)\n##### Figure: Training curves on ImageNet-1K. (Left): ResNet/ResNeXt-50 with the same complexity (~4.1 billion FLOPs, ~25 million parameters); (Right): ResNet/ResNeXt-101 with the same complexity (~7.8 billion FLOPs, ~44 million parameters).\n-----\n\n### Citation\nIf you use ResNeXt in your research, please cite the paper:\n```\n@article{Xie2016,\n  title={Aggregated Residual Transformations for Deep Neural Networks},\n  author={Saining Xie and Ross Girshick and Piotr Dollár and Zhuowen Tu and Kaiming He},\n  journal={arXiv preprint arXiv:1611.05431},\n  year={2016}\n}\n```\n\n### Requirements and Dependencies\nSee the fb.resnet.torch [installation instructions](https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md) for a step-by-step guide.\n- Install [Torch](http://torch.ch/docs/getting-started.html) on a machine with CUDA GPU\n- Install [cuDNN v4 or v5](https://developer.nvidia.com/cudnn) and the Torch [cuDNN bindings](https://github.com/soumith/cudnn.torch/tree/R4)\n- Download the [ImageNet](http://image-net.org/download-images) dataset and [move validation images](https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset) to labeled subfolders\n\n### Training\n\nPlease follow [fb.resnet.torch](https://github.com/facebook/fb.resnet.torch) for the general usage of the code, including [how](https://github.com/facebook/fb.resnet.torch/tree/master/pretrained) to use pretrained ResNeXt models for your own task.\n\nThere are two new hyperparameters need to be specified to determine the bottleneck template:\n\n**-baseWidth** and **-cardinality**\n\n### 1x Complexity Configurations Reference Table\n\n| baseWidth | cardinality |\n|---------- | ----------- |\n| 64        | 1           |\n| 40        | 2           |\n| 24        | 4           |\n| 14        | 8           |\n| 4         | 32          |\n\n\nTo train ResNeXt-50 (32x4d) on 8 GPUs for ImageNet:\n```bash\nth main.lua -dataset imagenet -bottleneckType resnext_C -depth 50 -baseWidth 4 -cardinality 32 -batchSize 256 -nGPU 8 -nThreads 8 -shareGradInput true -data [imagenet-folder]\n```\n\nTo reproduce CIFAR results (e.g. ResNeXt 16x64d for cifar10) on 8 GPUs:\n```bash\nth main.lua -dataset cifar10 -bottleneckType resnext_C -depth 29 -baseWidth 64 -cardinality 16 -weightDecay 5e-4 -batchSize 128 -nGPU 8 -nThreads 8 -shareGradInput true\n```\nTo get comparable results using 2/4 GPUs, you should change the batch size and the corresponding learning rate:\n```bash\nth main.lua -dataset cifar10 -bottleneckType resnext_C -depth 29 -baseWidth 64 -cardinality 16 -weightDecay 5e-4 -batchSize 64 -nGPU 4 -LR 0.05 -nThreads 8 -shareGradInput true\nth main.lua -dataset cifar10 -bottleneckType resnext_C -depth 29 -baseWidth 64 -cardinality 16 -weightDecay 5e-4 -batchSize 32 -nGPU 2 -LR 0.025 -nThreads 8 -shareGradInput true\n```\nNote: CIFAR datasets will be automatically downloaded and processed for the first time. Note that in the arXiv paper CIFAR results are based on pre-activated bottleneck blocks and a batch size of 256. We found that better CIFAR test acurracy can be achieved using original bottleneck blocks and a batch size of 128.\n\n### ImageNet Pretrained Models\nImageNet pretrained models are licensed under CC BY-NC 4.0.\n\n[![CC BY-NC 4.0](https://i.creativecommons.org/l/by-nc/4.0/88x31.png)](https://creativecommons.org/licenses/by-nc/4.0/)\n\n#### Single-crop (224x224) validation error rate\n| Network             | GFLOPS | Top-1 Error |  Download   |\n| ------------------- | ------ | ----------- | ------------|\n| ResNet-50 (1x64d)   |  ~4.1  |  23.9        | [Original ResNet-50](https://github.com/facebook/fb.resnet.torch/tree/master/pretrained)       |\n| ResNeXt-50 (32x4d)  |  ~4.1  |  22.2        | [Download (191MB)](https://dl.fbaipublicfiles.com/resnext/imagenet_models/resnext_50_32x4d.t7)       |\n| ResNet-101 (1x64d)  |  ~7.8  |  22.0        | [Original ResNet-101](https://github.com/facebook/fb.resnet.torch/tree/master/pretrained)      |\n| ResNeXt-101 (32x4d) |  ~7.8  |  21.2        | [Download (338MB)](https://dl.fbaipublicfiles.com/resnext/imagenet_models/resnext_101_32x4d.t7)      |\n| ResNeXt-101 (64x4d) |  ~15.6 |  20.4        | [Download (638MB)](https://dl.fbaipublicfiles.com/resnext/imagenet_models/resnext_101_64x4d.t7)       |\n\n### Third-party re-implementations\n\nBesides our torch implementation, we recommend to see also the following third-party re-implementations and extensions:\n\n1. Training code in PyTorch [code](https://github.com/prlz77/ResNeXt.pytorch)\n1. Converting ImageNet pretrained model to PyTorch model and source. [code](https://github.com/clcarwin/convert_torch_to_pytorch)\n1. Training code in MXNet and pretrained ImageNet models [code](https://github.com/dmlc/mxnet/tree/master/example/image-classification#imagenet-1k)\n1. Caffe prototxt, pretrained ImageNet models (with ResNeXt-152), curves [code](https://github.com/cypw/ResNeXt-1)[code](https://github.com/terrychenism/ResNeXt)\n",
  "source_type": "meta_github_repository",
  "domain": "meta_ai",
  "keywords": [
    "neural network"
  ],
  "retrieval_tags": [
    "meta",
    "ai",
    "github",
    "repository",
    "research"
  ]
}