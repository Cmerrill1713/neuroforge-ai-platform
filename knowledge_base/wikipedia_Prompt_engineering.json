{
  "id": "wikipedia_Prompt_engineering",
  "title": "Prompt engineering",
  "url": "https://en.wikipedia.org/wiki/Prompt_engineering",
  "content": "Prompt engineering is the process of structuring or crafting an instruction in order to produce better outputs from a generative artificial intelligence (AI) model.[1]\n\nA prompt is natural language text describing the task that an AI should perform.[2] A prompt for a text-to-text language model can be a query, a command, or a longer statement including context, instructions, and conversation history. Prompt engineering may involve phrasing a query, specifying a style, choice of words and grammar,[3] providing relevant context, or describing a character for the AI to mimic.[1]\n\nWhen communicating with a text-to-image or a text-to-audio model, a typical prompt is a description of a desired output such as \"a high-quality photo of an astronaut riding a horse\"[4] or \"Lo-fi slow BPM electro chill with organic samples\".[5] Prompting a text-to-image model may involve adding, removing, or emphasizing words to achieve a desired subject, style, layout, lighting, and aesthetic.[6]\n\nIn 2018, researchers first proposed that all previously separate tasks in natural language processing (NLP) could be cast as a question-answering problem over a context. In addition, they trained a first single, joint, multi-task model that would answer any task-related question like \"What is the sentiment\" or \"Translate this sentence to German\" or \"Who is the president?\"[7]\n\nThe AI boom saw an increase in the amount of \"prompting technique\" to get the model to output the desired outcome and avoid nonsensical output, a process characterized by trial-and-error.[8] After the release of ChatGPT in 2022, prompt engineering was soon seen as an important business skill, albeit one with an uncertain economic future.[1]\n\nA repository for prompts reported that over 2,000 public prompts for around 170 datasets were available in February 2022.[9] In 2022, the chain-of-thought prompting technique was proposed by Google researchers.[10][11] In 2023, several text-to-text and text-to-image prompt databases were made publicly available.[12][13] The Personalized Image-Prompt (PIP) dataset, a generated image-text dataset that has been categorized by 3,115 users, has also been made available publicly in 2024.[14]\n\nMultiple distinct prompt engineering techniques have been published.\n\nAccording to Google Research, chain-of-thought (CoT) prompting is a technique that allows large language models (LLMs) to solve a problem as a series of intermediate steps before giving a final answer. In 2022, Google Brain reported that chain-of-thought prompting improves reasoning ability by inducing the model to answer a multi-step problem with steps of reasoning that mimic a train of thought.[10][15] Chain-of-thought techniques were developed to help LLMs handle multi-step reasoning tasks, such as arithmetic or commonsense reasoning questions.[16][17]\n\nFor example, given the question, \"Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\", Google claims that a CoT prompt might induce the LLM to answer \"A: The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. The answer is 9.\"[10] When applied to PaLM, a 540 billion parameter language model, according to Google, CoT prompting significantly aided the model, allowing it to perform comparably with task-specific fine-tuned models on several tasks, achieving state-of-the-art results at the time on the GSM8K mathematical reasoning benchmark.[10] It is possible to fine-tune models on CoT reasoning datasets to enhance this capability further and stimulate better interpretability.[18][19]\n\nAs originally proposed by Google,[10] each CoT prompt is accompanied by a set of input/output examples—called exemplars—to demonstrate the desired model output, making it a few-shot prompting technique. However, according to a later paper from researchers at Google and the University of Tokyo, simply appending the words \"Let's think step-by-step\"[20] was also effective, which allowed for CoT to be employed as a zero-shot technique.\n\nAn example format of few-shot CoT prompting with in-context exemplars:[21]\n\nAn example format of zero-shot CoT prompting:[20]\n\nIn-context learning, refers to a model's ability to temporarily learn from prompts. For example, a prompt may include a few examples for a model to learn from, such as asking the model to complete \"maison → house, chat → cat, chien →\" (the expected response being dog),[22] an approach called few-shot learning.[23]\n\nIn-context learning is an emergent ability[24] of large language models. It is an emergent property of model scale, meaning that breaks[25] in downstream scaling laws occur, leading to its efficacy increasing at a different rate in larger models than in smaller models.[24][10] Unlike training and fine-tuning, which produce lasting changes, in-context learning is temporary.[26] Training models to perform in-context learning can be viewed as a form of meta-learning, or \"learning to learn\".[27]\n\nSelf-Consistency performs several chain-of-thought rollouts, then selects the most commonly reached conclusion out of all the rollouts.[28][29]\n\nTree-of-thought prompting generalizes chain-of-thought by generating multiple lines of reasoning in parallel, with the ability to backtrack or explore other paths. It can use tree search algorithms like breadth-first, depth-first, or beam.[29][30]\n\nResearch consistently demonstrates that LLMs are highly sensitive to subtle variations in prompt formatting, structure, and linguistic properties. Some studies have shown up to 76 accuracy points across formatting changes in few-shot settings.[31] Linguistic features significantly influence prompt effectiveness—such as morphology, syntax, and lexico-semantic changes—which meaningfully enhance task performance across a variety of tasks.[3][32] Clausal syntax, for example, improves consistency and reduces uncertainty in knowledge retrieval.[33] This sensitivity persists even with larger model sizes, additional few-shot examples, or instruction tuning.\n\nTo address sensitivity of models and make them more robust, several methods have been proposed. FormatSpread facilitates systematic analysis by evaluating a range of plausible prompt formats, offering a more comprehensive performance interval.[31] Similarly, PromptEval estimates performance distributions across diverse prompts, enabling robust metrics such as performance quantiles and accurate evaluations under constrained budgets.[34]\n\nRetrieval-augmented generation (RAG) is a technique that enables generative artificial intelligence (Gen AI) models to retrieve and incorporate new information. It modifies interactions with an LLM so that the model responds to user queries with reference to a specified set of documents, using this information to supplement information from its pre-existing training data. This allows LLMs to use domain-specific and/or updated information.[35]\n\nRAG improves large language models by incorporating information retrieval before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce AI hallucinations, which have led to real-world issues like chatbots inventing policies or lawyers citing nonexistent legal cases. By dynamically retrieving information, RAG enables AI to provide more accurate responses without frequent retraining.[36]\n\nGraphRAG (coined by Microsoft Research) is a technique that extends RAG with the use of a knowledge graph (usually, LLM-generated) to allow the model to connect disparate pieces of information, synthesize insights, and holistically understand summarized semantic concepts over large data collections. It was shown to be effective on datasets like the Violent Incident Information from News Articles (VIINA).[37][38]\n\nEarlier work showed the effectiveness of using a knowledge graph for question answering using text-to-query generation.[39] These techniques can be combined to search across both unstructured and structured data, providing expanded context, and improved ranking.\n\nLLMs themselves can be used to compose prompts for LLMs.[40] The automatic prompt engineer algorithm uses one LLM to beam search over prompts for another LLM:[41][42]\n\nCoT examples can be generated by LLM themselves. In \"auto-CoT\", a library of questions are converted to vectors by a model such as BERT. The question vectors are clustered. Questions close to the centroid of each cluster are selected, in order to have a subset of diverse questions. An LLM does zero-shot CoT on each selected question. The question and the corresponding CoT answer are added to a dataset of demonstrations. These diverse demonstrations can then added to prompts for few-shot learning.[43]\n\nAutomatic prompt optimization techniques refine prompts for LLMs using test datasets and comparison metrics to determine whether changes improve performance. Methods such as MiPRO (Minimum Perturbation Prompt Optimization) update prompts with minimal edits,[44] while GEPA (Gradient-based Prompt Augmentation) applies gradient signals over model likelihoods.[45] There are also open-source implementations of such algorithms in frameworks like DSPy[46] and Opik.[47]\n\nIn 2022, text-to-image models like DALL-E 2, Stable Diffusion, and Midjourney were released to the public. These models take text prompts as input and use them to generate images.[48][6]\n\nEarly text-to-image models typically do not understand negation, grammar and sentence structure in the same way as large language models, and may thus require a different set of prompting techniques. The prompt \"a party with no cake\" may produce an image including a cake.[49] As an...",
  "source_type": "wikipedia_article",
  "domain": "general_knowledge",
  "keywords": [
    "wikipedia",
    "ai",
    "programming",
    "technology",
    "prompt engineering"
  ],
  "retrieval_tags": [
    "wikipedia",
    "article",
    "ai",
    "programming",
    "knowledge"
  ]
}