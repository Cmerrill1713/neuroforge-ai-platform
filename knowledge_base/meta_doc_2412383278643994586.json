{
  "id": "meta_doc_2412383278643994586",
  "title": "meta-llama (Meta Llama)",
  "description": "Meta Llama\nEnterprise\ncompany\nVerified\nhttps://ai.meta.com/llama/\nmeta-llama\nActivity Feed\nFollow\n62,860\nAI & ML interests\nNone defined yet.\nRecent Activity\nmlcu\nauthored\na paper\n4 days ago\nARE: Scaling Up Agent Environments and Evaluations\nBenjamin-eecs\nauthored\na paper\n26 days ago\nLLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model\njcklcn\nauthored\na paper\n26 days ago\nJointly Reinforcing Diversity and Quality in Language Model Generations\nView all activity\nTeam members\n605\n+571...",
  "url": "https://huggingface.co/meta-llama",
  "content": "Meta Llama\nEnterprise\ncompany\nVerified\nhttps://ai.meta.com/llama/\nmeta-llama\nActivity Feed\nFollow\n62,860\nAI & ML interests\nNone defined yet.\nRecent Activity\nmlcu\nauthored\na paper\n4 days ago\nARE: Scaling Up Agent Environments and Evaluations\nBenjamin-eecs\nauthored\na paper\n26 days ago\nLLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model\njcklcn\nauthored\na paper\n26 days ago\nJointly Reinforcing Diversity and Quality in Language Model Generations\nView all activity\nTeam members\n605\n+571\n+558\n+537\n+527\n+507\nOrganization Card\nCommunity\nAbout org cards\nThe Llama Family\nFrom Meta\nWelcome to the official Hugging Face organization for Llama, Llama Guard, and Prompt Guard models from Meta!\nIn order to access models here, please visit a repo of one of the three families and accept the license terms and acceptable use policy. Requests are processed hourly.\nIn this organization, you can find models in both the original Meta format as well as the Hugging Face transformers format. You can find:\nCurrent:\nLlama 4:\nThe Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.\nThese Llama 4 models mark the beginning of a new era for the Llama ecosystem. We are launching two efficient models in the Llama 4 series, Llama 4 Scout, a 17 billion parameter model with 16 experts, and Llama 4 Maverick, a 17 billion parameter model with 128 experts.\nHistory:\nLlama 3.3:\nThe Llama 3.3 is a text only instruct-tuned model in 70B size (text in/text out).\nLlama 3.2:\nThe Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out).\nLlama 3.2 Vision:\nThe Llama 3.2-Vision collection of multimodal large language models (LLMs) is a collection of pretrained and instruction-tuned image reasoning generative models in 11B and 90B sizes (text + images in / text out)\nLlama 3.1:\na collection of pretrained and fine-tuned text models with sizes ranging from 8 billion to 405 billion parameters pre-trained on ~15 trillion tokens.\nLlama 3.1 Evals:\na collection that provides detailed information on how we derived the reported benchmark metrics for the Llama 3.1 models, including the configurations, prompts and model responses used to generate evaluation results.\nLlama Guard 3:\na Llama-3.1-8B pretrained model, aligned to safeguard against the MLCommons standardized hazards taxonomy and designed to support Llama 3.1 capabilities.\nPrompt Guard:\na mDeBERTa-v3-base (86M backbone parameters and 192M word embedding parameters) fine-tuned multi-label model that categorizes input strings into 3 categories - benign, injection, and jailbreak. It is suitable to run as a filter prior to each call to an LLM in an application.\nLlama 2:\na collection of pretrained and fine-tuned text models ranging in scale from 7 billion to 70 billion parameters.\nCode Llama:\na collection of code-specialized versions of Llama 2 in three flavors (base model, Python specialist, and instruct tuned).\nLlama Guard:\na 8B Llama 3 safeguard model for classifying LLM inputs and responses.\nLearn more about the models at\nhttps://ai.meta.com/llama/\nCollections\n15\nLlama 4\nLlama 4 release\nmeta-llama/Llama-4-Scout-17B-16E-Instruct\nImage-Text-to-Text\n•\n109B\n•\nUpdated\nMay 22\n•\n417k\n•\n•\n1.1k\nmeta-llama/Llama-4-Scout-17B-16E\nImage-Text-to-Text\n•\n109B\n•\nUpdated\nApr 9\n•\n22.7k\n•\n203\nmeta-llama/Llama-4-Maverick-17B-128E-Instruct\nImage-Text-to-Text\n•\n402B\n•\nUpdated\nMay 22\n•\n19.2k\n•\n•\n410\nmeta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\nImage-Text-to-Text\n•\n402B\n•\nUpdated\nMay 22\n•\n122k\n•\n•\n135\nLlama 3.3\nThis collection hosts the transformers and original repos of the Llama 3.3\nmeta-llama/Llama-3.3-70B-Instruct\nText Generation\n•\n71B\n•\nUpdated\nDec 21, 2024\n•\n446k\n•\n•\n2.51k\nLlama 4\nLlama 4 release\nmeta-llama/Llama-4-Scout-17B-16E-Instruct\nImage-Text-to-Text\n•\n109B\n•\nUpdated\nMay 22\n•\n417k\n•\n•\n1.1k\nmeta-llama/Llama-4-Scout-17B-16E\nImage-Text-to-Text\n•\n109B\n•\nUpdated\nApr 9\n•\n22.7k\n•\n203\nmeta-llama/Llama-4-Maverick-17B-128E-Instruct\nImage-Text-to-Text\n•\n402B\n•\nUpdated\nMay 22\n•\n19.2k\n•\n•\n410\nmeta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\nImage-Text-to-Text\n•\n402B\n•\nUpdated\nMay 22\n•\n122k\n•\n•\n135\nLlama 3.3\nThis collection hosts the transformers and original repos of the Llama 3.3\nmeta-llama/Llama-3.3-70B-Instruct\nText Generation\n•\n71B\n•\nUpdated\nDec 21, 2024\n•\n446k\n•\n•\n2.51k\nView 15 collections\nmodels\n70\nSort: \n\t\tRecently updated\nmeta-llama/Meta-Llama-3-8B-Instruct\nText Generation\n•\n8B\n•\nUpdated\nJun 18\n•\n1.02M\n•\n•\n4.2k\nmeta-llama/Meta-Llama-3-70B-Instruct\nText Generation\n•\n71B\n•\nUpdated\nJun 18\n•\n754k\n•\n•\n1.49k\nmeta-llama/Llama-4-Maverick-17B-128E-Instruct\nImage-Text-to-Text\n•\n402B\n•\nUpdated\nMay 22\n•\n19.2k\n•\n•\n410\nmeta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\nImage-Text-to-Text\n•\n402B\n•\nUpdated\nMay 22\n•\n122k\n•\n•\n135\nmeta-llama/Llama-4-Scout-17B-16E-Instruct\nImage-Text-to-Text\n•\n109B\n•\nUpdated\nMay 22\n•\n417k\n•\n•\n1.1k\nmeta-llama/Llama-4-Maverick-17B-128E-Instruct-Original\nUpdated\nMay 10\n•\n6\n•\n38\nmeta-llama/Llama-Guard-4-12B\nImage-Text-to-Text\n•\n12B\n•\nUpdated\nApr 29\n•\n51.8k\n•\n•\n57\nmeta-llama/Llama-Prompt-Guard-2-86M\nText Classification\n•\n0.3B\n•\nUpdated\nApr 29\n•\n61.9k\n•\n•\n66\nmeta-llama/Llama-Prompt-Guard-2-22M\nText Classification\n•\n0.1B\n•\nUpdated\nApr 29\n•\n28.1k\n•\n21\nmeta-llama/Llama-4-Maverick-17B-128E\nImage-Text-to-Text\n•\n402B\n•\nUpdated\nApr 9\n•\n37.9k\n•\n86\nView 70\n\t\t\t\t\t\t\tmodels\ndatasets\n11\nSort: \n\t\tRecently updated\nmeta-llama/Llama-3.3-70B-Instruct-evals\nViewer\n•\nUpdated\nDec 6, 2024\n•\n41.3k\n•\n1.26k\n•\n41\nmeta-llama/Llama-3.1-70B-Instruct-evals\nViewer\n•\nUpdated\nOct 2, 2024\n•\n158k\n•\n154\n•\n12\nmeta-llama/Llama-3.1-70B-evals\nViewer\n•\nUpdated\nOct 2, 2024\n•\n79.7k\n•\n23\n•\n10\nmeta-llama/Llama-3.1-8B-Instruct-evals\nViewer\n•\nUpdated\nOct 2, 2024\n•\n158k\n•\n1.7k\n•\n32\nmeta-llama/Llama-3.1-405B-Instruct-evals\nViewer\n•\nUpdated\nOct 2, 2024\n•\n158k\n•\n65\n•\n23\nmeta-llama/Llama-3.1-405B-evals\nViewer\n•\nUpdated\nOct 2, 2024\n•\n79.7k\n•\n31\n•\n14\nmeta-llama/Llama-3.1-8B-evals\nViewer\n•\nUpdated\nOct 2, 2024\n•\n79.7k\n•\n469\n•\n23\nmeta-llama/Llama-3.2-3B-evals\nViewer\n•\nUpdated\nSep 25, 2024\n•\n48.6k\n•\n81\n•\n7\nmeta-llama/Llama-3.2-3B-Instruct-evals\nViewer\n•\nUpdated\nSep 25, 2024\n•\n142k\n•\n385\n•\n17\nmeta-llama/Llama-3.2-1B-evals\nViewer\n•\nUpdated\nSep 25, 2024\n•\n48.6k\n•\n155\n•\n8\nView 11\n\t\t\t\t\t\t\tdatasets",
  "source_type": "meta_documentation",
  "domain": "meta_ai",
  "keywords": [
    "meta",
    "ai",
    "llama",
    "transformer",
    "language model"
  ],
  "retrieval_tags": [
    "meta",
    "ai",
    "documentation",
    "guide"
  ]
}