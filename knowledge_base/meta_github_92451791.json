{
  "id": "meta_github_92451791",
  "title": "low-shot-shrink-hallucinate",
  "description": "Presenting Low-shot Visual Recognition by Shrinking and Hallucinating Features",
  "url": "https://github.com/facebookresearch/low-shot-shrink-hallucinate",
  "language": "Python",
  "stars": 310,
  "forks": 69,
  "created_at": "2017-05-25T23:10:20Z",
  "updated_at": "2025-04-30T20:57:24Z",
  "topics": [],
  "readme_content": "# Low-shot Learning by Shrinking and Hallucinating Features\n\nThis repository contains code associated with the following paper:<br>\n[Low-shot Visual Recognition by Shrinking and Hallucinating Features](https://arxiv.org/abs/1606.02819) <br>\n[Bharath Hariharan](http://home.bharathh.info/), [Ross Girshick](http://www.rossgirshick.info/)<br>\narxiv 2016.\n\nYou can find trained models [here](https://dl.fbaipublicfiles.com/low-shot-shrink-hallucinate/models.zip).\n\n\n## Prerequisites\nThis code uses [pytorch](http://pytorch.org/), [numpy](http://www.numpy.org/) and [h5py](http://www.h5py.org/). It requires GPUs and Cuda.\n\n## Running the code\n\nRunning a low-shot learning experiment will involve three or four steps:\n1.  Train a ConvNet representation\n2.  Save features from the ConvNet\n3.  (Optional) Train analogy-based generator\n4.  Use saved features to train and test on the low-shot learning benchmark.\nEach step is described below.\n\nThe scripts directory contains scripts required to generate results for the baseline representation, representations trained with the SGM loss or L2 regularization, and results with and without the analogy-based generation strategy.\n\n\n### Training a ConvNet representation\nTo train the ConvNet, we first need to specify the training and validation sets.\nThe training and validation datasets, together with data-augmentation and preprocessing steps, are specified through yaml files: see `base_classes_train_template.yaml` and `base_classes_val_template.yaml`.\nYou will need to specify the path to the directory containing ImageNet in each file.\n\nThe main entry point for training a ConvNet representation is `main.py`. For example, to train a ResNet10 representation with the sgm loss, run:\n\n    mkdir -p checkpoints/ResNet10_sgm\n    python ./main.py --model ResNet10 \\\n      --traincfg base_classes_train_template.yaml \\\n      --valcfg base_classes_val_template.yaml \\\n      --print_freq 10 --save_freq 10 \\\n      --aux_loss_wt 0.02 --aux_loss_type sgm \\\n      --checkpoint_dir checkpoints/ResNet10_sgm\nHere, `aux_loss_type` is the kind of auxilliary loss to use (`sgm` or `l2` or `batchsgm`), `aux_loss_wt` is the weight attached to this auxilliary loss, and `checkpoint_dir` is a cache directory to save the checkpoints.\n\nThe model checkpoints will be saved as epoch-number.tar. Training by default runs for 90 epochs, so the final model saved will be `89.tar`.\n\n### Saving features from the ConvNet\nThe next step is to save features from the trained ConvNet. This is fairly straightforward: first, create a directoryto save the features in, and then save the features for the train set and the validation set. Thus, for the ResNet10 model trained above:\n\n    mkdir -p features/ResNet10_sgm\n    python ./save_features.py \\\n      --cfg train_save_data.yaml \\\n      --outfile features/ResNet10_sgm/train.hdf5 \\\n      --modelfile checkpoints/ResNet10_sgm/89.tar \\\n      --model ResNet10\n    python ./save_features.py \\\n      --cfg val_save_data.yaml \\\n      --outfile features/ResNet10_sgm/val.hdf5 \\\n      --modelfile checkpoints/ResNet10_sgm/89.tar \\\n      --model ResNet10\n\n\n### Training the analogy-based generator\nThe entry point for training the analogy-based generator is `train_analogy_generator.py`.\nTo train the analogy based generation on the above representation, run:\n\n    mkdir generation\n    python ./train_analogy_generator.py \\\n      --lowshotmeta label_idx.json \\\n      --trainfile features/ResNet10_sgm/train.hdf5 \\\n      --outdir generation \\\n      --networkfile checkpoints/ResNet10_sgm/89.tar \\\n      --initlr 1\n\nHere, label_idx.json contains the split of base and novel classes, and is used to pick out the saved features corresponding to just the base classes.\nThe analogy generation has several steps and maintains a cache.\nThe final generator will be saved in generation/ResNet10_sgm/generator.tar\n\n### Running the low shot benchmark\nThe benchmark tests with 5 different settings for the number of novel category examples\n_n_ = {1,2,5,10,20}.\nThe benchmark is organized into 5 experiments, with each experiment corresponding to a fixed choice of _n_ examples for each category.\n\nThe main entry point for running the low shot benchmark is `low_shot.py`, which will run a single experiment for a single value of _n_. Thus, to run the benchmark, `low_shot.py` will have to be run 25 times. This design choice has been made to allow the 25 experiments to be run in parallel.\n\nThere is one final wrinkle. To allow cross-validation of hyperparameters, there are two different setups for the benchmark: a validation setup, and a test setup. The setups use different settings for the hyperparameters.\n\nTo run the benchmark, first create a results directory, and then run each experiment for each value of _n_. For example, running the first experiment with _n_=2 on the test setup will look like:\n\n    python ./low_shot.py --lowshotmeta label_idx.json \\\n      --experimentpath experiment_cfgs/splitfile_{:d}.json \\\n      --experimentid  1 --lowshotn 2 \\\n      --trainfile features/ResNet10_sgm/train.hdf5 \\\n      --testfile features/ResNet10_sgm/val.hdf5 \\\n      --outdir results \\\n      --lr 1 --wd 0.001 \\\n      --testsetup 1\n\nIf you want to use the analogy based generator, and generate till there are at least 5 examples per category, then you can run:\n\n    python ./low_shot.py --lowshotmeta label_idx.json \\\n      --experimentpath experiment_cfgs/splitfile_{:d}.json \\\n      --experimentid  1 --lowshotn 2 \\\n      --trainfile features/ResNet10_sgm/train.hdf5 \\\n      --testfile features/ResNet10_sgm/val.hdf5 \\\n      --outdir results \\\n      --lr 1 --wd 0.001 \\\n      --testsetup 1 \\\n      --max_per_label 5 \\\n      --generator_name analogies \\\n      --generator_file generation/ResNet10_sgm/generator.tar\n\nHere `generator_name` is the kind of generator to use; only analogy based generation is implemented, but other ways of generating data can easily be added (see below).\n\nOnce all the experiments are done, you can use the quick-and-dirty script `parse_results.py` to assemble the results:\n\n    python ./parse_results.py --resultsdir results \\\n      --repr ResNet10_sgm \\\n      --lr 1 --wd 0.001 \\\n      --max_per_label 5\n\n## Extensions\n\n### New losses\nIt is fairly easy to implement novel loss functions or forms of regularization. Such losses can be added to `losses.py`, and can make use of the scores, the features, and even the model weights. Create your own loss function, add it to the dictionary of auxiliary losses in `GenericLoss`, and specify how it should be called in the `__call__` function.\n\n### New generation strategies\nAgain, implementing new data generation strategies is also easy. Any generation strategy should provide two functions:\n\n  1. `init_generator` should be able to load whatever state you need to load from a single filename provided as input and return a generator.\n  2. `do_generate` should take four arguments: the original set of novel class feats, novel class labels, the generator produced by `init_generator` and the total number of examples per class we want to target. It should return a new set of novel class feats and novel class labels that include both the real and the generated examples.\n\nAdd any new generation strategy to `generation.py`.\n\n## Matching Networks\nThis repository also includes an implementation of [Matching Networks](https://arxiv.org/abs/1606.04080). Given a saved feature representation (such as the one above), you can train matching networks by running:\n\n    python matching_network.py --test 0 \\\n      --trainfile features/ResNet10_sgm/train.hdf5 \\\n      --lowshotmeta label_idx.json \\\n      --modelfile matching_network_sgm.tar\n\nThis will save the trained model in `matching_network_sgm.tar`.\nThen, test the model using:\n\n    python matching_network.py --test 1 \\\n      --trainfile features/ResNet10_sgm/train.hdf5 \\\n      --testfile features/ResNet10_sgm/val.hdf5 \\\n      --lowshotmeta label_idx.json \\\n      --modelfile matching_network_sgm.tar \\\n      --lowshotn 1 --experimentid 1 \\\n      --experimentpath experiment_cfgs/splitfile_{:d}.json \\\n      --outdir results\n\nAs in the benchmark above, this tests a single experiment for a single value of _n_.\n\n## New results\nThe initial implementation, corresponding to the original paper, was in Lua. For this release, we have switched to Pytorch. As such, there are small differences in the resulting numbers, although the trends are the same. The new numbers are below:\n\n_Top-1, Novel classes_\n\n| Representation      |\tLow-shot phase     | \tn=1 | \t2   | \t5   | \t10   | \t20   |\n| :------------\t      |  :-------------    | :----: | :---: | :---: | :----: | :---: |\n| Baseline\t          |  Baseline          |  2.77  | 10.78 | 26.38 | 35.46\t | 41.49 |\n| Baseline\t          |  Generation\t       |  9.17\t| 15.85\t| 25.47\t| 33.21\t | 40.41 |\n| SGM\t              |  Baseline\t       |  4.14\t| 13.08\t| 27.83\t| 36.04\t | 41.36 |\n| SGM\t              |  Generation\t       |  9.85\t| 17.32\t| 27.89\t| 36.17\t | 41.42 |\n| Batch SGM\t          |  Baseline\t       |  4.16\t| 13.01\t| 28.12\t| 36.56\t | 42.07 |\n| L2\t              |  Baseline\t       |  7.14\t| 16.75\t| 27.73\t| 32.32\t | 35.11 |\n| Baseline\t          |  Matching Networks |  18.33\t| 23.87\t| 31.08\t| 35.27\t | 38.45 |\n| Baseline (Resnet 50)|  Baseline          |  6.82  | 18.37 | 36.55 | 46.15  | 51.99 |\n| Baseline (Resnet 50)|  Generation\t       |  16.58\t| 25.38\t| 36.16\t| 44.53\t | 52.06 |\n| SGM (Resnet 50)\t  |  Baseline\t       |  10.23\t| 21.45\t| 37.25\t| 46.00\t | 51.83 |\n| SGM (Resnet 50)     |  Generation\t       |  15.77\t| 24.43\t| 37.22\t| 45.96\t | 51.82 |\n\n\n_Top-5, Novel classes_\n\n| Representation      |\tLow-shot phase     | \tn=1 | \t2   | \t5   | \t10   | \t20   |\n| :------------\t      |  :-------------    | :----: | :---: | :---: | :----: | :---: |\n| Baseline\t          |  Baseline          | 14.10\t| 33.34\t| 56.20\t| 66.15\t | 71.52 |\n| Baseline\t          |  Generation\t       | 29.68\t| 42.15\t| 56.13\t| 64.52\t | 70.56 |\n| SGM\t              |  Baseline\t       | 23.14\t| 42.37\t| 61.68\t| 69.60\t | 73.76 |\n| SGM\t              |  Generation\t       | 32.80\t| 46.37\t| 61.70\t| 69.71\t | 73.81 |\n| Batch SGM\t          |  Baseline\t       | 22.97\t| 42.35\t| 61.91\t| 69.91\t | 74.45 |\n| L2\t              |  Baseline\t       | 29.08\t| 47.42\t| 62.33\t| 67.96\t | 70.63 |\n| Baseline\t          |  Matching Networks | 41.27\t| 51.25\t| 62.13\t| 67.82\t | 71.78 |\n| Baseline (Resnet 50)|  Baseline          | 28.16\t| 51.03\t| 71.01\t| 78.39\t | 82.32 |\n| Baseline (Resnet 50)|  Generation\t       | 44.76\t| 58.98\t| 71.37\t| 77.65\t | 82.30 |\n| SGM (Resnet 50)\t  |  Baseline\t       | 37.81\t| 57.08\t| 72.78\t| 79.09\t | 82.61 |\n| SGM (Resnet 50)     |  Generation\t       | 45.11\t| 58.83\t| 72.76\t| 79.09\t | 82.61 |\n\n_Top-1, Base classes_\n\n| Representation      |\tLow-shot phase     | \tn=1 | \t2   | \t5   | \t10   | \t20   |\n| :------------\t      |  :-------------    | :----: | :---: | :---: | :----: | :---: |\n| Baseline\t          |  Baseline          | 71.04\t| 69.63\t| 65.67\t| 63.56\t | 62.83 |\n| Baseline\t          |  Generation\t       | 72.38\t| 70.12\t| 68.50\t| 68.11\t | 69.47 |\n| SGM\t              |  Baseline\t       | 75.76\t| 74.24\t| 70.82\t| 69.02\t | 68.29 |\n| SGM\t              |  Generation\t       | 72.62\t| 71.05\t| 70.86\t| 68.88\t | 68.24 |\n| Batch SGM\t          |  Baseline\t       | 75.75\t| 74.50\t| 70.83\t| 68.87\t | 68.04 |\n| L2\t              |  Baseline\t       | 74.50\t| 72.26\t| 69.99\t| 69.62\t | 69.42 |\n| Baseline\t          |  Matching Networks | 48.71\t| 52.10\t| 58.65\t| 62.55\t | 65.25 |\n| Baseline (Resnet 50)|  Baseline          | 83.16\t| 81.94\t| 78.36\t| 76.27\t | 75.32 |\n| Baseline (Resnet 50)|  Generation\t       | 79.39\t| 77.81\t| 76.86\t| 76.12\t | 75.27 |\n| SGM (Resnet 50)\t  |  Baseline\t       | 83.96\t| 82.52\t| 79.04\t| 76.78\t | 75.37 |\n| SGM (Resnet 50)     |  Generation\t       | 81.17\t| 79.60\t| 79.04\t| 76.84\t | 75.35 |\n\n_Top-5, Base classes_\n\n| Representation      |\tLow-shot phase     | \tn=1 | \t2   | \t5   | \t10   | \t20   |\n| :------------\t      |  :-------------    | :----: | :---: | :---: | :----: | :---: |\n| Baseline\t          |  Baseline          | 88.90\t| 87.53\t| 84.56\t| 83.23\t | 82.76 |\n| Baseline\t          |  Generation\t       | 88.32\t| 86.81\t| 85.61\t| 85.56\t | 86.97 |\n| SGM\t              |  Baseline\t       | 91.00\t| 89.32\t| 86.67\t| 85.51\t | 84.97 |\n| SGM\t              |  Generation\t       | 88.43\t| 87.12\t| 86.62\t| 85.49\t | 84.95 |\n| Batch SGM\t          |  Baseline\t       | 91.13\t| 89.35\t| 86.55\t| 85.38\t | 84.88 |\n| L2\t              |  Baseline\t       | 90.03\t| 87.84\t| 85.99\t| 85.63\t | 85.60 |\n| Baseline\t          |  Matching Networks | 76.70\t| 77.82\t| 80.59\t| 82.19\t | 83.27 |\n| Baseline (Resnet 50)|  Baseline          | 95.11\t| 94.02\t| 91.84\t| 90.71\t | 90.23 |\n| Baseline (Resnet 50)|  Generation\t       | 92.30\t| 91.26\t| 90.48\t| 90.33\t | 90.18 |\n| SGM (Resnet 50)\t  |  Baseline\t       | 95.25\t| 93.81\t| 91.32\t| 89.89\t | 89.28 |\n| SGM (Resnet 50)     |  Generation\t       | 92.94\t| 91.67\t| 91.35\t| 89.90\t | 89.21 |\n\n_Top-1, All classes_\n\n| Representation      |\tLow-shot phase     | \tn=1 | \t2   | \t5   | \t10   | \t20   |\n| :------------\t      |  :-------------    | :----: | :---: | :---: | :----: | :---: |\n| Baseline\t          |  Baseline          | 29.16\t| 33.53\t| 41.57\t| 46.33\t | 49.74 |\n| Baseline\t          |  Generation\t       | 33.60\t| 36.83\t| 42.11\t| 46.70\t | 51.65 |\n| SGM\t              |  Baseline\t       | 31.83\t| 36.73\t| 44.45\t| 48.79\t | 51.77 |\n| SGM\t              |  Generation\t       | 34.12\t| 38.09\t| 44.50\t| 48.82\t | 51.79 |\n| Batch SGM\t          |  Baseline\t       | 31.84\t| 36.78\t| 44.63\t| 49.05\t | 52.11 |\n| L2\t              |  Baseline\t       | 33.18\t| 38.21\t| 44.07\t| 46.74\t | 48.37 |\n| Baseline\t          |  Matching Networks | 30.08\t| 34.78\t| 41.74\t| 45.81\t | 48.81 |\n| Baseline (Resnet 50)|  Baseline          | 36.33\t| 42.95\t| 52.71\t| 57.79\t | 61.01 |\n| Baseline (Resnet 50)|  Generation\t       | 40.86\t| 45.65\t| 51.90\t| 56.74\t | 61.03 |\n| SGM (Resnet 50)\t  |  Baseline\t       | 38.73\t| 45.06\t| 53.40\t| 57.90\t | 60.93 |\n| SGM (Resnet 50)     |  Generation\t       | 41.05\t| 45.76\t| 53.39\t| 57.90\t | 60.92 |\n\n_Top-5, All classes_\n\n| Representation      |\tLow-shot phase     | \tn=1 | \t2   | \t5   | \t10   | \t20   |\n| :------------\t      |  :-------------    | :----: | :---: | :---: | :----: | :---: |\n| Baseline\t          |  Baseline          | 43.02\t| 54.29\t| 67.17\t| 72.75\t | 75.86 |\n| Baseline\t          |  Generation\t       | 52.35\t| 59.42\t| 67.53\t| 72.65\t | 76.91 |\n| SGM\t              |  Baseline\t       | 49.37\t| 60.52\t| 71.34\t| 75.75\t | 78.10 |\n| SGM\t              |  Generation\t       | 54.31\t| 62.12\t| 71.33\t| 75.81\t | 78.12 |\n| Batch SGM\t          |  Baseline\t       | 49.32\t| 60.52\t| 71.44\t| 75.89\t | 78.48 |\n| L2\t              |  Baseline\t       | 52.65\t| 63.05\t| 71.48\t| 74.79\t | 76.41 |\n| Baseline\t          |  Matching Networks | 54.97\t| 61.52\t| 69.27\t| 73.38\t | 76.22 |\n| Baseline (Resnet 50)|  Baseline          | 54.05\t| 67.65\t| 79.07\t| 83.15\t | 85.37 |\n| Baseline (Resnet 50)|  Generation\t       | 63.14\t| 71.45\t| 78.76\t| 82.55\t | 85.35 |\n| SGM (Resnet 50)\t  |  Baseline\t       | 60.02\t| 71.28\t| 79.95\t| 83.27\t | 85.19 |\n| SGM (Resnet 50)     |  Generation\t       | 63.60\t| 71.53\t| 79.95\t| 83.27\t | 85.16 |\n",
  "source_type": "meta_github_repository",
  "domain": "meta_ai",
  "keywords": [],
  "retrieval_tags": [
    "meta",
    "ai",
    "github",
    "repository",
    "research"
  ]
}