{
  "id": "github_957716837",
  "title": "just-prompt",
  "description": "just-prompt is an MCP server that provides a unified interface to top LLM providers (OpenAI, Anthropic, Google Gemini, Groq, DeepSeek, and Ollama)",
  "url": "https://github.com/disler/just-prompt",
  "language": "Python",
  "stars": 652,
  "forks": 120,
  "created_at": "2025-03-31T02:07:01Z",
  "updated_at": "2025-09-28T17:31:38Z",
  "topics": [],
  "readme_content": "# Just Prompt - A lightweight MCP server for LLM providers\n\n`just-prompt` is a Model Control Protocol (MCP) server that provides a unified interface to various Large Language Model (LLM) providers including OpenAI, Anthropic, Google Gemini, Groq, DeepSeek, and Ollama. See how we use the `ceo_and_board` tool to make [hard decisions easy with o3 here](https://youtu.be/LEMLntjfihA).\n\n<img src=\"images/just-prompt-logo.png\" alt=\"Just Prompt Logo\" width=\"700\" height=\"auto\">\n\n<img src=\"images/o3-as-a-ceo.png\" alt=\"Just Prompt Logo\" width=\"700\" height=\"auto\">\n\n\n## Tools\n\nThe following MCP tools are available in the server:\n\n- **`prompt`**: Send a prompt to multiple LLM models\n  - Parameters:\n    - `text`: The prompt text\n    - `models_prefixed_by_provider` (optional): List of models with provider prefixes. If not provided, uses default models.\n\n- **`prompt_from_file`**: Send a prompt from a file to multiple LLM models\n  - Parameters:\n    - `abs_file_path`: Absolute path to the file containing the prompt (must be an absolute path, not relative)\n    - `models_prefixed_by_provider` (optional): List of models with provider prefixes. If not provided, uses default models.\n\n- **`prompt_from_file_to_file`**: Send a prompt from a file to multiple LLM models and save responses as markdown files\n  - Parameters:\n    - `abs_file_path`: Absolute path to the file containing the prompt (must be an absolute path, not relative)\n    - `models_prefixed_by_provider` (optional): List of models with provider prefixes. If not provided, uses default models.\n    - `abs_output_dir` (default: \".\"): Absolute directory path to save the response markdown files to (must be an absolute path, not relative)\n\n- **`ceo_and_board`**: Send a prompt to multiple 'board member' models and have a 'CEO' model make a decision based on their responses\n  - Parameters:\n    - `abs_file_path`: Absolute path to the file containing the prompt (must be an absolute path, not relative)\n    - `models_prefixed_by_provider` (optional): List of models with provider prefixes to act as board members. If not provided, uses default models.\n    - `abs_output_dir` (default: \".\"): Absolute directory path to save the response files and CEO decision (must be an absolute path, not relative)\n    - `ceo_model` (default: \"openai:o3\"): Model to use for the CEO decision in format \"provider:model\"\n\n- **`list_providers`**: List all available LLM providers\n  - Parameters: None\n\n- **`list_models`**: List all available models for a specific LLM provider\n  - Parameters:\n    - `provider`: Provider to list models for (e.g., 'openai' or 'o')\n\n## Provider Prefixes\n> every model must be prefixed with the provider name\n>\n> use the short name for faster referencing\n\n- `o` or `openai`: OpenAI \n  - `o:gpt-4o-mini`\n  - `openai:gpt-4o-mini`\n- `a` or `anthropic`: Anthropic \n  - `a:claude-3-5-haiku`\n  - `anthropic:claude-3-5-haiku`\n- `g` or `gemini`: Google Gemini \n  - `g:gemini-2.5-pro-exp-03-25`\n  - `gemini:gemini-2.5-pro-exp-03-25`\n- `q` or `groq`: Groq \n  - `q:llama-3.1-70b-versatile`\n  - `groq:llama-3.1-70b-versatile`\n- `d` or `deepseek`: DeepSeek \n  - `d:deepseek-coder`\n  - `deepseek:deepseek-coder`\n- `l` or `ollama`: Ollama \n  - `l:llama3.1`\n  - `ollama:llama3.1`\n\n## Features\n\n- Unified API for multiple LLM providers\n- Support for text prompts from strings or files\n- Run multiple models in parallel\n- Automatic model name correction using the first model in the `--default-models` list\n- Ability to save responses to files\n- Easy listing of available providers and models\n\n## Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/yourusername/just-prompt.git\ncd just-prompt\n\n# Install with pip\nuv sync\n```\n\n### Environment Variables\n\nCreate a `.env` file with your API keys (you can copy the `.env.sample` file):\n\n```bash\ncp .env.sample .env\n```\n\nThen edit the `.env` file to add your API keys (or export them in your shell):\n\n```\nOPENAI_API_KEY=your_openai_api_key_here\nANTHROPIC_API_KEY=your_anthropic_api_key_here\nGEMINI_API_KEY=your_gemini_api_key_here\nGROQ_API_KEY=your_groq_api_key_here\nDEEPSEEK_API_KEY=your_deepseek_api_key_here\nOLLAMA_HOST=http://localhost:11434\n```\n\n## Claude Code Installation\n> In all these examples, replace the directory with the path to the just-prompt directory.\n\nDefault models set to `openai:o3:high`, `openai:o4-mini:high`, `anthropic:claude-opus-4-20250514`, `anthropic:claude-sonnet-4-20250514`, `gemini:gemini-2.5-pro-preview-03-25`, and `gemini:gemini-2.5-flash-preview-04-17`.\n\nIf you use Claude Code right out of the repository you can see in the .mcp.json file we set the default models to...\n\n```\n{\n  \"mcpServers\": {\n    \"just-prompt\": {\n      \"type\": \"stdio\",\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \".\",\n        \"run\",\n        \"just-prompt\",\n        \"--default-models\",\n        \"openai:o3:high,openai:o4-mini:high,anthropic:claude-opus-4-20250514,anthropic:claude-sonnet-4-20250514,gemini:gemini-2.5-pro-preview-03-25,gemini:gemini-2.5-flash-preview-04-17\"\n      ],\n      \"env\": {}\n    }\n  }\n}\n```\n\nThe `--default-models` parameter sets the models to use when none are explicitly provided to the API endpoints. The first model in the list is also used for model name correction when needed. This can be a list of models separated by commas.\n\nWhen starting the server, it will automatically check which API keys are available in your environment and inform you which providers you can use. If a key is missing, the provider will be listed as unavailable, but the server will still start and can be used with the providers that are available.\n\n### Using `mcp add-json`\n\nCopy this and paste it into claude code with BUT don't run until you copy the json\n\n```\nclaude mcp add just-prompt \"$(pbpaste)\"\n```\n\nJSON to copy\n\n```\n{\n    \"command\": \"uv\",\n    \"args\": [\"--directory\", \".\", \"run\", \"just-prompt\"]\n}\n```\n\nWith a custom default model set to `openai:gpt-4o`.\n\n```\n{\n    \"command\": \"uv\",\n    \"args\": [\"--directory\", \".\", \"run\", \"just-prompt\", \"--default-models\", \"openai:gpt-4o\"]\n}\n```\n\nWith multiple default models:\n\n```\n{\n    \"command\": \"uv\",\n    \"args\": [\"--directory\", \".\", \"run\", \"just-prompt\", \"--default-models\", \"openai:o3:high,openai:o4-mini:high,anthropic:claude-opus-4-20250514,anthropic:claude-sonnet-4-20250514,gemini:gemini-2.5-pro-preview-03-25,gemini:gemini-2.5-flash-preview-04-17\"]\n}\n```\n\n### Using `mcp add` with project scope\n\n```bash\n# With default models\nclaude mcp add just-prompt -s project \\\n  -- \\\n    uv --directory . \\\n    run just-prompt\n\n# With custom default model\nclaude mcp add just-prompt -s project \\\n  -- \\\n  uv --directory . \\\n  run just-prompt --default-models \"openai:gpt-4o\"\n\n# With multiple default models\nclaude mcp add just-prompt -s user \\\n  -- \\\n  uv --directory . \\\n  run just-prompt --default-models \"openai:o3:high,openai:o4-mini:high,anthropic:claude-opus-4-20250514,anthropic:claude-sonnet-4-20250514,gemini:gemini-2.5-pro-preview-03-25,gemini:gemini-2.5-flash-preview-04-17\"\n```\n\n\n## `mcp remove`\n\nclaude mcp remove just-prompt\n\n## Running Tests\n\n```bash\nuv run pytest\n```\n\n## Codebase Structure\n\n```\n.\n├── ai_docs/                   # Documentation for AI model details\n│   ├── extending_thinking_sonny.md\n│   ├── llm_providers_details.xml\n│   ├── openai-reasoning-effort.md\n│   └── pocket-pick-mcp-server-example.xml\n├── example_outputs/           # Example outputs from different models\n├── list_models.py             # Script to list available LLM models\n├── prompts/                   # Example prompt files\n├── pyproject.toml             # Python project configuration\n├── specs/                     # Project specifications\n│   ├── init-just-prompt.md\n│   ├── new-tool-llm-as-a-ceo.md\n│   └── oai-reasoning-levels.md\n├── src/                       # Source code directory\n│   └── just_prompt/\n│       ├── __init__.py\n│       ├── __main__.py\n│       ├── atoms/             # Core components\n│       │   ├── llm_providers/ # Individual provider implementations\n│       │   │   ├── anthropic.py\n│       │   │   ├── deepseek.py\n│       │   │   ├── gemini.py\n│       │   │   ├── groq.py\n│       │   │   ├── ollama.py\n│       │   │   └── openai.py\n│       │   └── shared/        # Shared utilities and data types\n│       │       ├── data_types.py\n│       │       ├── model_router.py\n│       │       ├── utils.py\n│       │       └── validator.py\n│       ├── molecules/         # Higher-level functionality\n│       │   ├── ceo_and_board_prompt.py\n│       │   ├── list_models.py\n│       │   ├── list_providers.py\n│       │   ├── prompt.py\n│       │   ├── prompt_from_file.py\n│       │   └── prompt_from_file_to_file.py\n│       ├── server.py          # MCP server implementation\n│       └── tests/             # Test directory\n│           ├── atoms/         # Tests for atoms\n│           │   ├── llm_providers/\n│           │   └── shared/\n│           └── molecules/     # Tests for molecules\n│               ├── test_ceo_and_board_prompt.py\n│               ├── test_list_models.py\n│               ├── test_list_providers.py\n│               ├── test_prompt.py\n│               ├── test_prompt_from_file.py\n│               └── test_prompt_from_file_to_file.py\n└── ultra_diff_review/         # Diff review outputs\n```\n\n## Context Priming\nREAD README.md, pyproject.toml, then run git ls-files, and 'eza --git-ignore --tree' to understand the context of the project.\n\n# Reasoning Effort with OpenAI o‑Series\n\nFor OpenAI o‑series reasoning models (`o4-mini`, `o3-mini`, `o3`) you can\ncontrol how much *internal* reasoning the model performs before producing a\nvisible answer.\n\nAppend one of the following suffixes to the model name (after the *provider*\nprefix):\n\n* `:low`   – minimal internal reasoning (faster, cheaper)\n* `:medium` – balanced (default if omitted)\n* `:high`  – thorough reasoning (slower, more tokens)\n\nExamples:\n\n* `openai:o4-mini:low`\n* `o:o4-mini:high`\n\nWhen a reasoning suffix is present, **just‑prompt** automatically switches to\nthe OpenAI *Responses* API (when available) and sets the corresponding\n`reasoning.effort` parameter.  If the installed OpenAI SDK is older, it\ngracefully falls back to the Chat Completions endpoint and embeds an internal\nsystem instruction to approximate the requested effort level.\n\n# Thinking Tokens with Claude\n\nThe Anthropic Claude models `claude-opus-4-20250514` and `claude-sonnet-4-20250514` support extended thinking capabilities using thinking tokens. This allows Claude to do more thorough thought processes before answering.\n\nYou can enable thinking tokens by adding a suffix to the model name in this format:\n- `anthropic:claude-opus-4-20250514:1k` - Use 1024 thinking tokens for Opus 4\n- `anthropic:claude-sonnet-4-20250514:4k` - Use 4096 thinking tokens for Sonnet 4\n- `anthropic:claude-opus-4-20250514:8000` - Use 8000 thinking tokens for Opus 4\n\nNotes:\n- Thinking tokens are supported for `claude-opus-4-20250514`, `claude-sonnet-4-20250514`, and `claude-3-7-sonnet-20250219` models\n- Valid thinking token budgets range from 1024 to 16000\n- Values outside this range will be automatically adjusted to be within range\n- You can specify the budget with k notation (1k, 4k, etc.) or with exact numbers (1024, 4096, etc.)\n\n# Thinking Budget with Gemini\n\nThe Google Gemini model `gemini-2.5-flash-preview-04-17` supports extended thinking capabilities using thinking budget. This allows Gemini to perform more thorough reasoning before providing a response.\n\nYou can enable thinking budget by adding a suffix to the model name in this format:\n- `gemini:gemini-2.5-flash-preview-04-17:1k` - Use 1024 thinking budget\n- `gemini:gemini-2.5-flash-preview-04-17:4k` - Use 4096 thinking budget\n- `gemini:gemini-2.5-flash-preview-04-17:8000` - Use 8000 thinking budget\n\nNotes:\n- Thinking budget is only supported for the `gemini-2.5-flash-preview-04-17` model\n- Valid thinking budget range from 0 to 24576\n- Values outside this range will be automatically adjusted to be within range\n- You can specify the budget with k notation (1k, 4k, etc.) or with exact numbers (1024, 4096, etc.)\n\n## Resources\n- https://docs.anthropic.com/en/api/models-list?q=list+models\n- https://github.com/googleapis/python-genai\n- https://platform.openai.com/docs/api-reference/models/list\n- https://api-docs.deepseek.com/api/list-models\n- https://github.com/ollama/ollama-python\n- https://github.com/openai/openai-python\n\n## Master AI Coding \nLearn to code with AI with foundational [Principles of AI Coding](https://agenticengineer.com/principled-ai-coding?y=jprompt)\n\nFollow the [IndyDevDan youtube channel](https://www.youtube.com/@indydevdan) for more AI coding tips and tricks.",
  "source_type": "github_repository",
  "domain": "software_development",
  "keywords": [
    "ai"
  ],
  "retrieval_tags": [
    "github",
    "repository",
    "code",
    "development"
  ]
}