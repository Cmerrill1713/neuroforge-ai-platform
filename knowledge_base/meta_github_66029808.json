{
  "id": "meta_github_66029808",
  "title": "multipathnet",
  "description": "A Torch implementation of the object detection network from \"A MultiPath Network for Object Detection\" (https://arxiv.org/abs/1604.02135)",
  "url": "https://github.com/facebookresearch/multipathnet",
  "language": "Lua",
  "stars": 1335,
  "forks": 274,
  "created_at": "2016-08-18T20:50:00Z",
  "updated_at": "2025-09-05T12:53:46Z",
  "topics": [],
  "readme_content": "MultiPath Network training code\n==========\n\nThe code provides functionality to train Fast R-CNN and MultiPath Networks in [Torch-7](http://torch.ch).<br>\nCorresponding paper: **A MultiPath Network for Object Detection** http://arxiv.org/abs/1604.02135\n\n![sheep](https://cloud.githubusercontent.com/assets/4953728/17826153/442d027a-666e-11e6-9a1e-2fac95a2d3ba.jpg)\n\nIf you use MultiPathNet in your research, please cite the relevant papers:\n\n```\n@INPROCEEDINGS{Zagoruyko2016Multipath,\n    author = {S. Zagoruyko and A. Lerer and T.-Y. Lin and P. O. Pinheiro and S. Gross and S. Chintala and P. Doll{\\'{a}}r},\n    title = {A MultiPath Network for Object Detection},\n    booktitle = {BMVC}\n    year = {2016}\n}\n```\n\n## Requirements\n\n* Linux\n* NVIDIA GPU with compute capability 3.5+\n\n## Installation\n\nThe code depends on Torch-7, fb.python and several other easy-to-install torch packages.<br>\nTo install Torch, follow http://torch.ch/docs/getting-started.html<br>\nThen install additional packages:\n\n```bash\nluarocks install inn\nluarocks install torchnet\nluarocks install fbpython\nluarocks install class\n```\n\nEvaluation relies on COCO API calls via python interface, because lua interface doesn't support it.\nLua API is used to load annotation files in \\*json to COCO API data structures. This doesn't work for proposal\nfiles as they're too big, so we provide converted proposals for sharpmask and selective search in torch format.\n\nFirst, clone https://github.com/pdollar/coco:\n\n```\ngit clone https://github.com/pdollar/coco\n```\n\nThen install LuaAPI:\n\n```\ncd coco\nluarocks make LuaAPI/rocks/coco-scm-1.rockspec\n```\n\nAnd PythonAPI:\n\n```\ncd coco/PythonAPI\nmake\n```\n\nYou might need to install Cython for this:\n\n```\nsudo apt-get install python-pip\nsudo pip install Cython\n```\n\nYou will have to add the path to PythonAPI to `PYTHONPATH`. Note that this won't work with anaconda as it ships\nwith it's own libraries which conflict with torch.\n\n### EC2 installation script\n\nThanks to @DeegC there is [scripts/ec2-install.sh](scripts/ec2-install.sh) script for quick EC2 setup.\n\n## Data preparation\n\nThe root folder should have a folder `data` with the following subfolders:\n\n```\nmodels/\nannotations/\nproposals/\n```\n\n`models` folder should contain AlexNet and VGG pretrained imagenet files downloaded from [here](#training). ResNets can resident in other places specified by `resnet_path` env variable.\n\n`annotations` should contain \\*json files downloaded from http://mscoco.org/external. There are \\*json annotation files for\nPASCAL VOC, MSCOCO, ImageNet and other datasets.\n\n`proposals` should contain \\*t7 files downloaded from here\nWe provide selective search VOC 2007 and VOC 2012 proposals converted from https://github.com/rbgirshick/fast-rcnn and SharpMask proposals for COCO 2015 converted from https://github.com/facebookresearch/deepmask, which can be used to compute proposals for new images as well.\n\nHere is an example structure:\n\n```\ndata\n|-- annotations\n|   |-- instances_train2014.json\n|   |-- instances_val2014.json\n|   |-- pascal_test2007.json\n|   |-- pascal_train2007.json\n|   |-- pascal_train2012.json\n|   |-- pascal_val2007.json\n|   `-- pascal_val2012.json\n|-- models\n|   |-- caffenet_fast_rcnn_iter_40000.t7\n|   |-- imagenet_pretrained_alexnet.t7\n|   |-- imagenet_pretrained_vgg.t7\n|   `-- vgg16_fast_rcnn_iter_40000.t7\n`-- proposals\n    |-- VOC2007\n    |   `-- selective_search\n    |       |-- test.t7\n    |       |-- train.t7\n    |       |-- trainval.t7\n    |       `-- val.t7\n    `-- coco\n        `-- sharpmask\n            |-- train.t7\n            `-- val.t7\n```\n\nDownload selective_search proposals for VOC2007:\n\n```bash\nwget https://dl.fbaipublicfiles.com/multipathnet/proposals/VOC2007/selective_search/train.t7\nwget https://dl.fbaipublicfiles.com/multipathnet/proposals/VOC2007/selective_search/val.t7\nwget https://dl.fbaipublicfiles.com/multipathnet/proposals/VOC2007/selective_search/trainval.t7\nwget https://dl.fbaipublicfiles.com/multipathnet/proposals/VOC2007/selective_search/test.t7\n```\n\nDownload sharpmask proposals for COCO:\n\n```bash\nwget https://dl.fbaipublicfiles.com/multipathnet/proposals/coco/sharpmask/train.t7\nwget https://dl.fbaipublicfiles.com/multipathnet/proposals/coco/sharpmask/val.t7\n```\n\nAs for the images themselves, provide paths to VOCDevkit and COCO in [config.lua](config.lua)\n\n## Running DeepMask with MultiPathNet on provided image\n\nWe provide an example of how to extract DeepMask or SharpMask proposals from an image and run recognition MultiPathNet\nto classify them, then do non-maximum suppression and draw the found objects.\n\n1. Clone DeepMask project into the root directory:\n\n  ```bash\n  git clone https://github.com/facebookresearch/deepmask\n  ```\n\n2. Download DeepMask or SharpMask network:\n\n  ```bash\n  cd data/models\n  # download SharpMask based on ResNet-50\n  wget https://dl.fbaipublicfiles.com/deepmask/models/sharpmask/model.t7 -O sharpmask.t7\n  ```\n\n3. Download recognition network:\n\n  ```bash\n  cd data/models\n  # download ResNet-18-based model trained on COCO with integral loss\n  wget https://dl.fbaipublicfiles.com/multipathnet/models/resnet18_integral_coco.t7\n  ```\n\n4. Make sure you have COCO validation .json files in `data/annotations/instances_val2014.json`\n\n5. Pick some image and run the script:\n\n  ```bash\n  th demo.lua -img ./deepmask/data/testImage.jpg\n  ```\n\nAnd you should see this image:\n\n![iterm2 4jpuod lua_khbaaq](https://cloud.githubusercontent.com/assets/4953728/17951035/69d6cb2e-6a5f-11e6-83b8-767c2ae0ae64.png)\n\nSee file [demo.lua](demo.lua) for details.\n\n## Training\n\nThe repository supports training Fast-RCNN and MultiPath networks with data and model multi-GPU paralellism.\nSupported base models are the following:\n\n* AlexNet trained in [caffe](https://github.com/bvlc/caffe) by Ross Girshick, [imagenet_pretrained_alexnet.t7](https://dl.fbaipublicfiles.com/multipathnet/models/imagenet_pretrained_alexnet.t7)\n* VGG trained in [caffe](https://github.com/bvlc/caffe) by Ross Girshick, [imagenet_pretrained_vgg.t7](https://dl.fbaipublicfiles.com/multipathnet/models/imagenet_pretrained_vgg.t7)\n* ResNets trained in torch with [fb.resnet.torch](https://github.com/facebook/fb.resnet.torch) by Sam Gross\n* inception-v3 trained in [tensorflow](https://github.com/tensorflow/tensorflow) by Google\n* Network-In-Network trained in torch with [imagenet-multiGPU.torch](https://github.com/soumith/imagenet-multiGPU.torch) by Sergey Zagoruyko\n\n### PASCAL VOC\n\nTo train Fast-RCNN on VOC2007 trainval with VGG base model and selective search proposals do:\n\n```bash\ntest_nsamples=1000 model=vgg ./scripts/train_fastrcnn_voc2007.sh\n```\n\nThe resulting mAP is slightly (~2 mAP) higher than original Fast-RCNN number. We should mention that the code is not exactly the same\nas we improved ROIPooling by fixing a few bugs, see https://github.com/szagoruyko/imagine-nn/pull/17\n\n### COCO\n\nTo train MultiPathNet with VGG-16 base model on 4 GPUs run:\n\n```bash\ntrain_nGPU=4 test_nGPU=1 ./scripts/train_multipathnet_coco.sh\n```\n\nHere is a graph visualization of the network (click to enlarge):\n\n<a href=\"https://dl.fbaipublicfiles.com/multipathnet/extra/multipathnet.pdf\">\n<img width=\"1109\" alt=\"multipathnet\" src=\"https://cloud.githubusercontent.com/assets/4953728/17974712/70baf428-6ae7-11e6-9881-e0dea8ab9c18.png\">\n</a>\n\nTo train ResNet-18 on COCO do:\n\n```bash\ntrain_nGPU=4 test_nGPU=1 model=resnet resnet_path=./data/models/resnet/resnet-18.t7 ./scripts/train_coco.sh\n```\n\n## Evaluation\n\n### PASCAL VOC\n\nWe provide original models from Fast-RCNN paper converted to torch format here:\n* [caffenet_fast_rcnn_iter_40000.t7](https://dl.fbaipublicfiles.com/multipathnet/models/caffenet_fast_rcnn_iter_40000.t7)\n* [vgg16_fast_rcnn_iter_40000.t7](https://dl.fbaipublicfiles.com/multipathnet/models/vgg16_fast_rcnn_iter_40000.t7)\n\nTo evaluate these models run:\n\n```bash\nmodel=data/models/caffenet_fast_rcnn_iter_40000.t7 ./scripts/eval_fastrcnn_voc2007.sh\nmodel=data/models/vgg_fast_rcnn_iter_40000.t7 ./scripts/eval_fastrcnn_voc2007.sh\n```\n\n### COCO\n\nEvaluate fast ResNet-18-based network trained with integral loss on COCO val5k split ([resnet18_integral_coco.t7](https://dl.fbaipublicfiles.com/multipathnet/models/resnet18_integral_coco.t7) 89MB):\n\n```bash\ntest_nGPU=4 test_nsamples=5000 ./scripts/eval_coco.sh\n```\n\nIt achieves 24.4 mAP using 400 SharpMask proposals per image:\n\n```\nAverage Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.244\nAverage Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.402\nAverage Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.268\nAverage Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.078\nAverage Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.266\nAverage Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.394\nAverage Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.249\nAverage Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.368\nAverage Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.377\nAverage Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.135\nAverage Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.444\nAverage Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.561\n```\n",
  "source_type": "meta_github_repository",
  "domain": "meta_ai",
  "keywords": [],
  "retrieval_tags": [
    "meta",
    "ai",
    "github",
    "repository",
    "research"
  ]
}