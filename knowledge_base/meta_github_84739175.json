{
  "id": "meta_github_84739175",
  "title": "fairseq-lua",
  "description": "Facebook AI Research Sequence-to-Sequence Toolkit",
  "url": "https://github.com/facebookresearch/fairseq-lua",
  "language": "Lua",
  "stars": 3738,
  "forks": 612,
  "created_at": "2017-03-12T16:13:32Z",
  "updated_at": "2025-09-24T04:30:15Z",
  "topics": [],
  "readme_content": "# Introduction\n\n***Note***: there is now a PyTorch version of this toolkit ([fairseq-py](https://github.com/pytorch/fairseq)) and new development efforts will focus on it. The Lua version is preserved here, but is provided without any support.\n\nThis is fairseq, a sequence-to-sequence learning toolkit for [Torch](http://torch.ch/) from Facebook AI Research tailored to Neural Machine Translation (NMT).\nIt implements the convolutional NMT models proposed in [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122) and [A Convolutional Encoder Model for Neural Machine Translation](https://arxiv.org/abs/1611.02344) as well as a standard LSTM-based model.\nIt features multi-GPU training on a single machine as well as fast beam search generation on both CPU and GPU.\nWe provide pre-trained models for English to French, English to German and English to Romanian translation.\n\n![Model](fairseq.gif)\n\n# Citation\n\nIf you use the code in your paper, then please cite it as:\n\n```\n@article{gehring2017convs2s,\n  author          = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},\n  title           = \"{Convolutional Sequence to Sequence Learning}\",\n  journal         = {ArXiv e-prints},\n  archivePrefix   = \"arXiv\",\n  eprinttype      = {arxiv},\n  eprint          = {1705.03122},\n  primaryClass    = \"cs.CL\",\n  keywords        = {Computer Science - Computation and Language},\n  year            = 2017,\n  month           = May,\n}\n```\n\nand\n\n```\n@article{gehring2016convenc,\n  author          = {Gehring, Jonas and Auli, Michael and Grangier, David and Dauphin, Yann N},\n  title           = \"{A Convolutional Encoder Model for Neural Machine Translation}\",\n  journal         = {ArXiv e-prints},\n  archivePrefix   = \"arXiv\",\n  eprinttype      = {arxiv},\n  eprint          = {1611.02344},\n  primaryClass    = \"cs.CL\",\n  keywords        = {Computer Science - Computation and Language},\n  year            = 2016,\n  month           = Nov,\n}\n```\n\n# Requirements and Installation\n* A computer running macOS or Linux\n* For training new models, you'll also need a NVIDIA GPU and [NCCL](https://github.com/NVIDIA/nccl)\n* A [Torch installation](http://torch.ch/docs/getting-started.html). For maximum speed, we recommend using LuaJIT and [Intel MKL](https://software.intel.com/en-us/intel-mkl).\n* A recent version [nn](https://github.com/torch/nn). The minimum required version is from May 5th, 2017. A simple `luarocks install nn` is sufficient to update your locally installed version.\n\nInstall fairseq by cloning the GitHub repository and running\n```\nluarocks make rocks/fairseq-scm-1.rockspec\n```\nLuaRocks will fetch and build any additional dependencies that may be missing.\nIn order to install the CPU-only version (which is only useful for translating new data with an existing model), do\n```\nluarocks make rocks/fairseq-cpu-scm-1.rockspec\n```\n\nThe LuaRocks installation provides a command-line tool that includes the following functionality:\n* `fairseq preprocess`: Data pre-processing: build vocabularies and binarize training data\n* `fairseq train`: Train a new model on one or multiple GPUs\n* `fairseq generate`: Translate pre-processed data with a trained model\n* `fairseq generate-lines`: Translate raw text with a trained model\n* `fairseq score`: BLEU scoring of generated translations against reference translations\n* `fairseq tofloat`: Convert a trained model to a CPU model\n* `fairseq optimize-fconv`: Optimize a fully convolutional model for generation. This can also be achieved by passing the `-fconvfast` flag to the generation scripts.\n\n# Quick Start\n\n## Training a New Model\n\n### Data Pre-processing\nThe fairseq source distribution contains an example pre-processing script for\nthe IWSLT14 German-English corpus.\nPre-process and binarize the data as follows:\n```\n$ cd data/\n$ bash prepare-iwslt14.sh\n$ cd ..\n$ TEXT=data/iwslt14.tokenized.de-en\n$ fairseq preprocess -sourcelang de -targetlang en \\\n  -trainpref $TEXT/train -validpref $TEXT/valid -testpref $TEXT/test \\\n  -thresholdsrc 3 -thresholdtgt 3 -destdir data-bin/iwslt14.tokenized.de-en\n```\nThis will write binarized data that can be used for model training to data-bin/iwslt14.tokenized.de-en.\n\n### Training\nUse `fairseq train` to train a new model.\nHere a few example settings that work well for the IWSLT14 dataset:\n```\n# Standard bi-directional LSTM model\n$ mkdir -p trainings/blstm\n$ fairseq train -sourcelang de -targetlang en -datadir data-bin/iwslt14.tokenized.de-en \\\n  -model blstm -nhid 512 -dropout 0.2 -dropout_hid 0 -optim adam -lr 0.0003125 -savedir trainings/blstm\n\n# Fully convolutional sequence-to-sequence model\n$ mkdir -p trainings/fconv\n$ fairseq train -sourcelang de -targetlang en -datadir data-bin/iwslt14.tokenized.de-en \\\n  -model fconv -nenclayer 4 -nlayer 3 -dropout 0.2 -optim nag -lr 0.25 -clip 0.1 \\\n  -momentum 0.99 -timeavg -bptt 0 -savedir trainings/fconv\n\n# Convolutional encoder, LSTM decoder\n$ mkdir -p trainings/convenc\n$ fairseq train -sourcelang de -targetlang en -datadir data-bin/iwslt14.tokenized.de-en \\\n  -model conv -nenclayer 6 -dropout 0.2 -dropout_hid 0 -savedir trainings/convenc\n```\n\nBy default, `fairseq train` will use all available GPUs on your machine.\nUse the [CUDA_VISIBLE_DEVICES](http://acceleware.com/blog/cudavisibledevices-masking-gpus) environment variable to select specific GPUs or `-ngpus` to change the number of GPU devices that will be used.\n\n### Generation\nOnce your model is trained, you can translate with it using `fairseq generate` (for binarized data) or `fairseq generate-lines` (for text).\nHere, we'll do it for a fully convolutional model:\n```\n# Optional: optimize for generation speed\n$ fairseq optimize-fconv -input_model trainings/fconv/model_best.th7 -output_model trainings/fconv/model_best_opt.th7\n\n# Translate some text\n$ DATA=data-bin/iwslt14.tokenized.de-en\n$ fairseq generate-lines -sourcedict $DATA/dict.de.th7 -targetdict $DATA/dict.en.th7 \\\n  -path trainings/fconv/model_best_opt.th7 -beam 10 -nbest 2\n| [target] Dictionary: 24738 types\n| [source] Dictionary: 35474 types\n> eine sprache ist ausdruck des menschlichen geistes .\nS\teine sprache ist ausdruck des menschlichen geistes .\nO\teine sprache ist ausdruck des menschlichen geistes .\nH\t-0.23804219067097\ta language is expression of human mind .\nA\t2 2 3 4 5 6 7 8 9\nH\t-0.23861141502857\ta language is expression of the human mind .\nA\t2 2 3 4 5 7 6 7 9 9\n```\n\n### CPU Generation\nUse `fairseq tofloat` to convert a trained model to use CPU-only operations (this has to be done on a GPU machine):\n```\n# Optional: optimize for generation speed\n$ fairseq optimize-fconv -input_model trainings/fconv/model_best.th7 -output_model trainings/fconv/model_best_opt.th7\n\n# Convert to float\n$ fairseq tofloat -input_model trainings/fconv/model_best_opt.th7 \\\n  -output_model trainings/fconv/model_best_opt-float.th7\n\n# Translate some text\n$ fairseq generate-lines -sourcedict $DATA/dict.de.th7 -targetdict $DATA/dict.en.th7 \\\n  -path trainings/fconv/model_best_opt-float.th7 -beam 10 -nbest 2\n> eine sprache ist ausdruck des menschlichen geistes .\nS\teine sprache ist ausdruck des menschlichen geistes .\nO\teine sprache ist ausdruck des menschlichen geistes .\nH\t-0.2380430996418\ta language is expression of human mind .\nA\t2 2 3 4 5 6 7 8 9\nH\t-0.23861189186573\ta language is expression of the human mind .\nA\t2 2 3 4 5 7 6 7 9 9\n```\n\n# Pre-trained Models\n\nGeneration with the binarized test sets can be run in batch mode as follows, e.g. for English-French on a GTX-1080ti:\n```\n$ fairseq generate -sourcelang en -targetlang fr -datadir data-bin/wmt14.en-fr -dataset newstest2014 \\\n  -path wmt14.en-fr.fconv-cuda/model.th7 -beam 5 -batchsize 128 | tee /tmp/gen.out\n...\n| Translated 3003 sentences (95451 tokens) in 136.3s (700.49 tokens/s)\n| Timings: setup 0.1s (0.1%), encoder 1.9s (1.4%), decoder 108.9s (79.9%), search_results 0.0s (0.0%), search_prune 12.5s (9.2%)\n| BLEU4 = 43.43, 68.2/49.2/37.4/28.8 (BP=0.996, ratio=1.004, sys_len=92087, ref_len=92448)\n\n# Word-level BLEU scoring:\n$ grep ^H /tmp/gen.out | cut -f3- | sed 's/@@ //g' > /tmp/gen.out.sys\n$ grep ^T /tmp/gen.out | cut -f2- | sed 's/@@ //g' > /tmp/gen.out.ref\n$ fairseq score -sys /tmp/gen.out.sys -ref /tmp/gen.out.ref\nBLEU4 = 40.55, 67.6/46.5/34.0/25.3 (BP=1.000, ratio=0.998, sys_len=81369, ref_len=81194)\n```\n\n# Join the fairseq community\n\n* Facebook page: https://www.facebook.com/groups/fairseq.users\n* Google group: https://groups.google.com/forum/#!forum/fairseq-users\n* Contact: [jgehring@fb.com](mailto:jgehring@fb.com), [michaelauli@fb.com](mailto:michaelauli@fb.com)\n\n# License\nfairseq is BSD-licensed.\nThe license applies to the pre-trained models as well.\nWe also provide an additional patent grant.\n",
  "source_type": "meta_github_repository",
  "domain": "meta_ai",
  "keywords": [
    "ai",
    "facebook",
    "research"
  ],
  "retrieval_tags": [
    "meta",
    "ai",
    "github",
    "repository",
    "research"
  ]
}