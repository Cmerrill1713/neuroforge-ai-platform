{
  "id": "github_729927728",
  "title": "llm-prompt-testing-quick-start",
  "description": "LLM Prompt Testing Quick Start",
  "url": "https://github.com/disler/llm-prompt-testing-quick-start",
  "language": "JavaScript",
  "stars": 73,
  "forks": 14,
  "created_at": "2023-12-10T19:21:29Z",
  "updated_at": "2025-08-27T01:13:22Z",
  "topics": [],
  "readme_content": "# LLM Evaluation & Testing quick start with [Promptfoo](https://www.promptfoo.dev)\n> Better, Faster, Cheaper Prompts with LLM Testing & Evaluation\n\n![Last LLM Standing Wins](imgs/last-llm-standing-wins.png)\n![Local On Device LLMs Are The Future](imgs/local-on-device-llms-are-the-future-but-not-yet.png)\n![Gemini Pro vs GPT-3.5 Turbo](imgs/prompt-testing-gemini-vs-gpt-3.5-turbo.png)\n![Fast, Cheap, Accurate](imgs/fast-cheap-accurate-prompt-testing-with-promptfoo.png)\n\n## Update on format\n- In EXPERIMENT Branch We're modernizing things a little bit\n- Run commands explicitly from package.json\n- Use the promptfoo library and additional node modules inside code (for grok and dotenv etc)\n- use .env and dotenv\n\n## Watch the video tutorials\n- Last LLM Standing Wins [Video Walk Through](https://youtu.be/Cy1Z8J0anKw). In this video we build a visual LLM benchmarking tool built on top of Promptfoo.\n- Check out the brief [Video Tutorial](https://youtu.be/KhINc5XwhKs) where we highlight the key features of Promptfoo and how to get started with this repo.\n- Compare [Gemini Pro vs GPT-3.5 Turbo](https://youtu.be/V_SyO0t7TZY) with Promptfoo.\n- Monitor the performance of [Local, On Device LLMs](https://youtu.be/urymhRw86Fc) with prompt testing\n\n\n## Setup\n\n### API Keys\n\n- To get started with OpenAI, set your OPENAI_API_KEY environment variable.\n  - `export OPENAI_API_KEY=<your key>`\n  - [OpenAI Setup Docs](https://promptfoo.dev/docs/providers/openai)\n- To get started with Gemini, set your VERTEX_API_KEY environment variable.\n  - `export VERTEX_API_KEY=<your key>`\n  - `export VERTEX_PROJECT_ID=<your google cloud project id>`\n  - [Gemini Setup Docs](https://promptfoo.dev/docs/providers/vertex)\n- To setup anthropic\n  - `export ANTHROPIC_API_KEY=<your key>\n  - [Anthropic Setup Docs](https://promptfoo.dev/docs/providers/anthropic)\n- To setup GROQ\n  - `export GROQ_API_KEY=`<your key>\n  - [Groq Setup Docs](https://promptfoo.dev/docs/providers/groq)\n\n### Global Install\n\n- Install promptfoo\n  - `npm install -g promptfoo`\n  - [Install Docs](https://www.promptfoo.dev/docs/installation)\n- cd into the directory you want to test\n- Run `promptfoo eval` to evaluate\n\n### Local Install\n- Install promptfoo\n  - `npm install promptfoo`\n  - [Install Docs](https://www.promptfoo.dev/docs/installation)\n- Update the `package.json` file to include the following scripts\n  - ```json\n    \"scripts\": {\n      \"eval\": \"promptfoo eval -c `./path/to/your/promptfooconfig.yaml`\",\n      \"view\": \"promptfoo view\"\n    }\n    ```\n  - See the package.json for examples\n\n### Opinionated Setup\n- I recommend using separate prompt.txt, test.txt, promptfooconfig.yaml with a dedicated directory and package.json script for each prompt you want to test.\n- This way you can create multiple test + prompt combinations.\n- For example this package.json: script section show cases running different tests\n```json\n\"scripts\": {\n    \"nlq_to_sql_ten\": \"source .env && promptfoo eval -c ./nlq_to_sql/promptfooconfig.yaml -t ./nlq_to_sql/test_ten.yaml -p ./nlq_to_sql/prompt.txt --no-cache --output ./nlq_to_sql/output_ten.json\",\n    \"nlq_to_sql_twenty\": \"source .env && promptfoo eval -c ./nlq_to_sql/promptfooconfig.yaml -t ./nlq_to_sql/test_twenty.yaml -p ./nlq_to_sql/prompt.txt --no-cache --output ./nlq_to_sql/output_twenty.json\",\n    \"view\": \"promptfoo view\"\n  },\n```\n\n### Promptfoo delay if you run into API rate limit issues (anthropic + grok)\n- You can set a delay between prompt tests by using the `PROMPTFOO_DELAY_MS` env variable.\n- [Delay Docs](https://www.promptfoo.dev/docs/providers/openai/#openai-rate-limits)\n\n### Install llamafile to test local models\n> You can reuse the `./custom_models/customModelBase.js` to test llama models locally. Or you can create a new .js file for your model. See [promptfoo custom model docs](https://www.promptfoo.dev/docs/providers/custom-api).\n- Read the instructions here and download the llama files\n  - https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file#quickstart\n  - I recommend installing `mistral-7b-instruct-v0.1-Q4_K_M-main.llamafile` for the best results for 4GB models\n- Place the model into the custom_models/ directory\n- Make sure the name of your model file matches the \n- Create a <your model name>.js file in custom_models/ for your model and inherit the CustomModelBase class. Use `custom_models/mistral-7b-v0.1-Q4.js` as a template.\n- Add the path to the `custom_models/<your model name>.js` in the ./\\*/promptfooconfig.yaml providers section\n- Run `promptfoo eval` to evaluate your model\n\n\n\n### Quickly test your prompts on different custom_models - \n- Use the `sh run_local_llm.sh` script to quickly test prompts on different custom_models. Update the prompt variable to be whatever prompt you want to test.\n\n*If you want to run OpenAI exclusively comment out other models in the ./\\*/promptfooconfig.yaml providers section.*\n\n## Commands\n\n`promptfoo eval` - load and evaluate in the current directory\n\n`promptfoo eval --no-cache` - load and evaluate in the current directory without using the cache\n\n`promptfoo view` - load the UI in the current directory\n\n## Prompt Evaluation Elements\n- Providers\n- Prompts\n- Assertions\n- Variables\n\n## Use Cases & Value Prop of LLM Testing & Evaluation\n\n- 💰 Save Money & Same Time (Resource Optimization)\n  - With LLM testing you can determine if you need GPT-4 or if you can save money and time with GPT-3\n  - You can find the minimum number of tokens you can use without sacrificing quality\n  - Compare different LLM providers to determine which is the best fit for your application\n- 👍 Ship with confidence (Validate Accuracy)\n  - Gain certainty that your prompt will generate the results you want\n  - Confidently generate json responses\n  - Compare prompts to determine which is more accurate\n- ✅ Prevent Regressions (Consistency)\n  - Ensure that the output of a prompt is within the bounds of your expectations\n  - Make sure that when you update your prompt it doesn't break your application\n  - With version control and CI/CD you can ensure your prompts are always working as expected\n\n## Organizational Pattern\n- `/<name of agent/test 1>`\n  - `/prompt.txt` - the prompt(s) to test\n  - `/test.yaml` - variables and assertions\n  - `/promptfooconfig.yaml` - llm config\n- `/<name of agent/test N>`\n  - `...`\n- `...`\n\n## Important Docs & Resources\n- Vertex Promptfoo Provider\n  - https://www.promptfoo.dev/docs/providers/vertex\n- Vertex AI Pricing\n  - https://cloud.google.com/vertex-ai/pricing\n- Great Breakdown of Gemini pro vs gpt-3.5\n  - https://klu.ai/blog/gemini-pro-vs-gpt-3-5-turbo\n  - https://www.promptfoo.dev/docs/guides/gemini-vs-gpt\n- Don't repeat test data\n  - https://www.promptfoo.dev/docs/configuration/guide#avoiding-repetition\n- Ensure output is in json format and the keys exist\n  - https://www.promptfoo.dev/docs/guides/evaluate-json/#ensuring-that-outputs-are-valid-json\n- Reference prompt, and test files using globs and lists\n  - https://www.promptfoo.dev/docs/configuration/parameters#prompts-from-file\n- Assertions ('equals', 'contains', 'is-json', 'levenshtein-distance', 'python', 'regex', 'llm-rubric', and more)\n  - https://www.promptfoo.dev/docs/configuration/expected-outputs/\n- Example using Scenarios for test assertion variables\n  - https://github.com/promptfoo/promptfoo/blob/main/examples/multiple-translations-scenarios/promptfooconfig.yaml\n- LLM Providers\n  - https://www.promptfoo.dev/docs/providers\n- Vertex Provider Src\n  - https://github.com/promptfoo/promptfoo/blob/main/src/providers/vertex.ts#L7-L22\n\n## Gemini Pro vs GPT-3.5 Turbo Highlights\n> Results generated by GPT-4 and then tweaked - take it with two grains of salt.\n- Resources: \n  - Source Blog: https://klu.ai/blog/gemini-pro-vs-gpt-3-5-turbo\n  - Vertex Pricing: https://cloud.google.com/vertex-ai/pricing\n  - Promptfoo: https://www.promptfoo.dev/docs/guides/gemini-vs-gpt\n\n#### Overall:\n- `Gemini-Pro ⚪️⚪️⚪️⚪️🟢|⚪️⚪️⚪️⚪️⚪️ GPT-3.5 Turbo`\n- Explanation: Gemini-Pro has a small to medium sized edge and is likely a better fit for most applications.\n\n#### Pricing Comparison:\n- `Gemini-Pro ⚪️⚪️⚪️⚪️⚪️|⚪️⚪️⚪️⚪️⚪️ GPT-3.5 Turbo`\n- Explanation: Gemini-Pro is the same price as GPT-3.5 Turbo.\n\n#### Speed:\n- `Gemini-Pro ⚪️🟢🟢🟢🟢|⚪️⚪️⚪️⚪️⚪️ GPT-3.5 Turbo`\n- Explanation: Gemini-Pro demonstrates superior speed, processing inputs faster than GPT-3.5 Turbo.\n\n#### Instruction Following:\n- `Gemini-Pro ⚪️⚪️🟢🟢🟢|⚪️⚪️⚪️⚪️⚪️ GPT-3.5 Turbo`\n- Explanation: Gemini-Pro excels at following instructions accurately, outperforming GPT-3.5 Turbo in this regard.\n\n#### Content Generation:\n- `Gemini-Pro ⚪️⚪️⚪️⚪️⚪️|🟢🟢⚪️⚪️⚪️ GPT-3.5 Turbo`\n- Explanation: GPT-3.5 Turbo has a slight advantage in content generation, producing more nuanced and varied outputs.\n\n#### Language Understanding:\n- `Gemini-Pro ⚪️⚪️🟢🟢🟢|⚪️⚪️⚪️⚪️⚪️ GPT-3.5 Turbo`\n- Explanation: Gemini-Pro shows superior language understanding, especially in complex comprehension tasks.\n\n#### Bias:\n- `Gemini-Pro ⚪️⚪️⚪️⚪️⚪️|🟢🟢🟢⚪️⚪️ GPT-3.5 Turbo`\n- Explanation: Gemini seems to exibit more google specific bias than GPT-3.5 Turbo.\n\n#### API Design and Developer Experience:\n- `Gemini-Pro ⚪️⚪️⚪️⚪️⚪️|🟢🟢⚪️⚪️⚪️ GPT-3.5 Turbo`\n- Explanation: OpenAIs API is much easier to use than Vertex AI's API.\n\n#### AI Alignment and Safety:\n- `Gemini-Pro ⚪️⚪️⚪️⚪️⚪️|🟢🟢🟢🟢⚪️ GPT-3.5 Turbo`\n- Explanation: Gemini-Pro is extremely restricted in its capabilities.\n\n#### Multimodal Capabilities:\n- `Gemini-Pro 🟢🟢🟢🟢🟢|⚪️⚪️⚪️⚪️⚪️ GPT-3.5 Turbo`\n- *Explanation: Gemini-Pro supports both text and image inputs, offering a significant advantage over GPT-3.5 Turbo, which is limited to text only.*\n\n\n\n## Great LLM Testing & Evaluation Patterns\n- Trim your prompts to the minimum number of tokens needed to generate the desired output\n- Always compare multiple LLM models to see if you need a expensive, slower model or if a cheaper, faster model will work\n- Add as many test assertions as possible to ensure your prompt is generating the output you expect\n- Your prompts don't 'always' need to validate every assertions, but they should always validate the most important assertions and most test cases\n- Use JSON as the output format for your prompt this makes it easy to validate the output\n- Isolate your prompts into separate files for readability and maintainability\n- Identify which parts of your prompt are variables and which are static then separate them into different test variables\n- Use the `--no-cache` flag to ensure you are always testing the latest version of your prompt\n- Use your users as THE primary source for your test cases. Testing every use case your users will encounter is the best way to ensure your prompt is working as expected. This is especially true for prompts that are used in production. \n- Focus on asserting an acceptable range of results over specific, exact results. LLMs are non-deterministic and will generate different results each time. Your tests should account for this.\n",
  "source_type": "github_repository",
  "domain": "software_development",
  "keywords": [],
  "retrieval_tags": [
    "github",
    "repository",
    "code",
    "development"
  ]
}