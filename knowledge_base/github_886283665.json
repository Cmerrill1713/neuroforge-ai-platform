{
  "id": "github_886283665",
  "title": "benchy",
  "description": "Benchmarks you can feel",
  "url": "https://github.com/disler/benchy",
  "language": "TypeScript",
  "stars": 438,
  "forks": 109,
  "created_at": "2024-11-10T17:04:41Z",
  "updated_at": "2025-09-26T19:59:20Z",
  "topics": [],
  "readme_content": "# BENCHY\n> Benchmarks you can **feel**\n>\n> We all love benchmarks, but there's nothing like a hands on vibe check. What if we could meet somewhere in the middle?\n> \n> Enter BENCHY. A chill, live benchmark tool that lets you see the performance, price, and speed of LLMs in a side by side comparison for SPECIFIC use cases.\n>\n> Watch the latest development [video here](https://youtu.be/f8RnRuaxee8)\n\n## Project Structure\n\nThis project is organized as a full-stack application with clear separation between frontend and backend:\n\n```\nbenchy/\n├── client/                 # Frontend Vue.js application\n│   ├── src/               # Vue source code\n│   │   ├── apis/          # API layer for all requests\n│   │   ├── components/    # Vue components\n│   │   ├── pages/         # Frontend per app pages\n│   │   ├── stores/        # Frontend state and prompts\n│   │   └── ...           # Other source files\n│   ├── public/            # Static assets\n│   ├── package.json       # Frontend dependencies\n│   ├── vite.config.ts     # Vite configuration\n│   ├── uno.config.ts      # UnoCSS configuration\n│   └── ...               # Other frontend config files\n├── server/                # Backend Python server\n│   ├── modules/           # Python modules\n│   ├── benchmark_data/    # Benchmark configurations\n│   ├── reports/          # Benchmark results\n│   ├── tests/            # Python tests\n│   ├── pyproject.toml    # Python dependencies\n│   ├── server.py         # Main server file\n│   ├── .env              # Server environment variables\n│   └── ...               # Other server files\n├── ai_docs/              # AI documentation\n├── images/               # Project images\n├── specs/                # Project specifications\n├── trees/                # Git worktrees directory\n├── .env                  # Root environment variables\n├── start.sh              # Convenience script to start both services\n└── ...                   # Other project files\n```\n\n### Quick Start\n```bash\n# Start both frontend and backend together\n./start.sh\n```\n\nThis will start:\n- Frontend dev server at `http://localhost:5173` (or next available port)\n- Backend API server at `http://localhost:8000`\n\n<img src=\"./images/o3-mini.png\" alt=\"deepseek-r1\" style=\"max-width: 800px;\">\n\n<img src=\"./images/deepseek-r1.png\" alt=\"deepseek-r1\" style=\"max-width: 800px;\">\n\n<img src=\"./images/o1-ai-coding-limit-testing.png\" alt=\"o1-ai-coding-limit-testing\" style=\"max-width: 800px;\">\n\n<img src=\"./images/m4-max-mac-book-pro-benchmarked.png\" alt=\"m4-mac-book-pro\" style=\"max-width: 800px;\">\n\n<img src=\"./images/parallel-function-calling.png\" alt=\"parallel-function-calling\" style=\"max-width: 800px;\">\n\n<img src=\"./images/perf-price-speed-pick-two.png\" alt=\"pick-two\" style=\"max-width: 800px;\">\n\n## Benchy Micro Apps\n- [Thought Bench](https://youtu.be/UgSGtBZnwEo)\n  - Goal: Compare multiple reasoning models side by side in parallel to analyze their thinking processes and responses\n  - Default models: Anthropic Claude 4.0 Sonnet/Opus, OpenAI o4-mini/o3, Anthropic Claude 3.7 Sonnet, Gemini 2.5 Flash/Pro, Ollama Qwen3/Gemma3/Devstral\n  - Watch the walk through [video here](https://youtu.be/UgSGtBZnwEo)\n  - Front end: [client/src/pages/ThoughtBench.vue](client/src/pages/ThoughtBench.vue)\n- [BIG AI Coding Updates to Benchy](https://youtu.be/y_ywOVQyafE)\n  - Watch the walk through [video here](https://youtu.be/y_ywOVQyafE)\n- [Iso Speed Bench](https://youtu.be/OwUm-4I22QI)\n  - Goal: Create a unified, config file based, multi-llm provider, yes/no evaluation based benchmark for high quality insights and iteration.\n  - Watch o3-mini vibe check, comparison, and benchmark [video here](https://youtu.be/K5xs669ANQo)\n  - Watch the M4 Unboxing and benchmark [video here](https://youtu.be/OwUm-4I22QI)\n  - Front end: [client/src/pages/IsoSpeedBench.vue](client/src/pages/IsoSpeedBench.vue)\n- [Long Tool Calling](https://youtu.be/ZlljCLhq814)\n  - Goal: Understand the best LLMs and techniques for LONG chains of tool calls / function calls (15+).\n  - Watch the walk through [video here](https://youtu.be/ZlljCLhq814)\n  - Front end: [client/src/pages/AppMultiToolCall.vue](client/src/pages/AppMultiToolCall.vue)\n- [Multi Autocomplete](https://youtu.be/1ObiaSiA8BQ)\n  - Goal: Understand [claude 3.5 haiku](https://www.anthropic.com/claude/haiku) & GPT-4o [predictive outputs](https://platform.openai.com/docs/guides/predicted-outputs) compared to existing models. \n  - Watch the walk through [video here](https://youtu.be/1ObiaSiA8BQ)\n  - Front end: [client/src/pages/AppMultiAutocomplete.vue](client/src/pages/AppMultiAutocomplete.vue)\n\n## Important Files\n\n### Frontend (client/)\n- `client/package.json` - Frontend dependencies and scripts\n- `client/src/stores/*` - Stores all frontend state and prompts\n- `client/src/apis/*` - API layer for all requests\n- `client/src/pages/*` - Frontend per app pages\n- `client/src/components/*` - Vue components\n- `client/vite.config.ts` - Vite build configuration\n- `client/uno.config.ts` - UnoCSS configuration\n\n### Backend (server/)\n- `server/server.py` - Main server routes and API endpoints\n- `server/pyproject.toml` - Python dependencies\n- `server/modules/llm_models.py` - All LLM model definitions\n- `server/modules/openai_llm.py` - OpenAI integration\n- `server/modules/anthropic_llm.py` - Anthropic integration\n- `server/modules/gemini_llm.py` - Google Gemini integration\n- `server/modules/ollama_llm.py` - Ollama integration\n- `server/modules/deepseek_llm.py` - Deepseek integration\n- `server/benchmark_data/*` - Benchmark configuration files\n- `server/reports/*` - Generated benchmark results\n\n### Configuration\n- `.env` - Root environment variables for API keys\n- `server/.env` - Server-specific environment variables\n- `start.sh` - Convenience script to start both services\n- `.claude/` - Claude Code configuration and commands\n  - `.claude/settings.local.json` - Claude Code permissions and settings\n  - `.claude/commands/prime.md` - Custom Claude commands for project context\n\n## Setup\n\n### Get API Keys & Models\n- [Anthropic](https://docs.anthropic.com/en/api/getting-started)\n- [Google Cloud](https://ai.google.dev/gemini-api/docs/api-key)\n- [OpenAI](https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key)\n- [Deepseek](https://platform.deepseek.com/)\n- [Ollama](https://ollama.ai/download)\n  - After installing Ollama, pull the required models:\n  ```bash\n  # Pull Llama 3.2 1B model\n  ollama pull llama3.2:1b\n  \n  # Pull Llama 3.2 latest (3B) model\n  ollama pull llama3.2:latest\n  \n  # Pull Qwen2.5 Coder 14B model\n  ollama pull qwen2.5-coder:14b\n\n  # Pull Deepseek R1 1.5B, 7b, 8b, 14b, 32b, 70b models\n  ollama pull deepseek-r1:1.5b\n  ollama pull deepseek-r1:latest\n  ollama pull deepseek-r1:8b\n  ollama pull deepseek-r1:14b\n  ollama pull deepseek-r1:32b\n  ollama pull deepseek-r1:70b\n\n  # Pull mistral-small 3\n  ollama pull mistral-small:latest\n\n  # Pull Phi-4 model\n  ollama pull phi4:latest\n\n  # Pull Falcon 3 10B model\n  ollama pull falcon3:10b\n\n  # Pull Qwen 3 14B model\n  ollama pull qwen3:14b\n\n  # Pull Gemma 3 4B model\n  ollama pull gemma3:4b\n\n  # Pull Devstral model\n  ollama pull devstral\n  ```\n\n### Frontend Setup (client/)\n```bash\n# Navigate to client directory\ncd client\n\n# Install dependencies using bun (recommended)\nbun install\n\n# Or using npm\nnpm install\n\n# Or using yarn\nyarn install\n\n# Start development server\nbun dev  # or npm run dev / yarn dev\n```\n\n### Backend Setup (server/)\n```bash\n# Navigate to server directory\ncd server\n\n# Create and activate virtual environment using uv\nuv sync\n\n# Set up environment variables\ncp ../.env.sample ../.env  # Root .env file\ncp .env.sample .env        # Server .env file\n\n# Set EVERY .env key with your API keys and settings in both files\nANTHROPIC_API_KEY=\nOPENAI_API_KEY=\nGEMINI_API_KEY=\nDEEPSEEK_API_KEY=\nFIREWORKS_API_KEY=\n\n# Start server\nuv run python server.py\n\n# Run tests\nuv run pytest (**beware will hit APIs and cost money**)\n```\n\n### Development Workflow\n```bash\n# Start both services at once (recommended)\n./start.sh\n\n# Or start them separately in different terminals:\n# Terminal 1: Frontend\ncd client && bun dev\n\n# Terminal 2: Backend  \ncd server && uv run python server.py\n```\n\n### Claude Code Integration\n\nThis project includes Claude Code configuration for enhanced development experience:\n\n- **Custom Commands**: Use the `/prime` command in Claude Code to quickly load project context\n- **Permissions**: Pre-configured permissions for common development tasks (mkdir, mv, ls)\n- **Project Context**: The `.claude/commands/prime.md` file automatically reads key project files and shows the directory structure\n\nTo use with Claude Code:\n1. Open the project in Claude Code\n2. Type `/prime` to load the project context\n3. Claude will have immediate understanding of the codebase structure and key files\n\n## Resources\n- https://github.com/simonw/llm?tab=readme-ov-file\n- https://github.com/openai/openai-python\n- https://platform.openai.com/docs/guides/predicted-outputs\n- https://community.openai.com/t/introducing-predicted-outputs/1004502\n- https://unocss.dev/integrations/vite\n- https://www.npmjs.com/package/vue-codemirror6\n- https://vuejs.org/guide/scaling-up/state-management\n- https://www.ag-grid.com/vue-data-grid/getting-started/\n- https://www.ag-grid.com/vue-data-grid/value-formatters/\n- https://llm.datasette.io/en/stable/index.html\n- https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/get-token-count\n- https://ai.google.dev/gemini-api/docs/tokens?lang=python\n- https://ai.google.dev/pricing#1_5flash\n- https://ai.google.dev/gemini-api/docs/structured-output?lang=python\n- https://platform.openai.com/docs/guides/structured-outputs\n- https://docs.anthropic.com/en/docs/build-with-claude/tool-use\n- https://ai.google.dev/gemini-api/docs/models/experimental-models\n- https://sqlparse.readthedocs.io/en/latest/intro.html\n- mlx: https://huggingface.co/mlx-community\n- ollama docs: https://github.com/ollama/ollama/blob/main/docs/api.md#examples\n- deepseek docs: https://platform.deepseek.com/usage\n\n## Multi-Agent Git Worktree\n\n### Why Run in Parallel\n\nLLMs are non-deterministic probabilistic machines - every run produces different results. \n\nThis is a feature, not a bug. We can leverage it to see **multiple versions of the future** and **choose the best outcome**.\n\nBy running multiple AI agents in parallel on separate git worktrees, you can:\n\n#### #1\nHedge against model failures on complex tasks with multiple outcomes (startups/big tech tech does this all the time)\n\n#### #2\nGet different perspectives on the same problem - choose the best implementation\n\n#### #3\nIsolate and delegate your engineering work to 2-N agents\n\n\n### How It Works\n\nGit worktrees allow you to duplicate your entire codebase into a new branch and directory:\n\n```bash\n# Create a directory for worktrees\nmkdir trees\n\n# Create three parallel worktrees for UI improvements\ngit worktree add -b ui_revamp-1 trees/ui_revamp-1\ngit worktree add -b ui_revamp-2 trees/ui_revamp-2\ngit worktree add -b ui_revamp-3 trees/ui_revamp-3\n\n# Copy environment variables to each worktree\ncp .env trees/ui_revamp-1/\ncp .env trees/ui_revamp-2/\ncp .env trees/ui_revamp-3/\n```\n\nThen run separate AI agents (like Claude Code) on each worktree with the same plan/prompt. Each agent works in isolation, producing different variations.\n\n### When to Use This Technique\n\n1. **Multiple Satisfactory Outcomes**: Perfect for UI work where many versions could be acceptable\n2. **Complex Tasks with Failure Risk**: If one agent might fail, run three and pick the winner\n3. **When You Have a Clear Plan**: The plan is the prompt - detailed planning enables parallel execution\n\n### Big Ideas\n\n- **Non-determinism is a feature**: Different versions give you options\n- **The plan is the prompt**: Great planning = great prompting\n- **Scale compute = scale impact**: Use more tokens to see more possibilities\n- **Pick and merge**: Choose the best version or combine elements from multiple\n\nThis advanced technique requires burning significant tokens (dollars per run) but enables you to:\n- Work on multiple timelines simultaneously\n- Get different perspectives on the same problem\n- Dramatically increase development velocity\n- See multiple versions of the future, literally\n\nAs AI models improve, this parallel approach will become increasingly powerful for leveraging their capabilities at scale.\n\n## Master AI Coding \nLearn to code with AI with foundational [Principles of AI Coding](https://agenticengineer.com/principled-ai-coding?y=benchy)\n\nFollow the [IndyDevDan youtube channel](https://www.youtube.com/@indydevdan) for more AI coding tips and tricks.",
  "source_type": "github_repository",
  "domain": "software_development",
  "keywords": [],
  "retrieval_tags": [
    "github",
    "repository",
    "code",
    "development"
  ]
}