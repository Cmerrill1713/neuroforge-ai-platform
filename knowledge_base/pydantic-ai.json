{
  "id": "pydantic-ai",
  "name": "Pydantic AI",
  "url": "https://github.com/pydantic/pydantic-ai",
  "description": "Pydantic AI - AI-powered data validation and processing",
  "source_type": "github_repository",
  "domain": "ai_framework",
  "keywords": [
    "pydantic",
    "ai",
    "python",
    "validation",
    "framework",
    "llm"
  ],
  "retrieval_tags": [
    "github",
    "repository",
    "pydantic",
    "ai",
    "python"
  ],
  "content": "=== README.md ===\n<div align=\"center\">\n  <a href=\"https://ai.pydantic.dev/\">\n    <picture>\n      <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://ai.pydantic.dev/img/pydantic-ai-dark.svg\">\n      <img src=\"https://ai.pydantic.dev/img/pydantic-ai-light.svg\" alt=\"Pydantic AI\">\n    </picture>\n  </a>\n</div>\n<div align=\"center\">\n  <h3>GenAI Agent Framework, the Pydantic way</h3>\n</div>\n<div align=\"center\">\n  <a href=\"https://github.com/pydantic/pydantic-ai/actions/workflows/ci.yml?query=branch%3Amain\"><img src=\"https://github.com/pydantic/pydantic-ai/actions/workflows/ci.yml/badge.svg?event=push\" alt=\"CI\"></a>\n  <a href=\"https://coverage-badge.samuelcolvin.workers.dev/redirect/pydantic/pydantic-ai\"><img src=\"https://coverage-badge.samuelcolvin.workers.dev/pydantic/pydantic-ai.svg\" alt=\"Coverage\"></a>\n  <a href=\"https://pypi.python.org/pypi/pydantic-ai\"><img src=\"https://img.shields.io/pypi/v/pydantic-ai.svg\" alt=\"PyPI\"></a>\n  <a href=\"https://github.com/pydantic/pydantic-ai\"><img src=\"https://img.shields.io/pypi/pyversions/pydantic-ai.svg\" alt=\"versions\"></a>\n  <a href=\"https://github.com/pydantic/pydantic-ai/blob/main/LICENSE\"><img src=\"https://img.shields.io/github/license/pydantic/pydantic-ai.svg?v\" alt=\"license\"></a>\n  <a href=\"https://logfire.pydantic.dev/docs/join-slack/\"><img src=\"https://img.shields.io/badge/Slack-Join%20Slack-4A154B?logo=slack\" alt=\"Join Slack\" /></a>\n</div>\n\n---\n\n**Documentation**: [ai.pydantic.dev](https://ai.pydantic.dev/)\n\n---\n\n### <em>Pydantic AI is a Python agent framework designed to help you quickly, confidently, and painlessly build production grade applications and workflows with Generative AI.</em>\n\n\nFastAPI revolutionized web development by offering an innovative and ergonomic design, built on the foundation of [Pydantic Validation](https://docs.pydantic.dev) and modern Python features like type hints.\n\nYet despite virtually every Python agent framework and LLM library using Pydantic Validation, when we began to use LLMs in [Pydantic Logfire](https://pydantic.dev/logfire), we couldn't find anything that gave us the same feeling.\n\nWe built Pydantic AI with one simple aim: to bring that FastAPI feeling to GenAI app and agent development.\n\n## Why use Pydantic AI\n\n1. **Built by the Pydantic Team**:\n[Pydantic Validation](https://docs.pydantic.dev/latest/) is the validation layer of the OpenAI SDK, the Google ADK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more. _Why use the derivative when you can go straight to the source?_ :smiley:\n\n2. **Model-agnostic**:\nSupports virtually every [model](https://ai.pydantic.dev/models/overview) and provider: OpenAI, Anthropic, Gemini, DeepSeek, Grok, Cohere, Mistral, and Perplexity; Azure AI Foundry, Amazon Bedrock, Google Vertex AI, Ollama, LiteLLM, Groq, OpenRouter, Together AI, Fireworks AI, Cerebras, Hugging Face, GitHub, Heroku, Vercel. If your favorite model or provider is not listed, you can easily implement a [custom model](https://ai.pydantic.dev/models/overview#custom-models).\n\n3. **Seamless Observability**:\nTightly [integrates](https://ai.pydantic.dev/logfire) with [Pydantic Logfire](https://pydantic.dev/logfire), our general-purpose OpenTelemetry observability platform, for real-time debugging, evals-based performance monitoring, and behavior, tracing, and cost tracking. If you already have an observability platform that supports OTel, you can [use that too](https://ai.pydantic.dev/logfire#alternative-observability-backends).\n\n4. **Fully Type-safe**:\nDesigned to give your IDE or AI coding agent as much context as possible for auto-completion and [type checking](https://ai.pydantic.dev/agents#static-type-checking), moving entire classes of errors from runtime to write-time for a bit of that Rust \"if it compiles, it works\" feel.\n\n5. **Powerful Evals**:\nEnables you to systematically test and [evaluate](https://ai.pydantic.dev/evals) the performance and accuracy of the agentic systems you build, and monitor the performance over time in Pydantic Logfire.\n\n6. **MCP, A2A, and AG-UI**:\nIntegrates the [Model Context Protocol](https://ai.pydantic.dev/mcp/client), [Agent2Agent](https://ai.pydantic.dev/a2a), and [AG-UI](https://ai.pydantic.dev/ag-ui) standards to give your agent access to external tools and data, let it interoperate with other agents, and build interactive applications with streaming event-based communication.\n\n7. **Human-in-the-Loop Tool Approval**:\nEasily lets you flag that certain tool calls [require approval](https://ai.pydantic.dev/deferred-tools#human-in-the-loop-tool-approval) before they can proceed, possibly depending on tool call arguments, conversation history, or user preferences.\n\n8. **Durable Execution**:\nEnables you to build [durable agents](https://ai.pydantic.dev/durable_execution/overview/) that can preserve their progress across transient API failures and application errors or restarts, and handle long-running, asynchronous, and human-in-the-loop workflows with production-grade reliability.\n\n9. **Streamed Outputs**:\nProvides the ability to [stream](https://ai.pydantic.dev/output#streamed-results) structured output continuously, with immediate validation, ensuring real time access to generated data.\n\n10. **Graph Support**:\nProvides a powerful way to define [graphs](https://ai.pydantic.dev/graph) using type hints, for use in complex applications where standard control flow can degrade to spaghetti code.\n\nRealistically though, no list is going to be as convincing as [giving it a try](#next-steps) and seeing how it makes you feel!\n\n## Hello World Example\n\nHere's a minimal example of Pydantic AI:\n\n```python\nfrom pydantic_ai import Agent\n\n# Define a very simple agent including the model to use, you can also set the model when running the agent.\nagent = Agent(\n    'anthropic:claude-sonnet-4-0',\n    # Register static instructions using a keyword argument to the agent.\n    # For more complex dynamically-generated instructions, see the example below.\n    instructions='Be concise, reply with one sentence.',\n)\n\n# Run the agent synchronously, conducting a conversation with the LLM.\nresult = agent.run_sync('Where does \"hello world\" come from?')\nprint(result.output)\n\"\"\"\nThe first known use of \"hello, world\" was in a 1974 textbook about the C programming language.\n\"\"\"\n```\n\n_(This example is complete, it can be run \"as is\", assuming you've [installed the `pydantic_ai` package](https://ai.pydantic.dev/install))_\n\nThe exchange will be very short: Pydantic AI will send the instructions and the user prompt to the LLM, and the model will return a text response.\n\nNot very interesting yet, but we can easily add [tools](https://ai.pydantic.dev/tools), [dynamic instructions](https://ai.pydantic.dev/agents#instructions), and [structured outputs](https://ai.pydantic.dev/output) to build more powerful agents.\n\n## Tools & Dependency Injection Example\n\nHere is a concise example using Pydantic AI to build a support agent for a bank:\n\n**(Better documented example [in the docs](https://ai.pydantic.dev/#tools-dependency-injection-example))**\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic import BaseModel, Field\nfrom pydantic_ai import Agent, RunContext\n\nfrom bank_database import DatabaseConn\n\n\n# SupportDependencies is used to pass data, connections, and logic into the model that will be needed when running\n# instructions and tool functions. Dependency injection provides a type-safe way to customise the behavior of your agents.\n@dataclass\nclass SupportDependencies:\n    customer_id: int\n    db: DatabaseConn\n\n\n# This Pydantic model defines the structure of the output returned by the agent.\nclass SupportOutput(BaseModel):\n    support_advice: str = Field(description='Advice returned to the customer')\n    block_card: bool = Field(description=\"Whether to block the customer's card\")\n    risk: int = Field(description='Risk level of query', ge=0, le=10)\n\n\n# This agent will act as first-tier support in a bank.\n# Agents are generic in the type of dependencies they accept and the type of output they return.\n# In this case, the support agent has type `Agent[SupportDependencies, SupportOutput]`.\nsupport_agent = Agent(\n    'openai:gpt-5',\n    deps_type=SupportDependencies,\n    # The response from the agent will, be guaranteed to be a SupportOutput,\n    # if validation fails the agent is prompted to try again.\n    output_type=SupportOutput,\n    instructions=(\n        'You are a support agent in our bank, give the '\n        'customer support and judge the risk level of their query.'\n    ),\n)\n\n\n# Dynamic instructions can make use of dependency injection.\n# Dependencies are carried via the `RunContext` argument, which is parameterized with the `deps_type` from above.\n# If the type annotation here is wrong, static type checkers will catch it.\n@support_agent.instructions\nasync def add_customer_name(ctx: RunContext[SupportDependencies]) -> str:\n    customer_name = await ctx.deps.db.customer_name(id=ctx.deps.customer_id)\n    return f\"The customer's name is {customer_name!r}\"\n\n\n# The `tool` decorator let you register functions which the LLM may call while responding to a user.\n# Again, dependencies are carried via `RunContext`, any other arguments become the tool schema passed to the LLM.\n# Pydantic is used to validate these arguments, and errors are passed back to the LLM so it can retry.\n@support_agent.tool\nasync def customer_balance(\n        ctx: RunContext[SupportDependencies], include_pending: bool\n) -> float:\n    \"\"\"Returns the customer's current account balance.\"\"\"\n    # The docstring of a tool is also passed to the LLM as the description of the tool.\n    # Parameter descriptions are extracted from the docstring and added to the parameter schema sent to the LLM.\n    balance = await ctx.deps.db.customer_balance(\n        id=ctx.deps.customer_id,\n        include_pending=include_pending,\n    )\n    return balance\n\n\n...  # In a real use case, you'd add more tools and a longer system prompt\n\n\nasync def main():\n    deps = SupportDependencies(customer_id=123, db=DatabaseConn())\n    # Run the agent asynchronously, conducting a conversation with the LLM until a final response is reached.\n    # Even in this fairly simple case, the agent will exchange multiple messages with the LLM as tools are called to retrieve an output.\n    result = await support_agent.run('What is my balance?', deps=deps)\n    # The `result.output` will be validated with Pydantic to guarantee it is a `SupportOutput`. Since the agent is generic,\n    # it'll also be typed as a `SupportOutput` to aid with static type checking.\n    print(result.output)\n    \"\"\"\n    support_advice='Hello John, your current account balance, including pending transactions, is $123.45.' block_card=False risk=1\n    \"\"\"\n\n    result = await support_agent.run('I just lost my card!', deps=deps)\n    print(result.output)\n    \"\"\"\n    support_advice=\"I'm sorry to hear that, John. We are temporarily blocking your card to prevent unauthorized transactions.\" block_card=True risk=8\n    \"\"\"\n```\n\n## Next Steps\n\nTo try Pydantic AI for yourself, [install it](https://ai.pydantic.dev/install) and follow the instructions [in the examples](https://ai.pydantic.dev/examples/setup).\n\nRead the [docs](https://ai.pydantic.dev/agents/) to learn more about building applications with Pydantic AI.\n\nRead the [API Reference](https://ai.pydantic.dev/api/agent/) to understand Pydantic AI's interface.\n\nJoin [Slack](https://logfire.pydantic.dev/docs/join-slack/) or file an issue on [GitHub](https://github.com/pydantic/pydantic-ai/issues) if you have any questions.\n\n\n=== pyproject.toml ===\n[build-system]\nrequires = [\"hatchling\", \"uv-dynamic-versioning>=0.7.0\"]\nbuild-backend = \"hatchling.build\"\n\n[tool.hatch.version]\nsource = \"uv-dynamic-versioning\"\n\n[tool.uv-dynamic-versioning]\nvcs = \"git\"\nstyle = \"pep440\"\nbump = true\n\n[project]\nname = \"pydantic-ai\"\ndynamic = [\"version\", \"dependencies\", \"optional-dependencies\"]\ndescription = \"Agent Framework / shim to use Pydantic with LLMs\"\nauthors = [\n    { name = \"Samuel Colvin\", email = \"samuel@pydantic.dev\" },\n    { name = \"Marcelo Trylesinski\", email = \"marcelotryle@gmail.com\" },\n    { name = \"David Montague\", email = \"david@pydantic.dev\" },\n    { name = \"Alex Hall\", email = \"alex@pydantic.dev\" },\n    { name = \"Douwe Maan\", email = \"douwe@pydantic.dev\" },\n]\nlicense = \"MIT\"\nlicense-files = [\"LICENSE\"]\nreadme = \"README.md\"\nclassifiers = [\n    \"Development Status :: 5 - Production/Stable\",\n    \"Programming Language :: Python\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3 :: Only\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Programming Language :: Python :: 3.13\",\n    \"Intended Audience :: Developers\",\n    \"Intended Audience :: Information Technology\",\n    \"Operating System :: OS Independent\",\n    \"Topic :: Internet\",\n    \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n    \"Topic :: Software Development :: Libraries :: Python Modules\",\n    \"Framework :: Pydantic\",\n    \"Framework :: Pydantic :: 2\",\n]\nrequires-python = \">=3.10\"\n\n[tool.hatch.metadata.hooks.uv-dynamic-versioning]\ndependencies = [\n    \"pydantic-ai-slim[openai,vertexai,google,groq,anthropic,mistral,cohere,bedrock,huggingface,cli,mcp,evals,ag-ui,retries,temporal,logfire]=={{ version }}\",\n]\n\n[tool.hatch.metadata.hooks.uv-dynamic-versioning.optional-dependencies]\nexamples = [\"pydantic-ai-examples=={{ version }}\"]\na2a = [\"fasta2a>=0.4.1\"]\ndbos = [\"pydantic-ai-slim[dbos]=={{ version }}\"]\n\n[project.urls]\nHomepage = \"https://ai.pydantic.dev\"\nSource = \"https://github.com/pydantic/pydantic-ai\"\nDocumentation = \"https://ai.pydantic.dev\"\nChangelog = \"https://github.com/pydantic/pydantic-ai/releases\"\n\n[project.scripts]\npai = \"pydantic_ai._cli:cli_exit\" # TODO remove this when clai has been out for a while\n\n[tool.uv.sources]\npydantic-ai = { workspace = true }\npydantic-ai-slim = { workspace = true }\npydantic-evals = { workspace = true }\npydantic-graph = { workspace = true }\npydantic-ai-examples = { workspace = true }\n\n[tool.uv.workspace]\nmembers = [\n    \"pydantic_ai_slim\",\n    \"pydantic_evals\",\n    \"pydantic_graph\",\n    \"clai\",\n    \"examples\",\n]\n\n[tool.uv]\ndefault-groups = [\"dev\", \"lint\", \"docs\"]\n\n[dependency-groups]\ndev = [\n    \"anyio>=4.5.0\",\n    \"asgi-lifespan>=2.1.0\",\n    \"devtools>=0.12.2\",\n    \"coverage[toml]>=7.10.3\",\n    \"dirty-equals>=0.9.0\",\n    \"duckduckgo-search>=7.0.0\",\n    \"inline-snapshot>=0.19.3\",\n    \"pytest>=8.3.3\",\n    \"pytest-examples>=0.0.18\",\n    \"pytest-mock>=3.14.0\",\n    \"pytest-pretty>=1.3.0\",\n    \"pytest-recording>=0.13.2\",\n    \"diff-cover>=9.2.0\",\n    \"boto3-stubs[bedrock-runtime]\",\n    \"strict-no-cover @ git+https://github.com/pydantic/strict-no-cover.git@7fc59da2c4dff919db2095a0f0e47101b657131d\",\n    \"pytest-xdist>=3.6.1\",\n    # Needed for PyCharm users\n    \"pip>=25.2\",\n    \"genai-prices>=0.0.22\",\n    \"mcp-run-python>=0.0.20\",\n]\nlint = [\"mypy>=1.11.2\", \"pyright>=1.1.390\", \"ruff>=0.6.9\"]\ndocs = [\n    \"pydantic-ai[a2a]\",\n    \"black>=24.10.0\",\n    \"mkdocs>=1.6.1\",\n    \"mkdocs-glightbox>=0.4.0\",\n    \"mkdocs-llmstxt>=0.2.0\",\n    'mkdocs-redirects>=1.2.2',\n    \"mkdocs-material[imaging]>=9.5.45\",\n    \"mkdocstrings-python>=1.12.2\",\n    \"griffe-warnings-deprecated>=1.1.0\",\n]\ndocs-upload = [\"algoliasearch>=4.12.0\", \"pydantic>=2.10.1\"]\n\n[tool.hatch.build.targets.wheel]\nonly-include = [\"/README.md\"]\n\n[tool.hatch.build.targets.sdist]\ninclude = [\"/README.md\", \"/Makefile\", \"/tests\"]\n\n[tool.ruff]\nline-length = 120\ntarget-version = \"py310\"\ninclude = [\n    \"pydantic_ai_slim/**/*.py\",\n    \"pydantic_evals/**/*.py\",\n    \"pydantic_graph/**/*.py\",\n    \"examples/**/*.py\",\n    \"clai/**/*.py\",\n    \"tests/**/*.py\",\n    \"docs/**/*.py\",\n]\n\n[tool.ruff.lint]\nextend-select = [\n    \"Q\",\n    \"RUF100\",\n    \"RUF018\", # https://docs.astral.sh/ruff/rules/assignment-in-assert/\n    \"C90\",\n    \"UP\",\n    \"I\",\n    \"D\",\n    \"TID251\",\n]\nflake8-quotes = { inline-quotes = \"single\", multiline-quotes = \"double\" }\nmccabe = { max-complexity = 15 }\nignore = [\n    \"D100\", # ignore missing docstring in module\n    \"D102\", # ignore missing docstring in public method\n    \"D104\", # ignore missing docstring in public package\n    \"D105\", # ignore missing docstring in magic methods\n    \"D107\", # ignore missing docstring in __init__ methods\n]\n\n[tool.ruff.lint.isort]\ncombine-as-imports = true\nknown-first-party = [\"pydantic_ai\", \"pydantic_evals\", \"pydantic_graph\"]\n# weird issue with ruff thinking fasta2a is still editable\nknown-third-party = [\"fasta2a\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n[tool.ruff.lint.flake8-tidy-imports.banned-api]\n\"typing.TypedDict\".msg = \"Use typing_extensions.TypedDict instead.\"\n\n[tool.ruff.format]\n# don't format python in docstrings, pytest-examples takes care of it\ndocstring-code-format = false\nquote-style = \"single\"\n\n[tool.ruff.lint.per-file-ignores]\n\"examples/**/*.py\" = [\"D101\", \"D103\"]\n\"tests/**/*.py\" = [\"D\"]\n\"docs/**/*.py\" = [\"D\"]\n\n[tool.pyright]\npythonVersion = \"3.12\"\ntypeCheckingMode = \"strict\"\nreportMissingTypeStubs = false\nreportUnnecessaryIsInstance = false\nreportUnnecessaryTypeIgnoreComment = true\nreportMissingModuleSource = false\ninclude = [\n    \"pydantic_ai_slim\",\n    \"pydantic_evals\",\n    \"pydantic_graph\",\n    \"tests\",\n    \"examples\",\n    \"clai\",\n]\nvenvPath = '.'\nvenv = \".venv\"\n# see https://github.com/microsoft/pyright/issues/7771 - we don't want to error on decorated functions in tests\n# which are not otherwise used\nexecutionEnvironments = [\n    { root = \"tests\", reportUnusedFunction = false, reportPrivateImportUsage = false },\n]\nexclude = [\n    \"examples/pydantic_ai_examples/weather_agent_gradio.py\",\n    \"pydantic_ai_slim/pydantic_ai/ext/aci.py\",               # aci-sdk is too niche to be added as an (optional) dependency\n]\n\n[tool.mypy]\nfiles = \"tests/typed_agent.py,tests/typed_graph.py\"\nstrict = true\n\n[tool.pytest.ini_options]\ntestpaths = [\"tests\", \"docs/.hooks\"]\nxfail_strict = true\nfilterwarnings = [\n    \"error\",\n    # Issue with python-multipart - we don't want to bump the minimum version of starlette.\n    \"ignore::PendingDeprecationWarning:starlette\",\n    # mistralai accesses model_fields on the instance, which is deprecated in Pydantic 2.11.\n    \"ignore:Accessing the 'model_fields' attribute\",\n    # boto3\n    \"ignore::DeprecationWarning:botocore.*\",\n    \"ignore::RuntimeWarning:pydantic_ai.mcp\",\n    # uvicorn (mcp server)\n    \"ignore:websockets.legacy is deprecated.*:DeprecationWarning:websockets.legacy\",\n    \"ignore:websockets.server.WebSocketServerProtocol is deprecated:DeprecationWarning\",\n    # random resource warnings; I suspect these are coming from vendor SDKs when running examples..\n    \"ignore:unclosed <socket:ResourceWarning\",\n    \"ignore:unclosed event loop:ResourceWarning\",\n]\n\n# https://coverage.readthedocs.io/en/latest/config.html#run\n[tool.coverage.run]\npatch = [\"subprocess\"]\nconcurrency = [\"multiprocessing\", \"thread\"]\n# We use a subdirectory for coverage data to avoid noisy coverage data files.\ndata_file = \".coverage/.coverage\"\n# required to avoid warnings about files created by create_module fixture\ninclude = [\n    \"pydantic_ai_slim/**/*.py\",\n    \"pydantic_evals/**/*.py\",\n    \"pydantic_graph/**/*.py\",\n    \"tests/**/*.py\",\n]\nomit = [\n    \"tests/test_live.py\",\n    \"tests/example_modules/*.py\",\n    \"pydantic_ai_slim/pydantic_ai/ext/aci.py\", # aci-sdk is too niche to be added as an (optional) dependency\n]\nbranch = true\n# Disable include-ignored warnings as --source is enabled automatically causing a self conflict as per:\n# https://github.com/pytest-dev/pytest-cov/issues/532\n# https://github.com/pytest-dev/pytest-cov/issues/369\n# This prevents coverage being generated by pytest-cov which has direct editor support in VS Code,\n# making it super useful to check coverage while writing tests.\ndisable_warnings = [\"include-ignored\"]\n\n[tool.coverage.paths]\n# Allow CI run assets to be downloaded an replicated locally.\nsource = [\n    \".\",\n    \"/home/runner/work/pydantic-ai/pydantic-ai\",\n    \"/System/Volumes/Data/home/runner/work/pydantic-ai/pydantic-ai\",\n]\n\n# https://coverage.readthedocs.io/en/latest/config.html#report\n[tool.coverage.report]\nfail_under = 100\nskip_covered = true\nshow_missing = true\nignore_errors = true\nprecision = 2\nexclude_lines = [\n    # `# pragma: no cover` is standard marker for code that's not covered, this will error if code is covered\n    'pragma: no cover',\n    # use `# pragma: lax no cover` if you want to ignore cases where (some of) the code is covered\n    'pragma: lax no cover',\n    'raise NotImplementedError',\n    'if TYPE_CHECKING:',\n    'if typing.TYPE_CHECKING:',\n    '@overload',\n    '@deprecated',\n    '@typing.overload',\n    '@abstractmethod',\n    '\\(Protocol\\):$',\n    'typing.assert_never',\n    '$\\s*assert_never\\(',\n    'if __name__ == .__main__.:',\n    'except ImportError as _import_error:',\n    '$\\s*pass$',\n    'assert False',\n]\n\n[tool.logfire]\nignore_no_config = true\n\n[tool.inline-snapshot]\nformat-command = \"ruff format --stdin-filename {filename}\"\n\n[tool.inline-snapshot.shortcuts]\nsnap-fix = [\"create\", \"fix\"]\nsnap = [\"create\"]\n\n[tool.codespell]\n# Ref: https://github.com/codespell-project/codespell#using-a-config-file\nskip = '.git*,*.svg,*.lock,*.css,*.yaml'\ncheck-hidden = true\n# Ignore \"formatting\" like **L**anguage\nignore-regex = '\\*\\*[A-Z]\\*\\*[a-z]+\\b'\nignore-words-list = 'asend,aci'\n\n\n=== LICENSE ===\nThe MIT License (MIT)\n\nCopyright (c) Pydantic Services Inc. 2024 to present\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n\n=== pydantic_ai_slim/pydantic_ai/_output.py ===\nfrom __future__ import annotations as _annotations\n\nimport inspect\nimport json\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Awaitable, Callable, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import TYPE_CHECKING, Any, Generic, Literal, cast, overload\n\nfrom pydantic import Json, TypeAdapter, ValidationError\nfrom pydantic_core import SchemaValidator, to_json\nfrom typing_extensions import Self, TypedDict, TypeVar, assert_never\n\nfrom . import _function_schema, _utils, messages as _messages\nfrom ._run_context import AgentDepsT, RunContext\nfrom .exceptions import ModelRetry, ToolRetryError, UserError\nfrom .output import (\n    DeferredToolRequests,\n    NativeOutput,\n    OutputDataT,\n    OutputMode,\n    OutputSpec,\n    OutputTypeOrFunction,\n    PromptedOutput,\n    StructuredOutputMode,\n    TextOutput,\n    TextOutputFunc,\n    ToolOutput,\n    _OutputSpecItem,  # type: ignore[reportPrivateUsage]\n)\nfrom .tools import GenerateToolJsonSchema, ObjectJsonSchema, ToolDefinition\nfrom .toolsets.abstract import AbstractToolset, ToolsetTool\n\nif TYPE_CHECKING:\n    from .profiles import ModelProfile\n\nT = TypeVar('T')\n\"\"\"An invariant TypeVar.\"\"\"\nOutputDataT_inv = TypeVar('OutputDataT_inv', default=str)\n\"\"\"\nAn invariant type variable for the result data of a model.\n\nWe need to use an invariant typevar for `OutputValidator` and `OutputValidatorFunc` because the output data type is used\nin both the input and output of a `OutputValidatorFunc`. This can theoretically lead to some issues assuming that types\npossessing OutputValidator's are covariant in the result data type, but in practice this is rarely an issue, and\nchanging it would have negative consequences for the ergonomics of the library.\n\nAt some point, it may make sense to change the input to OutputValidatorFunc to be `Any` or `object` as doing that would\nresolve these potential variance issues.\n\"\"\"\n\nOutputValidatorFunc = (\n    Callable[[RunContext[AgentDepsT], OutputDataT_inv], OutputDataT_inv]\n    | Callable[[RunContext[AgentDepsT], OutputDataT_inv], Awaitable[OutputDataT_inv]]\n    | Callable[[OutputDataT_inv], OutputDataT_inv]\n    | Callable[[OutputDataT_inv], Awaitable[OutputDataT_inv]]\n)\n\"\"\"\nA function that always takes and returns the same type of data (which is the result type of an agent run), and:\n\n* may or may not take [`RunContext`][pydantic_ai.tools.RunContext] as a first argument\n* may or may not be async\n\nUsage `OutputValidatorFunc[AgentDepsT, T]`.\n\"\"\"\n\n\nDEFAULT_OUTPUT_TOOL_NAME = 'final_result'\nDEFAULT_OUTPUT_TOOL_DESCRIPTION = 'The final response which ends this conversation'\n\n\nasync def execute_traced_output_function(\n    function_schema: _function_schema.FunctionSchema,\n    run_context: RunContext[AgentDepsT],\n    args: dict[str, Any] | Any,\n    wrap_validation_errors: bool = True,\n) -> Any:\n    \"\"\"Execute an output function within a traced span with error handling.\n\n    This function executes the output function within an OpenTelemetry span for observability,\n    automatically records the function response, and handles ModelRetry exceptions by converting\n    them to ToolRetryError when wrap_validation_errors is True.\n\n    Args:\n        function_schema: The function schema containing the function to execute\n        run_context: The current run context containing tracing and tool information\n        args: Arguments to pass to the function\n        wrap_validation_errors: If True, wrap ModelRetry exceptions in ToolRetryError\n\n    Returns:\n        The result of the function execution\n\n    Raises:\n        ToolRetryError: When wrap_validation_errors is True and a ModelRetry is caught\n        ModelRetry: When wrap_validation_errors is False and a ModelRetry occurs\n    \"\"\"\n    # Set up span attributes\n    tool_name = run_context.tool_name or getattr(function_schema.function, '__name__', 'output_function')\n    attributes = {\n        'gen_ai.tool.name': tool_name,\n        'logfire.msg': f'running output function: {tool_name}',\n    }\n    if run_context.tool_call_id:\n        attributes['gen_ai.tool.call.id'] = run_context.tool_call_id\n    if run_context.trace_include_content:\n        attributes['tool_arguments'] = to_json(args).decode()\n        attributes['logfire.json_schema'] = json.dumps(\n            {\n                'type': 'object',\n                'properties': {\n                    'tool_arguments': {'type': 'object'},\n                    'tool_response': {'type': 'object'},\n                },\n            }\n        )\n\n    with run_context.tracer.start_as_current_span('running output function', attributes=attributes) as span:\n        try:\n            output = await function_schema.call(args, run_context)\n        except ModelRetry as r:\n            if wrap_validation_errors:\n                m = _messages.RetryPromptPart(\n                    content=r.message,\n                    tool_name=run_context.tool_name,\n                )\n                if run_context.tool_call_id:\n                    m.tool_call_id = run_context.tool_call_id  # pragma: no cover\n                raise ToolRetryError(m) from r\n            else:\n                raise\n\n        # Record response if content inclusion is enabled\n        if run_context.trace_include_content and span.is_recording():\n            from .models.instrumented import InstrumentedModel\n\n            span.set_attribute(\n                'tool_response',\n                output if isinstance(output, str) else json.dumps(InstrumentedModel.serialize_any(output)),\n            )\n\n        return output\n\n\n@dataclass\nclass OutputValidator(Generic[AgentDepsT, OutputDataT_inv]):\n    function: OutputValidatorFunc[AgentDepsT, OutputDataT_inv]\n    _takes_ctx: bool = field(init=False)\n    _is_async: bool = field(init=False)\n\n    def __post_init__(self):\n        self._takes_ctx = len(inspect.signature(self.function).parameters) > 1\n        self._is_async = _utils.is_async_callable(self.function)\n\n    async def validate(\n        self,\n        result: T,\n        run_context: RunContext[AgentDepsT],\n        wrap_validation_errors: bool = True,\n    ) -> T:\n        \"\"\"Validate a result but calling the function.\n\n        Args:\n            result: The result data after Pydantic validation the message content.\n            run_context: The current run context.\n            wrap_validation_errors: If true, wrap the validation errors in a retry message.\n\n        Returns:\n            Result of either the validated result data (ok) or a retry message (Err).\n        \"\"\"\n        if self._takes_ctx:\n            args = run_context, result\n        else:\n            args = (result,)\n\n        try:\n            if self._is_async:\n                function = cast(Callable[[Any], Awaitable[T]], self.function)\n                result_data = await function(*args)\n            else:\n                function = cast(Callable[[Any], T], self.function)\n                result_data = await _utils.run_in_executor(function, *args)\n        except ModelRetry as r:\n            if wrap_validation_errors:\n                m = _messages.RetryPromptPart(\n                    content=r.message,\n                    tool_name=run_context.tool_name,\n                )\n                if run_context.tool_call_id:  # pragma: no cover\n                    m.tool_call_id = run_context.tool_call_id\n                raise ToolRetryError(m) from r\n            else:\n                raise r\n        else:\n            return result_data\n\n\n@dataclass\nclass BaseOutputSchema(ABC, Generic[OutputDataT]):\n    allows_deferred_tools: bool\n\n    @abstractmethod\n    def with_default_mode(self, mode: StructuredOutputMode) -> OutputSchema[OutputDataT]:\n        raise NotImplementedError()\n\n    @property\n    def toolset(self) -> OutputToolset[Any] | None:\n        \"\"\"Get the toolset for this output schema.\"\"\"\n        return None\n\n\n@dataclass(init=False)\nclass OutputSchema(BaseOutputSchema[OutputDataT], ABC):\n    \"\"\"Model the final output from an agent run.\"\"\"\n\n    @classmethod\n    @overload\n    def build(\n        cls,\n        output_spec: OutputSpec[OutputDataT],\n        *,\n        default_mode: StructuredOutputMode,\n        name: str | None = None,\n        description: str | None = None,\n        strict: bool | None = None,\n    ) -> OutputSchema[OutputDataT]: ...\n\n    @classmethod\n    @overload\n    def build(\n        cls,\n        output_spec: OutputSpec[OutputDataT],\n        *,\n        default_mode: None = None,\n        name: str | None = None,\n        description: str | None = None,\n        strict: bool | None = None,\n    ) -> BaseOutputSchema[OutputDataT]: ...\n\n    @classmethod\n    def build(  # noqa: C901\n        cls,\n        output_spec: OutputSpec[OutputDataT],\n        *,\n        default_mode: StructuredOutputMode | None = None,\n        name: str | None = None,\n        description: str | None = None,\n        strict: bool | None = None,\n    ) -> BaseOutputSchema[OutputDataT]:\n        \"\"\"Build an OutputSchema dataclass from an output type.\"\"\"\n        raw_outputs = _flatten_output_spec(output_spec)\n\n        outputs = [output for output in raw_outputs if output is not DeferredToolRequests]\n        allows_deferred_tools = len(outputs) < len(raw_outputs)\n        if len(outputs) == 0 and allows_deferred_tools:\n            raise UserError('At least one output type must be provided other than `DeferredToolRequests`.')\n\n        if output := next((output for output in outputs if isinstance(output, NativeOutput)), None):\n            if len(outputs) > 1:\n                raise UserError('`NativeOutput` must be the only output type.')  # pragma: no cover\n\n            return NativeOutputSchema(\n                processor=cls._build_processor(\n                    _flatten_output_spec(output.outputs),\n                    name=output.name,\n                    description=output.description,\n                    strict=output.strict,\n                ),\n                allows_deferred_tools=allows_deferred_tools,\n            )\n        elif output := next((output for output in outputs if isinstance(output, PromptedOutput)), None):\n            if len(outputs) > 1:\n                raise UserError('`PromptedOutput` must be the only output type.')  # pragma: no cover\n\n            return PromptedOutputSchema(\n                processor=cls._build_processor(\n                    _flatten_output_spec(output.outputs),\n                    name=output.name,\n                    description=output.description,\n                ),\n                template=output.template,\n                allows_deferred_tools=allows_deferred_tools,\n            )\n\n        text_outputs: Sequence[type[str] | TextOutput[OutputDataT]] = []\n        tool_outputs: Sequence[ToolOutput[OutputDataT]] = []\n        other_outputs: Sequence[OutputTypeOrFunction[OutputDataT]] = []\n        for output in outputs:\n            if output is str:\n                text_outputs.append(cast(type[str], output))\n            elif isinstance(output, TextOutput):\n                text_outputs.append(output)\n            elif isinstance(output, ToolOutput):\n                tool_outputs.append(output)\n            elif isinstance(output, NativeOutput):\n                # We can never get here because this is checked for above.\n                raise UserError('`NativeOutput` must be the only output type.')  # pragma: no cover\n            elif isinstance(output, PromptedOutput):\n                # We can never get here because this is checked for above.\n                raise UserError('`PromptedOutput` must be the only output type.')  # pragma: no cover\n            else:\n                other_outputs.append(output)\n\n        toolset = OutputToolset.build(tool_outputs + other_outputs, name=name, description=description, strict=strict)\n\n        if len(text_outputs) > 0:\n            if len(text_outputs) > 1:\n                raise UserError('Only one `str` or `TextOutput` is allowed.')\n            text_output = text_outputs[0]\n\n            text_output_schema = None\n            if isinstance(text_output, TextOutput):\n                text_output_schema = PlainTextOutputProcessor(text_output.output_function)\n\n            if toolset:\n                return ToolOrTextOutputSchema(\n                    processor=text_output_schema,\n                    toolset=toolset,\n                    allows_deferred_tools=allows_deferred_tools,\n                )\n            else:\n                return PlainTextOutputSchema(processor=text_output_schema, allows_deferred_tools=allows_deferred_tools)\n\n        if len(tool_outputs) > 0:\n            return ToolOutputSchema(toolset=toolset, allows_deferred_tools=allows_deferred_tools)\n\n        if len(other_outputs) > 0:\n            schema = OutputSchemaWithoutMode(\n                processor=cls._build_processor(other_outputs, name=name, description=description, strict=strict),\n                toolset=toolset,\n                allows_deferred_tools=allows_deferred_tools,\n            )\n            if default_mode:\n                schema = schema.with_default_mode(default_mode)\n            return schema\n\n        raise UserError('At least one output type must be provided.')\n\n    @staticmethod\n    def _build_processor(\n        outputs: Sequence[OutputTypeOrFunction[OutputDataT]],\n        name: str | None = None,\n        description: str | None = None,\n        strict: bool | None = None,\n    ) -> ObjectOutputProcessor[OutputDataT] | UnionOutputProcessor[OutputDataT]:\n        outputs = _flatten_output_spec(outputs)\n        if len(outputs) == 1:\n            return ObjectOutputProcessor(output=outputs[0], name=name, description=description, strict=strict)\n\n        return UnionOutputProcessor(outputs=outputs, strict=strict, name=name, description=description)\n\n    @property\n    @abstractmethod\n    def mode(self) -> OutputMode:\n        raise NotImplementedError()\n\n    @abstractmethod\n    def raise_if_unsupported(self, profile: ModelProfile) -> None:\n        \"\"\"Raise an error if the mode is not supported by the model.\"\"\"\n        raise NotImplementedError()\n\n    def with_default_mode(self, mode: StructuredOutputMode) -> OutputSchema[OutputDataT]:\n        return self\n\n\n@dataclass(init=False)\nclass OutputSchemaWithoutMode(BaseOutputSchema[OutputDataT]):\n    processor: ObjectOutputProcessor[OutputDataT] | UnionOutputProcessor[OutputDataT]\n    _toolset: OutputToolset[Any] | None\n\n    def __init__(\n        self,\n        processor: ObjectOutputProcessor[OutputDataT] | UnionOutputProcessor[OutputDataT],\n        toolset: OutputToolset[Any] | None,\n        allows_deferred_tools: bool,\n    ):\n        super().__init__(allows_deferred_tools)\n        self.processor = processor\n        self._toolset = toolset\n\n    def with_default_mode(self, mode: StructuredOutputMode) -> OutputSchema[OutputDataT]:\n        if mode == 'native':\n            return NativeOutputSchema(processor=self.processor, allows_deferred_tools=self.allows_deferred_tools)\n        elif mode == 'prompted':\n            return PromptedOutputSchema(processor=self.processor, allows_deferred_tools=self.allows_deferred_tools)\n        elif mode == 'tool':\n            return ToolOutputSchema(toolset=self.toolset, allows_deferred_tools=self.allows_deferred_tools)\n        else:\n            assert_never(mode)\n\n    @property\n    def toolset(self) -> OutputToolset[Any] | None:\n        \"\"\"Get the toolset for this output schema.\"\"\"\n        # We return a toolset here as they're checked for name conflicts with other toolsets in the Agent constructor.\n        # At that point we may not know yet what output mode we're going to use if no model was provided or it was deferred until agent.run time,\n        # but we cover ourselves just in case we end up using the tool output mode.\n        return self._toolset\n\n\nclass TextOutputSchema(OutputSchema[OutputDataT], ABC):\n    @abstractmethod\n    async def process(\n        self,\n        text: str,\n        run_context: RunContext[AgentDepsT],\n        allow_partial: bool = False,\n        wrap_validation_errors: bool = True,\n    ) -> OutputDataT:\n        raise NotImplementedError()\n\n\n@dataclass\nclass PlainTextOutputSchema(TextOutputSchema[OutputDataT]):\n    processor: PlainTextOutputProcessor[OutputDataT] | None = None\n\n    @property\n    def mode(self) -> OutputMode:\n        return 'text'\n\n    def raise_if_unsupported(self, profile: ModelProfile) -> None:\n        \"\"\"Raise an error if the mode is not supported by the model.\"\"\"\n        pass\n\n    async def process(\n        self,\n        text: str,\n        run_context: RunContext[AgentDepsT],\n        allow_partial: bool = False,\n        wrap_validation_errors: bool = True,\n    ) -> OutputDataT:\n        \"\"\"Validate an output message.\n\n        Args:\n            text: The output text to validate.\n            run_context: The current run context.\n            allow_partial: If true, allow partial validation.\n            wrap_validation_errors: If true, wrap the validation errors in a retry message.\n\n        Returns:\n            Either the validated output data (left) or a retry message (right).\n        \"\"\"\n        if self.processor is None:\n            return cast(OutputDataT, text)\n\n        return await self.processor.process(\n            text, run_context, allow_partial=allow_partial, wrap_validation_errors=wrap_validation_errors\n        )\n\n\n@dataclass\nclass StructuredTextOutputSchema(TextOutputSchema[OutputDataT], ABC):\n    processor: ObjectOutputProcessor[OutputDataT] | UnionOutputProcessor[OutputDataT]\n\n    @property\n    def object_def(self) -> OutputObjectDefinition:\n        return self.processor.object_def\n\n\n@dataclass\nclass NativeOutputSchema(StructuredTextOutputSchema[OutputDataT]):\n    @property\n    def mode(self) -> OutputMode:\n        return 'native'\n\n    def raise_if_unsupported(self, profile: ModelProfile) -> None:\n        \"\"\"Raise an error if the mode is not supported by the model.\"\"\"\n        if not profile.supports_json_schema_output:\n            raise UserError('Native structured output is not supported by the model.')\n\n    async def process(\n        self,\n        text: str,\n        run_context: RunContext[AgentDepsT],\n        allow_partial: bool = False,\n        wrap_validation_errors: bool = True,\n    ) -> OutputDataT:\n        \"\"\"Validate an output message.\n\n        Args:\n            text: The output text to validate.\n            run_context: The current run context.\n            allow_partial: If true, allow partial validation.\n            wrap_validation_errors: If true, wrap the validation errors in a retry message.\n\n        Returns:\n            Either the validated output data (left) or a retry message (right).\n        \"\"\"\n        return await self.processor.process(\n            text, run_context, allow_partial=allow_partial, wrap_validation_errors=wrap_validation_errors\n        )\n\n\n@dataclass\nclass PromptedOutputSchema(StructuredTextOutputSchema[OutputDataT]):\n    template: str | None = None\n\n    @property\n    def mode(self) -> OutputMode:\n        return 'prompted'\n\n    def raise_if_unsupported(self, profile: ModelProfile) -> None:\n        \"\"\"Raise an error if the mode is not supported by the model.\"\"\"\n        pass\n\n    def instructions(self, default_template: str) -> str:\n        \"\"\"Get instructions to tell model to output JSON matching the schema.\"\"\"\n        template = self.template or default_template\n\n        if '{schema}' not in template:\n            template = '\\n\\n'.join([template, '{schema}'])\n\n        object_def = self.object_def\n        schema = object_def.json_schema.copy()\n        if object_def.name:\n            schema['title'] = object_def.name\n        if object_def.description:\n            schema['description'] = object_def.description\n\n        return template.format(schema=json.dumps(schema))\n\n    async def process(\n        self,\n        text: str,\n        run_context: RunContext[AgentDepsT],\n        allow_partial: bool = False,\n        wrap_validation_errors: bool = True,\n    ) -> OutputDataT:\n        \"\"\"Validate an output message.\n\n        Args:\n            text: The output text to validate.\n            run_context: The current run context.\n            allow_partial: If true, allow partial validation.\n            wrap_validation_errors: If true, wrap the validation errors in a retry message.\n\n        Returns:\n            Either the validated output data (left) or a retry message (right).\n        \"\"\"\n        text = _utils.strip_markdown_fences(text)\n\n        return await self.processor.process(\n            text, run_context, allow_partial=allow_partial, wrap_validation_errors=wrap_validation_errors\n        )\n\n\n@dataclass(init=False)\nclass ToolOutputSchema(OutputSchema[OutputDataT]):\n    _toolset: OutputToolset[Any] | None\n\n    def __init__(self, toolset: OutputToolset[Any] | None, allows_deferred_tools: bool):\n        super().__init__(allows_deferred_tools)\n        self._toolset = toolset\n\n    @property\n    def mode(self) -> OutputMode:\n        return 'tool'\n\n    def raise_if_unsupported(self, profile: ModelProfile) -> None:\n        \"\"\"Raise an error if the mode is not supported by the model.\"\"\"\n        if not profile.supports_tools:\n            raise UserError('Output tools are not supported by the model.')\n\n    @property\n    def toolset(self) -> OutputToolset[Any] | None:\n        \"\"\"Get the toolset for this output schema.\"\"\"\n        return self._toolset\n\n\n@dataclass(init=False)\nclass ToolOrTextOutputSchema(ToolOutputSchema[OutputDataT], PlainTextOutputSchema[OutputDataT]):\n    def __init__(\n        self,\n        processor: PlainTextOutputProcessor[OutputDataT] | None,\n        toolset: OutputToolset[Any] | None,\n        allows_deferred_tools: bool,\n    ):\n        super().__init__(toolset=toolset, allows_deferred_tools=allows_deferred_tools)\n        self.processor = processor\n\n    @property\n    def mode(self) -> OutputMode:\n        return 'tool_or_text'\n\n\n@dataclass\nclass OutputObjectDefinition:\n    json_schema: ObjectJsonSchema\n    name: str | None = None\n    description: str | None = None\n    strict: bool | None = None\n\n\n@dataclass(init=False)\nclass BaseOutputProcessor(ABC, Generic[OutputDataT]):\n    @abstractmethod\n    async def process(\n        self,\n        data: str,\n        run_context: RunContext[AgentDepsT],\n        allow_partial: bool = False,\n        wrap_validation_errors: bool = True,\n    ) -> OutputDataT:\n        \"\"\"Process an output message, performing validation and (if necessary) calling the output function.\"\"\"\n        raise NotImplementedError()\n\n\n@dataclass(init=False)\nclass ObjectOutputProcessor(BaseOutputProcessor[OutputDataT]):\n    object_def: OutputObjectDefinition\n    outer_typed_dict_key: str | None = None\n    validator: SchemaValidator\n    _function_schema: _function_schema.FunctionSchema | None = None\n\n    def __init__(\n        self,\n        output: OutputTypeOrFunction[OutputDataT],\n        *,\n        name: str | None = None,\n        description: str | None = None,\n        strict: bool | None = None,\n    ):\n        if inspect.isfunction(output) or inspect.ismethod(output):\n            self._function_schema = _function_schema.function_schema(output, GenerateToolJsonSchema)\n            self.validator = self._function_schema.validator\n            json_schema = self._function_schema.json_schema\n            json_schema['description'] = self._function_schema.description\n        else:\n            json_schema_type_adapter: TypeAdapter[Any]\n            validation_type_adapter: TypeAdapter[Any]\n            if _utils.is_model_like(output):\n                json_schema_type_adapter = validation_type_adapter = TypeAdapter(output)\n            else:\n                self.outer_typed_dict_key = 'response'\n                output_type: type[OutputDataT] = cast(type[OutputDataT], output)\n\n                response_data_typed_dict = TypedDict(  # noqa: UP013\n                    'response_data_typed_dict',\n                    {'response': output_type},  # pyright: ignore[reportInvalidTypeForm]\n                )\n                json_schema_type_adapter = TypeAdapter(response_data_typed_dict)\n\n                # More lenient validator: allow either the native type or a JSON string containing it\n                # i.e. `response: OutputDataT | Json[OutputDataT]`, as some models don't follow the schema correctly,\n                # e.g. `BedrockConverseModel('us.meta.llama3-2-11b-instruct-v1:0')`\n                response_validation_typed_dict = TypedDict(  # noqa: UP013\n                    'response_validation_typed_dict',\n                    {'response': output_type | Json[output_type]},  # pyright: ignore[reportInvalidTypeForm]\n                )\n                validation_type_adapter = TypeAdapter(response_validation_typed_dict)\n\n            # Really a PluggableSchemaValidator, but it's API-compatible\n            self.validator = cast(SchemaValidator, validation_type_adapter.validator)\n            json_schema = _utils.check_object_json_schema(\n                json_schema_type_adapter.json_schema(schema_generator=GenerateToolJsonSchema)\n            )\n\n            if self.outer_typed_dict_key:\n                # including `response_data_typed_dict` as a title here doesn't add anything and could confuse the LLM\n                json_schema.pop('title')\n\n        if name is None and (json_schema_title := json_schema.get('title', None)):\n            name = json_schema_title\n\n        if json_schema_description := json_schema.pop('description', None):\n            if description is None:\n                description = json_schema_description\n            else:\n                description = f'{description}. {json_schema_description}'\n\n        self.object_def = OutputObjectDefinition(\n            name=name or getattr(output, '__name__', None),\n            description=description,\n            json_schema=json_schema,\n            strict=strict,\n        )\n\n    async def process(\n        self,\n        data: str | dict[str, Any] | None,\n        run_context: RunContext[AgentDepsT],\n        allow_partial: bool = False,\n        wrap_validation_errors: bool = True,\n    ) -> OutputDataT:\n        \"\"\"Process an output message, performing validation and (if necessary) calling the output function.\n\n        Args:\n            data: The output data to validate.\n            run_context: The current run context.\n            allow_partial: If true, allow partial validation.\n            wrap_validation_errors: If true, wrap the validation errors in a retry message.\n\n        Returns:\n            Either the validated output data (left) or a retry message (right).\n        \"\"\"\n        try:\n            output = self.validate(data, allow_partial)\n        except ValidationError as e:\n            if wrap_validation_errors:\n                m = _messages.RetryPromptPart(\n                    content=e.errors(include_url=False),\n                )\n                raise ToolRetryError(m) from e\n            else:\n                raise\n\n        output = await self.call(output, run_context, wrap_validation_errors)\n\n        return output\n\n    def validate(\n        self,\n        data: str | dict[str, Any] | None,\n        allow_partial: bool = False,\n    ) -> dict[str, Any]:\n        pyd_allow_partial: Literal['off', 'trailing-strings'] = 'trailing-strings' if allow_partial else 'off'\n        if isinstance(data, str):\n            return self.validator.validate_json(data or '{}', allow_partial=pyd_allow_partial)\n        else:\n            return self.validator.validate_python(data or {}, allow_partial=pyd_allow_partial)\n\n    async def call(\n        self,\n        output: Any,\n        run_context: RunContext[AgentDepsT],\n        wrap_validation_errors: bool = True,\n    ):\n        if k := self.outer_typed_dict_key:\n            output = output[k]\n\n        if self._function_schema:\n            output = await execute_traced_output_function(\n                self._function_schema, run_context, output, wrap_validation_errors\n            )\n\n        return output\n\n\n@dataclass\nclass UnionOutputResult:\n    kind: str\n    data: ObjectJsonSchema\n\n\n@dataclass\nclass UnionOutputModel:\n    result: UnionOutputResult\n\n\n@dataclass(init=False)\nclass UnionOutputProcessor(BaseOutputProcessor[OutputDataT]):\n    object_def: OutputObjectDefinition\n    _union_processor: ObjectOutputProcessor[UnionOutputModel]\n    _processors: dict[str, ObjectOutputProcessor[OutputDataT]]\n\n    def __init__(\n        self,\n        outputs: Sequence[OutputTypeOrFunction[OutputDataT]],\n        *,\n        name: str | None = None,\n        description: str | None = None,\n        strict: bool | None = None,\n    ):\n        self._union_processor = ObjectOutputProcessor(output=UnionOutputModel)\n\n        json_schemas: list[ObjectJsonSchema] = []\n        self._processors = {}\n        for output in outputs:\n            processor = ObjectOutputProcessor(output=output, strict=strict)\n            object_def = processor.object_def\n\n            object_key = object_def.name or output.__name__\n            i = 1\n            original_key = object_key\n            while object_key in self._processors:\n                i += 1\n                object_key = f'{original_key}_{i}'\n\n            self._processors[object_key] = processor\n\n            json_schema = object_def.json_schema\n            if object_def.name:  # pragma: no branch\n                json_schema['title'] = object_def.name\n            if object_def.description:\n                json_schema['description'] = object_def.description\n\n            json_schemas.append(json_schema)\n\n        json_schemas, all_defs = _utils.merge_json_schema_defs(json_schemas)\n\n        discriminated_json_schemas: list[ObjectJsonSchema] = []\n        for object_key, json_schema in zip(self._processors.keys(), json_schemas):\n            title = json_schema.pop('title', None)\n            description = json_schema.pop('description', None)\n\n            discriminated_json_schema = {\n                'type': 'object',\n                'properties': {\n                    'kind': {\n                        'type': 'string',\n                        'const': object_key,\n                    },\n                    'data': json_schema,\n                },\n                'required': ['kind', 'data'],\n                'additionalProperties': False,\n            }\n            if title:  # pragma: no branch\n                discriminated_json_schema['title'] = title\n            if description:\n                discriminated_json_schema['description'] = description\n\n            discriminated_json_schemas.append(discriminated_json_schema)\n\n        json_schema = {\n            'type': 'object',\n            'properties': {\n                'result': {\n                    'anyOf': discriminated_json_schemas,\n                }\n            },\n            'required': ['result'],\n            'additionalProperties': False,\n        }\n        if all_defs:\n            json_schema['$defs'] = all_defs\n\n        self.object_def = OutputObjectDefinition(\n            json_schema=json_schema,\n            strict=strict,\n            name=name,\n            description=description,\n        )\n\n    async def process(\n        self,\n        data: str | dict[str, Any] | None,\n        run_context: RunContext[AgentDepsT],\n        allow_partial: bool = False,\n        wrap_validation_errors: bool = True,\n    ) -> OutputDataT:\n        union_object = await self._union_processor.process(\n            data, run_context, allow_partial=allow_partial, wrap_validation_errors=wrap_validation_errors\n        )\n\n        result = union_object.result\n        kind = result.kind\n        data = result.data\n        try:\n            processor = self._processors[kind]\n        except KeyError as e:  # pragma: no cover\n            if wrap_validation_errors:\n                m = _messages.RetryPromptPart(content=f'Invalid kind: {kind}')\n                raise ToolRetryError(m) from e\n            else:\n                raise\n\n        return await processor.process(\n            data, run_context, allow_partial=allow_partial, wrap_validation_errors=wrap_validation_errors\n        )\n\n\n@dataclass(init=False)\nclass PlainTextOutputProcessor(BaseOutputProcessor[OutputDataT]):\n    _function_schema: _function_schema.FunctionSchema\n    _str_argument_name: str\n\n    def __init__(\n        self,\n        output_function: TextOutputFunc[OutputDataT],\n    ):\n        self._function_schema = _function_schema.function_schema(output_function, GenerateToolJsonSchema)\n\n        arguments_schema = self._function_schema.json_schema.get('properties', {})\n        argument_name = next(iter(arguments_schema.keys()), None)\n        if argument_name and arguments_schema.get(argument_name, {}).get('type') == 'string':\n            self._str_argument_name = argument_name\n            return\n\n        raise UserError('TextOutput must take a function taking a `str`')\n\n    @property\n    def object_def(self) -> None:\n        return None  # pragma: no cover\n\n    async def process(\n        self,\n        data: str,\n        run_context: RunContext[AgentDepsT],\n        allow_partial: bool = False,\n        wrap_validation_errors: bool = True,\n    ) -> OutputDataT:\n        args = {self._str_argument_name: data}\n        output = await execute_traced_output_function(self._function_schema, run_context, args, wrap_validation_errors)\n\n        return cast(OutputDataT, output)\n\n\n@dataclass(init=False)\nclass OutputToolset(AbstractToolset[AgentDepsT]):\n    \"\"\"A toolset that contains contains output tools for agent output types.\"\"\"\n\n    _tool_defs: list[ToolDefinition]\n    \"\"\"The tool definitions for the output tools in this toolset.\"\"\"\n    processors: dict[str, ObjectOutputProcessor[Any]]\n    \"\"\"The processors for the output tools in this toolset.\"\"\"\n    max_retries: int\n    output_validators: list[OutputValidator[AgentDepsT, Any]]\n\n    @classmethod\n    def build(\n        cls,\n        outputs: list[OutputTypeOrFunction[OutputDataT] | ToolOutput[OutputDataT]],\n        name: str | None = None,\n        description: str | None = None,\n        strict: bool | None = None,\n    ) -> Self | None:\n        if len(outputs) == 0:\n            return None\n\n        processors: dict[str, ObjectOutputProcessor[Any]] = {}\n        tool_defs: list[ToolDefinition] = []\n\n        default_name = name or DEFAULT_OUTPUT_TOOL_NAME\n        default_description = description\n        default_strict = strict\n\n        multiple = len(outputs) > 1\n        for output in outputs:\n            name = None\n            description = None\n            strict = None\n            if isinstance(output, ToolOutput):\n                # do we need to error on conflicts here? (DavidM): If this is internal maybe doesn't matter, if public, use overloads\n                name = output.name\n                description = output.description\n                strict = output.strict\n\n                output = output.output\n\n            description = description or default_description\n            if strict is None:\n                strict = default_strict\n\n            processor = ObjectOutputProcessor(output=output, description=description, strict=strict)\n            object_def = processor.object_def\n\n            if name is None:\n                name = default_name\n                if multiple:\n                    name += f'_{object_def.name}'\n\n            i = 1\n            original_name = name\n            while name in processors:\n                i += 1\n                name = f'{original_name}_{i}'\n\n            description = object_def.description\n            if not description:\n                description = DEFAULT_OUTPUT_TOOL_DESCRIPTION\n                if multiple:\n                    description = f'{object_def.name}: {description}'\n\n            tool_def = ToolDefinition(\n                name=name,\n                description=description,\n                parameters_json_schema=object_def.json_schema,\n                strict=object_def.strict,\n                outer_typed_dict_key=processor.outer_typed_dict_key,\n                kind='output',\n            )\n            processors[name] = processor\n            tool_defs.append(tool_def)\n\n        return cls(processors=processors, tool_defs=tool_defs)\n\n    def __init__(\n        self,\n        tool_defs: list[ToolDefinition],\n        processors: dict[str, ObjectOutputProcessor[Any]],\n        max_retries: int = 1,\n        output_validators: list[OutputValidator[AgentDepsT, Any]] | None = None,\n    ):\n        self.processors = processors\n        self._tool_defs = tool_defs\n        self.max_retries = max_retries\n        self.output_validators = output_validators or []\n\n    @property\n    def id(self) -> str | None:\n        return '<output>'  # pragma: no cover\n\n    @property\n    def label(self) -> str:\n        return \"the agent's output tools\"\n\n    async def get_tools(self, ctx: RunContext[AgentDepsT]) -> dict[str, ToolsetTool[AgentDepsT]]:\n        return {\n            tool_def.name: ToolsetTool(\n                toolset=self,\n                tool_def=tool_def,\n                max_retries=self.max_retries,\n                args_validator=self.processors[tool_def.name].validator,\n            )\n            for tool_def in self._tool_defs\n        }\n\n    async def call_tool(\n        self, name: str, tool_args: dict[str, Any], ctx: RunContext[AgentDepsT], tool: ToolsetTool[AgentDepsT]\n    ) -> Any:\n        output = await self.processors[name].call(tool_args, ctx, wrap_validation_errors=False)\n        for validator in self.output_validators:\n            output = await validator.validate(output, ctx, wrap_validation_errors=False)\n        return output\n\n\n@overload\ndef _flatten_output_spec(\n    output_spec: OutputTypeOrFunction[T] | Sequence[OutputTypeOrFunction[T]],\n) -> Sequence[OutputTypeOrFunction[T]]: ...\n\n\n@overload\ndef _flatten_output_spec(output_spec: OutputSpec[T]) -> Sequence[_OutputSpecItem[T]]: ...\n\n\ndef _flatten_output_spec(output_spec: OutputSpec[T]) -> Sequence[_OutputSpecItem[T]]:\n    outputs: Sequence[OutputSpec[T]]\n    if isinstance(output_spec, Sequence):\n        outputs = output_spec\n    else:\n        outputs = (output_spec,)\n\n    outputs_flat: list[_OutputSpecItem[T]] = []\n    for output in outputs:\n        if isinstance(output, Sequence):\n            outputs_flat.extend(_flatten_output_spec(cast(OutputSpec[T], output)))\n        elif union_types := _utils.get_union_args(output):\n            outputs_flat.extend(union_types)\n        else:\n            outputs_flat.append(cast(_OutputSpecItem[T], output))\n    return outputs_flat\n\n\n=== pydantic_ai_slim/pydantic_ai/retries.py ===\n\"\"\"Retries utilities based on tenacity, especially for HTTP requests.\n\nThis module provides HTTP transport wrappers and wait strategies that integrate with\nthe tenacity library to add retry capabilities to HTTP requests. The transports can be\nused with HTTP clients that support custom transports (such as httpx), while the wait\nstrategies can be used with any tenacity retry decorator.\n\nThe module includes:\n- TenacityTransport: Synchronous HTTP transport with retry capabilities\n- AsyncTenacityTransport: Asynchronous HTTP transport with retry capabilities\n- wait_retry_after: Wait strategy that respects HTTP Retry-After headers\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom types import TracebackType\n\nfrom httpx import (\n    AsyncBaseTransport,\n    AsyncHTTPTransport,\n    BaseTransport,\n    HTTPStatusError,\n    HTTPTransport,\n    Request,\n    Response,\n)\n\ntry:\n    from tenacity import RetryCallState, RetryError, retry, wait_exponential\nexcept ImportError as _import_error:\n    raise ImportError(\n        'Please install `tenacity` to use the retries utilities, '\n        'you can use the `retries` optional group — `pip install \"pydantic-ai-slim[retries]\"`'\n    ) from _import_error\n\nfrom collections.abc import Awaitable, Callable\nfrom datetime import datetime, timezone\nfrom email.utils import parsedate_to_datetime\nfrom typing import TYPE_CHECKING, Any, cast\n\nfrom typing_extensions import TypedDict\n\nif TYPE_CHECKING:\n    from tenacity.asyncio.retry import RetryBaseT\n    from tenacity.retry import RetryBaseT as SyncRetryBaseT\n    from tenacity.stop import StopBaseT\n    from tenacity.wait import WaitBaseT\n\n__all__ = ['RetryConfig', 'TenacityTransport', 'AsyncTenacityTransport', 'wait_retry_after']\n\n\nclass RetryConfig(TypedDict, total=False):\n    \"\"\"The configuration for tenacity-based retrying.\n\n    These are precisely the arguments to the tenacity `retry` decorator, and they are generally\n    used internally by passing them to that decorator via `@retry(**config)` or similar.\n\n    All fields are optional, and if not provided, the default values from the `tenacity.retry` decorator will be used.\n    \"\"\"\n\n    sleep: Callable[[int | float], None | Awaitable[None]]\n    \"\"\"A sleep strategy to use for sleeping between retries.\n\n    Tenacity's default for this argument is `tenacity.nap.sleep`.\"\"\"\n\n    stop: StopBaseT\n    \"\"\"\n    A stop strategy to determine when to stop retrying.\n\n    Tenacity's default for this argument is `tenacity.stop.stop_never`.\"\"\"\n\n    wait: WaitBaseT\n    \"\"\"\n    A wait strategy to determine how long to wait between retries.\n\n    Tenacity's default for this argument is `tenacity.wait.wait_none`.\"\"\"\n\n    retry: SyncRetryBaseT | RetryBaseT\n    \"\"\"A retry strategy to determine which exceptions should trigger a retry.\n\n    Tenacity's default for this argument is `tenacity.retry.retry_if_exception_type()`.\"\"\"\n\n    before: Callable[[RetryCallState], None | Awaitable[None]]\n    \"\"\"\n    A callable that is called before each retry attempt.\n\n    Tenacity's default for this argument is `tenacity.before.before_nothing`.\"\"\"\n\n    after: Callable[[RetryCallState], None | Awaitable[None]]\n    \"\"\"\n    A callable that is called after each retry attempt.\n\n    Tenacity's default for this argument is `tenacity.after.after_nothing`.\"\"\"\n\n    before_sleep: Callable[[RetryCallState], None | Awaitable[None]] | None\n    \"\"\"\n    An optional callable that is called before sleeping between retries.\n\n    Tenacity's default for this argument is `None`.\"\"\"\n\n    reraise: bool\n    \"\"\"Whether to reraise the last exception if the retry attempts are exhausted, or raise a RetryError instead.\n\n    Tenacity's default for this argument is `False`.\"\"\"\n\n    retry_error_cls: type[RetryError]\n    \"\"\"The exception class to raise when the retry attempts are exhausted and `reraise` is False.\n\n    Tenacity's default for this argument is `tenacity.RetryError`.\"\"\"\n\n    retry_error_callback: Callable[[RetryCallState], Any | Awaitable[Any]] | None\n    \"\"\"An optional callable that is called when the retry attempts are exhausted and `reraise` is False.\n\n    Tenacity's default for this argument is `None`.\"\"\"\n\n\nclass TenacityTransport(BaseTransport):\n    \"\"\"Synchronous HTTP transport with tenacity-based retry functionality.\n\n    This transport wraps another BaseTransport and adds retry capabilities using the tenacity library.\n    It can be configured to retry requests based on various conditions such as specific exception types,\n    response status codes, or custom validation logic.\n\n    The transport works by intercepting HTTP requests and responses, allowing the tenacity controller\n    to determine when and how to retry failed requests. The validate_response function can be used\n    to convert HTTP responses into exceptions that trigger retries.\n\n    Args:\n        wrapped: The underlying transport to wrap and add retry functionality to.\n        config: The arguments to use for the tenacity `retry` decorator, including retry conditions,\n            wait strategy, stop conditions, etc. See the tenacity docs for more info.\n        validate_response: Optional callable that takes a Response and can raise an exception\n            to be handled by the controller if the response should trigger a retry.\n            Common use case is to raise exceptions for certain HTTP status codes.\n            If None, no response validation is performed.\n\n    Example:\n        ```python\n        from httpx import Client, HTTPStatusError, HTTPTransport\n        from tenacity import retry_if_exception_type, stop_after_attempt\n\n        from pydantic_ai.retries import RetryConfig, TenacityTransport, wait_retry_after\n\n        transport = TenacityTransport(\n            RetryConfig(\n                retry=retry_if_exception_type(HTTPStatusError),\n                wait=wait_retry_after(max_wait=300),\n                stop=stop_after_attempt(5),\n                reraise=True\n            ),\n            HTTPTransport(),\n            validate_response=lambda r: r.raise_for_status()\n        )\n        client = Client(transport=transport)\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        config: RetryConfig,\n        wrapped: BaseTransport | None = None,\n        validate_response: Callable[[Response], Any] | None = None,\n    ):\n        self.config = config\n        self.wrapped = wrapped or HTTPTransport()\n        self.validate_response = validate_response\n\n    def handle_request(self, request: Request) -> Response:\n        \"\"\"Handle an HTTP request with retry logic.\n\n        Args:\n            request: The HTTP request to handle.\n\n        Returns:\n            The HTTP response.\n\n        Raises:\n            RuntimeError: If the retry controller did not make any attempts.\n            Exception: Any exception raised by the wrapped transport or validation function.\n        \"\"\"\n\n        @retry(**self.config)\n        def handle_request(req: Request) -> Response:\n            response = self.wrapped.handle_request(req)\n\n            # this is normally set by httpx _after_ calling this function, but we want the request in the validator:\n            response.request = req\n\n            if self.validate_response:\n                try:\n                    self.validate_response(response)\n                except Exception:\n                    response.close()\n                    raise\n            return response\n\n        return handle_request(request)\n\n    def __enter__(self) -> TenacityTransport:\n        self.wrapped.__enter__()\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None = None,\n        exc_value: BaseException | None = None,\n        traceback: TracebackType | None = None,\n    ) -> None:\n        self.wrapped.__exit__(exc_type, exc_value, traceback)\n\n    def close(self) -> None:\n        self.wrapped.close()  # pragma: no cover\n\n\nclass AsyncTenacityTransport(AsyncBaseTransport):\n    \"\"\"Asynchronous HTTP transport with tenacity-based retry functionality.\n\n    This transport wraps another AsyncBaseTransport and adds retry capabilities using the tenacity library.\n    It can be configured to retry requests based on various conditions such as specific exception types,\n    response status codes, or custom validation logic.\n\n    The transport works by intercepting HTTP requests and responses, allowing the tenacity controller\n    to determine when and how to retry failed requests. The validate_response function can be used\n    to convert HTTP responses into exceptions that trigger retries.\n\n    Args:\n        wrapped: The underlying async transport to wrap and add retry functionality to.\n        config: The arguments to use for the tenacity `retry` decorator, including retry conditions,\n            wait strategy, stop conditions, etc. See the tenacity docs for more info.\n        validate_response: Optional callable that takes a Response and can raise an exception\n            to be handled by the controller if the response should trigger a retry.\n            Common use case is to raise exceptions for certain HTTP status codes.\n            If None, no response validation is performed.\n\n    Example:\n        ```python\n        from httpx import AsyncClient, HTTPStatusError\n        from tenacity import retry_if_exception_type, stop_after_attempt\n\n        from pydantic_ai.retries import AsyncTenacityTransport, RetryConfig, wait_retry_after\n\n        transport = AsyncTenacityTransport(\n            RetryConfig(\n                retry=retry_if_exception_type(HTTPStatusError),\n                wait=wait_retry_after(max_wait=300),\n                stop=stop_after_attempt(5),\n                reraise=True\n            ),\n            validate_response=lambda r: r.raise_for_status()\n        )\n        client = AsyncClient(transport=transport)\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        config: RetryConfig,\n        wrapped: AsyncBaseTransport | None = None,\n        validate_response: Callable[[Response], Any] | None = None,\n    ):\n        self.config = config\n        self.wrapped = wrapped or AsyncHTTPTransport()\n        self.validate_response = validate_response\n\n    async def handle_async_request(self, request: Request) -> Response:\n        \"\"\"Handle an async HTTP request with retry logic.\n\n        Args:\n            request: The HTTP request to handle.\n\n        Returns:\n            The HTTP response.\n\n        Raises:\n            RuntimeError: If the retry controller did not make any attempts.\n            Exception: Any exception raised by the wrapped transport or validation function.\n        \"\"\"\n\n        @retry(**self.config)\n        async def handle_async_request(req: Request) -> Response:\n            response = await self.wrapped.handle_async_request(req)\n\n            # this is normally set by httpx _after_ calling this function, but we want the request in the validator:\n            response.request = req\n\n            if self.validate_response:\n                try:\n                    self.validate_response(response)\n                except Exception:\n                    await response.aclose()\n                    raise\n            return response\n\n        return await handle_async_request(request)\n\n    async def __aenter__(self) -> AsyncTenacityTransport:\n        await self.wrapped.__aenter__()\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None = None,\n        exc_value: BaseException | None = None,\n        traceback: TracebackType | None = None,\n    ) -> None:\n        await self.wrapped.__aexit__(exc_type, exc_value, traceback)\n\n    async def aclose(self) -> None:\n        await self.wrapped.aclose()\n\n\ndef wait_retry_after(\n    fallback_strategy: Callable[[RetryCallState], float] | None = None, max_wait: float = 300\n) -> Callable[[RetryCallState], float]:\n    \"\"\"Create a tenacity-compatible wait strategy that respects HTTP Retry-After headers.\n\n    This wait strategy checks if the exception contains an HTTPStatusError with a\n    Retry-After header, and if so, waits for the time specified in the header.\n    If no header is present or parsing fails, it falls back to the provided strategy.\n\n    The Retry-After header can be in two formats:\n    - An integer representing seconds to wait\n    - An HTTP date string representing when to retry\n\n    Args:\n        fallback_strategy: Wait strategy to use when no Retry-After header is present\n                          or parsing fails. Defaults to exponential backoff with max 60s.\n        max_wait: Maximum time to wait in seconds, regardless of header value.\n                 Defaults to 300 (5 minutes).\n\n    Returns:\n        A wait function that can be used with tenacity retry decorators.\n\n    Example:\n        ```python\n        from httpx import AsyncClient, HTTPStatusError\n        from tenacity import retry_if_exception_type, stop_after_attempt\n\n        from pydantic_ai.retries import AsyncTenacityTransport, RetryConfig, wait_retry_after\n\n        transport = AsyncTenacityTransport(\n            RetryConfig(\n                retry=retry_if_exception_type(HTTPStatusError),\n                wait=wait_retry_after(max_wait=120),\n                stop=stop_after_attempt(5),\n                reraise=True\n            ),\n            validate_response=lambda r: r.raise_for_status()\n        )\n        client = AsyncClient(transport=transport)\n        ```\n    \"\"\"\n    if fallback_strategy is None:\n        fallback_strategy = wait_exponential(multiplier=1, max=60)\n\n    def wait_func(state: RetryCallState) -> float:\n        exc = state.outcome.exception() if state.outcome else None\n        if isinstance(exc, HTTPStatusError):\n            retry_after = exc.response.headers.get('retry-after')\n            if retry_after:\n                try:\n                    # Try parsing as seconds first\n                    wait_seconds = int(retry_after)\n                    return min(float(wait_seconds), max_wait)\n                except ValueError:\n                    # Try parsing as HTTP date\n                    try:\n                        retry_time = cast(datetime, parsedate_to_datetime(retry_after))\n                        assert isinstance(retry_time, datetime)\n                        now = datetime.now(timezone.utc)\n                        wait_seconds = (retry_time - now).total_seconds()\n\n                        if wait_seconds > 0:\n                            return min(wait_seconds, max_wait)\n                    except (ValueError, TypeError, AssertionError):\n                        # If date parsing fails, fall back to fallback strategy\n                        pass\n\n        # Use fallback strategy\n        return fallback_strategy(state)\n\n    return wait_func\n\n\n=== pydantic_ai_slim/pydantic_ai/_thinking_part.py ===\nfrom __future__ import annotations as _annotations\n\nfrom pydantic_ai.messages import TextPart, ThinkingPart\n\n\ndef split_content_into_text_and_thinking(content: str, thinking_tags: tuple[str, str]) -> list[ThinkingPart | TextPart]:\n    \"\"\"Split a string into text and thinking parts.\n\n    Some models don't return the thinking part as a separate part, but rather as a tag in the content.\n    This function splits the content into text and thinking parts.\n    \"\"\"\n    start_tag, end_tag = thinking_tags\n    parts: list[ThinkingPart | TextPart] = []\n\n    start_index = content.find(start_tag)\n    while start_index >= 0:\n        before_think, content = content[:start_index], content[start_index + len(start_tag) :]\n        if before_think:\n            parts.append(TextPart(content=before_think))\n        end_index = content.find(end_tag)\n        if end_index >= 0:\n            think_content, content = content[:end_index], content[end_index + len(end_tag) :]\n            parts.append(ThinkingPart(content=think_content))\n        else:\n            # We lose the `<think>` tag, but it shouldn't matter.\n            parts.append(TextPart(content=content))\n            content = ''\n        start_index = content.find(start_tag)\n    if content:\n        parts.append(TextPart(content=content))\n    return parts\n\n\n=== pydantic_ai_slim/pydantic_ai/run.py ===\nfrom __future__ import annotations as _annotations\n\nimport dataclasses\nfrom collections.abc import AsyncIterator\nfrom copy import deepcopy\nfrom datetime import datetime\nfrom typing import TYPE_CHECKING, Any, Generic, Literal, overload\n\nfrom pydantic_graph import End, GraphRun, GraphRunContext\n\nfrom . import (\n    _agent_graph,\n    exceptions,\n    messages as _messages,\n    usage as _usage,\n)\nfrom .output import OutputDataT\nfrom .tools import AgentDepsT\n\nif TYPE_CHECKING:\n    from .result import FinalResult\n\n\n@dataclasses.dataclass(repr=False)\nclass AgentRun(Generic[AgentDepsT, OutputDataT]):\n    \"\"\"A stateful, async-iterable run of an [`Agent`][pydantic_ai.agent.Agent].\n\n    You generally obtain an `AgentRun` instance by calling `async with my_agent.iter(...) as agent_run:`.\n\n    Once you have an instance, you can use it to iterate through the run's nodes as they execute. When an\n    [`End`][pydantic_graph.nodes.End] is reached, the run finishes and [`result`][pydantic_ai.agent.AgentRun.result]\n    becomes available.\n\n    Example:\n    ```python\n    from pydantic_ai import Agent\n\n    agent = Agent('openai:gpt-4o')\n\n    async def main():\n        nodes = []\n        # Iterate through the run, recording each node along the way:\n        async with agent.iter('What is the capital of France?') as agent_run:\n            async for node in agent_run:\n                nodes.append(node)\n        print(nodes)\n        '''\n        [\n            UserPromptNode(\n                user_prompt='What is the capital of France?',\n                instructions_functions=[],\n                system_prompts=(),\n                system_prompt_functions=[],\n                system_prompt_dynamic_functions={},\n            ),\n            ModelRequestNode(\n                request=ModelRequest(\n                    parts=[\n                        UserPromptPart(\n                            content='What is the capital of France?',\n                            timestamp=datetime.datetime(...),\n                        )\n                    ]\n                )\n            ),\n            CallToolsNode(\n                model_response=ModelResponse(\n                    parts=[TextPart(content='The capital of France is Paris.')],\n                    usage=RequestUsage(input_tokens=56, output_tokens=7),\n                    model_name='gpt-4o',\n                    timestamp=datetime.datetime(...),\n                )\n            ),\n            End(data=FinalResult(output='The capital of France is Paris.')),\n        ]\n        '''\n        print(agent_run.result.output)\n        #> The capital of France is Paris.\n    ```\n\n    You can also manually drive the iteration using the [`next`][pydantic_ai.agent.AgentRun.next] method for\n    more granular control.\n    \"\"\"\n\n    _graph_run: GraphRun[\n        _agent_graph.GraphAgentState, _agent_graph.GraphAgentDeps[AgentDepsT, Any], FinalResult[OutputDataT]\n    ]\n\n    @overload\n    def _traceparent(self, *, required: Literal[False]) -> str | None: ...\n    @overload\n    def _traceparent(self) -> str: ...\n    def _traceparent(self, *, required: bool = True) -> str | None:\n        traceparent = self._graph_run._traceparent(required=False)  # type: ignore[reportPrivateUsage]\n        if traceparent is None and required:  # pragma: no cover\n            raise AttributeError('No span was created for this agent run')\n        return traceparent\n\n    @property\n    def ctx(self) -> GraphRunContext[_agent_graph.GraphAgentState, _agent_graph.GraphAgentDeps[AgentDepsT, Any]]:\n        \"\"\"The current context of the agent run.\"\"\"\n        return GraphRunContext[_agent_graph.GraphAgentState, _agent_graph.GraphAgentDeps[AgentDepsT, Any]](\n            state=self._graph_run.state, deps=self._graph_run.deps\n        )\n\n    @property\n    def next_node(\n        self,\n    ) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:\n        \"\"\"The next node that will be run in the agent graph.\n\n        This is the next node that will be used during async iteration, or if a node is not passed to `self.next(...)`.\n        \"\"\"\n        next_node = self._graph_run.next_node\n        if isinstance(next_node, End):\n            return next_node\n        if _agent_graph.is_agent_node(next_node):\n            return next_node\n        raise exceptions.AgentRunError(f'Unexpected node type: {type(next_node)}')  # pragma: no cover\n\n    @property\n    def result(self) -> AgentRunResult[OutputDataT] | None:\n        \"\"\"The final result of the run if it has ended, otherwise `None`.\n\n        Once the run returns an [`End`][pydantic_graph.nodes.End] node, `result` is populated\n        with an [`AgentRunResult`][pydantic_ai.agent.AgentRunResult].\n        \"\"\"\n        graph_run_result = self._graph_run.result\n        if graph_run_result is None:\n            return None\n        return AgentRunResult(\n            graph_run_result.output.output,\n            graph_run_result.output.tool_name,\n            graph_run_result.state,\n            self._graph_run.deps.new_message_index,\n            self._traceparent(required=False),\n        )\n\n    def __aiter__(\n        self,\n    ) -> AsyncIterator[_agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]]:\n        \"\"\"Provide async-iteration over the nodes in the agent run.\"\"\"\n        return self\n\n    async def __anext__(\n        self,\n    ) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:\n        \"\"\"Advance to the next node automatically based on the last returned node.\"\"\"\n        next_node = await self._graph_run.__anext__()\n        if _agent_graph.is_agent_node(node=next_node):\n            return next_node\n        assert isinstance(next_node, End), f'Unexpected node type: {type(next_node)}'\n        return next_node\n\n    async def next(\n        self,\n        node: _agent_graph.AgentNode[AgentDepsT, OutputDataT],\n    ) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:\n        \"\"\"Manually drive the agent run by passing in the node you want to run next.\n\n        This lets you inspect or mutate the node before continuing execution, or skip certain nodes\n        under dynamic conditions. The agent run should be stopped when you return an [`End`][pydantic_graph.nodes.End]\n        node.\n\n        Example:\n        ```python\n        from pydantic_ai import Agent\n        from pydantic_graph import End\n\n        agent = Agent('openai:gpt-4o')\n\n        async def main():\n            async with agent.iter('What is the capital of France?') as agent_run:\n                next_node = agent_run.next_node  # start with the first node\n                nodes = [next_node]\n                while not isinstance(next_node, End):\n                    next_node = await agent_run.next(next_node)\n                    nodes.append(next_node)\n                # Once `next_node` is an End, we've finished:\n                print(nodes)\n                '''\n                [\n                    UserPromptNode(\n                        user_prompt='What is the capital of France?',\n                        instructions_functions=[],\n                        system_prompts=(),\n                        system_prompt_functions=[],\n                        system_prompt_dynamic_functions={},\n                    ),\n                    ModelRequestNode(\n                        request=ModelRequest(\n                            parts=[\n                                UserPromptPart(\n                                    content='What is the capital of France?',\n                                    timestamp=datetime.datetime(...),\n                                )\n                            ]\n                        )\n                    ),\n                    CallToolsNode(\n                        model_response=ModelResponse(\n                            parts=[TextPart(content='The capital of France is Paris.')],\n                            usage=RequestUsage(input_tokens=56, output_tokens=7),\n                            model_name='gpt-4o',\n                            timestamp=datetime.datetime(...),\n                        )\n                    ),\n                    End(data=FinalResult(output='The capital of France is Paris.')),\n                ]\n                '''\n                print('Final result:', agent_run.result.output)\n                #> Final result: The capital of France is Paris.\n        ```\n\n        Args:\n            node: The node to run next in the graph.\n\n        Returns:\n            The next node returned by the graph logic, or an [`End`][pydantic_graph.nodes.End] node if\n            the run has completed.\n        \"\"\"\n        # Note: It might be nice to expose a synchronous interface for iteration, but we shouldn't do it\n        # on this class, or else IDEs won't warn you if you accidentally use `for` instead of `async for` to iterate.\n        next_node = await self._graph_run.next(node)\n        if _agent_graph.is_agent_node(next_node):\n            return next_node\n        assert isinstance(next_node, End), f'Unexpected node type: {type(next_node)}'\n        return next_node\n\n    def usage(self) -> _usage.RunUsage:\n        \"\"\"Get usage statistics for the run so far, including token usage, model requests, and so on.\"\"\"\n        return self._graph_run.state.usage\n\n    def __repr__(self) -> str:  # pragma: no cover\n        result = self._graph_run.result\n        result_repr = '<run not finished>' if result is None else repr(result.output)\n        return f'<{type(self).__name__} result={result_repr} usage={self.usage()}>'\n\n\n@dataclasses.dataclass\nclass AgentRunResult(Generic[OutputDataT]):\n    \"\"\"The final result of an agent run.\"\"\"\n\n    output: OutputDataT\n    \"\"\"The output data from the agent run.\"\"\"\n\n    _output_tool_name: str | None = dataclasses.field(repr=False)\n    _state: _agent_graph.GraphAgentState = dataclasses.field(repr=False)\n    _new_message_index: int = dataclasses.field(repr=False)\n    _traceparent_value: str | None = dataclasses.field(repr=False)\n\n    @overload\n    def _traceparent(self, *, required: Literal[False]) -> str | None: ...\n    @overload\n    def _traceparent(self) -> str: ...\n    def _traceparent(self, *, required: bool = True) -> str | None:\n        if self._traceparent_value is None and required:  # pragma: no cover\n            raise AttributeError('No span was created for this agent run')\n        return self._traceparent_value\n\n    def _set_output_tool_return(self, return_content: str) -> list[_messages.ModelMessage]:\n        \"\"\"Set return content for the output tool.\n\n        Useful if you want to continue the conversation and want to set the response to the output tool call.\n        \"\"\"\n        if not self._output_tool_name:\n            raise ValueError('Cannot set output tool return content when the return type is `str`.')\n\n        messages = self._state.message_history\n        last_message = messages[-1]\n        for idx, part in enumerate(last_message.parts):\n            if isinstance(part, _messages.ToolReturnPart) and part.tool_name == self._output_tool_name:\n                # Only do deepcopy when we have to modify\n                copied_messages = list(messages)\n                copied_last = deepcopy(last_message)\n                copied_last.parts[idx].content = return_content  # type: ignore[misc]\n                copied_messages[-1] = copied_last\n                return copied_messages\n\n        raise LookupError(f'No tool call found with tool name {self._output_tool_name!r}.')\n\n    def all_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:\n        \"\"\"Return the history of _messages.\n\n        Args:\n            output_tool_return_content: The return content of the tool call to set in the last message.\n                This provides a convenient way to modify the content of the output tool call if you want to continue\n                the conversation and want to set the response to the output tool call. If `None`, the last message will\n                not be modified.\n\n        Returns:\n            List of messages.\n        \"\"\"\n        if output_tool_return_content is not None:\n            return self._set_output_tool_return(output_tool_return_content)\n        else:\n            return self._state.message_history\n\n    def all_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:\n        \"\"\"Return all messages from [`all_messages`][pydantic_ai.agent.AgentRunResult.all_messages] as JSON bytes.\n\n        Args:\n            output_tool_return_content: The return content of the tool call to set in the last message.\n                This provides a convenient way to modify the content of the output tool call if you want to continue\n                the conversation and want to set the response to the output tool call. If `None`, the last message will\n                not be modified.\n\n        Returns:\n            JSON bytes representing the messages.\n        \"\"\"\n        return _messages.ModelMessagesTypeAdapter.dump_json(\n            self.all_messages(output_tool_return_content=output_tool_return_content)\n        )\n\n    def new_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:\n        \"\"\"Return new messages associated with this run.\n\n        Messages from older runs are excluded.\n\n        Args:\n            output_tool_return_content: The return content of the tool call to set in the last message.\n                This provides a convenient way to modify the content of the output tool call if you want to continue\n                the conversation and want to set the response to the output tool call. If `None`, the last message will\n                not be modified.\n\n        Returns:\n            List of new messages.\n        \"\"\"\n        return self.all_messages(output_tool_return_content=output_tool_return_content)[self._new_message_index :]\n\n    def new_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:\n        \"\"\"Return new messages from [`new_messages`][pydantic_ai.agent.AgentRunResult.new_messages] as JSON bytes.\n\n        Args:\n            output_tool_return_content: The return content of the tool call to set in the last message.\n                This provides a convenient way to modify the content of the output tool call if you want to continue\n                the conversation and want to set the response to the output tool call. If `None`, the last message will\n                not be modified.\n\n        Returns:\n            JSON bytes representing the new messages.\n        \"\"\"\n        return _messages.ModelMessagesTypeAdapter.dump_json(\n            self.new_messages(output_tool_return_content=output_tool_return_content)\n        )\n\n    def usage(self) -> _usage.RunUsage:\n        \"\"\"Return the usage of the whole run.\"\"\"\n        return self._state.usage\n\n    def timestamp(self) -> datetime:\n        \"\"\"Return the timestamp of last response.\"\"\"\n        model_response = self.all_messages()[-1]\n        assert isinstance(model_response, _messages.ModelResponse)\n        return model_response.timestamp\n\n\n=== pydantic_ai_slim/pydantic_ai/format_prompt.py ===\nfrom __future__ import annotations as _annotations\n\nfrom collections.abc import Iterable, Iterator, Mapping\nfrom dataclasses import asdict, dataclass, field, fields, is_dataclass\nfrom datetime import date\nfrom typing import Any, Literal\nfrom xml.etree import ElementTree\n\nfrom pydantic import BaseModel\n\n__all__ = ('format_as_xml',)\n\nfrom pydantic.fields import ComputedFieldInfo, FieldInfo\n\n\ndef format_as_xml(\n    obj: Any,\n    root_tag: str | None = None,\n    item_tag: str = 'item',\n    none_str: str = 'null',\n    indent: str | None = '  ',\n    include_field_info: Literal['once'] | bool = False,\n) -> str:\n    \"\"\"Format a Python object as XML.\n\n    This is useful since LLMs often find it easier to read semi-structured data (e.g. examples) as XML,\n    rather than JSON etc.\n\n    Supports: `str`, `bytes`, `bytearray`, `bool`, `int`, `float`, `date`, `datetime`, `Mapping`,\n    `Iterable`, `dataclass`, and `BaseModel`.\n\n    Args:\n        obj: Python Object to serialize to XML.\n        root_tag: Outer tag to wrap the XML in, use `None` to omit the outer tag.\n        item_tag: Tag to use for each item in an iterable (e.g. list), this is overridden by the class name\n            for dataclasses and Pydantic models.\n        none_str: String to use for `None` values.\n        indent: Indentation string to use for pretty printing.\n        include_field_info: Whether to include attributes like Pydantic `Field` attributes and dataclasses `field()`\n            `metadata` as XML attributes. In both cases the allowed `Field` attributes and `field()` metadata keys are\n            `title` and `description`. If a field is repeated in the data (e.g. in a list) by setting `once`\n            the attributes are included only in the first occurrence of an XML element relative to the same field.\n\n    Returns:\n        XML representation of the object.\n\n    Example:\n    ```python {title=\"format_as_xml_example.py\" lint=\"skip\"}\n    from pydantic_ai import format_as_xml\n\n    print(format_as_xml({'name': 'John', 'height': 6, 'weight': 200}, root_tag='user'))\n    '''\n    <user>\n      <name>John</name>\n      <height>6</height>\n      <weight>200</weight>\n    </user>\n    '''\n    ```\n    \"\"\"\n    el = _ToXml(\n        data=obj,\n        item_tag=item_tag,\n        none_str=none_str,\n        include_field_info=include_field_info,\n    ).to_xml(root_tag)\n    if root_tag is None and el.text is None:\n        join = '' if indent is None else '\\n'\n        return join.join(_rootless_xml_elements(el, indent))\n    else:\n        if indent is not None:\n            ElementTree.indent(el, space=indent)\n        return ElementTree.tostring(el, encoding='unicode')\n\n\n@dataclass\nclass _ToXml:\n    data: Any\n    item_tag: str\n    none_str: str\n    include_field_info: Literal['once'] | bool\n    # a map of Pydantic and dataclasses Field paths to their metadata:\n    # a field unique string representation and its class\n    _fields_info: dict[str, tuple[str, FieldInfo | ComputedFieldInfo]] = field(default_factory=dict)\n    # keep track of fields we have extracted attributes from\n    _included_fields: set[str] = field(default_factory=set)\n    # keep track of class names for dataclasses and Pydantic models, that occur in lists\n    _element_names: dict[str, str] = field(default_factory=dict)\n    # flag for parsing dataclasses and Pydantic models once\n    _is_info_extracted: bool = False\n    _FIELD_ATTRIBUTES = ('title', 'description')\n\n    def to_xml(self, tag: str | None = None) -> ElementTree.Element:\n        return self._to_xml(value=self.data, path='', tag=tag)\n\n    def _to_xml(self, value: Any, path: str, tag: str | None = None) -> ElementTree.Element:\n        element = self._create_element(self.item_tag if tag is None else tag, path)\n        if value is None:\n            element.text = self.none_str\n        elif isinstance(value, str):\n            element.text = value\n        elif isinstance(value, bytes | bytearray):\n            element.text = value.decode(errors='ignore')\n        elif isinstance(value, bool | int | float):\n            element.text = str(value)\n        elif isinstance(value, date):\n            element.text = value.isoformat()\n        elif isinstance(value, Mapping):\n            if tag is None and path in self._element_names:\n                element.tag = self._element_names[path]\n            self._mapping_to_xml(element, value, path)  # pyright: ignore[reportUnknownArgumentType]\n        elif is_dataclass(value) and not isinstance(value, type):\n            self._init_structure_info()\n            if tag is None:\n                element.tag = value.__class__.__name__\n            self._mapping_to_xml(element, asdict(value), path)\n        elif isinstance(value, BaseModel):\n            self._init_structure_info()\n            if tag is None:\n                element.tag = value.__class__.__name__\n            # by dumping the model we loose all metadata in nested data structures,\n            # but we have collected it when called _init_structure_info\n            self._mapping_to_xml(element, value.model_dump(), path)\n        elif isinstance(value, Iterable):\n            for n, item in enumerate(value):  # pyright: ignore[reportUnknownVariableType,reportUnknownArgumentType]\n                element.append(self._to_xml(value=item, path=f'{path}.[{n}]' if path else f'[{n}]'))\n        else:\n            raise TypeError(f'Unsupported type for XML formatting: {type(value)}')\n        return element\n\n    def _create_element(self, tag: str, path: str) -> ElementTree.Element:\n        element = ElementTree.Element(tag)\n        if path in self._fields_info:\n            field_repr, field_info = self._fields_info[path]\n            if self.include_field_info and self.include_field_info != 'once' or field_repr not in self._included_fields:\n                field_attributes = self._extract_attributes(field_info)\n                for k, v in field_attributes.items():\n                    element.set(k, v)\n                self._included_fields.add(field_repr)\n        return element\n\n    def _init_structure_info(self):\n        \"\"\"Create maps with all data information (fields info and class names), if not already created.\"\"\"\n        if not self._is_info_extracted:\n            self._parse_data_structures(self.data)\n            self._is_info_extracted = True\n\n    def _mapping_to_xml(\n        self,\n        element: ElementTree.Element,\n        mapping: Mapping[Any, Any],\n        path: str = '',\n    ) -> None:\n        for key, value in mapping.items():\n            if isinstance(key, int):\n                key = str(key)\n            elif not isinstance(key, str):\n                raise TypeError(f'Unsupported key type for XML formatting: {type(key)}, only str and int are allowed')\n            element.append(self._to_xml(value=value, path=f'{path}.{key}' if path else key, tag=key))\n\n    def _parse_data_structures(\n        self,\n        value: Any,\n        path: str = '',\n    ):\n        \"\"\"Parse data structures as dataclasses or Pydantic models to extract element names and attributes.\"\"\"\n        if value is None or isinstance(value, (str | int | float | date | bytearray | bytes | bool)):\n            return\n        elif isinstance(value, Mapping):\n            for k, v in value.items():  # pyright: ignore[reportUnknownVariableType]\n                self._parse_data_structures(v, f'{path}.{k}' if path else f'{k}')\n        elif is_dataclass(value) and not isinstance(value, type):\n            self._element_names[path] = value.__class__.__name__\n            for field in fields(value):\n                new_path = f'{path}.{field.name}' if path else field.name\n                if self.include_field_info and field.metadata:\n                    attributes = {k: v for k, v in field.metadata.items() if k in self._FIELD_ATTRIBUTES}\n                    if attributes:\n                        field_repr = f'{value.__class__.__name__}.{field.name}'\n                        self._fields_info[new_path] = (field_repr, FieldInfo(**attributes))\n                self._parse_data_structures(getattr(value, field.name), new_path)\n        elif isinstance(value, BaseModel):\n            self._element_names[path] = value.__class__.__name__\n            for model_fields in (value.__class__.model_fields, value.__class__.model_computed_fields):\n                for field, info in model_fields.items():\n                    new_path = f'{path}.{field}' if path else field\n                    if self.include_field_info and (isinstance(info, ComputedFieldInfo) or not info.exclude):\n                        field_repr = f'{value.__class__.__name__}.{field}'\n                        self._fields_info[new_path] = (field_repr, info)\n                    self._parse_data_structures(getattr(value, field), new_path)\n        elif isinstance(value, Iterable):\n            for n, item in enumerate(value):  # pyright: ignore[reportUnknownVariableType,reportUnknownArgumentType]\n                new_path = f'{path}.[{n}]' if path else f'[{n}]'\n                self._parse_data_structures(item, new_path)\n\n    @classmethod\n    def _extract_attributes(cls, info: FieldInfo | ComputedFieldInfo) -> dict[str, str]:\n        return {attr: str(value) for attr in cls._FIELD_ATTRIBUTES if (value := getattr(info, attr, None)) is not None}\n\n\ndef _rootless_xml_elements(root: ElementTree.Element, indent: str | None) -> Iterator[str]:\n    for sub_element in root:\n        if indent is not None:\n            ElementTree.indent(sub_element, space=indent)\n        yield ElementTree.tostring(sub_element, encoding='unicode')\n\n\n=== pydantic_ai_slim/pydantic_ai/_mcp.py ===\nimport base64\nfrom collections.abc import Sequence\nfrom typing import Literal\n\nfrom . import exceptions, messages\n\ntry:\n    from mcp import types as mcp_types\nexcept ImportError as _import_error:\n    raise ImportError(\n        'Please install the `mcp` package to use the MCP server, '\n        'you can use the `mcp` optional group — `pip install \"pydantic-ai-slim[mcp]\"`'\n    ) from _import_error\n\n\ndef map_from_mcp_params(params: mcp_types.CreateMessageRequestParams) -> list[messages.ModelMessage]:\n    \"\"\"Convert from MCP create message request parameters to pydantic-ai messages.\"\"\"\n    pai_messages: list[messages.ModelMessage] = []\n    request_parts: list[messages.ModelRequestPart] = []\n    if params.systemPrompt:\n        request_parts.append(messages.SystemPromptPart(content=params.systemPrompt))\n    response_parts: list[messages.ModelResponsePart] = []\n    for msg in params.messages:\n        content = msg.content\n        if msg.role == 'user':\n            # if there are any response parts, add a response message wrapping them\n            if response_parts:\n                pai_messages.append(messages.ModelResponse(parts=response_parts))\n                response_parts = []\n\n            # TODO(Marcelo): We can reuse the `_map_tool_result_part` from the mcp module here.\n            if isinstance(content, mcp_types.TextContent):\n                user_part_content: str | Sequence[messages.UserContent] = content.text\n            else:\n                # image content\n                user_part_content = [\n                    messages.BinaryContent(data=base64.b64decode(content.data), media_type=content.mimeType)\n                ]\n\n            request_parts.append(messages.UserPromptPart(content=user_part_content))\n        else:\n            # role is assistant\n            # if there are any request parts, add a request message wrapping them\n            if request_parts:\n                pai_messages.append(messages.ModelRequest(parts=request_parts))\n                request_parts = []\n\n            response_parts.append(map_from_sampling_content(content))\n\n    if response_parts:\n        pai_messages.append(messages.ModelResponse(parts=response_parts))\n    if request_parts:\n        pai_messages.append(messages.ModelRequest(parts=request_parts))\n    return pai_messages\n\n\ndef map_from_pai_messages(pai_messages: list[messages.ModelMessage]) -> tuple[str, list[mcp_types.SamplingMessage]]:\n    \"\"\"Convert from pydantic-ai messages to MCP sampling messages.\n\n    Returns:\n        A tuple containing the system prompt and a list of sampling messages.\n    \"\"\"\n    sampling_msgs: list[mcp_types.SamplingMessage] = []\n\n    def add_msg(\n        role: Literal['user', 'assistant'],\n        content: mcp_types.TextContent | mcp_types.ImageContent | mcp_types.AudioContent,\n    ):\n        sampling_msgs.append(mcp_types.SamplingMessage(role=role, content=content))\n\n    system_prompt: list[str] = []\n    for pai_message in pai_messages:\n        if isinstance(pai_message, messages.ModelRequest):\n            if pai_message.instructions is not None:\n                system_prompt.append(pai_message.instructions)\n\n            for part in pai_message.parts:\n                if isinstance(part, messages.SystemPromptPart):\n                    system_prompt.append(part.content)\n                if isinstance(part, messages.UserPromptPart):\n                    if isinstance(part.content, str):\n                        add_msg('user', mcp_types.TextContent(type='text', text=part.content))\n                    else:\n                        for chunk in part.content:\n                            if isinstance(chunk, str):\n                                add_msg('user', mcp_types.TextContent(type='text', text=chunk))\n                            elif isinstance(chunk, messages.BinaryContent) and chunk.is_image:\n                                add_msg(\n                                    'user',\n                                    mcp_types.ImageContent(\n                                        type='image',\n                                        data=base64.b64decode(chunk.data).decode(),\n                                        mimeType=chunk.media_type,\n                                    ),\n                                )\n                            # TODO(Marcelo): Add support for audio content.\n                            else:\n                                raise NotImplementedError(f'Unsupported content type: {type(chunk)}')\n        else:\n            add_msg('assistant', map_from_model_response(pai_message))\n    return ''.join(system_prompt), sampling_msgs\n\n\ndef map_from_model_response(model_response: messages.ModelResponse) -> mcp_types.TextContent:\n    \"\"\"Convert from a model response to MCP text content.\"\"\"\n    text_parts: list[str] = []\n    for part in model_response.parts:\n        if isinstance(part, messages.TextPart):\n            text_parts.append(part.content)\n        # TODO(Marcelo): We should ignore ThinkingPart here.\n        else:\n            raise exceptions.UnexpectedModelBehavior(f'Unexpected part type: {type(part).__name__}, expected TextPart')\n    return mcp_types.TextContent(type='text', text=''.join(text_parts))\n\n\ndef map_from_sampling_content(\n    content: mcp_types.TextContent | mcp_types.ImageContent | mcp_types.AudioContent,\n) -> messages.TextPart:\n    \"\"\"Convert from sampling content to a pydantic-ai text part.\"\"\"\n    if isinstance(content, mcp_types.TextContent):  # pragma: no branch\n        return messages.TextPart(content=content.text)\n    else:\n        raise NotImplementedError('Image and Audio responses in sampling are not yet supported')\n\n\n=== pydantic_ai_slim/pydantic_ai/tools.py ===\nfrom __future__ import annotations as _annotations\n\nfrom collections.abc import Awaitable, Callable, Sequence\nfrom dataclasses import KW_ONLY, dataclass, field, replace\nfrom typing import Annotated, Any, Concatenate, Generic, Literal, TypeAlias, cast\n\nfrom pydantic import Discriminator, Tag\nfrom pydantic.json_schema import GenerateJsonSchema, JsonSchemaValue\nfrom pydantic_core import SchemaValidator, core_schema\nfrom typing_extensions import ParamSpec, Self, TypeVar\n\nfrom . import _function_schema, _utils\nfrom ._run_context import AgentDepsT, RunContext\nfrom .exceptions import ModelRetry\nfrom .messages import RetryPromptPart, ToolCallPart, ToolReturn\n\n__all__ = (\n    'AgentDepsT',\n    'DocstringFormat',\n    'RunContext',\n    'SystemPromptFunc',\n    'ToolFuncContext',\n    'ToolFuncPlain',\n    'ToolFuncEither',\n    'ToolParams',\n    'ToolPrepareFunc',\n    'ToolsPrepareFunc',\n    'Tool',\n    'ObjectJsonSchema',\n    'ToolDefinition',\n    'DeferredToolRequests',\n    'DeferredToolResults',\n    'ToolApproved',\n    'ToolDenied',\n)\n\n\nToolParams = ParamSpec('ToolParams', default=...)\n\"\"\"Retrieval function param spec.\"\"\"\n\nSystemPromptFunc: TypeAlias = (\n    Callable[[RunContext[AgentDepsT]], str]\n    | Callable[[RunContext[AgentDepsT]], Awaitable[str]]\n    | Callable[[], str]\n    | Callable[[], Awaitable[str]]\n)\n\"\"\"A function that may or maybe not take `RunContext` as an argument, and may or may not be async.\n\nUsage `SystemPromptFunc[AgentDepsT]`.\n\"\"\"\n\nToolFuncContext: TypeAlias = Callable[Concatenate[RunContext[AgentDepsT], ToolParams], Any]\n\"\"\"A tool function that takes `RunContext` as the first argument.\n\nUsage `ToolContextFunc[AgentDepsT, ToolParams]`.\n\"\"\"\nToolFuncPlain: TypeAlias = Callable[ToolParams, Any]\n\"\"\"A tool function that does not take `RunContext` as the first argument.\n\nUsage `ToolPlainFunc[ToolParams]`.\n\"\"\"\nToolFuncEither: TypeAlias = ToolFuncContext[AgentDepsT, ToolParams] | ToolFuncPlain[ToolParams]\n\"\"\"Either kind of tool function.\n\nThis is just a union of [`ToolFuncContext`][pydantic_ai.tools.ToolFuncContext] and\n[`ToolFuncPlain`][pydantic_ai.tools.ToolFuncPlain].\n\nUsage `ToolFuncEither[AgentDepsT, ToolParams]`.\n\"\"\"\nToolPrepareFunc: TypeAlias = Callable[[RunContext[AgentDepsT], 'ToolDefinition'], Awaitable['ToolDefinition | None']]\n\"\"\"Definition of a function that can prepare a tool definition at call time.\n\nSee [tool docs](../tools-advanced.md#tool-prepare) for more information.\n\nExample — here `only_if_42` is valid as a `ToolPrepareFunc`:\n\n```python {noqa=\"I001\"}\nfrom pydantic_ai import RunContext, Tool\nfrom pydantic_ai.tools import ToolDefinition\n\nasync def only_if_42(\n    ctx: RunContext[int], tool_def: ToolDefinition\n) -> ToolDefinition | None:\n    if ctx.deps == 42:\n        return tool_def\n\ndef hitchhiker(ctx: RunContext[int], answer: str) -> str:\n    return f'{ctx.deps} {answer}'\n\nhitchhiker = Tool(hitchhiker, prepare=only_if_42)\n```\n\nUsage `ToolPrepareFunc[AgentDepsT]`.\n\"\"\"\n\nToolsPrepareFunc: TypeAlias = Callable[\n    [RunContext[AgentDepsT], list['ToolDefinition']], Awaitable['list[ToolDefinition] | None']\n]\n\"\"\"Definition of a function that can prepare the tool definition of all tools for each step.\nThis is useful if you want to customize the definition of multiple tools or you want to register\na subset of tools for a given step.\n\nExample — here `turn_on_strict_if_openai` is valid as a `ToolsPrepareFunc`:\n\n```python {noqa=\"I001\"}\nfrom dataclasses import replace\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.tools import ToolDefinition\n\n\nasync def turn_on_strict_if_openai(\n    ctx: RunContext[None], tool_defs: list[ToolDefinition]\n) -> list[ToolDefinition] | None:\n    if ctx.model.system == 'openai':\n        return [replace(tool_def, strict=True) for tool_def in tool_defs]\n    return tool_defs\n\nagent = Agent('openai:gpt-4o', prepare_tools=turn_on_strict_if_openai)\n```\n\nUsage `ToolsPrepareFunc[AgentDepsT]`.\n\"\"\"\n\nDocstringFormat: TypeAlias = Literal['google', 'numpy', 'sphinx', 'auto']\n\"\"\"Supported docstring formats.\n\n* `'google'` — [Google-style](https://google.github.io/styleguide/pyguide.html#381-docstrings) docstrings.\n* `'numpy'` — [Numpy-style](https://numpydoc.readthedocs.io/en/latest/format.html) docstrings.\n* `'sphinx'` — [Sphinx-style](https://sphinx-rtd-tutorial.readthedocs.io/en/latest/docstrings.html#the-sphinx-docstring-format) docstrings.\n* `'auto'` — Automatically infer the format based on the structure of the docstring.\n\"\"\"\n\n\n@dataclass(kw_only=True)\nclass DeferredToolRequests:\n    \"\"\"Tool calls that require approval or external execution.\n\n    This can be used as an agent's `output_type` and will be used as the output of the agent run if the model called any deferred tools.\n\n    Results can be passed to the next agent run using a [`DeferredToolResults`][pydantic_ai.tools.DeferredToolResults] object with the same tool call IDs.\n\n    See [deferred tools docs](../deferred-tools.md#deferred-tools) for more information.\n    \"\"\"\n\n    calls: list[ToolCallPart] = field(default_factory=list)\n    \"\"\"Tool calls that require external execution.\"\"\"\n    approvals: list[ToolCallPart] = field(default_factory=list)\n    \"\"\"Tool calls that require human-in-the-loop approval.\"\"\"\n\n\n@dataclass(kw_only=True)\nclass ToolApproved:\n    \"\"\"Indicates that a tool call has been approved and that the tool function should be executed.\"\"\"\n\n    override_args: dict[str, Any] | None = None\n    \"\"\"Optional tool call arguments to use instead of the original arguments.\"\"\"\n\n    kind: Literal['tool-approved'] = 'tool-approved'\n\n\n@dataclass\nclass ToolDenied:\n    \"\"\"Indicates that a tool call has been denied and that a denial message should be returned to the model.\"\"\"\n\n    message: str = 'The tool call was denied.'\n    \"\"\"The message to return to the model.\"\"\"\n\n    _: KW_ONLY\n\n    kind: Literal['tool-denied'] = 'tool-denied'\n\n\ndef _deferred_tool_call_result_discriminator(x: Any) -> str | None:\n    if isinstance(x, dict):\n        if 'kind' in x:\n            return cast(str, x['kind'])\n        elif 'part_kind' in x:\n            return cast(str, x['part_kind'])\n    else:\n        if hasattr(x, 'kind'):\n            return cast(str, x.kind)\n        elif hasattr(x, 'part_kind'):\n            return cast(str, x.part_kind)\n    return None\n\n\nDeferredToolApprovalResult: TypeAlias = Annotated[ToolApproved | ToolDenied, Discriminator('kind')]\n\"\"\"Result for a tool call that required human-in-the-loop approval.\"\"\"\nDeferredToolCallResult: TypeAlias = Annotated[\n    Annotated[ToolReturn, Tag('tool-return')]\n    | Annotated[ModelRetry, Tag('model-retry')]\n    | Annotated[RetryPromptPart, Tag('retry-prompt')],\n    Discriminator(_deferred_tool_call_result_discriminator),\n]\n\"\"\"Result for a tool call that required external execution.\"\"\"\nDeferredToolResult = DeferredToolApprovalResult | DeferredToolCallResult\n\"\"\"Result for a tool call that required approval or external execution.\"\"\"\n\n\n@dataclass(kw_only=True)\nclass DeferredToolResults:\n    \"\"\"Results for deferred tool calls from a previous run that required approval or external execution.\n\n    The tool call IDs need to match those from the [`DeferredToolRequests`][pydantic_ai.output.DeferredToolRequests] output object from the previous run.\n\n    See [deferred tools docs](../deferred-tools.md#deferred-tools) for more information.\n    \"\"\"\n\n    calls: dict[str, DeferredToolCallResult | Any] = field(default_factory=dict)\n    \"\"\"Map of tool call IDs to results for tool calls that required external execution.\"\"\"\n    approvals: dict[str, bool | DeferredToolApprovalResult] = field(default_factory=dict)\n    \"\"\"Map of tool call IDs to results for tool calls that required human-in-the-loop approval.\"\"\"\n\n\nA = TypeVar('A')\n\n\nclass GenerateToolJsonSchema(GenerateJsonSchema):\n    def typed_dict_schema(self, schema: core_schema.TypedDictSchema) -> JsonSchemaValue:\n        json_schema = super().typed_dict_schema(schema)\n        # Workaround for https://github.com/pydantic/pydantic/issues/12123\n        if 'additionalProperties' not in json_schema:  # pragma: no branch\n            extra = schema.get('extra_behavior') or schema.get('config', {}).get('extra_fields_behavior')\n            if extra == 'allow':\n                extras_schema = schema.get('extras_schema', None)\n                if extras_schema is not None:\n                    json_schema['additionalProperties'] = self.generate_inner(extras_schema) or True\n                else:\n                    json_schema['additionalProperties'] = True  # pragma: no cover\n            elif extra == 'forbid':\n                json_schema['additionalProperties'] = False\n        return json_schema\n\n    def _named_required_fields_schema(self, named_required_fields: Sequence[tuple[str, bool, Any]]) -> JsonSchemaValue:\n        # Remove largely-useless property titles\n        s = super()._named_required_fields_schema(named_required_fields)\n        for p in s.get('properties', {}):\n            s['properties'][p].pop('title', None)\n        return s\n\n\n@dataclass(init=False)\nclass Tool(Generic[AgentDepsT]):\n    \"\"\"A tool function for an agent.\"\"\"\n\n    function: ToolFuncEither[AgentDepsT]\n    takes_ctx: bool\n    max_retries: int | None\n    name: str\n    description: str | None\n    prepare: ToolPrepareFunc[AgentDepsT] | None\n    docstring_format: DocstringFormat\n    require_parameter_descriptions: bool\n    strict: bool | None\n    sequential: bool\n    requires_approval: bool\n    metadata: dict[str, Any] | None\n    function_schema: _function_schema.FunctionSchema\n    \"\"\"\n    The base JSON schema for the tool's parameters.\n\n    This schema may be modified by the `prepare` function or by the Model class prior to including it in an API request.\n    \"\"\"\n\n    def __init__(\n        self,\n        function: ToolFuncEither[AgentDepsT],\n        *,\n        takes_ctx: bool | None = None,\n        max_retries: int | None = None,\n        name: str | None = None,\n        description: str | None = None,\n        prepare: ToolPrepareFunc[AgentDepsT] | None = None,\n        docstring_format: DocstringFormat = 'auto',\n        require_parameter_descriptions: bool = False,\n        schema_generator: type[GenerateJsonSchema] = GenerateToolJsonSchema,\n        strict: bool | None = None,\n        sequential: bool = False,\n        requires_approval: bool = False,\n        metadata: dict[str, Any] | None = None,\n        function_schema: _function_schema.FunctionSchema | None = None,\n    ):\n        \"\"\"Create a new tool instance.\n\n        Example usage:\n\n        ```python {noqa=\"I001\"}\n        from pydantic_ai import Agent, RunContext, Tool\n\n        async def my_tool(ctx: RunContext[int], x: int, y: int) -> str:\n            return f'{ctx.deps} {x} {y}'\n\n        agent = Agent('test', tools=[Tool(my_tool)])\n        ```\n\n        or with a custom prepare method:\n\n        ```python {noqa=\"I001\"}\n\n        from pydantic_ai import Agent, RunContext, Tool\n        from pydantic_ai.tools import ToolDefinition\n\n        async def my_tool(ctx: RunContext[int], x: int, y: int) -> str:\n            return f'{ctx.deps} {x} {y}'\n\n        async def prep_my_tool(\n            ctx: RunContext[int], tool_def: ToolDefinition\n        ) -> ToolDefinition | None:\n            # only register the tool if `deps == 42`\n            if ctx.deps == 42:\n                return tool_def\n\n        agent = Agent('test', tools=[Tool(my_tool, prepare=prep_my_tool)])\n        ```\n\n\n        Args:\n            function: The Python function to call as the tool.\n            takes_ctx: Whether the function takes a [`RunContext`][pydantic_ai.tools.RunContext] first argument,\n                this is inferred if unset.\n            max_retries: Maximum number of retries allowed for this tool, set to the agent default if `None`.\n            name: Name of the tool, inferred from the function if `None`.\n            description: Description of the tool, inferred from the function if `None`.\n            prepare: custom method to prepare the tool definition for each step, return `None` to omit this\n                tool from a given step. This is useful if you want to customise a tool at call time,\n                or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].\n            docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].\n                Defaults to `'auto'`, such that the format is inferred from the structure of the docstring.\n            require_parameter_descriptions: If True, raise an error if a parameter description is missing. Defaults to False.\n            schema_generator: The JSON schema generator class to use. Defaults to `GenerateToolJsonSchema`.\n            strict: Whether to enforce JSON schema compliance (only affects OpenAI).\n                See [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] for more info.\n            sequential: Whether the function requires a sequential/serial execution environment. Defaults to False.\n            requires_approval: Whether this tool requires human-in-the-loop approval. Defaults to False.\n                See the [tools documentation](../deferred-tools.md#human-in-the-loop-tool-approval) for more info.\n            metadata: Optional metadata for the tool. This is not sent to the model but can be used for filtering and tool behavior customization.\n            function_schema: The function schema to use for the tool. If not provided, it will be generated.\n        \"\"\"\n        self.function = function\n        self.function_schema = function_schema or _function_schema.function_schema(\n            function,\n            schema_generator,\n            takes_ctx=takes_ctx,\n            docstring_format=docstring_format,\n            require_parameter_descriptions=require_parameter_descriptions,\n        )\n        self.takes_ctx = self.function_schema.takes_ctx\n        self.max_retries = max_retries\n        self.name = name or function.__name__\n        self.description = description or self.function_schema.description\n        self.prepare = prepare\n        self.docstring_format = docstring_format\n        self.require_parameter_descriptions = require_parameter_descriptions\n        self.strict = strict\n        self.sequential = sequential\n        self.requires_approval = requires_approval\n        self.metadata = metadata\n\n    @classmethod\n    def from_schema(\n        cls,\n        function: Callable[..., Any],\n        name: str,\n        description: str | None,\n        json_schema: JsonSchemaValue,\n        takes_ctx: bool = False,\n        sequential: bool = False,\n    ) -> Self:\n        \"\"\"Creates a Pydantic tool from a function and a JSON schema.\n\n        Args:\n            function: The function to call.\n                This will be called with keywords only, and no validation of\n                the arguments will be performed.\n            name: The unique name of the tool that clearly communicates its purpose\n            description: Used to tell the model how/when/why to use the tool.\n                You can provide few-shot examples as a part of the description.\n            json_schema: The schema for the function arguments\n            takes_ctx: An optional boolean parameter indicating whether the function\n                accepts the context object as an argument.\n            sequential: Whether the function requires a sequential/serial execution environment. Defaults to False.\n\n        Returns:\n            A Pydantic tool that calls the function\n        \"\"\"\n        function_schema = _function_schema.FunctionSchema(\n            function=function,\n            description=description,\n            validator=SchemaValidator(schema=core_schema.any_schema()),\n            json_schema=json_schema,\n            takes_ctx=takes_ctx,\n            is_async=_utils.is_async_callable(function),\n        )\n\n        return cls(\n            function,\n            takes_ctx=takes_ctx,\n            name=name,\n            description=description,\n            function_schema=function_schema,\n            sequential=sequential,\n        )\n\n    @property\n    def tool_def(self):\n        return ToolDefinition(\n            name=self.name,\n            description=self.description,\n            parameters_json_schema=self.function_schema.json_schema,\n            strict=self.strict,\n            sequential=self.sequential,\n            metadata=self.metadata,\n        )\n\n    async def prepare_tool_def(self, ctx: RunContext[AgentDepsT]) -> ToolDefinition | None:\n        \"\"\"Get the tool definition.\n\n        By default, this method creates a tool definition, then either returns it, or calls `self.prepare`\n        if it's set.\n\n        Returns:\n            return a `ToolDefinition` or `None` if the tools should not be registered for this run.\n        \"\"\"\n        base_tool_def = self.tool_def\n\n        if self.requires_approval and not ctx.tool_call_approved:\n            base_tool_def = replace(base_tool_def, kind='unapproved')\n\n        if self.prepare is not None:\n            return await self.prepare(ctx, base_tool_def)\n        else:\n            return base_tool_def\n\n\nObjectJsonSchema: TypeAlias = dict[str, Any]\n\"\"\"Type representing JSON schema of an object, e.g. where `\"type\": \"object\"`.\n\nThis type is used to define tools parameters (aka arguments) in [ToolDefinition][pydantic_ai.tools.ToolDefinition].\n\nWith PEP-728 this should be a TypedDict with `type: Literal['object']`, and `extra_parts=Any`\n\"\"\"\n\nToolKind: TypeAlias = Literal['function', 'output', 'external', 'unapproved']\n\"\"\"Kind of tool.\"\"\"\n\n\n@dataclass(repr=False, kw_only=True)\nclass ToolDefinition:\n    \"\"\"Definition of a tool passed to a model.\n\n    This is used for both function tools and output tools.\n    \"\"\"\n\n    name: str\n    \"\"\"The name of the tool.\"\"\"\n\n    parameters_json_schema: ObjectJsonSchema = field(default_factory=lambda: {'type': 'object', 'properties': {}})\n    \"\"\"The JSON schema for the tool's parameters.\"\"\"\n\n    description: str | None = None\n    \"\"\"The description of the tool.\"\"\"\n\n    outer_typed_dict_key: str | None = None\n    \"\"\"The key in the outer [TypedDict] that wraps an output tool.\n\n    This will only be set for output tools which don't have an `object` JSON schema.\n    \"\"\"\n\n    strict: bool | None = None\n    \"\"\"Whether to enforce (vendor-specific) strict JSON schema validation for tool calls.\n\n    Setting this to `True` while using a supported model generally imposes some restrictions on the tool's JSON schema\n    in exchange for guaranteeing the API responses strictly match that schema.\n\n    When `False`, the model may be free to generate other properties or types (depending on the vendor).\n    When `None` (the default), the value will be inferred based on the compatibility of the parameters_json_schema.\n\n    Note: this is currently only supported by OpenAI models.\n    \"\"\"\n\n    sequential: bool = False\n    \"\"\"Whether this tool requires a sequential/serial execution environment.\"\"\"\n\n    kind: ToolKind = field(default='function')\n    \"\"\"The kind of tool:\n\n    - `'function'`: a tool that will be executed by Pydantic AI during an agent run and has its result returned to the model\n    - `'output'`: a tool that passes through an output value that ends the run\n    - `'external'`: a tool whose result will be produced outside of the Pydantic AI agent run in which it was called, because it depends on an upstream service (or user) or could take longer to generate than it's reasonable to keep the agent process running.\n        See the [tools documentation](../deferred-tools.md#deferred-tools) for more info.\n    - `'unapproved'`: a tool that requires human-in-the-loop approval.\n        See the [tools documentation](../deferred-tools.md#human-in-the-loop-tool-approval) for more info.\n    \"\"\"\n\n    metadata: dict[str, Any] | None = None\n    \"\"\"Tool metadata that can be set by the toolset this tool came from. It is not sent to the model, but can be used for filtering and tool behavior customization.\n\n    For MCP tools, this contains the `meta`, `annotations`, and `output_schema` fields from the tool definition.\n    \"\"\"\n\n    @property\n    def defer(self) -> bool:\n        \"\"\"Whether calls to this tool will be deferred.\n\n        See the [tools documentation](../deferred-tools.md#deferred-tools) for more info.\n        \"\"\"\n        return self.kind in ('external', 'unapproved')\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n\n=== pydantic_ai_slim/pydantic_ai/_cli.py ===\nfrom __future__ import annotations as _annotations\n\nimport argparse\nimport asyncio\nimport importlib\nimport os\nimport sys\nfrom asyncio import CancelledError\nfrom collections.abc import Sequence\nfrom contextlib import ExitStack\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom typing import Any, cast\n\nfrom typing_inspection.introspection import get_literal_values\n\nfrom . import __version__\nfrom ._run_context import AgentDepsT\nfrom .agent import AbstractAgent, Agent\nfrom .exceptions import UserError\nfrom .messages import ModelMessage, TextPart\nfrom .models import KnownModelName, infer_model\nfrom .output import OutputDataT\n\ntry:\n    import argcomplete\n    import pyperclip\n    from prompt_toolkit import PromptSession\n    from prompt_toolkit.auto_suggest import AutoSuggestFromHistory, Suggestion\n    from prompt_toolkit.buffer import Buffer\n    from prompt_toolkit.document import Document\n    from prompt_toolkit.history import FileHistory\n    from rich.console import Console, ConsoleOptions, RenderResult\n    from rich.live import Live\n    from rich.markdown import CodeBlock, Heading, Markdown\n    from rich.status import Status\n    from rich.style import Style\n    from rich.syntax import Syntax\n    from rich.text import Text\nexcept ImportError as _import_error:\n    raise ImportError(\n        'Please install `rich`, `prompt-toolkit`, `pyperclip` and `argcomplete` to use the Pydantic AI CLI, '\n        'you can use the `cli` optional group — `pip install \"pydantic-ai-slim[cli]\"`'\n    ) from _import_error\n\n\n__all__ = 'cli', 'cli_exit'\n\n\nPYDANTIC_AI_HOME = Path.home() / '.pydantic-ai'\n\"\"\"The home directory for Pydantic AI CLI.\n\nThis folder is used to store the prompt history and configuration.\n\"\"\"\n\nPROMPT_HISTORY_FILENAME = 'prompt-history.txt'\n\n\nclass SimpleCodeBlock(CodeBlock):\n    \"\"\"Customized code blocks in markdown.\n\n    This avoids a background color which messes up copy-pasting and sets the language name as dim prefix and suffix.\n    \"\"\"\n\n    def __rich_console__(self, console: Console, options: ConsoleOptions) -> RenderResult:\n        code = str(self.text).rstrip()\n        yield Text(self.lexer_name, style='dim')\n        yield Syntax(code, self.lexer_name, theme=self.theme, background_color='default', word_wrap=True)\n        yield Text(f'/{self.lexer_name}', style='dim')\n\n\nclass LeftHeading(Heading):\n    \"\"\"Customized headings in markdown to stop centering and prepend markdown style hashes.\"\"\"\n\n    def __rich_console__(self, console: Console, options: ConsoleOptions) -> RenderResult:\n        # note we use `Style(bold=True)` not `self.style_name` here to disable underlining which is ugly IMHO\n        yield Text(f'{\"#\" * int(self.tag[1:])} {self.text.plain}', style=Style(bold=True))\n\n\nMarkdown.elements.update(\n    fence=SimpleCodeBlock,\n    heading_open=LeftHeading,\n)\n\n\ncli_agent = Agent()\n\n\n@cli_agent.system_prompt\ndef cli_system_prompt() -> str:\n    now_utc = datetime.now(timezone.utc)\n    tzinfo = now_utc.astimezone().tzinfo\n    tzname = tzinfo.tzname(now_utc) if tzinfo else ''\n    return f\"\"\"\\\nHelp the user by responding to their request, the output should be concise and always written in markdown.\nThe current date and time is {datetime.now()} {tzname}.\nThe user is running {sys.platform}.\"\"\"\n\n\ndef cli_exit(prog_name: str = 'pai'):  # pragma: no cover\n    \"\"\"Run the CLI and exit.\"\"\"\n    sys.exit(cli(prog_name=prog_name))\n\n\ndef cli(  # noqa: C901\n    args_list: Sequence[str] | None = None, *, prog_name: str = 'pai', default_model: str = 'openai:gpt-4.1'\n) -> int:\n    \"\"\"Run the CLI and return the exit code for the process.\"\"\"\n    parser = argparse.ArgumentParser(\n        prog=prog_name,\n        description=f\"\"\"\\\nPydantic AI CLI v{__version__}\\n\\n\n\nSpecial prompts:\n* `/exit` - exit the interactive mode (ctrl-c and ctrl-d also work)\n* `/markdown` - show the last markdown output of the last question\n* `/multiline` - toggle multiline mode\n* `/cp` - copy the last response to clipboard\n\"\"\",\n        formatter_class=argparse.RawTextHelpFormatter,\n    )\n    parser.add_argument('prompt', nargs='?', help='AI Prompt, if omitted fall into interactive mode')\n    arg = parser.add_argument(\n        '-m',\n        '--model',\n        nargs='?',\n        help=f'Model to use, in format \"<provider>:<model>\" e.g. \"openai:gpt-4.1\" or \"anthropic:claude-sonnet-4-0\". Defaults to \"{default_model}\".',\n    )\n    # we don't want to autocomplete or list models that don't include the provider,\n    # e.g. we want to show `openai:gpt-4o` but not `gpt-4o`\n    qualified_model_names = [n for n in get_literal_values(KnownModelName.__value__) if ':' in n]\n    arg.completer = argcomplete.ChoicesCompleter(qualified_model_names)  # type: ignore[reportPrivateUsage]\n    parser.add_argument(\n        '-a',\n        '--agent',\n        help='Custom Agent to use, in format \"module:variable\", e.g. \"mymodule.submodule:my_agent\"',\n    )\n    parser.add_argument(\n        '-l',\n        '--list-models',\n        action='store_true',\n        help='List all available models and exit',\n    )\n    parser.add_argument(\n        '-t',\n        '--code-theme',\n        nargs='?',\n        help='Which colors to use for code, can be \"dark\", \"light\" or any theme from pygments.org/styles/. Defaults to \"dark\" which works well on dark terminals.',\n        default='dark',\n    )\n    parser.add_argument('--no-stream', action='store_true', help='Disable streaming from the model')\n    parser.add_argument('--version', action='store_true', help='Show version and exit')\n\n    argcomplete.autocomplete(parser)\n    args = parser.parse_args(args_list)\n\n    console = Console()\n    name_version = f'[green]{prog_name} - Pydantic AI CLI v{__version__}[/green]'\n    if args.version:\n        console.print(name_version, highlight=False)\n        return 0\n    if args.list_models:\n        console.print(f'{name_version}\\n\\n[green]Available models:[/green]')\n        for model in qualified_model_names:\n            console.print(f'  {model}', highlight=False)\n        return 0\n\n    agent: Agent[None, str] = cli_agent\n    if args.agent:\n        sys.path.append(os.getcwd())\n        try:\n            module_path, variable_name = args.agent.split(':')\n        except ValueError:\n            console.print('[red]Error: Agent must be specified in \"module:variable\" format[/red]')\n            return 1\n\n        module = importlib.import_module(module_path)\n        agent = getattr(module, variable_name)\n        if not isinstance(agent, Agent):\n            console.print(f'[red]Error: {args.agent} is not an Agent instance[/red]')\n            return 1\n\n    model_arg_set = args.model is not None\n    if agent.model is None or model_arg_set:\n        try:\n            agent.model = infer_model(args.model or default_model)\n        except UserError as e:\n            console.print(f'Error initializing [magenta]{args.model}[/magenta]:\\n[red]{e}[/red]')\n            return 1\n\n    model_name = agent.model if isinstance(agent.model, str) else f'{agent.model.system}:{agent.model.model_name}'\n    if args.agent and model_arg_set:\n        console.print(\n            f'{name_version} using custom agent [magenta]{args.agent}[/magenta] with [magenta]{model_name}[/magenta]',\n            highlight=False,\n        )\n    elif args.agent:\n        console.print(f'{name_version} using custom agent [magenta]{args.agent}[/magenta]', highlight=False)\n    else:\n        console.print(f'{name_version} with [magenta]{model_name}[/magenta]', highlight=False)\n\n    stream = not args.no_stream\n    if args.code_theme == 'light':\n        code_theme = 'default'\n    elif args.code_theme == 'dark':\n        code_theme = 'monokai'\n    else:\n        code_theme = args.code_theme  # pragma: no cover\n\n    if prompt := cast(str, args.prompt):\n        try:\n            asyncio.run(ask_agent(agent, prompt, stream, console, code_theme))\n        except KeyboardInterrupt:\n            pass\n        return 0\n\n    try:\n        return asyncio.run(run_chat(stream, agent, console, code_theme, prog_name))\n    except KeyboardInterrupt:  # pragma: no cover\n        return 0\n\n\nasync def run_chat(\n    stream: bool,\n    agent: AbstractAgent[AgentDepsT, OutputDataT],\n    console: Console,\n    code_theme: str,\n    prog_name: str,\n    config_dir: Path | None = None,\n    deps: AgentDepsT = None,\n    message_history: list[ModelMessage] | None = None,\n) -> int:\n    prompt_history_path = (config_dir or PYDANTIC_AI_HOME) / PROMPT_HISTORY_FILENAME\n    prompt_history_path.parent.mkdir(parents=True, exist_ok=True)\n    prompt_history_path.touch(exist_ok=True)\n    session: PromptSession[Any] = PromptSession(history=FileHistory(str(prompt_history_path)))\n\n    multiline = False\n    messages: list[ModelMessage] = message_history[:] if message_history else []\n\n    while True:\n        try:\n            auto_suggest = CustomAutoSuggest(['/markdown', '/multiline', '/exit', '/cp'])\n            text = await session.prompt_async(f'{prog_name} ➤ ', auto_suggest=auto_suggest, multiline=multiline)\n        except (KeyboardInterrupt, EOFError):  # pragma: no cover\n            return 0\n\n        if not text.strip():\n            continue\n\n        ident_prompt = text.lower().strip().replace(' ', '-')\n        if ident_prompt.startswith('/'):\n            exit_value, multiline = handle_slash_command(ident_prompt, messages, multiline, console, code_theme)\n            if exit_value is not None:\n                return exit_value\n        else:\n            try:\n                messages = await ask_agent(agent, text, stream, console, code_theme, deps, messages)\n            except CancelledError:  # pragma: no cover\n                console.print('[dim]Interrupted[/dim]')\n            except Exception as e:  # pragma: no cover\n                cause = getattr(e, '__cause__', None)\n                console.print(f'\\n[red]{type(e).__name__}:[/red] {e}')\n                if cause:\n                    console.print(f'[dim]Caused by: {cause}[/dim]')\n\n\nasync def ask_agent(\n    agent: AbstractAgent[AgentDepsT, OutputDataT],\n    prompt: str,\n    stream: bool,\n    console: Console,\n    code_theme: str,\n    deps: AgentDepsT = None,\n    messages: list[ModelMessage] | None = None,\n) -> list[ModelMessage]:\n    status = Status('[dim]Working on it…[/dim]', console=console)\n\n    if not stream:\n        with status:\n            result = await agent.run(prompt, message_history=messages, deps=deps)\n        content = str(result.output)\n        console.print(Markdown(content, code_theme=code_theme))\n        return result.all_messages()\n\n    with status, ExitStack() as stack:\n        async with agent.iter(prompt, message_history=messages, deps=deps) as agent_run:\n            live = Live('', refresh_per_second=15, console=console, vertical_overflow='ellipsis')\n            async for node in agent_run:\n                if Agent.is_model_request_node(node):\n                    async with node.stream(agent_run.ctx) as handle_stream:\n                        status.stop()  # stopping multiple times is idempotent\n                        stack.enter_context(live)  # entering multiple times is idempotent\n\n                        async for content in handle_stream.stream_output(debounce_by=None):\n                            live.update(Markdown(str(content), code_theme=code_theme))\n\n        assert agent_run.result is not None\n        return agent_run.result.all_messages()\n\n\nclass CustomAutoSuggest(AutoSuggestFromHistory):\n    def __init__(self, special_suggestions: list[str] | None = None):\n        super().__init__()\n        self.special_suggestions = special_suggestions or []\n\n    def get_suggestion(self, buffer: Buffer, document: Document) -> Suggestion | None:  # pragma: no cover\n        # Get the suggestion from history\n        suggestion = super().get_suggestion(buffer, document)\n\n        # Check for custom suggestions\n        text = document.text_before_cursor.strip()\n        for special in self.special_suggestions:\n            if special.startswith(text):\n                return Suggestion(special[len(text) :])\n        return suggestion\n\n\ndef handle_slash_command(\n    ident_prompt: str, messages: list[ModelMessage], multiline: bool, console: Console, code_theme: str\n) -> tuple[int | None, bool]:\n    if ident_prompt == '/markdown':\n        try:\n            parts = messages[-1].parts\n        except IndexError:\n            console.print('[dim]No markdown output available.[/dim]')\n        else:\n            console.print('[dim]Markdown output of last question:[/dim]\\n')\n            for part in parts:\n                if part.part_kind == 'text':\n                    console.print(\n                        Syntax(\n                            part.content,\n                            lexer='markdown',\n                            theme=code_theme,\n                            word_wrap=True,\n                            background_color='default',\n                        )\n                    )\n\n    elif ident_prompt == '/multiline':\n        multiline = not multiline\n        if multiline:\n            console.print(\n                'Enabling multiline mode. [dim]Press [Meta+Enter] or [Esc] followed by [Enter] to accept input.[/dim]'\n            )\n        else:\n            console.print('Disabling multiline mode.')\n        return None, multiline\n    elif ident_prompt == '/exit':\n        console.print('[dim]Exiting…[/dim]')\n        return 0, multiline\n    elif ident_prompt == '/cp':\n        try:\n            parts = messages[-1].parts\n        except IndexError:\n            console.print('[dim]No output available to copy.[/dim]')\n        else:\n            text_to_copy = ''.join(part.content for part in parts if isinstance(part, TextPart))\n            text_to_copy = text_to_copy.strip()\n            if text_to_copy:\n                pyperclip.copy(text_to_copy)\n                console.print('[dim]Copied last output to clipboard.[/dim]')\n            else:\n                console.print('[dim]No text content to copy.[/dim]')\n    else:\n        console.print(f'[red]Unknown command[/red] [magenta]`{ident_prompt}`[/magenta]')\n    return None, multiline\n\n\n=== pydantic_ai_slim/pydantic_ai/__init__.py ===\nfrom importlib.metadata import version as _metadata_version\n\nfrom .agent import (\n    Agent,\n    CallToolsNode,\n    EndStrategy,\n    InstrumentationSettings,\n    ModelRequestNode,\n    UserPromptNode,\n    capture_run_messages,\n)\nfrom .builtin_tools import CodeExecutionTool, UrlContextTool, WebSearchTool, WebSearchUserLocation\nfrom .exceptions import (\n    AgentRunError,\n    ApprovalRequired,\n    CallDeferred,\n    FallbackExceptionGroup,\n    ModelHTTPError,\n    ModelRetry,\n    UnexpectedModelBehavior,\n    UsageLimitExceeded,\n    UserError,\n)\nfrom .format_prompt import format_as_xml\nfrom .messages import AudioUrl, BinaryContent, DocumentUrl, ImageUrl, VideoUrl\nfrom .output import NativeOutput, PromptedOutput, StructuredDict, TextOutput, ToolOutput\nfrom .settings import ModelSettings\nfrom .tools import DeferredToolRequests, DeferredToolResults, RunContext, Tool, ToolApproved, ToolDefinition, ToolDenied\nfrom .usage import RequestUsage, RunUsage, UsageLimits\n\n__all__ = (\n    '__version__',\n    # agent\n    'Agent',\n    'EndStrategy',\n    'CallToolsNode',\n    'ModelRequestNode',\n    'UserPromptNode',\n    'capture_run_messages',\n    'InstrumentationSettings',\n    # exceptions\n    'AgentRunError',\n    'CallDeferred',\n    'ApprovalRequired',\n    'ModelRetry',\n    'ModelHTTPError',\n    'FallbackExceptionGroup',\n    'UnexpectedModelBehavior',\n    'UsageLimitExceeded',\n    'UserError',\n    # messages\n    'ImageUrl',\n    'AudioUrl',\n    'VideoUrl',\n    'DocumentUrl',\n    'BinaryContent',\n    # tools\n    'Tool',\n    'ToolDefinition',\n    'RunContext',\n    'DeferredToolRequests',\n    'DeferredToolResults',\n    'ToolApproved',\n    'ToolDenied',\n    # builtin_tools\n    'WebSearchTool',\n    'WebSearchUserLocation',\n    'UrlContextTool',\n    'CodeExecutionTool',\n    # output\n    'ToolOutput',\n    'NativeOutput',\n    'PromptedOutput',\n    'TextOutput',\n    'StructuredDict',\n    # format_prompt\n    'format_as_xml',\n    # settings\n    'ModelSettings',\n    # usage\n    'RunUsage',\n    'RequestUsage',\n    'UsageLimits',\n)\n__version__ = _metadata_version('pydantic_ai_slim')\n\n\n=== pydantic_ai_slim/pydantic_ai/_function_schema.py ===\n\"\"\"Used to build pydantic validators and JSON schemas from functions.\n\nThis module has to use numerous internal Pydantic APIs and is therefore brittle to changes in Pydantic.\n\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nfrom collections.abc import Awaitable, Callable\nfrom dataclasses import dataclass, field\nfrom inspect import Parameter, signature\nfrom typing import TYPE_CHECKING, Any, Concatenate, cast, get_origin\n\nfrom pydantic import ConfigDict\nfrom pydantic._internal import _decorators, _generate_schema, _typing_extra\nfrom pydantic._internal._config import ConfigWrapper\nfrom pydantic.fields import FieldInfo\nfrom pydantic.json_schema import GenerateJsonSchema\nfrom pydantic.plugin._schema_validator import create_schema_validator\nfrom pydantic_core import SchemaValidator, core_schema\nfrom typing_extensions import ParamSpec, TypeIs, TypeVar\n\nfrom ._griffe import doc_descriptions\nfrom ._run_context import RunContext\nfrom ._utils import check_object_json_schema, is_async_callable, is_model_like, run_in_executor\n\nif TYPE_CHECKING:\n    from .tools import DocstringFormat, ObjectJsonSchema\n\n\n__all__ = ('function_schema',)\n\n\n@dataclass(kw_only=True)\nclass FunctionSchema:\n    \"\"\"Internal information about a function schema.\"\"\"\n\n    function: Callable[..., Any]\n    description: str | None\n    validator: SchemaValidator\n    json_schema: ObjectJsonSchema\n    # if not None, the function takes a single by that name (besides potentially `info`)\n    takes_ctx: bool\n    is_async: bool\n    single_arg_name: str | None = None\n    positional_fields: list[str] = field(default_factory=list)\n    var_positional_field: str | None = None\n\n    async def call(self, args_dict: dict[str, Any], ctx: RunContext[Any]) -> Any:\n        args, kwargs = self._call_args(args_dict, ctx)\n        if self.is_async:\n            function = cast(Callable[[Any], Awaitable[str]], self.function)\n            return await function(*args, **kwargs)\n        else:\n            function = cast(Callable[[Any], str], self.function)\n            return await run_in_executor(function, *args, **kwargs)\n\n    def _call_args(\n        self,\n        args_dict: dict[str, Any],\n        ctx: RunContext[Any],\n    ) -> tuple[list[Any], dict[str, Any]]:\n        if self.single_arg_name:\n            args_dict = {self.single_arg_name: args_dict}\n\n        args = [ctx] if self.takes_ctx else []\n        for positional_field in self.positional_fields:\n            args.append(args_dict.pop(positional_field))  # pragma: no cover\n        if self.var_positional_field:\n            args.extend(args_dict.pop(self.var_positional_field))\n\n        return args, args_dict\n\n\ndef function_schema(  # noqa: C901\n    function: Callable[..., Any],\n    schema_generator: type[GenerateJsonSchema],\n    takes_ctx: bool | None = None,\n    docstring_format: DocstringFormat = 'auto',\n    require_parameter_descriptions: bool = False,\n) -> FunctionSchema:\n    \"\"\"Build a Pydantic validator and JSON schema from a tool function.\n\n    Args:\n        function: The function to build a validator and JSON schema for.\n        takes_ctx: Whether the function takes a `RunContext` first argument.\n        docstring_format: The docstring format to use.\n        require_parameter_descriptions: Whether to require descriptions for all tool function parameters.\n        schema_generator: The JSON schema generator class to use.\n\n    Returns:\n        A `FunctionSchema` instance.\n    \"\"\"\n    if takes_ctx is None:\n        takes_ctx = _takes_ctx(function)\n\n    config = ConfigDict(title=function.__name__, use_attribute_docstrings=True)\n    config_wrapper = ConfigWrapper(config)\n    gen_schema = _generate_schema.GenerateSchema(config_wrapper)\n    errors: list[str] = []\n\n    try:\n        sig = signature(function)\n    except ValueError as e:\n        errors.append(str(e))\n        sig = signature(lambda: None)\n\n    type_hints = _typing_extra.get_function_type_hints(function)\n\n    var_kwargs_schema: core_schema.CoreSchema | None = None\n    fields: dict[str, core_schema.TypedDictField] = {}\n    positional_fields: list[str] = []\n    var_positional_field: str | None = None\n    decorators = _decorators.DecoratorInfos()\n\n    description, field_descriptions = doc_descriptions(function, sig, docstring_format=docstring_format)\n\n    if require_parameter_descriptions:\n        if takes_ctx:\n            parameters_without_ctx = set(\n                name for name in sig.parameters if not _is_call_ctx(sig.parameters[name].annotation)\n            )\n            missing_params = parameters_without_ctx - set(field_descriptions)\n        else:\n            missing_params = set(sig.parameters) - set(field_descriptions)\n\n        if missing_params:\n            errors.append(f'Missing parameter descriptions for {\", \".join(missing_params)}')\n\n    for index, (name, p) in enumerate(sig.parameters.items()):\n        if p.annotation is sig.empty:\n            if takes_ctx and index == 0:\n                # should be the `context` argument, skip\n                continue\n            # TODO warn?\n            annotation = Any\n        else:\n            annotation = type_hints[name]\n\n            if index == 0 and takes_ctx:\n                if not _is_call_ctx(annotation):\n                    errors.append('First parameter of tools that take context must be annotated with RunContext[...]')\n                continue\n            elif not takes_ctx and _is_call_ctx(annotation):\n                errors.append('RunContext annotations can only be used with tools that take context')\n                continue\n            elif index != 0 and _is_call_ctx(annotation):\n                errors.append('RunContext annotations can only be used as the first argument')\n                continue\n\n        field_name = p.name\n        if p.kind == Parameter.VAR_KEYWORD:\n            var_kwargs_schema = gen_schema.generate_schema(annotation)\n        else:\n            if p.kind == Parameter.VAR_POSITIONAL:\n                annotation = list[annotation]\n\n            required = p.default is Parameter.empty\n            # FieldInfo.from_annotated_attribute expects a type, `annotation` is Any\n            annotation = cast(type[Any], annotation)\n            if required:\n                field_info = FieldInfo.from_annotation(annotation)\n            else:\n                field_info = FieldInfo.from_annotated_attribute(annotation, p.default)\n            if field_info.description is None:\n                field_info.description = field_descriptions.get(field_name)\n\n            fields[field_name] = td_schema = gen_schema._generate_td_field_schema(  # pyright: ignore[reportPrivateUsage]\n                field_name,\n                field_info,\n                decorators,\n                required=required,\n            )\n            # noinspection PyTypeChecker\n            td_schema.setdefault('metadata', {})['is_model_like'] = is_model_like(annotation)\n\n            if p.kind == Parameter.POSITIONAL_ONLY:\n                positional_fields.append(field_name)\n            elif p.kind == Parameter.VAR_POSITIONAL:\n                var_positional_field = field_name\n\n    if errors:\n        from .exceptions import UserError\n\n        error_details = '\\n  '.join(errors)\n        raise UserError(f'Error generating schema for {function.__qualname__}:\\n  {error_details}')\n\n    core_config = config_wrapper.core_config(None)\n    # noinspection PyTypedDict\n    core_config['extra_fields_behavior'] = 'allow' if var_kwargs_schema else 'forbid'\n\n    schema, single_arg_name = _build_schema(fields, var_kwargs_schema, gen_schema, core_config)\n    schema = gen_schema.clean_schema(schema)\n    # noinspection PyUnresolvedReferences\n    schema_validator = create_schema_validator(\n        schema,\n        function,\n        function.__module__,\n        function.__qualname__,\n        'validate_call',\n        core_config,\n        config_wrapper.plugin_settings,\n    )\n    # PluggableSchemaValidator is api compatible with SchemaValidator\n    schema_validator = cast(SchemaValidator, schema_validator)\n    json_schema = schema_generator().generate(schema)\n\n    # workaround for https://github.com/pydantic/pydantic/issues/10785\n    # if we build a custom TypedDict schema (matches when `single_arg_name is None`), we manually set\n    # `additionalProperties` in the JSON Schema\n    if single_arg_name is not None and not description:\n        # if the tool description is not set, and we have a single parameter, take the description from that\n        # and set it on the tool\n        description = json_schema.pop('description', None)\n\n    return FunctionSchema(\n        description=description,\n        validator=schema_validator,\n        json_schema=check_object_json_schema(json_schema),\n        single_arg_name=single_arg_name,\n        positional_fields=positional_fields,\n        var_positional_field=var_positional_field,\n        takes_ctx=takes_ctx,\n        is_async=is_async_callable(function),\n        function=function,\n    )\n\n\nP = ParamSpec('P')\nR = TypeVar('R')\n\n\nWithCtx = Callable[Concatenate[RunContext[Any], P], R]\nWithoutCtx = Callable[P, R]\nTargetFunc = WithCtx[P, R] | WithoutCtx[P, R]\n\n\ndef _takes_ctx(function: TargetFunc[P, R]) -> TypeIs[WithCtx[P, R]]:\n    \"\"\"Check if a function takes a `RunContext` first argument.\n\n    Args:\n        function: The function to check.\n\n    Returns:\n        `True` if the function takes a `RunContext` as first argument, `False` otherwise.\n    \"\"\"\n    try:\n        sig = signature(function)\n    except ValueError:  # pragma: no cover\n        return False  # pragma: no cover\n    try:\n        first_param_name = next(iter(sig.parameters.keys()))\n    except StopIteration:\n        return False\n    else:\n        type_hints = _typing_extra.get_function_type_hints(function)\n        annotation = type_hints.get(first_param_name)\n        if annotation is None:\n            return False  # pragma: no cover\n        return True is not sig.empty and _is_call_ctx(annotation)\n\n\ndef _build_schema(\n    fields: dict[str, core_schema.TypedDictField],\n    var_kwargs_schema: core_schema.CoreSchema | None,\n    gen_schema: _generate_schema.GenerateSchema,\n    core_config: core_schema.CoreConfig,\n) -> tuple[core_schema.CoreSchema, str | None]:\n    \"\"\"Generate a typed dict schema for function parameters.\n\n    Args:\n        fields: The fields to generate a typed dict schema for.\n        var_kwargs_schema: The variable keyword arguments schema.\n        gen_schema: The `GenerateSchema` instance.\n        core_config: The core configuration.\n\n    Returns:\n        tuple of (generated core schema, single arg name).\n    \"\"\"\n    if len(fields) == 1 and var_kwargs_schema is None:\n        name = next(iter(fields))\n        td_field = fields[name]\n        if td_field['metadata']['is_model_like']:  # type: ignore\n            return td_field['schema'], name\n\n    td_schema = core_schema.typed_dict_schema(\n        fields,\n        config=core_config,\n        extras_schema=gen_schema.generate_schema(var_kwargs_schema) if var_kwargs_schema else None,\n    )\n    return td_schema, None\n\n\ndef _is_call_ctx(annotation: Any) -> bool:\n    \"\"\"Return whether the annotation is the `RunContext` class, parameterized or not.\"\"\"\n    return annotation is RunContext or get_origin(annotation) is RunContext\n\n\n=== pydantic_ai_slim/pydantic_ai/ag_ui.py ===\n\"\"\"Provides an AG-UI protocol adapter for the Pydantic AI agent.\n\nThis package provides seamless integration between pydantic-ai agents and ag-ui\nfor building interactive AI applications with streaming event-based communication.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport uuid\nfrom collections.abc import AsyncIterator, Awaitable, Callable, Iterable, Mapping, Sequence\nfrom dataclasses import Field, dataclass, field, replace\nfrom http import HTTPStatus\nfrom typing import (\n    Any,\n    ClassVar,\n    Final,\n    Generic,\n    Protocol,\n    TypeAlias,\n    TypeVar,\n    runtime_checkable,\n)\n\nfrom pydantic import BaseModel, ValidationError\n\nfrom . import _utils\nfrom ._agent_graph import CallToolsNode, ModelRequestNode\nfrom .agent import AbstractAgent, AgentRun, AgentRunResult\nfrom .exceptions import UserError\nfrom .messages import (\n    BaseToolCallPart,\n    BuiltinToolCallPart,\n    BuiltinToolReturnPart,\n    FunctionToolResultEvent,\n    ModelMessage,\n    ModelRequest,\n    ModelRequestPart,\n    ModelResponse,\n    ModelResponsePart,\n    ModelResponseStreamEvent,\n    PartDeltaEvent,\n    PartStartEvent,\n    SystemPromptPart,\n    TextPart,\n    TextPartDelta,\n    ThinkingPart,\n    ThinkingPartDelta,\n    ToolCallPart,\n    ToolCallPartDelta,\n    ToolReturnPart,\n    UserPromptPart,\n)\nfrom .models import KnownModelName, Model\nfrom .output import OutputDataT, OutputSpec\nfrom .settings import ModelSettings\nfrom .tools import AgentDepsT, DeferredToolRequests, ToolDefinition\nfrom .toolsets import AbstractToolset\nfrom .toolsets.external import ExternalToolset\nfrom .usage import RunUsage, UsageLimits\n\ntry:\n    from ag_ui.core import (\n        AssistantMessage,\n        BaseEvent,\n        DeveloperMessage,\n        EventType,\n        Message,\n        RunAgentInput,\n        RunErrorEvent,\n        RunFinishedEvent,\n        RunStartedEvent,\n        State,\n        SystemMessage,\n        TextMessageContentEvent,\n        TextMessageEndEvent,\n        TextMessageStartEvent,\n        ThinkingEndEvent,\n        ThinkingStartEvent,\n        ThinkingTextMessageContentEvent,\n        ThinkingTextMessageEndEvent,\n        ThinkingTextMessageStartEvent,\n        Tool as AGUITool,\n        ToolCallArgsEvent,\n        ToolCallEndEvent,\n        ToolCallResultEvent,\n        ToolCallStartEvent,\n        ToolMessage,\n        UserMessage,\n    )\n    from ag_ui.encoder import EventEncoder\nexcept ImportError as e:  # pragma: no cover\n    raise ImportError(\n        'Please install the `ag-ui-protocol` package to use `Agent.to_ag_ui()` method, '\n        'you can use the `ag-ui` optional group — `pip install \"pydantic-ai-slim[ag-ui]\"`'\n    ) from e\n\ntry:\n    from starlette.applications import Starlette\n    from starlette.middleware import Middleware\n    from starlette.requests import Request\n    from starlette.responses import Response, StreamingResponse\n    from starlette.routing import BaseRoute\n    from starlette.types import ExceptionHandler, Lifespan\nexcept ImportError as e:  # pragma: no cover\n    raise ImportError(\n        'Please install the `starlette` package to use `Agent.to_ag_ui()` method, '\n        'you can use the `ag-ui` optional group — `pip install \"pydantic-ai-slim[ag-ui]\"`'\n    ) from e\n\n\n__all__ = [\n    'SSE_CONTENT_TYPE',\n    'StateDeps',\n    'StateHandler',\n    'AGUIApp',\n    'OnCompleteFunc',\n    'handle_ag_ui_request',\n    'run_ag_ui',\n]\n\nSSE_CONTENT_TYPE: Final[str] = 'text/event-stream'\n\"\"\"Content type header value for Server-Sent Events (SSE).\"\"\"\n\nOnCompleteFunc: TypeAlias = Callable[[AgentRunResult[Any]], None] | Callable[[AgentRunResult[Any]], Awaitable[None]]\n\"\"\"Callback function type that receives the `AgentRunResult` of the completed run. Can be sync or async.\"\"\"\n\n_BUILTIN_TOOL_CALL_ID_PREFIX: Final[str] = 'pyd_ai_builtin'\n\n\nclass AGUIApp(Generic[AgentDepsT, OutputDataT], Starlette):\n    \"\"\"ASGI application for running Pydantic AI agents with AG-UI protocol support.\"\"\"\n\n    def __init__(\n        self,\n        agent: AbstractAgent[AgentDepsT, OutputDataT],\n        *,\n        # Agent.iter parameters.\n        output_type: OutputSpec[Any] | None = None,\n        model: Model | KnownModelName | str | None = None,\n        deps: AgentDepsT = None,\n        model_settings: ModelSettings | None = None,\n        usage_limits: UsageLimits | None = None,\n        usage: RunUsage | None = None,\n        infer_name: bool = True,\n        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n        # Starlette parameters.\n        debug: bool = False,\n        routes: Sequence[BaseRoute] | None = None,\n        middleware: Sequence[Middleware] | None = None,\n        exception_handlers: Mapping[Any, ExceptionHandler] | None = None,\n        on_startup: Sequence[Callable[[], Any]] | None = None,\n        on_shutdown: Sequence[Callable[[], Any]] | None = None,\n        lifespan: Lifespan[AGUIApp[AgentDepsT, OutputDataT]] | None = None,\n    ) -> None:\n        \"\"\"An ASGI application that handles every AG-UI request by running the agent.\n\n        Note that the `deps` will be the same for each request, with the exception of the AG-UI state that's\n        injected into the `state` field of a `deps` object that implements the [`StateHandler`][pydantic_ai.ag_ui.StateHandler] protocol.\n        To provide different `deps` for each request (e.g. based on the authenticated user),\n        use [`pydantic_ai.ag_ui.run_ag_ui`][pydantic_ai.ag_ui.run_ag_ui] or\n        [`pydantic_ai.ag_ui.handle_ag_ui_request`][pydantic_ai.ag_ui.handle_ag_ui_request] instead.\n\n        Args:\n            agent: The agent to run.\n\n            output_type: Custom output type to use for this run, `output_type` may only be used if the agent has\n                no output validators since output validators would expect an argument that matches the agent's\n                output type.\n            model: Optional model to use for this run, required if `model` was not set when creating the agent.\n            deps: Optional dependencies to use for this run.\n            model_settings: Optional settings to use for this model's request.\n            usage_limits: Optional limits on model request count or token usage.\n            usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n            infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n            toolsets: Optional additional toolsets for this run.\n\n            debug: Boolean indicating if debug tracebacks should be returned on errors.\n            routes: A list of routes to serve incoming HTTP and WebSocket requests.\n            middleware: A list of middleware to run for every request. A starlette application will always\n                automatically include two middleware classes. `ServerErrorMiddleware` is added as the very\n                outermost middleware, to handle any uncaught errors occurring anywhere in the entire stack.\n                `ExceptionMiddleware` is added as the very innermost middleware, to deal with handled\n                exception cases occurring in the routing or endpoints.\n            exception_handlers: A mapping of either integer status codes, or exception class types onto\n                callables which handle the exceptions. Exception handler callables should be of the form\n                `handler(request, exc) -> response` and may be either standard functions, or async functions.\n            on_startup: A list of callables to run on application startup. Startup handler callables do not\n                take any arguments, and may be either standard functions, or async functions.\n            on_shutdown: A list of callables to run on application shutdown. Shutdown handler callables do\n                not take any arguments, and may be either standard functions, or async functions.\n            lifespan: A lifespan context function, which can be used to perform startup and shutdown tasks.\n                This is a newer style that replaces the `on_startup` and `on_shutdown` handlers. Use one or\n                the other, not both.\n        \"\"\"\n        super().__init__(\n            debug=debug,\n            routes=routes,\n            middleware=middleware,\n            exception_handlers=exception_handlers,\n            on_startup=on_startup,\n            on_shutdown=on_shutdown,\n            lifespan=lifespan,\n        )\n\n        async def endpoint(request: Request) -> Response:\n            \"\"\"Endpoint to run the agent with the provided input data.\"\"\"\n            return await handle_ag_ui_request(\n                agent,\n                request,\n                output_type=output_type,\n                model=model,\n                deps=deps,\n                model_settings=model_settings,\n                usage_limits=usage_limits,\n                usage=usage,\n                infer_name=infer_name,\n                toolsets=toolsets,\n            )\n\n        self.router.add_route('/', endpoint, methods=['POST'], name='run_agent')\n\n\nasync def handle_ag_ui_request(\n    agent: AbstractAgent[AgentDepsT, Any],\n    request: Request,\n    *,\n    output_type: OutputSpec[Any] | None = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n    on_complete: OnCompleteFunc | None = None,\n) -> Response:\n    \"\"\"Handle an AG-UI request by running the agent and returning a streaming response.\n\n    Args:\n        agent: The agent to run.\n        request: The Starlette request (e.g. from FastAPI) containing the AG-UI run input.\n\n        output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n            output validators since output validators would expect an argument that matches the agent's output type.\n        model: Optional model to use for this run, required if `model` was not set when creating the agent.\n        deps: Optional dependencies to use for this run.\n        model_settings: Optional settings to use for this model's request.\n        usage_limits: Optional limits on model request count or token usage.\n        usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n        infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n        toolsets: Optional additional toolsets for this run.\n        on_complete: Optional callback function called when the agent run completes successfully.\n            The callback receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can access `all_messages()` and other result data.\n\n    Returns:\n        A streaming Starlette response with AG-UI protocol events.\n    \"\"\"\n    accept = request.headers.get('accept', SSE_CONTENT_TYPE)\n    try:\n        input_data = RunAgentInput.model_validate(await request.json())\n    except ValidationError as e:  # pragma: no cover\n        return Response(\n            content=json.dumps(e.json()),\n            media_type='application/json',\n            status_code=HTTPStatus.UNPROCESSABLE_ENTITY,\n        )\n\n    return StreamingResponse(\n        run_ag_ui(\n            agent,\n            input_data,\n            accept,\n            output_type=output_type,\n            model=model,\n            deps=deps,\n            model_settings=model_settings,\n            usage_limits=usage_limits,\n            usage=usage,\n            infer_name=infer_name,\n            toolsets=toolsets,\n            on_complete=on_complete,\n        ),\n        media_type=accept,\n    )\n\n\nasync def run_ag_ui(\n    agent: AbstractAgent[AgentDepsT, Any],\n    run_input: RunAgentInput,\n    accept: str = SSE_CONTENT_TYPE,\n    *,\n    output_type: OutputSpec[Any] | None = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,\n    on_complete: OnCompleteFunc | None = None,\n) -> AsyncIterator[str]:\n    \"\"\"Run the agent with the AG-UI run input and stream AG-UI protocol events.\n\n    Args:\n        agent: The agent to run.\n        run_input: The AG-UI run input containing thread_id, run_id, messages, etc.\n        accept: The accept header value for the run.\n\n        output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n            output validators since output validators would expect an argument that matches the agent's output type.\n        model: Optional model to use for this run, required if `model` was not set when creating the agent.\n        deps: Optional dependencies to use for this run.\n        model_settings: Optional settings to use for this model's request.\n        usage_limits: Optional limits on model request count or token usage.\n        usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n        infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n        toolsets: Optional additional toolsets for this run.\n        on_complete: Optional callback function called when the agent run completes successfully.\n            The callback receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can access `all_messages()` and other result data.\n\n    Yields:\n        Streaming event chunks encoded as strings according to the accept header value.\n    \"\"\"\n    encoder = EventEncoder(accept=accept)\n    if run_input.tools:\n        # AG-UI tools can't be prefixed as that would result in a mismatch between the tool names in the\n        # Pydantic AI events and actual AG-UI tool names, preventing the tool from being called. If any\n        # conflicts arise, the AG-UI tool should be renamed or a `PrefixedToolset` used for local toolsets.\n        toolset = _AGUIFrontendToolset[AgentDepsT](run_input.tools)\n        toolsets = [*toolsets, toolset] if toolsets else [toolset]\n\n    try:\n        yield encoder.encode(\n            RunStartedEvent(\n                thread_id=run_input.thread_id,\n                run_id=run_input.run_id,\n            ),\n        )\n\n        if not run_input.messages:\n            raise _NoMessagesError\n\n        raw_state: dict[str, Any] = run_input.state or {}\n        if isinstance(deps, StateHandler):\n            if isinstance(deps.state, BaseModel):\n                try:\n                    state = type(deps.state).model_validate(raw_state)\n                except ValidationError as e:  # pragma: no cover\n                    raise _InvalidStateError from e\n            else:\n                state = raw_state\n\n            deps = replace(deps, state=state)\n        elif raw_state:\n            raise UserError(\n                f'AG-UI state is provided but `deps` of type `{type(deps).__name__}` does not implement the `StateHandler` protocol: it needs to be a dataclass with a non-optional `state` field.'\n            )\n        else:\n            # `deps` not being a `StateHandler` is OK if there is no state.\n            pass\n\n        messages = _messages_from_ag_ui(run_input.messages)\n\n        async with agent.iter(\n            user_prompt=None,\n            output_type=[output_type or agent.output_type, DeferredToolRequests],\n            message_history=messages,\n            model=model,\n            deps=deps,\n            model_settings=model_settings,\n            usage_limits=usage_limits,\n            usage=usage,\n            infer_name=infer_name,\n            toolsets=toolsets,\n        ) as run:\n            async for event in _agent_stream(run):\n                yield encoder.encode(event)\n\n        if on_complete is not None and run.result is not None:\n            if _utils.is_async_callable(on_complete):\n                await on_complete(run.result)\n            else:\n                await _utils.run_in_executor(on_complete, run.result)\n    except _RunError as e:\n        yield encoder.encode(\n            RunErrorEvent(message=e.message, code=e.code),\n        )\n    except Exception as e:\n        yield encoder.encode(\n            RunErrorEvent(message=str(e)),\n        )\n        raise e\n    else:\n        yield encoder.encode(\n            RunFinishedEvent(\n                thread_id=run_input.thread_id,\n                run_id=run_input.run_id,\n            ),\n        )\n\n\nasync def _agent_stream(run: AgentRun[AgentDepsT, Any]) -> AsyncIterator[BaseEvent]:\n    \"\"\"Run the agent streaming responses using AG-UI protocol events.\n\n    Args:\n        run: The agent run to process.\n\n    Yields:\n        AG-UI Server-Sent Events (SSE).\n    \"\"\"\n    async for node in run:\n        stream_ctx = _RequestStreamContext()\n        if isinstance(node, ModelRequestNode):\n            async with node.stream(run.ctx) as request_stream:\n                async for agent_event in request_stream:\n                    async for msg in _handle_model_request_event(stream_ctx, agent_event):\n                        yield msg\n\n                if stream_ctx.part_end:  # pragma: no branch\n                    yield stream_ctx.part_end\n                    stream_ctx.part_end = None\n                if stream_ctx.thinking:\n                    yield ThinkingEndEvent(\n                        type=EventType.THINKING_END,\n                    )\n                    stream_ctx.thinking = False\n        elif isinstance(node, CallToolsNode):\n            async with node.stream(run.ctx) as handle_stream:\n                async for event in handle_stream:\n                    if isinstance(event, FunctionToolResultEvent):\n                        async for msg in _handle_tool_result_event(stream_ctx, event):\n                            yield msg\n\n\nasync def _handle_model_request_event(  # noqa: C901\n    stream_ctx: _RequestStreamContext,\n    agent_event: ModelResponseStreamEvent,\n) -> AsyncIterator[BaseEvent]:\n    \"\"\"Handle an agent event and yield AG-UI protocol events.\n\n    Args:\n        stream_ctx: The request stream context to manage state.\n        agent_event: The agent event to process.\n\n    Yields:\n        AG-UI Server-Sent Events (SSE) based on the agent event.\n    \"\"\"\n    if isinstance(agent_event, PartStartEvent):\n        if stream_ctx.part_end:\n            # End the previous part.\n            yield stream_ctx.part_end\n            stream_ctx.part_end = None\n\n        part = agent_event.part\n        if isinstance(part, ThinkingPart):  # pragma: no branch\n            if not stream_ctx.thinking:\n                yield ThinkingStartEvent(\n                    type=EventType.THINKING_START,\n                )\n                stream_ctx.thinking = True\n\n            if part.content:\n                yield ThinkingTextMessageStartEvent(\n                    type=EventType.THINKING_TEXT_MESSAGE_START,\n                )\n                yield ThinkingTextMessageContentEvent(\n                    type=EventType.THINKING_TEXT_MESSAGE_CONTENT,\n                    delta=part.content,\n                )\n                stream_ctx.part_end = ThinkingTextMessageEndEvent(\n                    type=EventType.THINKING_TEXT_MESSAGE_END,\n                )\n        else:\n            if stream_ctx.thinking:\n                yield ThinkingEndEvent(\n                    type=EventType.THINKING_END,\n                )\n                stream_ctx.thinking = False\n\n            if isinstance(part, TextPart):\n                message_id = stream_ctx.new_message_id()\n                yield TextMessageStartEvent(\n                    message_id=message_id,\n                )\n                if part.content:  # pragma: no branch\n                    yield TextMessageContentEvent(\n                        message_id=message_id,\n                        delta=part.content,\n                    )\n                stream_ctx.part_end = TextMessageEndEvent(\n                    message_id=message_id,\n                )\n            elif isinstance(part, BaseToolCallPart):\n                tool_call_id = part.tool_call_id\n                if isinstance(part, BuiltinToolCallPart):\n                    builtin_tool_call_id = '|'.join(\n                        [_BUILTIN_TOOL_CALL_ID_PREFIX, part.provider_name or '', tool_call_id]\n                    )\n                    stream_ctx.builtin_tool_call_ids[tool_call_id] = builtin_tool_call_id\n                    tool_call_id = builtin_tool_call_id\n\n                message_id = stream_ctx.message_id or stream_ctx.new_message_id()\n                yield ToolCallStartEvent(\n                    tool_call_id=tool_call_id,\n                    tool_call_name=part.tool_name,\n                    parent_message_id=message_id,\n                )\n                if part.args:\n                    yield ToolCallArgsEvent(\n                        tool_call_id=tool_call_id,\n                        delta=part.args_as_json_str(),\n                    )\n                stream_ctx.part_end = ToolCallEndEvent(\n                    tool_call_id=tool_call_id,\n                )\n            elif isinstance(part, BuiltinToolReturnPart):  # pragma: no branch\n                tool_call_id = stream_ctx.builtin_tool_call_ids[part.tool_call_id]\n                yield ToolCallResultEvent(\n                    message_id=stream_ctx.new_message_id(),\n                    type=EventType.TOOL_CALL_RESULT,\n                    role='tool',\n                    tool_call_id=tool_call_id,\n                    content=part.model_response_str(),\n                )\n\n    elif isinstance(agent_event, PartDeltaEvent):\n        delta = agent_event.delta\n        if isinstance(delta, TextPartDelta):\n            if delta.content_delta:  # pragma: no branch\n                yield TextMessageContentEvent(\n                    message_id=stream_ctx.message_id,\n                    delta=delta.content_delta,\n                )\n        elif isinstance(delta, ToolCallPartDelta):  # pragma: no branch\n            tool_call_id = delta.tool_call_id\n            assert tool_call_id, '`ToolCallPartDelta.tool_call_id` must be set'\n            if tool_call_id in stream_ctx.builtin_tool_call_ids:\n                tool_call_id = stream_ctx.builtin_tool_call_ids[tool_call_id]\n            yield ToolCallArgsEvent(\n                tool_call_id=tool_call_id,\n                delta=delta.args_delta if isinstance(delta.args_delta, str) else json.dumps(delta.args_delta),\n            )\n        elif isinstance(delta, ThinkingPartDelta):  # pragma: no branch\n            if delta.content_delta:  # pragma: no branch\n                if not isinstance(stream_ctx.part_end, ThinkingTextMessageEndEvent):\n                    yield ThinkingTextMessageStartEvent(\n                        type=EventType.THINKING_TEXT_MESSAGE_START,\n                    )\n                    stream_ctx.part_end = ThinkingTextMessageEndEvent(\n                        type=EventType.THINKING_TEXT_MESSAGE_END,\n                    )\n\n                yield ThinkingTextMessageContentEvent(\n                    type=EventType.THINKING_TEXT_MESSAGE_CONTENT,\n                    delta=delta.content_delta,\n                )\n\n\nasync def _handle_tool_result_event(\n    stream_ctx: _RequestStreamContext,\n    event: FunctionToolResultEvent,\n) -> AsyncIterator[BaseEvent]:\n    \"\"\"Convert a tool call result to AG-UI events.\n\n    Args:\n        stream_ctx: The request stream context to manage state.\n        event: The tool call result event to process.\n\n    Yields:\n        AG-UI Server-Sent Events (SSE).\n    \"\"\"\n    result = event.result\n    if not isinstance(result, ToolReturnPart):\n        return\n\n    yield ToolCallResultEvent(\n        message_id=stream_ctx.new_message_id(),\n        type=EventType.TOOL_CALL_RESULT,\n        role='tool',\n        tool_call_id=result.tool_call_id,\n        content=result.model_response_str(),\n    )\n\n    # Now check for AG-UI events returned by the tool calls.\n    possible_event = result.metadata or result.content\n    if isinstance(possible_event, BaseEvent):\n        yield possible_event\n    elif isinstance(possible_event, str | bytes):  # pragma: no branch\n        # Avoid iterable check for strings and bytes.\n        pass\n    elif isinstance(possible_event, Iterable):  # pragma: no branch\n        for item in possible_event:  # type: ignore[reportUnknownMemberType]\n            if isinstance(item, BaseEvent):  # pragma: no branch\n                yield item\n\n\ndef _messages_from_ag_ui(messages: list[Message]) -> list[ModelMessage]:\n    \"\"\"Convert a AG-UI history to a Pydantic AI one.\"\"\"\n    result: list[ModelMessage] = []\n    tool_calls: dict[str, str] = {}  # Tool call ID to tool name mapping.\n    request_parts: list[ModelRequestPart] | None = None\n    response_parts: list[ModelResponsePart] | None = None\n    for msg in messages:\n        if isinstance(msg, UserMessage | SystemMessage | DeveloperMessage) or (\n            isinstance(msg, ToolMessage) and not msg.tool_call_id.startswith(_BUILTIN_TOOL_CALL_ID_PREFIX)\n        ):\n            if request_parts is None:\n                request_parts = []\n                result.append(ModelRequest(parts=request_parts))\n                response_parts = None\n\n            if isinstance(msg, UserMessage):\n                request_parts.append(UserPromptPart(content=msg.content))\n            elif isinstance(msg, SystemMessage | DeveloperMessage):\n                request_parts.append(SystemPromptPart(content=msg.content))\n            else:\n                tool_call_id = msg.tool_call_id\n                tool_name = tool_calls.get(tool_call_id)\n                if tool_name is None:  # pragma: no cover\n                    raise _ToolCallNotFoundError(tool_call_id=tool_call_id)\n\n                request_parts.append(\n                    ToolReturnPart(\n                        tool_name=tool_name,\n                        content=msg.content,\n                        tool_call_id=tool_call_id,\n                    )\n                )\n\n        elif isinstance(msg, AssistantMessage) or (  # pragma: no branch\n            isinstance(msg, ToolMessage) and msg.tool_call_id.startswith(_BUILTIN_TOOL_CALL_ID_PREFIX)\n        ):\n            if response_parts is None:\n                response_parts = []\n                result.append(ModelResponse(parts=response_parts))\n                request_parts = None\n\n            if isinstance(msg, AssistantMessage):\n                if msg.content:\n                    response_parts.append(TextPart(content=msg.content))\n\n                if msg.tool_calls:\n                    for tool_call in msg.tool_calls:\n                        tool_call_id = tool_call.id\n                        tool_name = tool_call.function.name\n                        tool_calls[tool_call_id] = tool_name\n\n                        if tool_call_id.startswith(_BUILTIN_TOOL_CALL_ID_PREFIX):\n                            _, provider_name, tool_call_id = tool_call_id.split('|', 2)\n                            response_parts.append(\n                                BuiltinToolCallPart(\n                                    tool_name=tool_name,\n                                    args=tool_call.function.arguments,\n                                    tool_call_id=tool_call_id,\n                                    provider_name=provider_name,\n                                )\n                            )\n                        else:\n                            response_parts.append(\n                                ToolCallPart(\n                                    tool_name=tool_name,\n                                    tool_call_id=tool_call_id,\n                                    args=tool_call.function.arguments,\n                                )\n                            )\n            else:\n                tool_call_id = msg.tool_call_id\n                tool_name = tool_calls.get(tool_call_id)\n                if tool_name is None:  # pragma: no cover\n                    raise _ToolCallNotFoundError(tool_call_id=tool_call_id)\n                _, provider_name, tool_call_id = tool_call_id.split('|', 2)\n\n                response_parts.append(\n                    BuiltinToolReturnPart(\n                        tool_name=tool_name,\n                        content=msg.content,\n                        tool_call_id=tool_call_id,\n                        provider_name=provider_name,\n                    )\n                )\n\n    return result\n\n\n@runtime_checkable\nclass StateHandler(Protocol):\n    \"\"\"Protocol for state handlers in agent runs. Requires the class to be a dataclass with a `state` field.\"\"\"\n\n    # Has to be a dataclass so we can use `replace` to update the state.\n    # From https://github.com/python/typeshed/blob/9ab7fde0a0cd24ed7a72837fcb21093b811b80d8/stdlib/_typeshed/__init__.pyi#L352\n    __dataclass_fields__: ClassVar[dict[str, Field[Any]]]\n\n    @property\n    def state(self) -> State:\n        \"\"\"Get the current state of the agent run.\"\"\"\n        ...\n\n    @state.setter\n    def state(self, state: State) -> None:\n        \"\"\"Set the state of the agent run.\n\n        This method is called to update the state of the agent run with the\n        provided state.\n\n        Args:\n            state: The run state.\n\n        Raises:\n            InvalidStateError: If `state` does not match the expected model.\n        \"\"\"\n        ...\n\n\nStateT = TypeVar('StateT', bound=BaseModel)\n\"\"\"Type variable for the state type, which must be a subclass of `BaseModel`.\"\"\"\n\n\n@dataclass\nclass StateDeps(Generic[StateT]):\n    \"\"\"Provides AG-UI state management.\n\n    This class is used to manage the state of an agent run. It allows setting\n    the state of the agent run with a specific type of state model, which must\n    be a subclass of `BaseModel`.\n\n    The state is set using the `state` setter by the `Adapter` when the run starts.\n\n    Implements the `StateHandler` protocol.\n    \"\"\"\n\n    state: StateT\n\n\n@dataclass(repr=False)\nclass _RequestStreamContext:\n    \"\"\"Data class to hold request stream context.\"\"\"\n\n    message_id: str = ''\n    part_end: BaseEvent | None = None\n    thinking: bool = False\n    builtin_tool_call_ids: dict[str, str] = field(default_factory=dict)\n\n    def new_message_id(self) -> str:\n        \"\"\"Generate a new message ID for the request stream.\n\n        Assigns a new UUID to the `message_id` and returns it.\n\n        Returns:\n            A new message ID.\n        \"\"\"\n        self.message_id = str(uuid.uuid4())\n        return self.message_id\n\n\n@dataclass\nclass _RunError(Exception):\n    \"\"\"Exception raised for errors during agent runs.\"\"\"\n\n    message: str\n    code: str\n\n    def __str__(self) -> str:  # pragma: no cover\n        return self.message\n\n\n@dataclass\nclass _NoMessagesError(_RunError):\n    \"\"\"Exception raised when no messages are found in the input.\"\"\"\n\n    message: str = 'no messages found in the input'\n    code: str = 'no_messages'\n\n\n@dataclass\nclass _InvalidStateError(_RunError, ValidationError):\n    \"\"\"Exception raised when an invalid state is provided.\"\"\"\n\n    message: str = 'invalid state provided'\n    code: str = 'invalid_state'\n\n\nclass _ToolCallNotFoundError(_RunError, ValueError):\n    \"\"\"Exception raised when an tool result is present without a matching call.\"\"\"\n\n    def __init__(self, tool_call_id: str) -> None:\n        \"\"\"Initialize the exception with the tool call ID.\"\"\"\n        super().__init__(  # pragma: no cover\n            message=f'Tool call with ID {tool_call_id} not found in the history.',\n            code='tool_call_not_found',\n        )\n\n\nclass _AGUIFrontendToolset(ExternalToolset[AgentDepsT]):\n    def __init__(self, tools: list[AGUITool]):\n        super().__init__(\n            [\n                ToolDefinition(\n                    name=tool.name,\n                    description=tool.description,\n                    parameters_json_schema=tool.parameters,\n                )\n                for tool in tools\n            ]\n        )\n\n    @property\n    def label(self) -> str:\n        return 'the AG-UI frontend tools'  # pragma: no cover\n\n\n=== pydantic_ai_slim/pydantic_ai/result.py ===\nfrom __future__ import annotations as _annotations\n\nfrom collections.abc import AsyncIterator, Awaitable, Callable, Iterable\nfrom copy import deepcopy\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import Generic, cast, overload\n\nfrom pydantic import ValidationError\nfrom typing_extensions import TypeVar, deprecated\n\nfrom . import _utils, exceptions, messages as _messages, models\nfrom ._output import (\n    OutputDataT_inv,\n    OutputSchema,\n    OutputValidator,\n    OutputValidatorFunc,\n    PlainTextOutputSchema,\n    TextOutputSchema,\n    ToolOutputSchema,\n)\nfrom ._run_context import AgentDepsT, RunContext\nfrom ._tool_manager import ToolManager\nfrom .messages import ModelResponseStreamEvent\nfrom .output import (\n    DeferredToolRequests,\n    OutputDataT,\n    ToolOutput,\n)\nfrom .run import AgentRunResult\nfrom .usage import RunUsage, UsageLimits\n\n__all__ = (\n    'OutputDataT',\n    'OutputDataT_inv',\n    'ToolOutput',\n    'OutputValidatorFunc',\n)\n\n\nT = TypeVar('T')\n\"\"\"An invariant TypeVar.\"\"\"\n\n\n@dataclass(kw_only=True)\nclass AgentStream(Generic[AgentDepsT, OutputDataT]):\n    _raw_stream_response: models.StreamedResponse\n    _output_schema: OutputSchema[OutputDataT]\n    _model_request_parameters: models.ModelRequestParameters\n    _output_validators: list[OutputValidator[AgentDepsT, OutputDataT]]\n    _run_ctx: RunContext[AgentDepsT]\n    _usage_limits: UsageLimits | None\n    _tool_manager: ToolManager[AgentDepsT]\n\n    _agent_stream_iterator: AsyncIterator[ModelResponseStreamEvent] | None = field(default=None, init=False)\n    _initial_run_ctx_usage: RunUsage = field(init=False)\n\n    def __post_init__(self):\n        self._initial_run_ctx_usage = deepcopy(self._run_ctx.usage)\n\n    async def stream_output(self, *, debounce_by: float | None = 0.1) -> AsyncIterator[OutputDataT]:\n        \"\"\"Asynchronously stream the (validated) agent outputs.\"\"\"\n        async for response in self.stream_responses(debounce_by=debounce_by):\n            if self._raw_stream_response.final_result_event is not None:\n                try:\n                    yield await self.validate_response_output(response, allow_partial=True)\n                except ValidationError:\n                    pass\n        if self._raw_stream_response.final_result_event is not None:  # pragma: no branch\n            yield await self.validate_response_output(self._raw_stream_response.get())\n\n    async def stream_responses(self, *, debounce_by: float | None = 0.1) -> AsyncIterator[_messages.ModelResponse]:\n        \"\"\"Asynchronously stream the (unvalidated) model responses for the agent.\"\"\"\n        # if the message currently has any parts with content, yield before streaming\n        msg = self._raw_stream_response.get()\n        for part in msg.parts:\n            if part.has_content():\n                yield msg\n                break\n\n        async with _utils.group_by_temporal(self, debounce_by) as group_iter:\n            async for _items in group_iter:\n                yield self._raw_stream_response.get()  # current state of the response\n\n    async def stream_text(self, *, delta: bool = False, debounce_by: float | None = 0.1) -> AsyncIterator[str]:\n        \"\"\"Stream the text result as an async iterable.\n\n        !!! note\n            Result validators will NOT be called on the text result if `delta=True`.\n\n        Args:\n            delta: if `True`, yield each chunk of text as it is received, if `False` (default), yield the full text\n                up to the current point.\n            debounce_by: by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing.\n                Debouncing is particularly important for long structured responses to reduce the overhead of\n                performing validation as each token is received.\n        \"\"\"\n        if not isinstance(self._output_schema, PlainTextOutputSchema):\n            raise exceptions.UserError('stream_text() can only be used with text responses')\n\n        if delta:\n            async for text in self._stream_response_text(delta=True, debounce_by=debounce_by):\n                yield text\n        else:\n            async for text in self._stream_response_text(delta=False, debounce_by=debounce_by):\n                for validator in self._output_validators:\n                    text = await validator.validate(text, self._run_ctx)  # pragma: no cover\n                yield text\n\n    def get(self) -> _messages.ModelResponse:\n        \"\"\"Get the current state of the response.\"\"\"\n        return self._raw_stream_response.get()\n\n    def usage(self) -> RunUsage:\n        \"\"\"Return the usage of the whole run.\n\n        !!! note\n            This won't return the full usage until the stream is finished.\n        \"\"\"\n        return self._initial_run_ctx_usage + self._raw_stream_response.usage()\n\n    def timestamp(self) -> datetime:\n        \"\"\"Get the timestamp of the response.\"\"\"\n        return self._raw_stream_response.timestamp\n\n    async def get_output(self) -> OutputDataT:\n        \"\"\"Stream the whole response, validate the output and return it.\"\"\"\n        async for _ in self:\n            pass\n\n        return await self.validate_response_output(self._raw_stream_response.get())\n\n    async def validate_response_output(\n        self, message: _messages.ModelResponse, *, allow_partial: bool = False\n    ) -> OutputDataT:\n        \"\"\"Validate a structured result message.\"\"\"\n        final_result_event = self._raw_stream_response.final_result_event\n        if final_result_event is None:\n            raise exceptions.UnexpectedModelBehavior('Invalid response, unable to find output')  # pragma: no cover\n\n        output_tool_name = final_result_event.tool_name\n\n        if isinstance(self._output_schema, ToolOutputSchema) and output_tool_name is not None:\n            tool_call = next(\n                (\n                    part\n                    for part in message.parts\n                    if isinstance(part, _messages.ToolCallPart) and part.tool_name == output_tool_name\n                ),\n                None,\n            )\n            if tool_call is None:\n                raise exceptions.UnexpectedModelBehavior(  # pragma: no cover\n                    f'Invalid response, unable to find tool call for {output_tool_name!r}'\n                )\n            return await self._tool_manager.handle_call(\n                tool_call, allow_partial=allow_partial, wrap_validation_errors=False\n            )\n        elif deferred_tool_requests := _get_deferred_tool_requests(message.parts, self._tool_manager):\n            if not self._output_schema.allows_deferred_tools:\n                raise exceptions.UserError(\n                    'A deferred tool call was present, but `DeferredToolRequests` is not among output types. To resolve this, add `DeferredToolRequests` to the list of output types for this agent.'\n                )\n            return cast(OutputDataT, deferred_tool_requests)\n        elif isinstance(self._output_schema, TextOutputSchema):\n            text = ''\n            for part in message.parts:\n                if isinstance(part, _messages.TextPart):\n                    text += part.content\n                elif isinstance(part, _messages.BuiltinToolCallPart):\n                    # Text parts before a built-in tool call are essentially thoughts,\n                    # not part of the final result output, so we reset the accumulated text\n                    text = ''\n\n            result_data = await self._output_schema.process(\n                text, self._run_ctx, allow_partial=allow_partial, wrap_validation_errors=False\n            )\n            for validator in self._output_validators:\n                result_data = await validator.validate(result_data, self._run_ctx)\n            return result_data\n        else:\n            raise exceptions.UnexpectedModelBehavior(  # pragma: no cover\n                'Invalid response, unable to process text output'\n            )\n\n    async def _stream_response_text(\n        self, *, delta: bool = False, debounce_by: float | None = 0.1\n    ) -> AsyncIterator[str]:\n        \"\"\"Stream the response as an async iterable of text.\"\"\"\n\n        # Define a \"merged\" version of the iterator that will yield items that have already been retrieved\n        # and items that we receive while streaming. We define a dedicated async iterator for this so we can\n        # pass the combined stream to the group_by_temporal function within `_stream_text_deltas` below.\n        async def _stream_text_deltas_ungrouped() -> AsyncIterator[tuple[str, int]]:\n            # yields tuples of (text_content, part_index)\n            # we don't currently make use of the part_index, but in principle this may be useful\n            # so we retain it here for now to make possible future refactors simpler\n            msg = self._raw_stream_response.get()\n            for i, part in enumerate(msg.parts):\n                if isinstance(part, _messages.TextPart) and part.content:\n                    yield part.content, i\n\n            last_text_index: int | None = None\n            async for event in self._raw_stream_response:\n                if (\n                    isinstance(event, _messages.PartStartEvent)\n                    and isinstance(event.part, _messages.TextPart)\n                    and event.part.content\n                ):\n                    last_text_index = event.index\n                    yield event.part.content, event.index\n                elif (\n                    isinstance(event, _messages.PartDeltaEvent)\n                    and isinstance(event.delta, _messages.TextPartDelta)\n                    and event.delta.content_delta\n                ):\n                    last_text_index = event.index\n                    yield event.delta.content_delta, event.index\n                elif (\n                    isinstance(event, _messages.PartStartEvent)\n                    and isinstance(event.part, _messages.BuiltinToolCallPart)\n                    and last_text_index is not None\n                ):\n                    # Text parts that are interrupted by a built-in tool call should not be joined together directly\n                    yield '\\n\\n', event.index\n                    last_text_index = None\n\n        async def _stream_text_deltas() -> AsyncIterator[str]:\n            async with _utils.group_by_temporal(_stream_text_deltas_ungrouped(), debounce_by) as group_iter:\n                async for items in group_iter:\n                    # Note: we are currently just dropping the part index on the group here\n                    yield ''.join([content for content, _ in items])\n\n        if delta:\n            async for text in _stream_text_deltas():\n                yield text\n        else:\n            # a quick benchmark shows it's faster to build up a string with concat when we're\n            # yielding at each step\n            deltas: list[str] = []\n            async for text in _stream_text_deltas():\n                deltas.append(text)\n                yield ''.join(deltas)\n\n    def __aiter__(self) -> AsyncIterator[ModelResponseStreamEvent]:\n        \"\"\"Stream [`ModelResponseStreamEvent`][pydantic_ai.messages.ModelResponseStreamEvent]s.\"\"\"\n        if self._agent_stream_iterator is None:\n            self._agent_stream_iterator = _get_usage_checking_stream_response(\n                self._raw_stream_response, self._usage_limits, self.usage\n            )\n\n        return self._agent_stream_iterator\n\n\n@dataclass(init=False)\nclass StreamedRunResult(Generic[AgentDepsT, OutputDataT]):\n    \"\"\"Result of a streamed run that returns structured data via a tool call.\"\"\"\n\n    _all_messages: list[_messages.ModelMessage]\n    _new_message_index: int\n\n    _stream_response: AgentStream[AgentDepsT, OutputDataT] | None = None\n    _on_complete: Callable[[], Awaitable[None]] | None = None\n\n    _run_result: AgentRunResult[OutputDataT] | None = None\n\n    is_complete: bool = field(default=False, init=False)\n    \"\"\"Whether the stream has all been received.\n\n    This is set to `True` when one of\n    [`stream_output`][pydantic_ai.result.StreamedRunResult.stream_output],\n    [`stream_text`][pydantic_ai.result.StreamedRunResult.stream_text],\n    [`stream_responses`][pydantic_ai.result.StreamedRunResult.stream_responses] or\n    [`get_output`][pydantic_ai.result.StreamedRunResult.get_output] completes.\n    \"\"\"\n\n    @overload\n    def __init__(\n        self,\n        all_messages: list[_messages.ModelMessage],\n        new_message_index: int,\n        stream_response: AgentStream[AgentDepsT, OutputDataT] | None,\n        on_complete: Callable[[], Awaitable[None]] | None,\n    ) -> None: ...\n\n    @overload\n    def __init__(\n        self,\n        all_messages: list[_messages.ModelMessage],\n        new_message_index: int,\n        *,\n        run_result: AgentRunResult[OutputDataT],\n    ) -> None: ...\n\n    def __init__(\n        self,\n        all_messages: list[_messages.ModelMessage],\n        new_message_index: int,\n        stream_response: AgentStream[AgentDepsT, OutputDataT] | None = None,\n        on_complete: Callable[[], Awaitable[None]] | None = None,\n        run_result: AgentRunResult[OutputDataT] | None = None,\n    ) -> None:\n        self._all_messages = all_messages\n        self._new_message_index = new_message_index\n\n        self._stream_response = stream_response\n        self._on_complete = on_complete\n        self._run_result = run_result\n\n    def all_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:\n        \"\"\"Return the history of _messages.\n\n        Args:\n            output_tool_return_content: The return content of the tool call to set in the last message.\n                This provides a convenient way to modify the content of the output tool call if you want to continue\n                the conversation and want to set the response to the output tool call. If `None`, the last message will\n                not be modified.\n\n        Returns:\n            List of messages.\n        \"\"\"\n        # this is a method to be consistent with the other methods\n        if output_tool_return_content is not None:\n            raise NotImplementedError('Setting output tool return content is not supported for this result type.')\n        return self._all_messages\n\n    def all_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:  # pragma: no cover\n        \"\"\"Return all messages from [`all_messages`][pydantic_ai.result.StreamedRunResult.all_messages] as JSON bytes.\n\n        Args:\n            output_tool_return_content: The return content of the tool call to set in the last message.\n                This provides a convenient way to modify the content of the output tool call if you want to continue\n                the conversation and want to set the response to the output tool call. If `None`, the last message will\n                not be modified.\n\n        Returns:\n            JSON bytes representing the messages.\n        \"\"\"\n        return _messages.ModelMessagesTypeAdapter.dump_json(\n            self.all_messages(output_tool_return_content=output_tool_return_content)\n        )\n\n    def new_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:\n        \"\"\"Return new messages associated with this run.\n\n        Messages from older runs are excluded.\n\n        Args:\n            output_tool_return_content: The return content of the tool call to set in the last message.\n                This provides a convenient way to modify the content of the output tool call if you want to continue\n                the conversation and want to set the response to the output tool call. If `None`, the last message will\n                not be modified.\n\n        Returns:\n            List of new messages.\n        \"\"\"\n        return self.all_messages(output_tool_return_content=output_tool_return_content)[self._new_message_index :]\n\n    def new_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:  # pragma: no cover\n        \"\"\"Return new messages from [`new_messages`][pydantic_ai.result.StreamedRunResult.new_messages] as JSON bytes.\n\n        Args:\n            output_tool_return_content: The return content of the tool call to set in the last message.\n                This provides a convenient way to modify the content of the output tool call if you want to continue\n                the conversation and want to set the response to the output tool call. If `None`, the last message will\n                not be modified.\n\n        Returns:\n            JSON bytes representing the new messages.\n        \"\"\"\n        return _messages.ModelMessagesTypeAdapter.dump_json(\n            self.new_messages(output_tool_return_content=output_tool_return_content)\n        )\n\n    @deprecated('`StreamedRunResult.stream` is deprecated, use `stream_output` instead.')\n    async def stream(self, *, debounce_by: float | None = 0.1) -> AsyncIterator[OutputDataT]:\n        async for output in self.stream_output(debounce_by=debounce_by):\n            yield output\n\n    async def stream_output(self, *, debounce_by: float | None = 0.1) -> AsyncIterator[OutputDataT]:\n        \"\"\"Stream the output as an async iterable.\n\n        The pydantic validator for structured data will be called in\n        [partial mode](https://docs.pydantic.dev/dev/concepts/experimental/#partial-validation)\n        on each iteration.\n\n        Args:\n            debounce_by: by how much (if at all) to debounce/group the output chunks by. `None` means no debouncing.\n                Debouncing is particularly important for long structured outputs to reduce the overhead of\n                performing validation as each token is received.\n\n        Returns:\n            An async iterable of the response data.\n        \"\"\"\n        if self._run_result is not None:\n            yield self._run_result.output\n            await self._marked_completed()\n        elif self._stream_response is not None:\n            async for output in self._stream_response.stream_output(debounce_by=debounce_by):\n                yield output\n            await self._marked_completed(self._stream_response.get())\n        else:\n            raise ValueError('No stream response or run result provided')  # pragma: no cover\n\n    async def stream_text(self, *, delta: bool = False, debounce_by: float | None = 0.1) -> AsyncIterator[str]:\n        \"\"\"Stream the text result as an async iterable.\n\n        !!! note\n            Result validators will NOT be called on the text result if `delta=True`.\n\n        Args:\n            delta: if `True`, yield each chunk of text as it is received, if `False` (default), yield the full text\n                up to the current point.\n            debounce_by: by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing.\n                Debouncing is particularly important for long structured responses to reduce the overhead of\n                performing validation as each token is received.\n        \"\"\"\n        if self._run_result is not None:  # pragma: no cover\n            # We can't really get here, as `_run_result` is only set in `run_stream` when `CallToolsNode` produces `DeferredToolRequests` output\n            # as a result of a tool function raising `CallDeferred` or `ApprovalRequired`.\n            # That'll change if we ever support something like `raise EndRun(output: OutputT)` where `OutputT` could be `str`.\n            if not isinstance(self._run_result.output, str):\n                raise exceptions.UserError('stream_text() can only be used with text responses')\n            yield self._run_result.output\n            await self._marked_completed()\n        elif self._stream_response is not None:\n            async for text in self._stream_response.stream_text(delta=delta, debounce_by=debounce_by):\n                yield text\n            await self._marked_completed(self._stream_response.get())\n        else:\n            raise ValueError('No stream response or run result provided')  # pragma: no cover\n\n    @deprecated('`StreamedRunResult.stream_structured` is deprecated, use `stream_responses` instead.')\n    async def stream_structured(\n        self, *, debounce_by: float | None = 0.1\n    ) -> AsyncIterator[tuple[_messages.ModelResponse, bool]]:\n        async for msg, last in self.stream_responses(debounce_by=debounce_by):\n            yield msg, last\n\n    async def stream_responses(\n        self, *, debounce_by: float | None = 0.1\n    ) -> AsyncIterator[tuple[_messages.ModelResponse, bool]]:\n        \"\"\"Stream the response as an async iterable of Structured LLM Messages.\n\n        Args:\n            debounce_by: by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing.\n                Debouncing is particularly important for long structured responses to reduce the overhead of\n                performing validation as each token is received.\n\n        Returns:\n            An async iterable of the structured response message and whether that is the last message.\n        \"\"\"\n        if self._run_result is not None:\n            model_response = cast(_messages.ModelResponse, self.all_messages()[-1])\n            yield model_response, True\n            await self._marked_completed()\n        elif self._stream_response is not None:\n            # if the message currently has any parts with content, yield before streaming\n            async for msg in self._stream_response.stream_responses(debounce_by=debounce_by):\n                yield msg, False\n\n            msg = self._stream_response.get()\n            yield msg, True\n\n            await self._marked_completed(msg)\n        else:\n            raise ValueError('No stream response or run result provided')  # pragma: no cover\n\n    async def get_output(self) -> OutputDataT:\n        \"\"\"Stream the whole response, validate and return it.\"\"\"\n        if self._run_result is not None:\n            output = self._run_result.output\n            await self._marked_completed()\n            return output\n        elif self._stream_response is not None:\n            output = await self._stream_response.get_output()\n            await self._marked_completed(self._stream_response.get())\n            return output\n        else:\n            raise ValueError('No stream response or run result provided')  # pragma: no cover\n\n    def usage(self) -> RunUsage:\n        \"\"\"Return the usage of the whole run.\n\n        !!! note\n            This won't return the full usage until the stream is finished.\n        \"\"\"\n        if self._run_result is not None:\n            return self._run_result.usage()\n        elif self._stream_response is not None:\n            return self._stream_response.usage()\n        else:\n            raise ValueError('No stream response or run result provided')  # pragma: no cover\n\n    def timestamp(self) -> datetime:\n        \"\"\"Get the timestamp of the response.\"\"\"\n        if self._run_result is not None:\n            return self._run_result.timestamp()\n        elif self._stream_response is not None:\n            return self._stream_response.timestamp()\n        else:\n            raise ValueError('No stream response or run result provided')  # pragma: no cover\n\n    @deprecated('`validate_structured_output` is deprecated, use `validate_response_output` instead.')\n    async def validate_structured_output(\n        self, message: _messages.ModelResponse, *, allow_partial: bool = False\n    ) -> OutputDataT:\n        return await self.validate_response_output(message, allow_partial=allow_partial)\n\n    async def validate_response_output(\n        self, message: _messages.ModelResponse, *, allow_partial: bool = False\n    ) -> OutputDataT:\n        \"\"\"Validate a structured result message.\"\"\"\n        if self._run_result is not None:\n            return self._run_result.output\n        elif self._stream_response is not None:\n            return await self._stream_response.validate_response_output(message, allow_partial=allow_partial)\n        else:\n            raise ValueError('No stream response or run result provided')  # pragma: no cover\n\n    async def _marked_completed(self, message: _messages.ModelResponse | None = None) -> None:\n        self.is_complete = True\n        if message is not None:\n            self._all_messages.append(message)\n        if self._on_complete is not None:\n            await self._on_complete()\n\n\n@dataclass(repr=False)\nclass FinalResult(Generic[OutputDataT]):\n    \"\"\"Marker class storing the final output of an agent run and associated metadata.\"\"\"\n\n    output: OutputDataT\n    \"\"\"The final result data.\"\"\"\n\n    tool_name: str | None = None\n    \"\"\"Name of the final output tool; `None` if the output came from unstructured text content.\"\"\"\n\n    tool_call_id: str | None = None\n    \"\"\"ID of the tool call that produced the final output; `None` if the output came from unstructured text content.\"\"\"\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n\ndef _get_usage_checking_stream_response(\n    stream_response: models.StreamedResponse,\n    limits: UsageLimits | None,\n    get_usage: Callable[[], RunUsage],\n) -> AsyncIterator[ModelResponseStreamEvent]:\n    if limits is not None and limits.has_token_limits():\n\n        async def _usage_checking_iterator():\n            async for item in stream_response:\n                limits.check_tokens(get_usage())\n                yield item\n\n        return _usage_checking_iterator()\n    else:\n        return aiter(stream_response)\n\n\ndef _get_deferred_tool_requests(\n    parts: Iterable[_messages.ModelResponsePart], tool_manager: ToolManager[AgentDepsT]\n) -> DeferredToolRequests | None:\n    \"\"\"Get the deferred tool requests from the model response parts.\"\"\"\n    approvals: list[_messages.ToolCallPart] = []\n    calls: list[_messages.ToolCallPart] = []\n\n    for part in parts:\n        if isinstance(part, _messages.ToolCallPart):\n            tool_def = tool_manager.get_tool_def(part.tool_name)\n            if tool_def is not None:  # pragma: no branch\n                if tool_def.kind == 'unapproved':\n                    approvals.append(part)\n                elif tool_def.kind == 'external':\n                    calls.append(part)\n\n    if not calls and not approvals:\n        return None\n\n    return DeferredToolRequests(calls=calls, approvals=approvals)\n\n\n=== pydantic_ai_slim/pydantic_ai/_run_context.py ===\nfrom __future__ import annotations as _annotations\n\nimport dataclasses\nfrom collections.abc import Sequence\nfrom dataclasses import field\nfrom typing import TYPE_CHECKING, Generic\n\nfrom opentelemetry.trace import NoOpTracer, Tracer\nfrom typing_extensions import TypeVar\n\nfrom . import _utils, messages as _messages\n\nif TYPE_CHECKING:\n    from .models import Model\n    from .result import RunUsage\n\nAgentDepsT = TypeVar('AgentDepsT', default=None, contravariant=True)\n\"\"\"Type variable for agent dependencies.\"\"\"\n\n\n@dataclasses.dataclass(repr=False, kw_only=True)\nclass RunContext(Generic[AgentDepsT]):\n    \"\"\"Information about the current call.\"\"\"\n\n    deps: AgentDepsT\n    \"\"\"Dependencies for the agent.\"\"\"\n    model: Model\n    \"\"\"The model used in this run.\"\"\"\n    usage: RunUsage\n    \"\"\"LLM usage associated with the run.\"\"\"\n    prompt: str | Sequence[_messages.UserContent] | None = None\n    \"\"\"The original user prompt passed to the run.\"\"\"\n    messages: list[_messages.ModelMessage] = field(default_factory=list)\n    \"\"\"Messages exchanged in the conversation so far.\"\"\"\n    tracer: Tracer = field(default_factory=NoOpTracer)\n    \"\"\"The tracer to use for tracing the run.\"\"\"\n    trace_include_content: bool = False\n    \"\"\"Whether to include the content of the messages in the trace.\"\"\"\n    retries: dict[str, int] = field(default_factory=dict)\n    \"\"\"Number of retries for each tool so far.\"\"\"\n    tool_call_id: str | None = None\n    \"\"\"The ID of the tool call.\"\"\"\n    tool_name: str | None = None\n    \"\"\"Name of the tool being called.\"\"\"\n    retry: int = 0\n    \"\"\"Number of retries of this tool so far.\"\"\"\n    max_retries: int = 0\n    \"\"\"The maximum number of retries of this tool.\"\"\"\n    run_step: int = 0\n    \"\"\"The current step in the run.\"\"\"\n    tool_call_approved: bool = False\n    \"\"\"Whether a tool call that required approval has now been approved.\"\"\"\n\n    @property\n    def last_attempt(self) -> bool:\n        \"\"\"Whether this is the last attempt at running this tool before an error is raised.\"\"\"\n        return self.retry == self.max_retries\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n\n=== pydantic_ai_slim/pydantic_ai/_system_prompt.py ===\nfrom __future__ import annotations as _annotations\n\nimport inspect\nfrom collections.abc import Awaitable, Callable\nfrom dataclasses import dataclass, field\nfrom typing import Any, Generic, cast\n\nfrom . import _utils\nfrom ._run_context import AgentDepsT, RunContext\nfrom .tools import SystemPromptFunc\n\n\n@dataclass\nclass SystemPromptRunner(Generic[AgentDepsT]):\n    function: SystemPromptFunc[AgentDepsT]\n    dynamic: bool = False\n    _takes_ctx: bool = field(init=False)\n    _is_async: bool = field(init=False)\n\n    def __post_init__(self):\n        self._takes_ctx = len(inspect.signature(self.function).parameters) > 0\n        self._is_async = _utils.is_async_callable(self.function)\n\n    async def run(self, run_context: RunContext[AgentDepsT]) -> str:\n        if self._takes_ctx:\n            args = (run_context,)\n        else:\n            args = ()\n\n        if self._is_async:\n            function = cast(Callable[[Any], Awaitable[str]], self.function)\n            return await function(*args)\n        else:\n            function = cast(Callable[[Any], str], self.function)\n            return await _utils.run_in_executor(function, *args)\n\n\n=== pydantic_ai_slim/pydantic_ai/mcp.py ===\nfrom __future__ import annotations\n\nimport base64\nimport functools\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom asyncio import Lock\nfrom collections.abc import AsyncIterator, Awaitable, Callable, Sequence\nfrom contextlib import AbstractAsyncContextManager, AsyncExitStack, asynccontextmanager\nfrom dataclasses import field, replace\nfrom datetime import timedelta\nfrom pathlib import Path\nfrom typing import Annotated, Any\n\nimport anyio\nimport httpx\nimport pydantic_core\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\nfrom pydantic import BaseModel, Discriminator, Field, Tag\nfrom pydantic_core import CoreSchema, core_schema\nfrom typing_extensions import Self, assert_never, deprecated\n\nfrom pydantic_ai.tools import RunContext, ToolDefinition\n\nfrom .direct import model_request\nfrom .toolsets.abstract import AbstractToolset, ToolsetTool\n\ntry:\n    from mcp import types as mcp_types\n    from mcp.client.session import ClientSession, ElicitationFnT, LoggingFnT\n    from mcp.client.sse import sse_client\n    from mcp.client.stdio import StdioServerParameters, stdio_client\n    from mcp.client.streamable_http import GetSessionIdCallback, streamablehttp_client\n    from mcp.shared.context import RequestContext\n    from mcp.shared.exceptions import McpError\n    from mcp.shared.message import SessionMessage\nexcept ImportError as _import_error:\n    raise ImportError(\n        'Please install the `mcp` package to use the MCP server, '\n        'you can use the `mcp` optional group — `pip install \"pydantic-ai-slim[mcp]\"`'\n    ) from _import_error\n\n# after mcp imports so any import error maps to this file, not _mcp.py\nfrom . import _mcp, _utils, exceptions, messages, models\n\n__all__ = 'MCPServer', 'MCPServerStdio', 'MCPServerHTTP', 'MCPServerSSE', 'MCPServerStreamableHTTP', 'load_mcp_servers'\n\nTOOL_SCHEMA_VALIDATOR = pydantic_core.SchemaValidator(\n    schema=pydantic_core.core_schema.dict_schema(\n        pydantic_core.core_schema.str_schema(), pydantic_core.core_schema.any_schema()\n    )\n)\n\n\nclass MCPServer(AbstractToolset[Any], ABC):\n    \"\"\"Base class for attaching agents to MCP servers.\n\n    See <https://modelcontextprotocol.io> for more information.\n    \"\"\"\n\n    tool_prefix: str | None\n    \"\"\"A prefix to add to all tools that are registered with the server.\n\n    If not empty, will include a trailing underscore(`_`).\n\n    e.g. if `tool_prefix='foo'`, then a tool named `bar` will be registered as `foo_bar`\n    \"\"\"\n\n    log_level: mcp_types.LoggingLevel | None\n    \"\"\"The log level to set when connecting to the server, if any.\n\n    See <https://modelcontextprotocol.io/specification/2025-03-26/server/utilities/logging#logging> for more details.\n\n    If `None`, no log level will be set.\n    \"\"\"\n\n    log_handler: LoggingFnT | None\n    \"\"\"A handler for logging messages from the server.\"\"\"\n\n    timeout: float\n    \"\"\"The timeout in seconds to wait for the client to initialize.\"\"\"\n\n    read_timeout: float\n    \"\"\"Maximum time in seconds to wait for new messages before timing out.\n\n    This timeout applies to the long-lived connection after it's established.\n    If no new messages are received within this time, the connection will be considered stale\n    and may be closed. Defaults to 5 minutes (300 seconds).\n    \"\"\"\n\n    process_tool_call: ProcessToolCallback | None\n    \"\"\"Hook to customize tool calling and optionally pass extra metadata.\"\"\"\n\n    allow_sampling: bool\n    \"\"\"Whether to allow MCP sampling through this client.\"\"\"\n\n    sampling_model: models.Model | None\n    \"\"\"The model to use for sampling.\"\"\"\n\n    max_retries: int\n    \"\"\"The maximum number of times to retry a tool call.\"\"\"\n\n    elicitation_callback: ElicitationFnT | None = None\n    \"\"\"Callback function to handle elicitation requests from the server.\"\"\"\n\n    _id: str | None\n\n    _enter_lock: Lock = field(compare=False)\n    _running_count: int\n    _exit_stack: AsyncExitStack | None\n\n    _client: ClientSession\n    _read_stream: MemoryObjectReceiveStream[SessionMessage | Exception]\n    _write_stream: MemoryObjectSendStream[SessionMessage]\n\n    def __init__(\n        self,\n        tool_prefix: str | None = None,\n        log_level: mcp_types.LoggingLevel | None = None,\n        log_handler: LoggingFnT | None = None,\n        timeout: float = 5,\n        read_timeout: float = 5 * 60,\n        process_tool_call: ProcessToolCallback | None = None,\n        allow_sampling: bool = True,\n        sampling_model: models.Model | None = None,\n        max_retries: int = 1,\n        elicitation_callback: ElicitationFnT | None = None,\n        *,\n        id: str | None = None,\n    ):\n        self.tool_prefix = tool_prefix\n        self.log_level = log_level\n        self.log_handler = log_handler\n        self.timeout = timeout\n        self.read_timeout = read_timeout\n        self.process_tool_call = process_tool_call\n        self.allow_sampling = allow_sampling\n        self.sampling_model = sampling_model\n        self.max_retries = max_retries\n        self.elicitation_callback = elicitation_callback\n\n        self._id = id or tool_prefix\n\n        self.__post_init__()\n\n    def __post_init__(self):\n        self._enter_lock = Lock()\n        self._running_count = 0\n        self._exit_stack = None\n\n    @abstractmethod\n    @asynccontextmanager\n    async def client_streams(\n        self,\n    ) -> AsyncIterator[\n        tuple[\n            MemoryObjectReceiveStream[SessionMessage | Exception],\n            MemoryObjectSendStream[SessionMessage],\n        ]\n    ]:\n        \"\"\"Create the streams for the MCP server.\"\"\"\n        raise NotImplementedError('MCP Server subclasses must implement this method.')\n        yield\n\n    @property\n    def id(self) -> str | None:\n        return self._id\n\n    @property\n    def label(self) -> str:\n        if self.id:\n            return super().label  # pragma: no cover\n        else:\n            return repr(self)\n\n    @property\n    def tool_name_conflict_hint(self) -> str:\n        return 'Set the `tool_prefix` attribute to avoid name conflicts.'\n\n    async def list_tools(self) -> list[mcp_types.Tool]:\n        \"\"\"Retrieve tools that are currently active on the server.\n\n        Note:\n        - We don't cache tools as they might change.\n        - We also don't subscribe to the server to avoid complexity.\n        \"\"\"\n        async with self:  # Ensure server is running\n            result = await self._client.list_tools()\n        return result.tools\n\n    async def direct_call_tool(\n        self,\n        name: str,\n        args: dict[str, Any],\n        metadata: dict[str, Any] | None = None,\n    ) -> ToolResult:\n        \"\"\"Call a tool on the server.\n\n        Args:\n            name: The name of the tool to call.\n            args: The arguments to pass to the tool.\n            metadata: Request-level metadata (optional)\n\n        Returns:\n            The result of the tool call.\n\n        Raises:\n            ModelRetry: If the tool call fails.\n        \"\"\"\n        async with self:  # Ensure server is running\n            try:\n                result = await self._client.send_request(\n                    mcp_types.ClientRequest(\n                        mcp_types.CallToolRequest(\n                            method='tools/call',\n                            params=mcp_types.CallToolRequestParams(\n                                name=name,\n                                arguments=args,\n                                _meta=mcp_types.RequestParams.Meta(**metadata) if metadata else None,\n                            ),\n                        )\n                    ),\n                    mcp_types.CallToolResult,\n                )\n            except McpError as e:\n                raise exceptions.ModelRetry(e.error.message)\n\n        content = [await self._map_tool_result_part(part) for part in result.content]\n\n        if result.isError:\n            text = '\\n'.join(str(part) for part in content)\n            raise exceptions.ModelRetry(text)\n        else:\n            return content[0] if len(content) == 1 else content\n\n    async def call_tool(\n        self,\n        name: str,\n        tool_args: dict[str, Any],\n        ctx: RunContext[Any],\n        tool: ToolsetTool[Any],\n    ) -> ToolResult:\n        if self.tool_prefix:\n            name = name.removeprefix(f'{self.tool_prefix}_')\n            ctx = replace(ctx, tool_name=name)\n\n        if self.process_tool_call is not None:\n            return await self.process_tool_call(ctx, self.direct_call_tool, name, tool_args)\n        else:\n            return await self.direct_call_tool(name, tool_args)\n\n    async def get_tools(self, ctx: RunContext[Any]) -> dict[str, ToolsetTool[Any]]:\n        return {\n            name: self.tool_for_tool_def(\n                ToolDefinition(\n                    name=name,\n                    description=mcp_tool.description,\n                    parameters_json_schema=mcp_tool.inputSchema,\n                    metadata={\n                        'meta': mcp_tool.meta,\n                        'annotations': mcp_tool.annotations.model_dump() if mcp_tool.annotations else None,\n                        'output_schema': mcp_tool.outputSchema or None,\n                    },\n                ),\n            )\n            for mcp_tool in await self.list_tools()\n            if (name := f'{self.tool_prefix}_{mcp_tool.name}' if self.tool_prefix else mcp_tool.name)\n        }\n\n    def tool_for_tool_def(self, tool_def: ToolDefinition) -> ToolsetTool[Any]:\n        return ToolsetTool(\n            toolset=self,\n            tool_def=tool_def,\n            max_retries=self.max_retries,\n            args_validator=TOOL_SCHEMA_VALIDATOR,\n        )\n\n    async def __aenter__(self) -> Self:\n        \"\"\"Enter the MCP server context.\n\n        This will initialize the connection to the server.\n        If this server is an [`MCPServerStdio`][pydantic_ai.mcp.MCPServerStdio], the server will first be started as a subprocess.\n\n        This is a no-op if the MCP server has already been entered.\n        \"\"\"\n        async with self._enter_lock:\n            if self._running_count == 0:\n                async with AsyncExitStack() as exit_stack:\n                    self._read_stream, self._write_stream = await exit_stack.enter_async_context(self.client_streams())\n                    client = ClientSession(\n                        read_stream=self._read_stream,\n                        write_stream=self._write_stream,\n                        sampling_callback=self._sampling_callback if self.allow_sampling else None,\n                        elicitation_callback=self.elicitation_callback,\n                        logging_callback=self.log_handler,\n                        read_timeout_seconds=timedelta(seconds=self.read_timeout),\n                    )\n                    self._client = await exit_stack.enter_async_context(client)\n\n                    with anyio.fail_after(self.timeout):\n                        await self._client.initialize()\n\n                        if log_level := self.log_level:\n                            await self._client.set_logging_level(log_level)\n\n                    self._exit_stack = exit_stack.pop_all()\n            self._running_count += 1\n        return self\n\n    async def __aexit__(self, *args: Any) -> bool | None:\n        if self._running_count == 0:\n            raise ValueError('MCPServer.__aexit__ called more times than __aenter__')\n        async with self._enter_lock:\n            self._running_count -= 1\n            if self._running_count == 0 and self._exit_stack is not None:\n                await self._exit_stack.aclose()\n                self._exit_stack = None\n\n    @property\n    def is_running(self) -> bool:\n        \"\"\"Check if the MCP server is running.\"\"\"\n        return bool(self._running_count)\n\n    async def _sampling_callback(\n        self, context: RequestContext[ClientSession, Any], params: mcp_types.CreateMessageRequestParams\n    ) -> mcp_types.CreateMessageResult | mcp_types.ErrorData:\n        \"\"\"MCP sampling callback.\"\"\"\n        if self.sampling_model is None:\n            raise ValueError('Sampling model is not set')  # pragma: no cover\n\n        pai_messages = _mcp.map_from_mcp_params(params)\n        model_settings = models.ModelSettings()\n        if max_tokens := params.maxTokens:  # pragma: no branch\n            model_settings['max_tokens'] = max_tokens\n        if temperature := params.temperature:  # pragma: no branch\n            model_settings['temperature'] = temperature\n        if stop_sequences := params.stopSequences:  # pragma: no branch\n            model_settings['stop_sequences'] = stop_sequences\n\n        model_response = await model_request(self.sampling_model, pai_messages, model_settings=model_settings)\n        return mcp_types.CreateMessageResult(\n            role='assistant',\n            content=_mcp.map_from_model_response(model_response),\n            model=self.sampling_model.model_name,\n        )\n\n    async def _map_tool_result_part(\n        self, part: mcp_types.ContentBlock\n    ) -> str | messages.BinaryContent | dict[str, Any] | list[Any]:\n        # See https://github.com/jlowin/fastmcp/blob/main/docs/servers/tools.mdx#return-values\n\n        if isinstance(part, mcp_types.TextContent):\n            text = part.text\n            if text.startswith(('[', '{')):\n                try:\n                    return pydantic_core.from_json(text)\n                except ValueError:\n                    pass\n            return text\n        elif isinstance(part, mcp_types.ImageContent):\n            return messages.BinaryContent(data=base64.b64decode(part.data), media_type=part.mimeType)\n        elif isinstance(part, mcp_types.AudioContent):\n            # NOTE: The FastMCP server doesn't support audio content.\n            # See <https://github.com/modelcontextprotocol/python-sdk/issues/952> for more details.\n            return messages.BinaryContent(\n                data=base64.b64decode(part.data), media_type=part.mimeType\n            )  # pragma: no cover\n        elif isinstance(part, mcp_types.EmbeddedResource):\n            resource = part.resource\n            return self._get_content(resource)\n        elif isinstance(part, mcp_types.ResourceLink):\n            resource_result: mcp_types.ReadResourceResult = await self._client.read_resource(part.uri)\n            return (\n                self._get_content(resource_result.contents[0])\n                if len(resource_result.contents) == 1\n                else [self._get_content(resource) for resource in resource_result.contents]\n            )\n        else:\n            assert_never(part)\n\n    def _get_content(\n        self, resource: mcp_types.TextResourceContents | mcp_types.BlobResourceContents\n    ) -> str | messages.BinaryContent:\n        if isinstance(resource, mcp_types.TextResourceContents):\n            return resource.text\n        elif isinstance(resource, mcp_types.BlobResourceContents):\n            return messages.BinaryContent(\n                data=base64.b64decode(resource.blob), media_type=resource.mimeType or 'application/octet-stream'\n            )\n        else:\n            assert_never(resource)\n\n\nclass MCPServerStdio(MCPServer):\n    \"\"\"Runs an MCP server in a subprocess and communicates with it over stdin/stdout.\n\n    This class implements the stdio transport from the MCP specification.\n    See <https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio> for more information.\n\n    !!! note\n        Using this class as an async context manager will start the server as a subprocess when entering the context,\n        and stop it when exiting the context.\n\n    Example:\n    ```python {py=\"3.10\"}\n    from pydantic_ai import Agent\n    from pydantic_ai.mcp import MCPServerStdio\n\n    server = MCPServerStdio(  # (1)!\n        'uv', args=['run', 'mcp-run-python', 'stdio'], timeout=10\n    )\n    agent = Agent('openai:gpt-4o', toolsets=[server])\n\n    async def main():\n        async with agent:  # (2)!\n            ...\n    ```\n\n    1. See [MCP Run Python](https://github.com/pydantic/mcp-run-python) for more information.\n    2. This will start the server as a subprocess and connect to it.\n    \"\"\"\n\n    command: str\n    \"\"\"The command to run.\"\"\"\n\n    args: Sequence[str]\n    \"\"\"The arguments to pass to the command.\"\"\"\n\n    env: dict[str, str] | None\n    \"\"\"The environment variables the CLI server will have access to.\n\n    By default the subprocess will not inherit any environment variables from the parent process.\n    If you want to inherit the environment variables from the parent process, use `env=os.environ`.\n    \"\"\"\n\n    cwd: str | Path | None\n    \"\"\"The working directory to use when spawning the process.\"\"\"\n\n    # last fields are re-defined from the parent class so they appear as fields\n    tool_prefix: str | None\n    log_level: mcp_types.LoggingLevel | None\n    log_handler: LoggingFnT | None\n    timeout: float\n    read_timeout: float\n    process_tool_call: ProcessToolCallback | None\n    allow_sampling: bool\n    sampling_model: models.Model | None\n    max_retries: int\n    elicitation_callback: ElicitationFnT | None = None\n\n    def __init__(\n        self,\n        command: str,\n        args: Sequence[str],\n        *,\n        env: dict[str, str] | None = None,\n        cwd: str | Path | None = None,\n        tool_prefix: str | None = None,\n        log_level: mcp_types.LoggingLevel | None = None,\n        log_handler: LoggingFnT | None = None,\n        timeout: float = 5,\n        read_timeout: float = 5 * 60,\n        process_tool_call: ProcessToolCallback | None = None,\n        allow_sampling: bool = True,\n        sampling_model: models.Model | None = None,\n        max_retries: int = 1,\n        elicitation_callback: ElicitationFnT | None = None,\n        id: str | None = None,\n    ):\n        \"\"\"Build a new MCP server.\n\n        Args:\n            command: The command to run.\n            args: The arguments to pass to the command.\n            env: The environment variables to set in the subprocess.\n            cwd: The working directory to use when spawning the process.\n            tool_prefix: A prefix to add to all tools that are registered with the server.\n            log_level: The log level to set when connecting to the server, if any.\n            log_handler: A handler for logging messages from the server.\n            timeout: The timeout in seconds to wait for the client to initialize.\n            read_timeout: Maximum time in seconds to wait for new messages before timing out.\n            process_tool_call: Hook to customize tool calling and optionally pass extra metadata.\n            allow_sampling: Whether to allow MCP sampling through this client.\n            sampling_model: The model to use for sampling.\n            max_retries: The maximum number of times to retry a tool call.\n            elicitation_callback: Callback function to handle elicitation requests from the server.\n            id: An optional unique ID for the MCP server. An MCP server needs to have an ID in order to be used in a durable execution environment like Temporal, in which case the ID will be used to identify the server's activities within the workflow.\n        \"\"\"\n        self.command = command\n        self.args = args\n        self.env = env\n        self.cwd = cwd\n\n        super().__init__(\n            tool_prefix,\n            log_level,\n            log_handler,\n            timeout,\n            read_timeout,\n            process_tool_call,\n            allow_sampling,\n            sampling_model,\n            max_retries,\n            elicitation_callback,\n            id=id,\n        )\n\n    @classmethod\n    def __get_pydantic_core_schema__(cls, _: Any, __: Any) -> CoreSchema:\n        return core_schema.no_info_after_validator_function(\n            lambda dct: MCPServerStdio(**dct),\n            core_schema.typed_dict_schema(\n                {\n                    'command': core_schema.typed_dict_field(core_schema.str_schema()),\n                    'args': core_schema.typed_dict_field(core_schema.list_schema(core_schema.str_schema())),\n                    'env': core_schema.typed_dict_field(\n                        core_schema.dict_schema(core_schema.str_schema(), core_schema.str_schema()),\n                        required=False,\n                    ),\n                }\n            ),\n        )\n\n    @asynccontextmanager\n    async def client_streams(\n        self,\n    ) -> AsyncIterator[\n        tuple[\n            MemoryObjectReceiveStream[SessionMessage | Exception],\n            MemoryObjectSendStream[SessionMessage],\n        ]\n    ]:\n        server = StdioServerParameters(command=self.command, args=list(self.args), env=self.env, cwd=self.cwd)\n        async with stdio_client(server=server) as (read_stream, write_stream):\n            yield read_stream, write_stream\n\n    def __repr__(self) -> str:\n        repr_args = [\n            f'command={self.command!r}',\n            f'args={self.args!r}',\n        ]\n        if self.id:\n            repr_args.append(f'id={self.id!r}')\n        return f'{self.__class__.__name__}({\", \".join(repr_args)})'\n\n    def __eq__(self, value: object, /) -> bool:\n        if not isinstance(value, MCPServerStdio):\n            return False  # pragma: no cover\n        return (\n            self.command == value.command\n            and self.args == value.args\n            and self.env == value.env\n            and self.cwd == value.cwd\n        )\n\n\nclass _MCPServerHTTP(MCPServer):\n    url: str\n    \"\"\"The URL of the endpoint on the MCP server.\"\"\"\n\n    headers: dict[str, Any] | None\n    \"\"\"Optional HTTP headers to be sent with each request to the endpoint.\n\n    These headers will be passed directly to the underlying `httpx.AsyncClient`.\n    Useful for authentication, custom headers, or other HTTP-specific configurations.\n\n    !!! note\n        You can either pass `headers` or `http_client`, but not both.\n\n        See [`MCPServerHTTP.http_client`][pydantic_ai.mcp.MCPServerHTTP.http_client] for more information.\n    \"\"\"\n\n    http_client: httpx.AsyncClient | None\n    \"\"\"An `httpx.AsyncClient` to use with the endpoint.\n\n    This client may be configured to use customized connection parameters like self-signed certificates.\n\n    !!! note\n        You can either pass `headers` or `http_client`, but not both.\n\n        If you want to use both, you can pass the headers to the `http_client` instead.\n\n        ```python {py=\"3.10\" test=\"skip\"}\n        import httpx\n\n        from pydantic_ai.mcp import MCPServerSSE\n\n        http_client = httpx.AsyncClient(headers={'Authorization': 'Bearer ...'})\n        server = MCPServerSSE('http://localhost:3001/sse', http_client=http_client)\n        ```\n    \"\"\"\n\n    # last fields are re-defined from the parent class so they appear as fields\n    tool_prefix: str | None\n    log_level: mcp_types.LoggingLevel | None\n    log_handler: LoggingFnT | None\n    timeout: float\n    read_timeout: float\n    process_tool_call: ProcessToolCallback | None\n    allow_sampling: bool\n    sampling_model: models.Model | None\n    max_retries: int\n    elicitation_callback: ElicitationFnT | None = None\n\n    def __init__(\n        self,\n        url: str,\n        *,\n        headers: dict[str, str] | None = None,\n        http_client: httpx.AsyncClient | None = None,\n        id: str | None = None,\n        tool_prefix: str | None = None,\n        log_level: mcp_types.LoggingLevel | None = None,\n        log_handler: LoggingFnT | None = None,\n        timeout: float = 5,\n        read_timeout: float | None = None,\n        process_tool_call: ProcessToolCallback | None = None,\n        allow_sampling: bool = True,\n        sampling_model: models.Model | None = None,\n        max_retries: int = 1,\n        elicitation_callback: ElicitationFnT | None = None,\n        **_deprecated_kwargs: Any,\n    ):\n        \"\"\"Build a new MCP server.\n\n        Args:\n            url: The URL of the endpoint on the MCP server.\n            headers: Optional HTTP headers to be sent with each request to the endpoint.\n            http_client: An `httpx.AsyncClient` to use with the endpoint.\n            id: An optional unique ID for the MCP server. An MCP server needs to have an ID in order to be used in a durable execution environment like Temporal, in which case the ID will be used to identify the server's activities within the workflow.\n            tool_prefix: A prefix to add to all tools that are registered with the server.\n            log_level: The log level to set when connecting to the server, if any.\n            log_handler: A handler for logging messages from the server.\n            timeout: The timeout in seconds to wait for the client to initialize.\n            read_timeout: Maximum time in seconds to wait for new messages before timing out.\n            process_tool_call: Hook to customize tool calling and optionally pass extra metadata.\n            allow_sampling: Whether to allow MCP sampling through this client.\n            sampling_model: The model to use for sampling.\n            max_retries: The maximum number of times to retry a tool call.\n            elicitation_callback: Callback function to handle elicitation requests from the server.\n        \"\"\"\n        if 'sse_read_timeout' in _deprecated_kwargs:\n            if read_timeout is not None:\n                raise TypeError(\"'read_timeout' and 'sse_read_timeout' cannot be set at the same time.\")\n\n            warnings.warn(\n                \"'sse_read_timeout' is deprecated, use 'read_timeout' instead.\", DeprecationWarning, stacklevel=2\n            )\n            read_timeout = _deprecated_kwargs.pop('sse_read_timeout')\n\n        _utils.validate_empty_kwargs(_deprecated_kwargs)\n\n        if read_timeout is None:\n            read_timeout = 5 * 60\n\n        self.url = url\n        self.headers = headers\n        self.http_client = http_client\n\n        super().__init__(\n            tool_prefix,\n            log_level,\n            log_handler,\n            timeout,\n            read_timeout,\n            process_tool_call,\n            allow_sampling,\n            sampling_model,\n            max_retries,\n            elicitation_callback,\n            id=id,\n        )\n\n    @property\n    @abstractmethod\n    def _transport_client(\n        self,\n    ) -> Callable[\n        ...,\n        AbstractAsyncContextManager[\n            tuple[\n                MemoryObjectReceiveStream[SessionMessage | Exception],\n                MemoryObjectSendStream[SessionMessage],\n                GetSessionIdCallback,\n            ],\n        ]\n        | AbstractAsyncContextManager[\n            tuple[\n                MemoryObjectReceiveStream[SessionMessage | Exception],\n                MemoryObjectSendStream[SessionMessage],\n            ]\n        ],\n    ]: ...\n\n    @asynccontextmanager\n    async def client_streams(\n        self,\n    ) -> AsyncIterator[\n        tuple[\n            MemoryObjectReceiveStream[SessionMessage | Exception],\n            MemoryObjectSendStream[SessionMessage],\n        ]\n    ]:  # pragma: no cover\n        if self.http_client and self.headers:\n            raise ValueError('`http_client` is mutually exclusive with `headers`.')\n\n        transport_client_partial = functools.partial(\n            self._transport_client,\n            url=self.url,\n            timeout=self.timeout,\n            sse_read_timeout=self.read_timeout,\n        )\n\n        if self.http_client is not None:\n\n            def httpx_client_factory(\n                headers: dict[str, str] | None = None,\n                timeout: httpx.Timeout | None = None,\n                auth: httpx.Auth | None = None,\n            ) -> httpx.AsyncClient:\n                assert self.http_client is not None\n                return self.http_client\n\n            async with transport_client_partial(httpx_client_factory=httpx_client_factory) as (\n                read_stream,\n                write_stream,\n                *_,\n            ):\n                yield read_stream, write_stream\n        else:\n            async with transport_client_partial(headers=self.headers) as (read_stream, write_stream, *_):\n                yield read_stream, write_stream\n\n    def __repr__(self) -> str:  # pragma: no cover\n        repr_args = [\n            f'url={self.url!r}',\n        ]\n        if self.id:\n            repr_args.append(f'id={self.id!r}')\n        return f'{self.__class__.__name__}({\", \".join(repr_args)})'\n\n\nclass MCPServerSSE(_MCPServerHTTP):\n    \"\"\"An MCP server that connects over streamable HTTP connections.\n\n    This class implements the SSE transport from the MCP specification.\n    See <https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse> for more information.\n\n    !!! note\n        Using this class as an async context manager will create a new pool of HTTP connections to connect\n        to a server which should already be running.\n\n    Example:\n    ```python {py=\"3.10\"}\n    from pydantic_ai import Agent\n    from pydantic_ai.mcp import MCPServerSSE\n\n    server = MCPServerSSE('http://localhost:3001/sse')\n    agent = Agent('openai:gpt-4o', toolsets=[server])\n\n    async def main():\n        async with agent:  # (1)!\n            ...\n    ```\n\n    1. This will connect to a server running on `localhost:3001`.\n    \"\"\"\n\n    @classmethod\n    def __get_pydantic_core_schema__(cls, _: Any, __: Any) -> CoreSchema:\n        return core_schema.no_info_after_validator_function(\n            lambda dct: MCPServerSSE(**dct),\n            core_schema.typed_dict_schema(\n                {\n                    'url': core_schema.typed_dict_field(core_schema.str_schema()),\n                    'headers': core_schema.typed_dict_field(\n                        core_schema.dict_schema(core_schema.str_schema(), core_schema.str_schema()), required=False\n                    ),\n                }\n            ),\n        )\n\n    @property\n    def _transport_client(self):\n        return sse_client  # pragma: no cover\n\n    def __eq__(self, value: object, /) -> bool:\n        if not isinstance(value, MCPServerSSE):\n            return False  # pragma: no cover\n        return self.url == value.url\n\n\n@deprecated('The `MCPServerHTTP` class is deprecated, use `MCPServerSSE` instead.')\nclass MCPServerHTTP(MCPServerSSE):\n    \"\"\"An MCP server that connects over HTTP using the old SSE transport.\n\n    This class implements the SSE transport from the MCP specification.\n    See <https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse> for more information.\n\n    !!! note\n        Using this class as an async context manager will create a new pool of HTTP connections to connect\n        to a server which should already be running.\n\n    Example:\n    ```python {py=\"3.10\" test=\"skip\"}\n    from pydantic_ai import Agent\n    from pydantic_ai.mcp import MCPServerHTTP\n\n    server = MCPServerHTTP('http://localhost:3001/sse')\n    agent = Agent('openai:gpt-4o', toolsets=[server])\n\n    async def main():\n        async with agent:  # (2)!\n            ...\n    ```\n\n    1. This will connect to a server running on `localhost:3001`.\n    \"\"\"\n\n\nclass MCPServerStreamableHTTP(_MCPServerHTTP):\n    \"\"\"An MCP server that connects over HTTP using the Streamable HTTP transport.\n\n    This class implements the Streamable HTTP transport from the MCP specification.\n    See <https://modelcontextprotocol.io/introduction#streamable-http> for more information.\n\n    !!! note\n        Using this class as an async context manager will create a new pool of HTTP connections to connect\n        to a server which should already be running.\n\n    Example:\n    ```python {py=\"3.10\"}\n    from pydantic_ai import Agent\n    from pydantic_ai.mcp import MCPServerStreamableHTTP\n\n    server = MCPServerStreamableHTTP('http://localhost:8000/mcp')  # (1)!\n    agent = Agent('openai:gpt-4o', toolsets=[server])\n\n    async def main():\n        async with agent:  # (2)!\n            ...\n    ```\n    \"\"\"\n\n    @classmethod\n    def __get_pydantic_core_schema__(cls, _: Any, __: Any) -> CoreSchema:\n        return core_schema.no_info_after_validator_function(\n            lambda dct: MCPServerStreamableHTTP(**dct),\n            core_schema.typed_dict_schema(\n                {\n                    'url': core_schema.typed_dict_field(core_schema.str_schema()),\n                    'headers': core_schema.typed_dict_field(\n                        core_schema.dict_schema(core_schema.str_schema(), core_schema.str_schema()), required=False\n                    ),\n                }\n            ),\n        )\n\n    @property\n    def _transport_client(self):\n        return streamablehttp_client  # pragma: no cover\n\n    def __eq__(self, value: object, /) -> bool:\n        if not isinstance(value, MCPServerStreamableHTTP):\n            return False  # pragma: no cover\n        return self.url == value.url\n\n\nToolResult = (\n    str\n    | messages.BinaryContent\n    | dict[str, Any]\n    | list[Any]\n    | Sequence[str | messages.BinaryContent | dict[str, Any] | list[Any]]\n)\n\"\"\"The result type of an MCP tool call.\"\"\"\n\nCallToolFunc = Callable[[str, dict[str, Any], dict[str, Any] | None], Awaitable[ToolResult]]\n\"\"\"A function type that represents a tool call.\"\"\"\n\nProcessToolCallback = Callable[\n    [\n        RunContext[Any],\n        CallToolFunc,\n        str,\n        dict[str, Any],\n    ],\n    Awaitable[ToolResult],\n]\n\"\"\"A process tool callback.\n\nIt accepts a run context, the original tool call function, a tool name, and arguments.\n\nAllows wrapping an MCP server tool call to customize it, including adding extra request\nmetadata.\n\"\"\"\n\n\ndef _mcp_server_discriminator(value: dict[str, Any]) -> str | None:\n    if 'url' in value:\n        if value['url'].endswith('/sse'):\n            return 'sse'\n        return 'streamable-http'\n    return 'stdio'\n\n\nclass MCPServerConfig(BaseModel):\n    \"\"\"Configuration for MCP servers.\"\"\"\n\n    mcp_servers: Annotated[\n        dict[\n            str,\n            Annotated[\n                Annotated[MCPServerStdio, Tag('stdio')]\n                | Annotated[MCPServerStreamableHTTP, Tag('streamable-http')]\n                | Annotated[MCPServerSSE, Tag('sse')],\n                Discriminator(_mcp_server_discriminator),\n            ],\n        ],\n        Field(alias='mcpServers'),\n    ]\n\n\ndef load_mcp_servers(config_path: str | Path) -> list[MCPServerStdio | MCPServerStreamableHTTP | MCPServerSSE]:\n    \"\"\"Load MCP servers from a configuration file.\n\n    Args:\n        config_path: The path to the configuration file.\n\n    Returns:\n        A list of MCP servers.\n\n    Raises:\n        FileNotFoundError: If the configuration file does not exist.\n        ValidationError: If the configuration file does not match the schema.\n    \"\"\"\n    config_path = Path(config_path)\n\n    if not config_path.exists():\n        raise FileNotFoundError(f'Config file {config_path} not found')\n\n    config = MCPServerConfig.model_validate_json(config_path.read_bytes())\n    return list(config.mcp_servers.values())\n\n\n=== pydantic_ai_slim/pydantic_ai/_otel_messages.py ===\n\"\"\"Type definitions of OpenTelemetry GenAI spec message parts.\n\nBased on https://github.com/lmolkova/semantic-conventions/blob/eccd1f806e426a32c98271c3ce77585492d26de2/docs/gen-ai/non-normative/models.ipynb\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Literal, TypeAlias\n\nfrom pydantic import JsonValue\nfrom typing_extensions import NotRequired, TypedDict\n\n\nclass TextPart(TypedDict):\n    type: Literal['text']\n    content: NotRequired[str]\n\n\nclass ToolCallPart(TypedDict):\n    type: Literal['tool_call']\n    id: str\n    name: str\n    arguments: NotRequired[JsonValue]\n    builtin: NotRequired[bool]  # Not (currently?) part of the spec, used by Logfire\n\n\nclass ToolCallResponsePart(TypedDict):\n    type: Literal['tool_call_response']\n    id: str\n    name: str\n    result: NotRequired[JsonValue]\n    builtin: NotRequired[bool]  # Not (currently?) part of the spec, used by Logfire\n\n\nclass MediaUrlPart(TypedDict):\n    type: Literal['image-url', 'audio-url', 'video-url', 'document-url']\n    url: NotRequired[str]\n\n\nclass BinaryDataPart(TypedDict):\n    type: Literal['binary']\n    media_type: str\n    content: NotRequired[str]\n\n\nclass ThinkingPart(TypedDict):\n    type: Literal['thinking']\n    content: NotRequired[str]\n\n\nMessagePart: TypeAlias = 'TextPart | ToolCallPart | ToolCallResponsePart | MediaUrlPart | BinaryDataPart | ThinkingPart'\n\n\nRole = Literal['system', 'user', 'assistant']\n\n\nclass ChatMessage(TypedDict):\n    role: Role\n    parts: list[MessagePart]\n\n\nInputMessages: TypeAlias = list[ChatMessage]\n\n\nclass OutputMessage(ChatMessage):\n    finish_reason: NotRequired[str]\n\n\nOutputMessages: TypeAlias = list[OutputMessage]\n\n\n=== pydantic_ai_slim/pydantic_ai/_tool_manager.py ===\nfrom __future__ import annotations\n\nimport json\nfrom collections.abc import Iterator\nfrom contextlib import contextmanager\nfrom contextvars import ContextVar\nfrom dataclasses import dataclass, field, replace\nfrom typing import Any, Generic\n\nfrom opentelemetry.trace import Tracer\nfrom pydantic import ValidationError\nfrom typing_extensions import assert_never\n\nfrom . import messages as _messages\nfrom ._run_context import AgentDepsT, RunContext\nfrom .exceptions import ModelRetry, ToolRetryError, UnexpectedModelBehavior\nfrom .messages import ToolCallPart\nfrom .tools import ToolDefinition\nfrom .toolsets.abstract import AbstractToolset, ToolsetTool\nfrom .usage import UsageLimits\n\n_sequential_tool_calls_ctx_var: ContextVar[bool] = ContextVar('sequential_tool_calls', default=False)\n\n\n@dataclass\nclass ToolManager(Generic[AgentDepsT]):\n    \"\"\"Manages tools for an agent run step. It caches the agent run's toolset's tool definitions and handles calling tools and retries.\"\"\"\n\n    toolset: AbstractToolset[AgentDepsT]\n    \"\"\"The toolset that provides the tools for this run step.\"\"\"\n    ctx: RunContext[AgentDepsT] | None = None\n    \"\"\"The agent run context for a specific run step.\"\"\"\n    tools: dict[str, ToolsetTool[AgentDepsT]] | None = None\n    \"\"\"The cached tools for this run step.\"\"\"\n    failed_tools: set[str] = field(default_factory=set)\n    \"\"\"Names of tools that failed in this run step.\"\"\"\n\n    @classmethod\n    @contextmanager\n    def sequential_tool_calls(cls) -> Iterator[None]:\n        \"\"\"Run tool calls sequentially during the context.\"\"\"\n        token = _sequential_tool_calls_ctx_var.set(True)\n        try:\n            yield\n        finally:\n            _sequential_tool_calls_ctx_var.reset(token)\n\n    async def for_run_step(self, ctx: RunContext[AgentDepsT]) -> ToolManager[AgentDepsT]:\n        \"\"\"Build a new tool manager for the next run step, carrying over the retries from the current run step.\"\"\"\n        if self.ctx is not None:\n            if ctx.run_step == self.ctx.run_step:\n                return self\n\n            retries = {\n                failed_tool_name: self.ctx.retries.get(failed_tool_name, 0) + 1\n                for failed_tool_name in self.failed_tools\n            }\n            ctx = replace(ctx, retries=retries)\n\n        return self.__class__(\n            toolset=self.toolset,\n            ctx=ctx,\n            tools=await self.toolset.get_tools(ctx),\n        )\n\n    @property\n    def tool_defs(self) -> list[ToolDefinition]:\n        \"\"\"The tool definitions for the tools in this tool manager.\"\"\"\n        if self.tools is None:\n            raise ValueError('ToolManager has not been prepared for a run step yet')  # pragma: no cover\n\n        return [tool.tool_def for tool in self.tools.values()]\n\n    def should_call_sequentially(self, calls: list[ToolCallPart]) -> bool:\n        \"\"\"Whether to require sequential tool calls for a list of tool calls.\"\"\"\n        return _sequential_tool_calls_ctx_var.get() or any(\n            tool_def.sequential for call in calls if (tool_def := self.get_tool_def(call.tool_name))\n        )\n\n    def get_tool_def(self, name: str) -> ToolDefinition | None:\n        \"\"\"Get the tool definition for a given tool name, or `None` if the tool is unknown.\"\"\"\n        if self.tools is None:\n            raise ValueError('ToolManager has not been prepared for a run step yet')  # pragma: no cover\n\n        try:\n            return self.tools[name].tool_def\n        except KeyError:\n            return None\n\n    async def handle_call(\n        self,\n        call: ToolCallPart,\n        allow_partial: bool = False,\n        wrap_validation_errors: bool = True,\n        usage_limits: UsageLimits | None = None,\n    ) -> Any:\n        \"\"\"Handle a tool call by validating the arguments, calling the tool, and handling retries.\n\n        Args:\n            call: The tool call part to handle.\n            allow_partial: Whether to allow partial validation of the tool arguments.\n            wrap_validation_errors: Whether to wrap validation errors in a retry prompt part.\n            usage_limits: Optional usage limits to check before executing tools.\n        \"\"\"\n        if self.tools is None or self.ctx is None:\n            raise ValueError('ToolManager has not been prepared for a run step yet')  # pragma: no cover\n\n        if (tool := self.tools.get(call.tool_name)) and tool.tool_def.kind == 'output':\n            # Output tool calls are not traced and not counted\n            return await self._call_tool(call, allow_partial, wrap_validation_errors, count_tool_usage=False)\n        else:\n            return await self._call_tool_traced(\n                call,\n                allow_partial,\n                wrap_validation_errors,\n                self.ctx.tracer,\n                self.ctx.trace_include_content,\n                usage_limits,\n            )\n\n    async def _call_tool(\n        self,\n        call: ToolCallPart,\n        allow_partial: bool,\n        wrap_validation_errors: bool,\n        usage_limits: UsageLimits | None = None,\n        count_tool_usage: bool = True,\n    ) -> Any:\n        if self.tools is None or self.ctx is None:\n            raise ValueError('ToolManager has not been prepared for a run step yet')  # pragma: no cover\n\n        name = call.tool_name\n        tool = self.tools.get(name)\n        try:\n            if tool is None:\n                if self.tools:\n                    msg = f'Available tools: {\", \".join(f\"{name!r}\" for name in self.tools.keys())}'\n                else:\n                    msg = 'No tools available.'\n                raise ModelRetry(f'Unknown tool name: {name!r}. {msg}')\n\n            if tool.tool_def.defer:\n                raise RuntimeError('Deferred tools cannot be called')\n\n            ctx = replace(\n                self.ctx,\n                tool_name=name,\n                tool_call_id=call.tool_call_id,\n                retry=self.ctx.retries.get(name, 0),\n                max_retries=tool.max_retries,\n            )\n\n            pyd_allow_partial = 'trailing-strings' if allow_partial else 'off'\n            validator = tool.args_validator\n            if isinstance(call.args, str):\n                args_dict = validator.validate_json(call.args or '{}', allow_partial=pyd_allow_partial)\n            else:\n                args_dict = validator.validate_python(call.args or {}, allow_partial=pyd_allow_partial)\n\n            if usage_limits is not None and count_tool_usage:\n                usage_limits.check_before_tool_call(self.ctx.usage)\n\n            result = await self.toolset.call_tool(name, args_dict, ctx, tool)\n\n            if count_tool_usage:\n                self.ctx.usage.tool_calls += 1\n\n            return result\n        except (ValidationError, ModelRetry) as e:\n            max_retries = tool.max_retries if tool is not None else 1\n            current_retry = self.ctx.retries.get(name, 0)\n\n            if current_retry == max_retries:\n                raise UnexpectedModelBehavior(f'Tool {name!r} exceeded max retries count of {max_retries}') from e\n            else:\n                if wrap_validation_errors:\n                    if isinstance(e, ValidationError):\n                        m = _messages.RetryPromptPart(\n                            tool_name=name,\n                            content=e.errors(include_url=False, include_context=False),\n                            tool_call_id=call.tool_call_id,\n                        )\n                        e = ToolRetryError(m)\n                    elif isinstance(e, ModelRetry):\n                        m = _messages.RetryPromptPart(\n                            tool_name=name,\n                            content=e.message,\n                            tool_call_id=call.tool_call_id,\n                        )\n                        e = ToolRetryError(m)\n                    else:\n                        assert_never(e)\n\n                if not allow_partial:\n                    # If we're validating partial arguments, we don't want to count this as a failed tool as it may still succeed once the full arguments are received.\n                    self.failed_tools.add(name)\n\n                raise e\n\n    async def _call_tool_traced(\n        self,\n        call: ToolCallPart,\n        allow_partial: bool,\n        wrap_validation_errors: bool,\n        tracer: Tracer,\n        include_content: bool = False,\n        usage_limits: UsageLimits | None = None,\n    ) -> Any:\n        \"\"\"See <https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-spans/#execute-tool-span>.\"\"\"\n        span_attributes = {\n            'gen_ai.tool.name': call.tool_name,\n            # NOTE: this means `gen_ai.tool.call.id` will be included even if it was generated by pydantic-ai\n            'gen_ai.tool.call.id': call.tool_call_id,\n            **({'tool_arguments': call.args_as_json_str()} if include_content else {}),\n            'logfire.msg': f'running tool: {call.tool_name}',\n            # add the JSON schema so these attributes are formatted nicely in Logfire\n            'logfire.json_schema': json.dumps(\n                {\n                    'type': 'object',\n                    'properties': {\n                        **(\n                            {\n                                'tool_arguments': {'type': 'object'},\n                                'tool_response': {'type': 'object'},\n                            }\n                            if include_content\n                            else {}\n                        ),\n                        'gen_ai.tool.name': {},\n                        'gen_ai.tool.call.id': {},\n                    },\n                }\n            ),\n        }\n        with tracer.start_as_current_span('running tool', attributes=span_attributes) as span:\n            try:\n                tool_result = await self._call_tool(call, allow_partial, wrap_validation_errors, usage_limits)\n            except ToolRetryError as e:\n                part = e.tool_retry\n                if include_content and span.is_recording():\n                    span.set_attribute('tool_response', part.model_response())\n                raise e\n\n            if include_content and span.is_recording():\n                span.set_attribute(\n                    'tool_response',\n                    tool_result\n                    if isinstance(tool_result, str)\n                    else _messages.tool_return_ta.dump_json(tool_result).decode(),\n                )\n\n        return tool_result\n\n\n=== pydantic_ai_slim/pydantic_ai/_griffe.py ===\nfrom __future__ import annotations as _annotations\n\nimport logging\nimport re\nfrom collections.abc import Callable\nfrom contextlib import contextmanager\nfrom inspect import Signature\nfrom typing import TYPE_CHECKING, Any, Literal, cast\n\nfrom griffe import Docstring, DocstringSectionKind, Object as GriffeObject\n\nif TYPE_CHECKING:\n    from .tools import DocstringFormat\n\nDocstringStyle = Literal['google', 'numpy', 'sphinx']\n\n\ndef doc_descriptions(\n    func: Callable[..., Any],\n    sig: Signature,\n    *,\n    docstring_format: DocstringFormat,\n) -> tuple[str | None, dict[str, str]]:\n    \"\"\"Extract the function description and parameter descriptions from a function's docstring.\n\n    The function parses the docstring using the specified format (or infers it if 'auto')\n    and extracts both the main description and parameter descriptions. If a returns section\n    is present in the docstring, the main description will be formatted as XML.\n\n    Returns:\n        A tuple containing:\n        - str: Main description string, which may be either:\n            * Plain text if no returns section is present\n            * XML-formatted if returns section exists, including <summary> and <returns> tags\n        - dict[str, str]: Dictionary mapping parameter names to their descriptions\n    \"\"\"\n    doc = func.__doc__\n    if doc is None:\n        return None, {}\n\n    # see https://github.com/mkdocstrings/griffe/issues/293\n    parent = cast(GriffeObject, sig)\n\n    docstring_style = _infer_docstring_style(doc) if docstring_format == 'auto' else docstring_format\n    docstring = Docstring(\n        doc,\n        lineno=1,\n        parser=docstring_style,\n        parent=parent,\n        # https://mkdocstrings.github.io/griffe/reference/docstrings/#google-options\n        parser_options={'returns_named_value': False, 'returns_multiple_items': False},\n    )\n    with _disable_griffe_logging():\n        sections = docstring.parse()\n\n    params = {}\n    if parameters := next((p for p in sections if p.kind == DocstringSectionKind.parameters), None):\n        params = {p.name: p.description for p in parameters.value}\n\n    main_desc = ''\n    if main := next((p for p in sections if p.kind == DocstringSectionKind.text), None):\n        main_desc = main.value\n\n    if return_ := next((p for p in sections if p.kind == DocstringSectionKind.returns), None):\n        return_statement = return_.value[0]\n        return_desc = return_statement.description\n        return_type = return_statement.annotation\n        type_tag = f'<type>{return_type}</type>\\n' if return_type else ''\n        return_xml = f'<returns>\\n{type_tag}<description>{return_desc}</description>\\n</returns>'\n\n        if main_desc:\n            main_desc = f'<summary>{main_desc}</summary>\\n{return_xml}'\n        else:\n            main_desc = return_xml\n\n    return main_desc, params\n\n\ndef _infer_docstring_style(doc: str) -> DocstringStyle:\n    \"\"\"Simplistic docstring style inference.\"\"\"\n    for pattern, replacements, style in _docstring_style_patterns:\n        matches = (\n            re.search(pattern.format(replacement), doc, re.IGNORECASE | re.MULTILINE) for replacement in replacements\n        )\n        if any(matches):\n            return style\n    # fallback to google style\n    return 'google'\n\n\n# See https://github.com/mkdocstrings/griffe/issues/329#issuecomment-2425017804\n_docstring_style_patterns: list[tuple[str, list[str], DocstringStyle]] = [\n    (\n        r'\\n[ \\t]*:{0}([ \\t]+\\w+)*:([ \\t]+.+)?\\n',\n        [\n            'param',\n            'parameter',\n            'arg',\n            'argument',\n            'key',\n            'keyword',\n            'type',\n            'var',\n            'ivar',\n            'cvar',\n            'vartype',\n            'returns',\n            'return',\n            'rtype',\n            'raises',\n            'raise',\n            'except',\n            'exception',\n        ],\n        'sphinx',\n    ),\n    (\n        r'\\n[ \\t]*{0}:([ \\t]+.+)?\\n[ \\t]+.+',\n        [\n            'args',\n            'arguments',\n            'params',\n            'parameters',\n            'keyword args',\n            'keyword arguments',\n            'other args',\n            'other arguments',\n            'other params',\n            'other parameters',\n            'raises',\n            'exceptions',\n            'returns',\n            'yields',\n            'receives',\n            'examples',\n            'attributes',\n            'functions',\n            'methods',\n            'classes',\n            'modules',\n            'warns',\n            'warnings',\n        ],\n        'google',\n    ),\n    (\n        r'\\n[ \\t]*{0}\\n[ \\t]*---+\\n',\n        [\n            'deprecated',\n            'parameters',\n            'other parameters',\n            'returns',\n            'yields',\n            'receives',\n            'raises',\n            'warns',\n            'attributes',\n            'functions',\n            'methods',\n            'classes',\n            'modules',\n        ],\n        'numpy',\n    ),\n]\n\n\n@contextmanager\ndef _disable_griffe_logging():\n    # Hacky, but suggested here: https://github.com/mkdocstrings/griffe/issues/293#issuecomment-2167668117\n    old_level = logging.root.getEffectiveLevel()\n    logging.root.setLevel(logging.ERROR)\n    yield\n    logging.root.setLevel(old_level)\n\n\n=== pydantic_ai_slim/pydantic_ai/messages.py ===\nfrom __future__ import annotations as _annotations\n\nimport base64\nimport hashlib\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Sequence\nfrom dataclasses import KW_ONLY, dataclass, field, replace\nfrom datetime import datetime\nfrom mimetypes import guess_type\nfrom typing import TYPE_CHECKING, Annotated, Any, Literal, TypeAlias, cast, overload\n\nimport pydantic\nimport pydantic_core\nfrom genai_prices import calc_price, types as genai_types\nfrom opentelemetry._events import Event  # pyright: ignore[reportPrivateImportUsage]\nfrom typing_extensions import deprecated\n\nfrom . import _otel_messages, _utils\nfrom ._utils import generate_tool_call_id as _generate_tool_call_id, now_utc as _now_utc\nfrom .exceptions import UnexpectedModelBehavior\nfrom .usage import RequestUsage\n\nif TYPE_CHECKING:\n    from .models.instrumented import InstrumentationSettings\n\n\nAudioMediaType: TypeAlias = Literal['audio/wav', 'audio/mpeg', 'audio/ogg', 'audio/flac', 'audio/aiff', 'audio/aac']\nImageMediaType: TypeAlias = Literal['image/jpeg', 'image/png', 'image/gif', 'image/webp']\nDocumentMediaType: TypeAlias = Literal[\n    'application/pdf',\n    'text/plain',\n    'text/csv',\n    'application/vnd.openxmlformats-officedocument.wordprocessingml.document',\n    'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',\n    'text/html',\n    'text/markdown',\n    'application/vnd.ms-excel',\n]\nVideoMediaType: TypeAlias = Literal[\n    'video/x-matroska',\n    'video/quicktime',\n    'video/mp4',\n    'video/webm',\n    'video/x-flv',\n    'video/mpeg',\n    'video/x-ms-wmv',\n    'video/3gpp',\n]\n\nAudioFormat: TypeAlias = Literal['wav', 'mp3', 'oga', 'flac', 'aiff', 'aac']\nImageFormat: TypeAlias = Literal['jpeg', 'png', 'gif', 'webp']\nDocumentFormat: TypeAlias = Literal['csv', 'doc', 'docx', 'html', 'md', 'pdf', 'txt', 'xls', 'xlsx']\nVideoFormat: TypeAlias = Literal['mkv', 'mov', 'mp4', 'webm', 'flv', 'mpeg', 'mpg', 'wmv', 'three_gp']\n\nFinishReason: TypeAlias = Literal[\n    'stop',\n    'length',\n    'content_filter',\n    'tool_call',\n    'error',\n]\n\"\"\"Reason the model finished generating the response, normalized to OpenTelemetry values.\"\"\"\n\n\n@dataclass(repr=False)\nclass SystemPromptPart:\n    \"\"\"A system prompt, generally written by the application developer.\n\n    This gives the model context and guidance on how to respond.\n    \"\"\"\n\n    content: str\n    \"\"\"The content of the prompt.\"\"\"\n\n    _: KW_ONLY\n\n    timestamp: datetime = field(default_factory=_now_utc)\n    \"\"\"The timestamp of the prompt.\"\"\"\n\n    dynamic_ref: str | None = None\n    \"\"\"The ref of the dynamic system prompt function that generated this part.\n\n    Only set if system prompt is dynamic, see [`system_prompt`][pydantic_ai.Agent.system_prompt] for more information.\n    \"\"\"\n\n    part_kind: Literal['system-prompt'] = 'system-prompt'\n    \"\"\"Part type identifier, this is available on all parts as a discriminator.\"\"\"\n\n    def otel_event(self, settings: InstrumentationSettings) -> Event:\n        return Event(\n            'gen_ai.system.message',\n            body={'role': 'system', **({'content': self.content} if settings.include_content else {})},\n        )\n\n    def otel_message_parts(self, settings: InstrumentationSettings) -> list[_otel_messages.MessagePart]:\n        return [_otel_messages.TextPart(type='text', **{'content': self.content} if settings.include_content else {})]\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n\ndef _multi_modal_content_identifier(identifier: str | bytes) -> str:\n    \"\"\"Generate stable identifier for multi-modal content to help LLM in finding a specific file in tool call responses.\"\"\"\n    if isinstance(identifier, str):\n        identifier = identifier.encode('utf-8')\n    return hashlib.sha1(identifier).hexdigest()[:6]\n\n\n@dataclass(init=False, repr=False)\nclass FileUrl(ABC):\n    \"\"\"Abstract base class for any URL-based file.\"\"\"\n\n    url: str\n    \"\"\"The URL of the file.\"\"\"\n\n    _: KW_ONLY\n\n    force_download: bool = False\n    \"\"\"If the model supports it:\n\n    * If True, the file is downloaded and the data is sent to the model as bytes.\n    * If False, the URL is sent directly to the model and no download is performed.\n    \"\"\"\n\n    vendor_metadata: dict[str, Any] | None = None\n    \"\"\"Vendor-specific metadata for the file.\n\n    Supported by:\n    - `GoogleModel`: `VideoUrl.vendor_metadata` is used as `video_metadata`: https://ai.google.dev/gemini-api/docs/video-understanding#customize-video-processing\n    \"\"\"\n\n    _media_type: Annotated[str | None, pydantic.Field(alias='media_type', default=None, exclude=True)] = field(\n        compare=False, default=None\n    )\n\n    identifier: str | None = None\n    \"\"\"The identifier of the file, such as a unique ID. generating one from the url if not explicitly set\n\n    This identifier can be provided to the model in a message to allow it to refer to this file in a tool call argument,\n    and the tool can look up the file in question by iterating over the message history and finding the matching `FileUrl`.\n\n    This identifier is only automatically passed to the model when the `FileUrl` is returned by a tool.\n    If you're passing the `FileUrl` as a user message, it's up to you to include a separate text part with the identifier,\n    e.g. \"This is file <identifier>:\" preceding the `FileUrl`.\n    \"\"\"\n\n    def __init__(\n        self,\n        url: str,\n        *,\n        force_download: bool = False,\n        vendor_metadata: dict[str, Any] | None = None,\n        media_type: str | None = None,\n        identifier: str | None = None,\n    ) -> None:\n        self.url = url\n        self.force_download = force_download\n        self.vendor_metadata = vendor_metadata\n        self._media_type = media_type\n        self.identifier = identifier or _multi_modal_content_identifier(url)\n\n    @pydantic.computed_field\n    @property\n    def media_type(self) -> str:\n        \"\"\"Return the media type of the file, based on the URL or the provided `media_type`.\"\"\"\n        return self._media_type or self._infer_media_type()\n\n    @abstractmethod\n    def _infer_media_type(self) -> str:\n        \"\"\"Infer the media type of the file based on the URL.\"\"\"\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def format(self) -> str:\n        \"\"\"The file format.\"\"\"\n        raise NotImplementedError\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n\n@dataclass(init=False, repr=False)\nclass VideoUrl(FileUrl):\n    \"\"\"A URL to a video.\"\"\"\n\n    url: str\n    \"\"\"The URL of the video.\"\"\"\n\n    _: KW_ONLY\n\n    kind: Literal['video-url'] = 'video-url'\n    \"\"\"Type identifier, this is available on all parts as a discriminator.\"\"\"\n\n    def __init__(\n        self,\n        url: str,\n        *,\n        force_download: bool = False,\n        vendor_metadata: dict[str, Any] | None = None,\n        media_type: str | None = None,\n        kind: Literal['video-url'] = 'video-url',\n        identifier: str | None = None,\n        # Required for inline-snapshot which expects all dataclass `__init__` methods to take all field names as kwargs.\n        _media_type: str | None = None,\n    ) -> None:\n        super().__init__(\n            url=url,\n            force_download=force_download,\n            vendor_metadata=vendor_metadata,\n            media_type=media_type or _media_type,\n            identifier=identifier,\n        )\n        self.kind = kind\n\n    def _infer_media_type(self) -> VideoMediaType:\n        \"\"\"Return the media type of the video, based on the url.\"\"\"\n        if self.url.endswith('.mkv'):\n            return 'video/x-matroska'\n        elif self.url.endswith('.mov'):\n            return 'video/quicktime'\n        elif self.url.endswith('.mp4'):\n            return 'video/mp4'\n        elif self.url.endswith('.webm'):\n            return 'video/webm'\n        elif self.url.endswith('.flv'):\n            return 'video/x-flv'\n        elif self.url.endswith(('.mpeg', '.mpg')):\n            return 'video/mpeg'\n        elif self.url.endswith('.wmv'):\n            return 'video/x-ms-wmv'\n        elif self.url.endswith('.three_gp'):\n            return 'video/3gpp'\n        # Assume that YouTube videos are mp4 because there would be no extension\n        # to infer from. This should not be a problem, as Gemini disregards media\n        # type for YouTube URLs.\n        elif self.is_youtube:\n            return 'video/mp4'\n        else:\n            raise ValueError(\n                f'Could not infer media type from video URL: {self.url}. Explicitly provide a `media_type` instead.'\n            )\n\n    @property\n    def is_youtube(self) -> bool:\n        \"\"\"True if the URL has a YouTube domain.\"\"\"\n        return self.url.startswith(('https://youtu.be/', 'https://youtube.com/', 'https://www.youtube.com/'))\n\n    @property\n    def format(self) -> VideoFormat:\n        \"\"\"The file format of the video.\n\n        The choice of supported formats were based on the Bedrock Converse API. Other APIs don't require to use a format.\n        \"\"\"\n        return _video_format_lookup[self.media_type]\n\n\n@dataclass(init=False, repr=False)\nclass AudioUrl(FileUrl):\n    \"\"\"A URL to an audio file.\"\"\"\n\n    url: str\n    \"\"\"The URL of the audio file.\"\"\"\n\n    _: KW_ONLY\n\n    kind: Literal['audio-url'] = 'audio-url'\n    \"\"\"Type identifier, this is available on all parts as a discriminator.\"\"\"\n\n    def __init__(\n        self,\n        url: str,\n        *,\n        force_download: bool = False,\n        vendor_metadata: dict[str, Any] | None = None,\n        media_type: str | None = None,\n        kind: Literal['audio-url'] = 'audio-url',\n        identifier: str | None = None,\n        # Required for inline-snapshot which expects all dataclass `__init__` methods to take all field names as kwargs.\n        _media_type: str | None = None,\n    ) -> None:\n        super().__init__(\n            url=url,\n            force_download=force_download,\n            vendor_metadata=vendor_metadata,\n            media_type=media_type or _media_type,\n            identifier=identifier,\n        )\n        self.kind = kind\n\n    def _infer_media_type(self) -> AudioMediaType:\n        \"\"\"Return the media type of the audio file, based on the url.\n\n        References:\n        - Gemini: https://ai.google.dev/gemini-api/docs/audio#supported-formats\n        \"\"\"\n        if self.url.endswith('.mp3'):\n            return 'audio/mpeg'\n        if self.url.endswith('.wav'):\n            return 'audio/wav'\n        if self.url.endswith('.flac'):\n            return 'audio/flac'\n        if self.url.endswith('.oga'):\n            return 'audio/ogg'\n        if self.url.endswith('.aiff'):\n            return 'audio/aiff'\n        if self.url.endswith('.aac'):\n            return 'audio/aac'\n\n        raise ValueError(\n            f'Could not infer media type from audio URL: {self.url}. Explicitly provide a `media_type` instead.'\n        )\n\n    @property\n    def format(self) -> AudioFormat:\n        \"\"\"The file format of the audio file.\"\"\"\n        return _audio_format_lookup[self.media_type]\n\n\n@dataclass(init=False, repr=False)\nclass ImageUrl(FileUrl):\n    \"\"\"A URL to an image.\"\"\"\n\n    url: str\n    \"\"\"The URL of the image.\"\"\"\n\n    _: KW_ONLY\n\n    kind: Literal['image-url'] = 'image-url'\n    \"\"\"Type identifier, this is available on all parts as a discriminator.\"\"\"\n\n    def __init__(\n        self,\n        url: str,\n        *,\n        force_download: bool = False,\n        vendor_metadata: dict[str, Any] | None = None,\n        media_type: str | None = None,\n        kind: Literal['image-url'] = 'image-url',\n        identifier: str | None = None,\n        # Required for inline-snapshot which expects all dataclass `__init__` methods to take all field names as kwargs.\n        _media_type: str | None = None,\n    ) -> None:\n        super().__init__(\n            url=url,\n            force_download=force_download,\n            vendor_metadata=vendor_metadata,\n            media_type=media_type or _media_type,\n            identifier=identifier,\n        )\n        self.kind = kind\n\n    def _infer_media_type(self) -> ImageMediaType:\n        \"\"\"Return the media type of the image, based on the url.\"\"\"\n        if self.url.endswith(('.jpg', '.jpeg')):\n            return 'image/jpeg'\n        elif self.url.endswith('.png'):\n            return 'image/png'\n        elif self.url.endswith('.gif'):\n            return 'image/gif'\n        elif self.url.endswith('.webp'):\n            return 'image/webp'\n        else:\n            raise ValueError(\n                f'Could not infer media type from image URL: {self.url}. Explicitly provide a `media_type` instead.'\n            )\n\n    @property\n    def format(self) -> ImageFormat:\n        \"\"\"The file format of the image.\n\n        The choice of supported formats were based on the Bedrock Converse API. Other APIs don't require to use a format.\n        \"\"\"\n        return _image_format_lookup[self.media_type]\n\n\n@dataclass(init=False, repr=False)\nclass DocumentUrl(FileUrl):\n    \"\"\"The URL of the document.\"\"\"\n\n    url: str\n    \"\"\"The URL of the document.\"\"\"\n\n    _: KW_ONLY\n\n    kind: Literal['document-url'] = 'document-url'\n    \"\"\"Type identifier, this is available on all parts as a discriminator.\"\"\"\n\n    def __init__(\n        self,\n        url: str,\n        *,\n        force_download: bool = False,\n        vendor_metadata: dict[str, Any] | None = None,\n        media_type: str | None = None,\n        kind: Literal['document-url'] = 'document-url',\n        identifier: str | None = None,\n        # Required for inline-snapshot which expects all dataclass `__init__` methods to take all field names as kwargs.\n        _media_type: str | None = None,\n    ) -> None:\n        super().__init__(\n            url=url,\n            force_download=force_download,\n            vendor_metadata=vendor_metadata,\n            media_type=media_type or _media_type,\n            identifier=identifier,\n        )\n        self.kind = kind\n\n    def _infer_media_type(self) -> str:\n        \"\"\"Return the media type of the document, based on the url.\"\"\"\n        # Common document types are hardcoded here as mime-type support for these\n        # extensions varies across operating systems.\n        if self.url.endswith(('.md', '.mdx', '.markdown')):\n            return 'text/markdown'\n        elif self.url.endswith('.asciidoc'):\n            return 'text/x-asciidoc'\n        elif self.url.endswith('.txt'):\n            return 'text/plain'\n        elif self.url.endswith('.pdf'):\n            return 'application/pdf'\n        elif self.url.endswith('.rtf'):\n            return 'application/rtf'\n        elif self.url.endswith('.docx'):\n            return 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'\n        elif self.url.endswith('.xlsx'):\n            return 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'\n\n        type_, _ = guess_type(self.url)\n        if type_ is None:\n            raise ValueError(\n                f'Could not infer media type from document URL: {self.url}. Explicitly provide a `media_type` instead.'\n            )\n        return type_\n\n    @property\n    def format(self) -> DocumentFormat:\n        \"\"\"The file format of the document.\n\n        The choice of supported formats were based on the Bedrock Converse API. Other APIs don't require to use a format.\n        \"\"\"\n        media_type = self.media_type\n        try:\n            return _document_format_lookup[media_type]\n        except KeyError as e:\n            raise ValueError(f'Unknown document media type: {media_type}') from e\n\n\n@dataclass(init=False, repr=False)\nclass BinaryContent:\n    \"\"\"Binary content, e.g. an audio or image file.\"\"\"\n\n    data: bytes\n    \"\"\"The binary data.\"\"\"\n\n    _: KW_ONLY\n\n    media_type: AudioMediaType | ImageMediaType | DocumentMediaType | str\n    \"\"\"The media type of the binary data.\"\"\"\n\n    identifier: str\n    \"\"\"Identifier for the binary content, such as a unique ID. generating one from the data if not explicitly set\n    This identifier can be provided to the model in a message to allow it to refer to this file in a tool call argument,\n    and the tool can look up the file in question by iterating over the message history and finding the matching `BinaryContent`.\n\n    This identifier is only automatically passed to the model when the `BinaryContent` is returned by a tool.\n    If you're passing the `BinaryContent` as a user message, it's up to you to include a separate text part with the identifier,\n    e.g. \"This is file <identifier>:\" preceding the `BinaryContent`.\n    \"\"\"\n\n    vendor_metadata: dict[str, Any] | None = None\n    \"\"\"Vendor-specific metadata for the file.\n\n    Supported by:\n    - `GoogleModel`: `BinaryContent.vendor_metadata` is used as `video_metadata`: https://ai.google.dev/gemini-api/docs/video-understanding#customize-video-processing\n    \"\"\"\n\n    kind: Literal['binary'] = 'binary'\n    \"\"\"Type identifier, this is available on all parts as a discriminator.\"\"\"\n\n    def __init__(\n        self,\n        data: bytes,\n        *,\n        media_type: AudioMediaType | ImageMediaType | DocumentMediaType | str,\n        identifier: str | None = None,\n        vendor_metadata: dict[str, Any] | None = None,\n        kind: Literal['binary'] = 'binary',\n    ) -> None:\n        self.data = data\n        self.media_type = media_type\n        self.identifier = identifier or _multi_modal_content_identifier(data)\n        self.vendor_metadata = vendor_metadata\n        self.kind = kind\n\n    @property\n    def is_audio(self) -> bool:\n        \"\"\"Return `True` if the media type is an audio type.\"\"\"\n        return self.media_type.startswith('audio/')\n\n    @property\n    def is_image(self) -> bool:\n        \"\"\"Return `True` if the media type is an image type.\"\"\"\n        return self.media_type.startswith('image/')\n\n    @property\n    def is_video(self) -> bool:\n        \"\"\"Return `True` if the media type is a video type.\"\"\"\n        return self.media_type.startswith('video/')\n\n    @property\n    def is_document(self) -> bool:\n        \"\"\"Return `True` if the media type is a document type.\"\"\"\n        return self.media_type in _document_format_lookup\n\n    @property\n    def format(self) -> str:\n        \"\"\"The file format of the binary content.\"\"\"\n        try:\n            if self.is_audio:\n                return _audio_format_lookup[self.media_type]\n            elif self.is_image:\n                return _image_format_lookup[self.media_type]\n            elif self.is_video:\n                return _video_format_lookup[self.media_type]\n            else:\n                return _document_format_lookup[self.media_type]\n        except KeyError as e:\n            raise ValueError(f'Unknown media type: {self.media_type}') from e\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n\nMultiModalContent = ImageUrl | AudioUrl | DocumentUrl | VideoUrl | BinaryContent\nUserContent: TypeAlias = str | MultiModalContent\n\n\n@dataclass(repr=False)\nclass ToolReturn:\n    \"\"\"A structured return value for tools that need to provide both a return value and custom content to the model.\n\n    This class allows tools to return complex responses that include:\n    - A return value for actual tool return\n    - Custom content (including multi-modal content) to be sent to the model as a UserPromptPart\n    - Optional metadata for application use\n    \"\"\"\n\n    return_value: Any\n    \"\"\"The return value to be used in the tool response.\"\"\"\n\n    _: KW_ONLY\n\n    content: str | Sequence[UserContent] | None = None\n    \"\"\"The content to be sent to the model as a UserPromptPart.\"\"\"\n\n    metadata: Any = None\n    \"\"\"Additional data that can be accessed programmatically by the application but is not sent to the LLM.\"\"\"\n\n    kind: Literal['tool-return'] = 'tool-return'\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n\n_document_format_lookup: dict[str, DocumentFormat] = {\n    'application/pdf': 'pdf',\n    'text/plain': 'txt',\n    'text/csv': 'csv',\n    'application/vnd.openxmlformats-officedocument.wordprocessingml.document': 'docx',\n    'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet': 'xlsx',\n    'text/html': 'html',\n    'text/markdown': 'md',\n    'application/vnd.ms-excel': 'xls',\n}\n_audio_format_lookup: dict[str, AudioFormat] = {\n    'audio/mpeg': 'mp3',\n    'audio/wav': 'wav',\n    'audio/flac': 'flac',\n    'audio/ogg': 'oga',\n    'audio/aiff': 'aiff',\n    'audio/aac': 'aac',\n}\n_image_format_lookup: dict[str, ImageFormat] = {\n    'image/jpeg': 'jpeg',\n    'image/png': 'png',\n    'image/gif': 'gif',\n    'image/webp': 'webp',\n}\n_video_format_lookup: dict[str, VideoFormat] = {\n    'video/x-matroska': 'mkv',\n    'video/quicktime': 'mov',\n    'video/mp4': 'mp4',\n    'video/webm': 'webm',\n    'video/x-flv': 'flv',\n    'video/mpeg': 'mpeg',\n    'video/x-ms-wmv': 'wmv',\n    'video/3gpp': 'three_gp',\n}\n\n\n@dataclass(repr=False)\nclass UserPromptPart:\n    \"\"\"A user prompt, generally written by the end user.\n\n    Content comes from the `user_prompt` parameter of [`Agent.run`][pydantic_ai.agent.AbstractAgent.run],\n    [`Agent.run_sync`][pydantic_ai.agent.AbstractAgent.run_sync], and [`Agent.run_stream`][pydantic_ai.agent.AbstractAgent.run_stream].\n    \"\"\"\n\n    content: str | Sequence[UserContent]\n    \"\"\"The content of the prompt.\"\"\"\n\n    _: KW_ONLY\n\n    timestamp: datetime = field(default_factory=_now_utc)\n    \"\"\"The timestamp of the prompt.\"\"\"\n\n    part_kind: Literal['user-prompt'] = 'user-prompt'\n    \"\"\"Part type identifier, this is available on all parts as a discriminator.\"\"\"\n\n    def otel_event(self, settings: InstrumentationSettings) -> Event:\n        content = [{'kind': part.pop('type'), **part} for part in self.otel_message_parts(settings)]\n        for part in content:\n            if part['kind'] == 'binary' and 'content' in part:\n                part['binary_content'] = part.pop('content')\n        content = [\n            part['content'] if part == {'kind': 'text', 'content': part.get('content')} else part for part in content\n        ]\n        if content in ([{'kind': 'text'}], [self.content]):\n            content = content[0]\n        return Event('gen_ai.user.message', body={'content': content, 'role': 'user'})\n\n    def otel_message_parts(self, settings: InstrumentationSettings) -> list[_otel_messages.MessagePart]:\n        parts: list[_otel_messages.MessagePart] = []\n        content: Sequence[UserContent] = [self.content] if isinstance(self.content, str) else self.content\n        for part in content:\n            if isinstance(part, str):\n                parts.append(\n                    _otel_messages.TextPart(type='text', **({'content': part} if settings.include_content else {}))\n                )\n            elif isinstance(part, ImageUrl | AudioUrl | DocumentUrl | VideoUrl):\n                parts.append(\n                    _otel_messages.MediaUrlPart(\n                        type=part.kind,\n                        **{'url': part.url} if settings.include_content else {},\n                    )\n                )\n            elif isinstance(part, BinaryContent):\n                converted_part = _otel_messages.BinaryDataPart(type='binary', media_type=part.media_type)\n                if settings.include_content and settings.include_binary_content:\n                    converted_part['content'] = base64.b64encode(part.data).decode()\n                parts.append(converted_part)\n            else:\n                parts.append({'type': part.kind})  # pragma: no cover\n        return parts\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n\ntool_return_ta: pydantic.TypeAdapter[Any] = pydantic.TypeAdapter(\n    Any, config=pydantic.ConfigDict(defer_build=True, ser_json_bytes='base64', val_json_bytes='base64')\n)\n\n\n@dataclass(repr=False)\nclass BaseToolReturnPart:\n    \"\"\"Base class for tool return parts.\"\"\"\n\n    tool_name: str\n    \"\"\"The name of the \"tool\" was called.\"\"\"\n\n    content: Any\n    \"\"\"The return value.\"\"\"\n\n    tool_call_id: str = field(default_factory=_generate_tool_call_id)\n    \"\"\"The tool call identifier, this is used by some models including OpenAI.\n\n    In case the tool call id is not provided by the model, Pydantic AI will generate a random one.\n    \"\"\"\n\n    _: KW_ONLY\n\n    metadata: Any = None\n    \"\"\"Additional data that can be accessed programmatically by the application but is not sent to the LLM.\"\"\"\n\n    timestamp: datetime = field(default_factory=_now_utc)\n    \"\"\"The timestamp, when the tool returned.\"\"\"\n\n    def model_response_str(self) -> str:\n        \"\"\"Return a string representation of the content for the model.\"\"\"\n        if isinstance(self.content, str):\n            return self.content\n        else:\n            return tool_return_ta.dump_json(self.content).decode()\n\n    def model_response_object(self) -> dict[str, Any]:\n        \"\"\"Return a dictionary representation of the content, wrapping non-dict types appropriately.\"\"\"\n        # gemini supports JSON dict return values, but no other JSON types, hence we wrap anything else in a dict\n        if isinstance(self.content, dict):\n            return tool_return_ta.dump_python(self.content, mode='json')  # pyright: ignore[reportUnknownMemberType]\n        else:\n            return {'return_value': tool_return_ta.dump_python(self.content, mode='json')}\n\n    def otel_event(self, settings: InstrumentationSettings) -> Event:\n        return Event(\n            'gen_ai.tool.message',\n            body={\n                **({'content': self.content} if settings.include_content else {}),\n                'role': 'tool',\n                'id': self.tool_call_id,\n                'name': self.tool_name,\n            },\n        )\n\n    def otel_message_parts(self, settings: InstrumentationSettings) -> list[_otel_messages.MessagePart]:\n        from .models.instrumented import InstrumentedModel\n\n        part = _otel_messages.ToolCallResponsePart(\n            type='tool_call_response',\n            id=self.tool_call_id,\n            name=self.tool_name,\n        )\n\n        if settings.include_content and self.content is not None:\n            part['result'] = InstrumentedModel.serialize_any(self.content)\n\n        return [part]\n\n    def has_content(self) -> bool:\n        \"\"\"Return `True` if the tool return has content.\"\"\"\n        return self.content is not None  # pragma: no cover\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n\n@dataclass(repr=False)\nclass ToolReturnPart(BaseToolReturnPart):\n    \"\"\"A tool return message, this encodes the result of running a tool.\"\"\"\n\n    _: KW_ONLY\n\n    part_kind: Literal['tool-return'] = 'tool-return'\n    \"\"\"Part type identifier, this is available on all parts as a discriminator.\"\"\"\n\n\n@dataclass(repr=False)\nclass BuiltinToolReturnPart(BaseToolReturnPart):\n    \"\"\"A tool return message from a built-in tool.\"\"\"\n\n    _: KW_ONLY\n\n    provider_name: str | None = None\n    \"\"\"The name of the provider that generated the response.\"\"\"\n\n    part_kind: Literal['builtin-tool-return'] = 'builtin-tool-return'\n    \"\"\"Part type identifier, this is available on all parts as a discriminator.\"\"\"\n\n\nerror_details_ta = pydantic.TypeAdapter(list[pydantic_core.ErrorDetails], config=pydantic.ConfigDict(defer_build=True))\n\n\n@dataclass(repr=False)\nclass RetryPromptPart:\n    \"\"\"A message back to a model asking it to try again.\n\n    This can be sent for a number of reasons:\n\n    * Pydantic validation of tool arguments failed, here content is derived from a Pydantic\n      [`ValidationError`][pydantic_core.ValidationError]\n    * a tool raised a [`ModelRetry`][pydantic_ai.exceptions.ModelRetry] exception\n    * no tool was found for the tool name\n    * the model returned plain text when a structured response was expected\n    * Pydantic validation of a structured response failed, here content is derived from a Pydantic\n      [`ValidationError`][pydantic_core.ValidationError]\n    * an output validator raised a [`ModelRetry`][pydantic_ai.exceptions.ModelRetry] exception\n    \"\"\"\n\n    content: list[pydantic_core.ErrorDetails] | str\n    \"\"\"Details of why and how the model should retry.\n\n    If the retry was triggered by a [`ValidationError`][pydantic_core.ValidationError], this will be a list of\n    error details.\n    \"\"\"\n\n    _: KW_ONLY\n\n    tool_name: str | None = None\n    \"\"\"The name of the tool that was called, if any.\"\"\"\n\n    tool_call_id: str = field(default_factory=_generate_tool_call_id)\n    \"\"\"The tool call identifier, this is used by some models including OpenAI.\n\n    In case the tool call id is not provided by the model, Pydantic AI will generate a random one.\n    \"\"\"\n\n    timestamp: datetime = field(default_factory=_now_utc)\n    \"\"\"The timestamp, when the retry was triggered.\"\"\"\n\n    part_kind: Literal['retry-prompt'] = 'retry-prompt'\n    \"\"\"Part type identifier, this is available on all parts as a discriminator.\"\"\"\n\n    def model_response(self) -> str:\n        \"\"\"Return a string message describing why the retry is requested.\"\"\"\n        if isinstance(self.content, str):\n            if self.tool_name is None:\n                description = f'Validation feedback:\\n{self.content}'\n            else:\n                description = self.content\n        else:\n            json_errors = error_details_ta.dump_json(self.content, exclude={'__all__': {'ctx'}}, indent=2)\n            description = f'{len(self.content)} validation errors: {json_errors.decode()}'\n        return f'{description}\\n\\nFix the errors and try again.'\n\n    def otel_event(self, settings: InstrumentationSettings) -> Event:\n        if self.tool_name is None:\n            return Event('gen_ai.user.message', body={'content': self.model_response(), 'role': 'user'})\n        else:\n            return Event(\n                'gen_ai.tool.message',\n                body={\n                    **({'content': self.model_response()} if settings.include_content else {}),\n                    'role': 'tool',\n                    'id': self.tool_call_id,\n                    'name': self.tool_name,\n                },\n            )\n\n    def otel_message_parts(self, settings: InstrumentationSettings) -> list[_otel_messages.MessagePart]:\n        if self.tool_name is None:\n            return [_otel_messages.TextPart(type='text', content=self.model_response())]\n        else:\n            part = _otel_messages.ToolCallResponsePart(\n                type='tool_call_response',\n                id=self.tool_call_id,\n                name=self.tool_name,\n            )\n\n            if settings.include_content:\n                part['result'] = self.model_response()\n\n            return [part]\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n\nModelRequestPart = Annotated[\n    SystemPromptPart | UserPromptPart | ToolReturnPart | RetryPromptPart, pydantic.Discriminator('part_kind')\n]\n\"\"\"A message part sent by Pydantic AI to a model.\"\"\"\n\n\n@dataclass(repr=False)\nclass ModelRequest:\n    \"\"\"A request generated by Pydantic AI and sent to a model, e.g. a message from the Pydantic AI app to the model.\"\"\"\n\n    parts: Sequence[ModelRequestPart]\n    \"\"\"The parts of the user message.\"\"\"\n\n    _: KW_ONLY\n\n    instructions: str | None = None\n    \"\"\"The instructions for the model.\"\"\"\n\n    kind: Literal['request'] = 'request'\n    \"\"\"Message type identifier, this is available on all parts as a discriminator.\"\"\"\n\n    @classmethod\n    def user_text_prompt(cls, user_prompt: str, *, instructions: str | None = None) -> ModelRequest:\n        \"\"\"Create a `ModelRequest` with a single user prompt as text.\"\"\"\n        return cls(parts=[UserPromptPart(user_prompt)], instructions=instructions)\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n\n@dataclass(repr=False)\nclass TextPart:\n    \"\"\"A plain text response from a model.\"\"\"\n\n    content: str\n    \"\"\"The text content of the response.\"\"\"\n\n    _: KW_ONLY\n\n    id: str | None = None\n    \"\"\"An optional identifier of the text part.\"\"\"\n\n    part_kind: Literal['text'] = 'text'\n    \"\"\"Part type identifier, this is available on all parts as a discriminator.\"\"\"\n\n    def has_content(self) -> bool:\n        \"\"\"Return `True` if the text content is non-empty.\"\"\"\n        return bool(self.content)\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n\n@dataclass(repr=False)\nclass ThinkingPart:\n    \"\"\"A thinking response from a model.\"\"\"\n\n    content: str\n    \"\"\"The thinking content of the response.\"\"\"\n\n    _: KW_ONLY\n\n    id: str | None = None\n    \"\"\"The identifier of the thinking part.\"\"\"\n\n    signature: str | None = None\n    \"\"\"The signature of the thinking.\n\n    Supported by:\n\n    * Anthropic (corresponds to the `signature` field)\n    * Bedrock (corresponds to the `signature` field)\n    * Google (corresponds to the `thought_signature` field)\n    * OpenAI (corresponds to the `encrypted_content` field)\n    \"\"\"\n\n    provider_name: str | None = None\n    \"\"\"The name of the provider that generated the response.\n\n    Signatures are only sent back to the same provider.\n    \"\"\"\n\n    part_kind: Literal['thinking'] = 'thinking'\n    \"\"\"Part type identifier, this is available on all parts as a discriminator.\"\"\"\n\n    def has_content(self) -> bool:\n        \"\"\"Return `True` if the thinking content is non-empty.\"\"\"\n        return bool(self.content)\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n\n@dataclass(repr=False)\nclass BaseToolCallPart:\n    \"\"\"A tool call from a model.\"\"\"\n\n    tool_name: str\n    \"\"\"The name of the tool to call.\"\"\"\n\n    args: str | dict[str, Any] | None = None\n    \"\"\"The arguments to pass to the tool.\n\n    This is stored either as a JSON string or a Python dictionary depending on how data was received.\n    \"\"\"\n\n    tool_call_id: str = field(default_factory=_generate_tool_call_id)\n    \"\"\"The tool call identifier, this is used by some models including OpenAI.\n\n    In case the tool call id is not provided by the model, Pydantic AI will generate a random one.\n    \"\"\"\n\n    def args_as_dict(self) -> dict[str, Any]:\n        \"\"\"Return the arguments as a Python dictionary.\n\n        This is just for convenience with models that require dicts as input.\n        \"\"\"\n        if not self.args:\n            return {}\n        if isinstance(self.args, dict):\n            return self.args\n        args = pydantic_core.from_json(self.args)\n        assert isinstance(args, dict), 'args should be a dict'\n        return cast(dict[str, Any], args)\n\n    def args_as_json_str(self) -> str:\n        \"\"\"Return the arguments as a JSON string.\n\n        This is just for convenience with models that require JSON strings as input.\n        \"\"\"\n        if not self.args:\n            return '{}'\n        if isinstance(self.args, str):\n            return self.args\n        return pydantic_core.to_json(self.args).decode()\n\n    def has_content(self) -> bool:\n        \"\"\"Return `True` if the arguments contain any data.\"\"\"\n        if isinstance(self.args, dict):\n            # TODO: This should probably return True if you have the value False, or 0, etc.\n            #   It makes sense to me to ignore empty strings, but not sure about empty lists or dicts\n            return any(self.args.values())\n        else:\n            return bool(self.args)\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n\n@dataclass(repr=False)\nclass ToolCallPart(BaseToolCallPart):\n    \"\"\"A tool call from a model.\"\"\"\n\n    _: KW_ONLY\n\n    part_kind: Literal['tool-call'] = 'tool-call'\n    \"\"\"Part type identifier, this is available on all parts as a discriminator.\"\"\"\n\n\n@dataclass(repr=False)\nclass BuiltinToolCallPart(BaseToolCallPart):\n    \"\"\"A tool call to a built-in tool.\"\"\"\n\n    _: KW_ONLY\n\n    provider_name: str | None = None\n    \"\"\"The name of the provider that generated the response.\n\n    Built-in tool calls are only sent back to the same provider.\n    \"\"\"\n\n    part_kind: Literal['builtin-tool-call'] = 'builtin-tool-call'\n    \"\"\"Part type identifier, this is available on all parts as a discriminator.\"\"\"\n\n\nModelResponsePart = Annotated[\n    TextPart | ToolCallPart | BuiltinToolCallPart | BuiltinToolReturnPart | ThinkingPart,\n    pydantic.Discriminator('part_kind'),\n]\n\"\"\"A message part returned by a model.\"\"\"\n\n\n@dataclass(repr=False)\nclass ModelResponse:\n    \"\"\"A response from a model, e.g. a message from the model to the Pydantic AI app.\"\"\"\n\n    parts: Sequence[ModelResponsePart]\n    \"\"\"The parts of the model message.\"\"\"\n\n    _: KW_ONLY\n\n    usage: RequestUsage = field(default_factory=RequestUsage)\n    \"\"\"Usage information for the request.\n\n    This has a default to make tests easier, and to support loading old messages where usage will be missing.\n    \"\"\"\n\n    model_name: str | None = None\n    \"\"\"The name of the model that generated the response.\"\"\"\n\n    timestamp: datetime = field(default_factory=_now_utc)\n    \"\"\"The timestamp of the response.\n\n    If the model provides a timestamp in the response (as OpenAI does) that will be used.\n    \"\"\"\n\n    kind: Literal['response'] = 'response'\n    \"\"\"Message type identifier, this is available on all parts as a discriminator.\"\"\"\n\n    provider_name: str | None = None\n    \"\"\"The name of the LLM provider that generated the response.\"\"\"\n\n    provider_details: Annotated[\n        dict[str, Any] | None,\n        # `vendor_details` is deprecated, but we still want to support deserializing model responses stored in a DB before the name was changed\n        pydantic.Field(validation_alias=pydantic.AliasChoices('provider_details', 'vendor_details')),\n    ] = None\n    \"\"\"Additional provider-specific details in a serializable format.\n\n    This allows storing selected vendor-specific data that isn't mapped to standard ModelResponse fields.\n    For OpenAI models, this may include 'logprobs', 'finish_reason', etc.\n    \"\"\"\n\n    provider_response_id: Annotated[\n        str | None,\n        # `vendor_id` is deprecated, but we still want to support deserializing model responses stored in a DB before the name was changed\n        pydantic.Field(validation_alias=pydantic.AliasChoices('provider_response_id', 'vendor_id')),\n    ] = None\n    \"\"\"request ID as specified by the model provider. This can be used to track the specific request to the model.\"\"\"\n\n    finish_reason: FinishReason | None = None\n    \"\"\"Reason the model finished generating the response, normalized to OpenTelemetry values.\"\"\"\n\n    @deprecated('`price` is deprecated, use `cost` instead')\n    def price(self) -> genai_types.PriceCalculation:  # pragma: no cover\n        return self.cost()\n\n    def cost(self) -> genai_types.PriceCalculation:\n        \"\"\"Calculate the cost of the usage.\n\n        Uses [`genai-prices`](https://github.com/pydantic/genai-prices).\n        \"\"\"\n        assert self.model_name, 'Model name is required to calculate price'\n        return calc_price(\n            self.usage,\n            self.model_name,\n            provider_id=self.provider_name,\n            genai_request_timestamp=self.timestamp,\n        )\n\n    def otel_events(self, settings: InstrumentationSettings) -> list[Event]:\n        \"\"\"Return OpenTelemetry events for the response.\"\"\"\n        result: list[Event] = []\n\n        def new_event_body():\n            new_body: dict[str, Any] = {'role': 'assistant'}\n            ev = Event('gen_ai.assistant.message', body=new_body)\n            result.append(ev)\n            return new_body\n\n        body = new_event_body()\n        for part in self.parts:\n            if isinstance(part, ToolCallPart):\n                body.setdefault('tool_calls', []).append(\n                    {\n                        'id': part.tool_call_id,\n                        'type': 'function',\n                        'function': {\n                            'name': part.tool_name,\n                            **({'arguments': part.args} if settings.include_content else {}),\n                        },\n                    }\n                )\n            elif isinstance(part, TextPart | ThinkingPart):\n                kind = part.part_kind\n                body.setdefault('content', []).append(\n                    {'kind': kind, **({'text': part.content} if settings.include_content else {})}\n                )\n\n        if content := body.get('content'):\n            text_content = content[0].get('text')\n            if content == [{'kind': 'text', 'text': text_content}]:\n                body['content'] = text_content\n\n        return result\n\n    def otel_message_parts(self, settings: InstrumentationSettings) -> list[_otel_messages.MessagePart]:\n        parts: list[_otel_messages.MessagePart] = []\n        for part in self.parts:\n            if isinstance(part, TextPart):\n                parts.append(\n                    _otel_messages.TextPart(\n                        type='text',\n                        **({'content': part.content} if settings.include_content else {}),\n                    )\n                )\n            elif isinstance(part, ThinkingPart):\n                parts.append(\n                    _otel_messages.ThinkingPart(\n                        type='thinking',\n                        **({'content': part.content} if settings.include_content else {}),\n                    )\n                )\n            elif isinstance(part, BaseToolCallPart):\n                call_part = _otel_messages.ToolCallPart(type='tool_call', id=part.tool_call_id, name=part.tool_name)\n                if isinstance(part, BuiltinToolCallPart):\n                    call_part['builtin'] = True\n                if settings.include_content and part.args is not None:\n                    from .models.instrumented import InstrumentedModel\n\n                    if isinstance(part.args, str):\n                        call_part['arguments'] = part.args\n                    else:\n                        call_part['arguments'] = {k: InstrumentedModel.serialize_any(v) for k, v in part.args.items()}\n\n                parts.append(call_part)\n            elif isinstance(part, BuiltinToolReturnPart):\n                return_part = _otel_messages.ToolCallResponsePart(\n                    type='tool_call_response',\n                    id=part.tool_call_id,\n                    name=part.tool_name,\n                    builtin=True,\n                )\n                if settings.include_content and part.content is not None:  # pragma: no branch\n                    from .models.instrumented import InstrumentedModel\n\n                    return_part['result'] = InstrumentedModel.serialize_any(part.content)\n\n                parts.append(return_part)\n        return parts\n\n    @property\n    @deprecated('`vendor_details` is deprecated, use `provider_details` instead')\n    def vendor_details(self) -> dict[str, Any] | None:\n        return self.provider_details\n\n    @property\n    @deprecated('`vendor_id` is deprecated, use `provider_response_id` instead')\n    def vendor_id(self) -> str | None:\n        return self.provider_response_id\n\n    @property\n    @deprecated('`provider_request_id` is deprecated, use `provider_response_id` instead')\n    def provider_request_id(self) -> str | None:\n        return self.provider_response_id\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n\nModelMessage = Annotated[ModelRequest | ModelResponse, pydantic.Discriminator('kind')]\n\"\"\"Any message sent to or returned by a model.\"\"\"\n\nModelMessagesTypeAdapter = pydantic.TypeAdapter(\n    list[ModelMessage], config=pydantic.ConfigDict(defer_build=True, ser_json_bytes='base64', val_json_bytes='base64')\n)\n\"\"\"Pydantic [`TypeAdapter`][pydantic.type_adapter.TypeAdapter] for (de)serializing messages.\"\"\"\n\n\n@dataclass(repr=False)\nclass TextPartDelta:\n    \"\"\"A partial update (delta) for a `TextPart` to append new text content.\"\"\"\n\n    content_delta: str\n    \"\"\"The incremental text content to add to the existing `TextPart` content.\"\"\"\n\n    _: KW_ONLY\n\n    part_delta_kind: Literal['text'] = 'text'\n    \"\"\"Part delta type identifier, used as a discriminator.\"\"\"\n\n    def apply(self, part: ModelResponsePart) -> TextPart:\n        \"\"\"Apply this text delta to an existing `TextPart`.\n\n        Args:\n            part: The existing model response part, which must be a `TextPart`.\n\n        Returns:\n            A new `TextPart` with updated text content.\n\n        Raises:\n            ValueError: If `part` is not a `TextPart`.\n        \"\"\"\n        if not isinstance(part, TextPart):\n            raise ValueError('Cannot apply TextPartDeltas to non-TextParts')  # pragma: no cover\n        return replace(part, content=part.content + self.content_delta)\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n\n@dataclass(repr=False, kw_only=True)\nclass ThinkingPartDelta:\n    \"\"\"A partial update (delta) for a `ThinkingPart` to append new thinking content.\"\"\"\n\n    content_delta: str | None = None\n    \"\"\"The incremental thinking content to add to the existing `ThinkingPart` content.\"\"\"\n\n    signature_delta: str | None = None\n    \"\"\"Optional signature delta.\n\n    Note this is never treated as a delta — it can replace None.\n    \"\"\"\n\n    provider_name: str | None = None\n    \"\"\"Optional provider name for the thinking part.\n\n    Signatures are only sent back to the same provider.\n    \"\"\"\n\n    part_delta_kind: Literal['thinking'] = 'thinking'\n    \"\"\"Part delta type identifier, used as a discriminator.\"\"\"\n\n    @overload\n    def apply(self, part: ModelResponsePart) -> ThinkingPart: ...\n\n    @overload\n    def apply(self, part: ModelResponsePart | ThinkingPartDelta) -> ThinkingPart | ThinkingPartDelta: ...\n\n    def apply(self, part: ModelResponsePart | ThinkingPartDelta) -> ThinkingPart | ThinkingPartDelta:\n        \"\"\"Apply this thinking delta to an existing `ThinkingPart`.\n\n        Args:\n            part: The existing model response part, which must be a `ThinkingPart`.\n\n        Returns:\n            A new `ThinkingPart` with updated thinking content.\n\n        Raises:\n            ValueError: If `part` is not a `ThinkingPart`.\n        \"\"\"\n        if isinstance(part, ThinkingPart):\n            new_content = part.content + self.content_delta if self.content_delta else part.content\n            new_signature = self.signature_delta if self.signature_delta is not None else part.signature\n            new_provider_name = self.provider_name if self.provider_name is not None else part.provider_name\n            return replace(part, content=new_content, signature=new_signature, provider_name=new_provider_name)\n        elif isinstance(part, ThinkingPartDelta):\n            if self.content_delta is None and self.signature_delta is None:\n                raise ValueError('Cannot apply ThinkingPartDelta with no content or signature')\n            if self.content_delta is not None:\n                part = replace(part, content_delta=(part.content_delta or '') + self.content_delta)\n            if self.signature_delta is not None:\n                part = replace(part, signature_delta=self.signature_delta)\n            if self.provider_name is not None:\n                part = replace(part, provider_name=self.provider_name)\n            return part\n        raise ValueError(  # pragma: no cover\n            f'Cannot apply ThinkingPartDeltas to non-ThinkingParts or non-ThinkingPartDeltas ({part=}, {self=})'\n        )\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n\n@dataclass(repr=False, kw_only=True)\nclass ToolCallPartDelta:\n    \"\"\"A partial update (delta) for a `ToolCallPart` to modify tool name, arguments, or tool call ID.\"\"\"\n\n    tool_name_delta: str | None = None\n    \"\"\"Incremental text to add to the existing tool name, if any.\"\"\"\n\n    args_delta: str | dict[str, Any] | None = None\n    \"\"\"Incremental data to add to the tool arguments.\n\n    If this is a string, it will be appended to existing JSON arguments.\n    If this is a dict, it will be merged with existing dict arguments.\n    \"\"\"\n\n    tool_call_id: str | None = None\n    \"\"\"Optional tool call identifier, this is used by some models including OpenAI.\n\n    Note this is never treated as a delta — it can replace None, but otherwise if a\n    non-matching value is provided an error will be raised.\"\"\"\n\n    part_delta_kind: Literal['tool_call'] = 'tool_call'\n    \"\"\"Part delta type identifier, used as a discriminator.\"\"\"\n\n    def as_part(self) -> ToolCallPart | None:\n        \"\"\"Convert this delta to a fully formed `ToolCallPart` if possible, otherwise return `None`.\n\n        Returns:\n            A `ToolCallPart` if `tool_name_delta` is set, otherwise `None`.\n        \"\"\"\n        if self.tool_name_delta is None:\n            return None\n\n        return ToolCallPart(self.tool_name_delta, self.args_delta, self.tool_call_id or _generate_tool_call_id())\n\n    @overload\n    def apply(self, part: ModelResponsePart) -> ToolCallPart | BuiltinToolCallPart: ...\n\n    @overload\n    def apply(\n        self, part: ModelResponsePart | ToolCallPartDelta\n    ) -> ToolCallPart | BuiltinToolCallPart | ToolCallPartDelta: ...\n\n    def apply(\n        self, part: ModelResponsePart | ToolCallPartDelta\n    ) -> ToolCallPart | BuiltinToolCallPart | ToolCallPartDelta:\n        \"\"\"Apply this delta to a part or delta, returning a new part or delta with the changes applied.\n\n        Args:\n            part: The existing model response part or delta to update.\n\n        Returns:\n            Either a new `ToolCallPart` or `BuiltinToolCallPart`, or an updated `ToolCallPartDelta`.\n\n        Raises:\n            ValueError: If `part` is neither a `ToolCallPart`, `BuiltinToolCallPart`, nor a `ToolCallPartDelta`.\n            UnexpectedModelBehavior: If applying JSON deltas to dict arguments or vice versa.\n        \"\"\"\n        if isinstance(part, ToolCallPart | BuiltinToolCallPart):\n            return self._apply_to_part(part)\n\n        if isinstance(part, ToolCallPartDelta):\n            return self._apply_to_delta(part)\n\n        raise ValueError(  # pragma: no cover\n            f'Can only apply ToolCallPartDeltas to ToolCallParts, BuiltinToolCallParts, or ToolCallPartDeltas, not {part}'\n        )\n\n    def _apply_to_delta(self, delta: ToolCallPartDelta) -> ToolCallPart | BuiltinToolCallPart | ToolCallPartDelta:\n        \"\"\"Internal helper to apply this delta to another delta.\"\"\"\n        if self.tool_name_delta:\n            # Append incremental text to the existing tool_name_delta\n            updated_tool_name_delta = (delta.tool_name_delta or '') + self.tool_name_delta\n            delta = replace(delta, tool_name_delta=updated_tool_name_delta)\n\n        if isinstance(self.args_delta, str):\n            if isinstance(delta.args_delta, dict):\n                raise UnexpectedModelBehavior(\n                    f'Cannot apply JSON deltas to non-JSON tool arguments ({delta=}, {self=})'\n                )\n            updated_args_delta = (delta.args_delta or '') + self.args_delta\n            delta = replace(delta, args_delta=updated_args_delta)\n        elif isinstance(self.args_delta, dict):\n            if isinstance(delta.args_delta, str):\n                raise UnexpectedModelBehavior(\n                    f'Cannot apply dict deltas to non-dict tool arguments ({delta=}, {self=})'\n                )\n            updated_args_delta = {**(delta.args_delta or {}), **self.args_delta}\n            delta = replace(delta, args_delta=updated_args_delta)\n\n        if self.tool_call_id:\n            delta = replace(delta, tool_call_id=self.tool_call_id)\n\n        # If we now have enough data to create a full ToolCallPart, do so\n        if delta.tool_name_delta is not None:\n            return ToolCallPart(delta.tool_name_delta, delta.args_delta, delta.tool_call_id or _generate_tool_call_id())\n\n        return delta\n\n    def _apply_to_part(self, part: ToolCallPart | BuiltinToolCallPart) -> ToolCallPart | BuiltinToolCallPart:\n        \"\"\"Internal helper to apply this delta directly to a `ToolCallPart` or `BuiltinToolCallPart`.\"\"\"\n        if self.tool_name_delta:\n            # Append incremental text to the existing tool_name\n            tool_name = part.tool_name + self.tool_name_delta\n            part = replace(part, tool_name=tool_name)\n\n        if isinstance(self.args_delta, str):\n            if isinstance(part.args, dict):\n                raise UnexpectedModelBehavior(f'Cannot apply JSON deltas to non-JSON tool arguments ({part=}, {self=})')\n            updated_json = (part.args or '') + self.args_delta\n            part = replace(part, args=updated_json)\n        elif isinstance(self.args_delta, dict):\n            if isinstance(part.args, str):\n                raise UnexpectedModelBehavior(f'Cannot apply dict deltas to non-dict tool arguments ({part=}, {self=})')\n            updated_dict = {**(part.args or {}), **self.args_delta}\n            part = replace(part, args=updated_dict)\n\n        if self.tool_call_id:\n            part = replace(part, tool_call_id=self.tool_call_id)\n        return part\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n\nModelResponsePartDelta = Annotated[\n    TextPartDelta | ThinkingPartDelta | ToolCallPartDelta, pydantic.Discriminator('part_delta_kind')\n]\n\"\"\"A partial update (delta) for any model response part.\"\"\"\n\n\n@dataclass(repr=False, kw_only=True)\nclass PartStartEvent:\n    \"\"\"An event indicating that a new part has started.\n\n    If multiple `PartStartEvent`s are received with the same index,\n    the new one should fully replace the old one.\n    \"\"\"\n\n    index: int\n    \"\"\"The index of the part within the overall response parts list.\"\"\"\n\n    part: ModelResponsePart\n    \"\"\"The newly started `ModelResponsePart`.\"\"\"\n\n    event_kind: Literal['part_start'] = 'part_start'\n    \"\"\"Event type identifier, used as a discriminator.\"\"\"\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n\n@dataclass(repr=False, kw_only=True)\nclass PartDeltaEvent:\n    \"\"\"An event indicating a delta update for an existing part.\"\"\"\n\n    index: int\n    \"\"\"The index of the part within the overall response parts list.\"\"\"\n\n    delta: ModelResponsePartDelta\n    \"\"\"The delta to apply to the specified part.\"\"\"\n\n    event_kind: Literal['part_delta'] = 'part_delta'\n    \"\"\"Event type identifier, used as a discriminator.\"\"\"\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n\n@dataclass(repr=False, kw_only=True)\nclass FinalResultEvent:\n    \"\"\"An event indicating the response to the current model request matches the output schema and will produce a result.\"\"\"\n\n    tool_name: str | None\n    \"\"\"The name of the output tool that was called. `None` if the result is from text content and not from a tool.\"\"\"\n    tool_call_id: str | None\n    \"\"\"The tool call ID, if any, that this result is associated with.\"\"\"\n    event_kind: Literal['final_result'] = 'final_result'\n    \"\"\"Event type identifier, used as a discriminator.\"\"\"\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n\nModelResponseStreamEvent = Annotated[\n    PartStartEvent | PartDeltaEvent | FinalResultEvent, pydantic.Discriminator('event_kind')\n]\n\"\"\"An event in the model response stream, starting a new part, applying a delta to an existing one, or indicating the final result.\"\"\"\n\n\n@dataclass(repr=False)\nclass FunctionToolCallEvent:\n    \"\"\"An event indicating the start to a call to a function tool.\"\"\"\n\n    part: ToolCallPart\n    \"\"\"The (function) tool call to make.\"\"\"\n\n    _: KW_ONLY\n\n    event_kind: Literal['function_tool_call'] = 'function_tool_call'\n    \"\"\"Event type identifier, used as a discriminator.\"\"\"\n\n    @property\n    def tool_call_id(self) -> str:\n        \"\"\"An ID used for matching details about the call to its result.\"\"\"\n        return self.part.tool_call_id\n\n    @property\n    @deprecated('`call_id` is deprecated, use `tool_call_id` instead.')\n    def call_id(self) -> str:\n        \"\"\"An ID used for matching details about the call to its result.\"\"\"\n        return self.part.tool_call_id  # pragma: no cover\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n\n@dataclass(repr=False)\nclass FunctionToolResultEvent:\n    \"\"\"An event indicating the result of a function tool call.\"\"\"\n\n    result: ToolReturnPart | RetryPromptPart\n    \"\"\"The result of the call to the function tool.\"\"\"\n\n    _: KW_ONLY\n\n    event_kind: Literal['function_tool_result'] = 'function_tool_result'\n    \"\"\"Event type identifier, used as a discriminator.\"\"\"\n\n    @property\n    def tool_call_id(self) -> str:\n        \"\"\"An ID used to match the result to its original call.\"\"\"\n        return self.result.tool_call_id\n\n    __repr__ = _utils.dataclasses_no_defaults_repr\n\n\n@deprecated(\n    '`BuiltinToolCallEvent` is deprecated, look for `PartStartEvent` and `PartDeltaEvent` with `BuiltinToolCallPart` instead.'\n)\n@dataclass(repr=False)\nclass BuiltinToolCallEvent:\n    \"\"\"An event indicating the start to a call to a built-in tool.\"\"\"\n\n    part: BuiltinToolCallPart\n    \"\"\"The built-in tool call to make.\"\"\"\n\n    _: KW_ONLY\n\n    event_kind: Literal['builtin_tool_call'] = 'builtin_tool_call'\n    \"\"\"Event type identifier, used as a discriminator.\"\"\"\n\n\n@deprecated(\n    '`BuiltinToolResultEvent` is deprecated, look for `PartStartEvent` and `PartDeltaEvent` with `BuiltinToolReturnPart` instead.'\n)\n@dataclass(repr=False)\nclass BuiltinToolResultEvent:\n    \"\"\"An event indicating the result of a built-in tool call.\"\"\"\n\n    result: BuiltinToolReturnPart\n    \"\"\"The result of the call to the built-in tool.\"\"\"\n\n    _: KW_ONLY\n\n    event_kind: Literal['builtin_tool_result'] = 'builtin_tool_result'\n    \"\"\"Event type identifier, used as a discriminator.\"\"\"\n\n\nHandleResponseEvent = Annotated[\n    FunctionToolCallEvent\n    | FunctionToolResultEvent\n    | BuiltinToolCallEvent  # pyright: ignore[reportDeprecated]\n    | BuiltinToolResultEvent,  # pyright: ignore[reportDeprecated]\n    pydantic.Discriminator('event_kind'),\n]\n\"\"\"An event yielded when handling a model response, indicating tool calls and results.\"\"\"\n\nAgentStreamEvent = Annotated[ModelResponseStreamEvent | HandleResponseEvent, pydantic.Discriminator('event_kind')]\n\"\"\"An event in the agent stream: model response stream events and response-handling events.\"\"\"\n\n\n=== pydantic_ai_slim/pydantic_ai/settings.py ===\nfrom __future__ import annotations\n\nfrom httpx import Timeout\nfrom typing_extensions import TypedDict\n\n\nclass ModelSettings(TypedDict, total=False):\n    \"\"\"Settings to configure an LLM.\n\n    Here we include only settings which apply to multiple models / model providers,\n    though not all of these settings are supported by all models.\n    \"\"\"\n\n    max_tokens: int\n    \"\"\"The maximum number of tokens to generate before stopping.\n\n    Supported by:\n\n    * Gemini\n    * Anthropic\n    * OpenAI\n    * Groq\n    * Cohere\n    * Mistral\n    * Bedrock\n    * MCP Sampling\n    \"\"\"\n\n    temperature: float\n    \"\"\"Amount of randomness injected into the response.\n\n    Use `temperature` closer to `0.0` for analytical / multiple choice, and closer to a model's\n    maximum `temperature` for creative and generative tasks.\n\n    Note that even with `temperature` of `0.0`, the results will not be fully deterministic.\n\n    Supported by:\n\n    * Gemini\n    * Anthropic\n    * OpenAI\n    * Groq\n    * Cohere\n    * Mistral\n    * Bedrock\n    \"\"\"\n\n    top_p: float\n    \"\"\"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.\n\n    So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n\n    You should either alter `temperature` or `top_p`, but not both.\n\n    Supported by:\n\n    * Gemini\n    * Anthropic\n    * OpenAI\n    * Groq\n    * Cohere\n    * Mistral\n    * Bedrock\n    \"\"\"\n\n    timeout: float | Timeout\n    \"\"\"Override the client-level default timeout for a request, in seconds.\n\n    Supported by:\n\n    * Gemini\n    * Anthropic\n    * OpenAI\n    * Groq\n    * Mistral\n    \"\"\"\n\n    parallel_tool_calls: bool\n    \"\"\"Whether to allow parallel tool calls.\n\n    Supported by:\n\n    * OpenAI (some models, not o1)\n    * Groq\n    * Anthropic\n    \"\"\"\n\n    seed: int\n    \"\"\"The random seed to use for the model, theoretically allowing for deterministic results.\n\n    Supported by:\n\n    * OpenAI\n    * Groq\n    * Cohere\n    * Mistral\n    * Gemini\n    \"\"\"\n\n    presence_penalty: float\n    \"\"\"Penalize new tokens based on whether they have appeared in the text so far.\n\n    Supported by:\n\n    * OpenAI\n    * Groq\n    * Cohere\n    * Gemini\n    * Mistral\n    \"\"\"\n\n    frequency_penalty: float\n    \"\"\"Penalize new tokens based on their existing frequency in the text so far.\n\n    Supported by:\n\n    * OpenAI\n    * Groq\n    * Cohere\n    * Gemini\n    * Mistral\n    \"\"\"\n\n    logit_bias: dict[str, int]\n    \"\"\"Modify the likelihood of specified tokens appearing in the completion.\n\n    Supported by:\n\n    * OpenAI\n    * Groq\n    \"\"\"\n\n    stop_sequences: list[str]\n    \"\"\"Sequences that will cause the model to stop generating.\n\n    Supported by:\n\n    * OpenAI\n    * Anthropic\n    * Bedrock\n    * Mistral\n    * Groq\n    * Cohere\n    * Google\n    \"\"\"\n\n    extra_headers: dict[str, str]\n    \"\"\"Extra headers to send to the model.\n\n    Supported by:\n\n    * OpenAI\n    * Anthropic\n    * Groq\n    \"\"\"\n\n    extra_body: object\n    \"\"\"Extra body to send to the model.\n\n    Supported by:\n\n    * OpenAI\n    * Anthropic\n    * Groq\n    \"\"\"\n\n\ndef merge_model_settings(base: ModelSettings | None, overrides: ModelSettings | None) -> ModelSettings | None:\n    \"\"\"Merge two sets of model settings, preferring the overrides.\n\n    A common use case is: merge_model_settings(<agent settings>, <run settings>)\n    \"\"\"\n    # Note: we may want merge recursively if/when we add non-primitive values\n    if base and overrides:\n        return base | overrides\n    else:\n        return base or overrides\n\n",
  "file_count": 25,
  "total_size": 330371
}