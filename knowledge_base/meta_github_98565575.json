{
  "id": "meta_github_98565575",
  "title": "ReAgent",
  "description": "A platform for Reasoning systems (Reinforcement Learning, Contextual Bandits, etc.)",
  "url": "https://github.com/facebookresearch/ReAgent",
  "language": "Python",
  "stars": 3665,
  "forks": 525,
  "created_at": "2017-07-27T17:53:21Z",
  "updated_at": "2025-09-22T07:38:54Z",
  "topics": [],
  "readme_content": "![Banner](logo/reagent_banner.png)\n\n### ReAgent is officially archived and no longer maintained. For latest support on production-ready reinforcement learning open-source library, please refer to [Pearl](https://github.com/facebookresearch/Pearl/) - Production-ready Reinforcement Learning AI Agent Library, by the Applied Reinforcement Learning team @ Meta.\n\n[![Support Ukraine](https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&labelColor=005BBB)](https://opensource.fb.com/support-ukraine)\n[![License](https://img.shields.io/badge/license-BSD%203--Clause-brightgreen)](LICENSE)\n[![CircleCI](https://circleci.com/gh/facebookresearch/ReAgent/tree/main.svg?style=shield)](https://circleci.com/gh/facebookresearch/ReAgent/tree/main)\n[![codecov](https://codecov.io/gh/facebookresearch/ReAgent/branch/main/graph/badge.svg)](https://codecov.io/gh/facebookresearch/ReAgent)\n---\n\n### Overview\nReAgent is an open source end-to-end platform for applied reinforcement learning (RL) developed and used at Facebook. ReAgent is built in Python and uses PyTorch for modeling and training and TorchScript for model serving. The platform contains workflows to train popular deep RL algorithms and includes data preprocessing, feature transformation, distributed training, counterfactual policy evaluation, and optimized serving. For more detailed information about ReAgent see the release post [here](https://research.fb.com/publications/horizon-facebooks-open-source-applied-reinforcement-learning-platform/) and white paper [here](https://arxiv.org/abs/1811.00260).\n\nThe platform was once named \"Horizon\" but we have adopted the name \"ReAgent\" recently to emphasize its broader scope in decision making and reasoning.\n\n### Algorithms Supported\n\nClassic Off-Policy algorithms:\n- Discrete-Action [DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)\n- Parametric-Action DQN\n- [Double DQN](https://arxiv.org/abs/1509.06461), [Dueling DQN](https://arxiv.org/abs/1511.06581), [Dueling Double DQN](https://arxiv.org/abs/1710.02298)\n- Distributional RL: [C51](https://arxiv.org/abs/1707.06887) and [QR-DQN](https://arxiv.org/abs/1710.10044)\n- [Twin Delayed DDPG](https://arxiv.org/abs/1802.09477) (TD3)\n- [Soft Actor-Critic](https://arxiv.org/abs/1801.01290) (SAC)\n- [Critic Regularized Regression](https://arxiv.org/abs/2006.15134) (CRR)\n- [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347) (PPO)\n\nRL for recommender systems:\n- [Seq2Slate](https://arxiv.org/abs/1810.02019)\n- [SlateQ](https://arxiv.org/abs/1905.12767)\n\nCounterfactual Evaluation:\n- [Doubly Robust](https://arxiv.org/abs/1612.01205) (for bandits)\n- [Doubly Robust](https://arxiv.org/abs/1511.03722) (for sequential decisions)\n- [MAGIC](https://arxiv.org/abs/1604.00923)\n\nMulti-Arm and Contextual Bandits:\n- [UCB1](https://www.cs.bham.ac.uk/internal/courses/robotics/lectures/ucb1.pdf)\n- [MetricUCB](https://arxiv.org/abs/0809.4882)\n- [Thompson Sampling](https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf)\n- [LinUCB](https://arxiv.org/abs/1003.0146)\n\n\nOthers:\n- [Cross-Entropy Method](http://web.mit.edu/6.454/www/www_fall_2003/gew/CEtutorial.pdf)\n- [Synthetic Return for Credit Assignment](https://arxiv.org/abs/2102.12425)\n\n\n### Installation\nReAgent can be installed via. Docker or manually. Detailed instructions on how to install ReAgent can be found [here](docs/installation.rst).\n\n### Tutorial\nReAgent is designed for large-scale, distributed recommendation/optimization tasks where we don’t have access to a simulator.\nIn this environment, it is typically better to train offline on batches of data, and release new policies slowly over time.\nBecause the policy updates slowly and in batches, we use off-policy algorithms. To test a new policy without deploying it,\nwe rely on counter-factual policy evaluation (CPE), a set of techniques for estimating a policy based on the actions of another policy.\n\nWe also have a set of tools to facilitate applying RL in real-world applications:\n- Domain Analysis Tool, which analyzes state/action feature importance and identifies whether the problem is a suitable for applying batch RL\n- Behavior Cloning, which clones from the logging policy to bootstrap the learning policy safely\n\nDetailed instructions on how to use ReAgent can be found [here](docs/usage.rst).\n\n\n### License\nReAgent is released under a BSD 3-Clause license.  Find out more about it [here](LICENSE).\n\n[Terms of Use](https://opensource.facebook.com/legal/terms) | [Privacy Policy](https://opensource.facebook.com/legal/privacy) | Copyright © 2022 Meta Platforms, Inc\n\n\n### Citing\n```\n@article{gauci2018horizon,\n  title={Horizon: Facebook's Open Source Applied Reinforcement Learning Platform},\n  author={Gauci, Jason and Conti, Edoardo and Liang, Yitao and Virochsiri, Kittipat and Chen, Zhengxing and He, Yuchen and Kaden, Zachary and Narayanan, Vivek and Ye, Xiaohui},\n  journal={arXiv preprint arXiv:1811.00260},\n  year={2018}\n}\n```\n",
  "source_type": "meta_github_repository",
  "domain": "meta_ai",
  "keywords": [],
  "retrieval_tags": [
    "meta",
    "ai",
    "github",
    "repository",
    "research"
  ]
}