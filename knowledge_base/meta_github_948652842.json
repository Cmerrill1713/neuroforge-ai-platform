{
  "id": "meta_github_948652842",
  "title": "llama-prompt-ops",
  "description": "An open-source tool for general prompt optimization.",
  "url": "https://github.com/meta-llama/llama-prompt-ops",
  "language": "Python",
  "stars": 634,
  "forks": 81,
  "created_at": "2025-03-14T17:59:40Z",
  "updated_at": "2025-09-27T04:23:48Z",
  "topics": [],
  "readme_content": "<h1 align=\"center\"> Llama Prompt Ops </h1>\n\n## What is llama-prompt-ops?\n<p align=\"center\">\n  <a href=\"https://pypi.org/project/llama-prompt-ops/\"><img src=\"https://img.shields.io/pypi/v/llama-prompt-ops.svg\" /></a>\n</p>\n<p align=\"center\">\n  <a href=\"https://llama.developer.meta.com/?utm_source=llama-prompt-ops&utm_medium=readme&utm_campaign=main\"><img src=\"https://img.shields.io/badge/Llama_API-Join_Waitlist-brightgreen?logo=meta\" /></a>\n  <a href=\"https://llama.developer.meta.com/docs?utm_source=llama-prompt-ops&utm_medium=readme&utm_campaign=main\"><img src=\"https://img.shields.io/badge/Llama_API-Documentation-4BA9FE?logo=meta\" /></a>\n\n</p>\n\n<p align=\"center\">\n  <a href=\"https://github.com/meta-llama/llama-models/blob/main/models/?utm_source=llama-prompt-ops&utm_medium=readme&utm_campaign=main\"><img alt=\"Llama Model cards\" src=\"https://img.shields.io/badge/Llama_OSS-Model_cards-green?logo=meta\" /></a>\n  <a href=\"https://www.llama.com/docs/overview/?utm_source=llama-prompt-ops&utm_medium=readme&utm_campaign=main\"><img alt=\"Llama Documentation\" src=\"https://img.shields.io/badge/Llama_OSS-Documentation-4BA9FE?logo=meta\" /></a>\n  <a href=\"https://huggingface.co/meta-llama\"><img alt=\"Hugging Face meta-llama\" src=\"https://img.shields.io/badge/Hugging_Face-meta--llama-yellow?logo=huggingface\" /></a>\n\n</p>\n<p align=\"center\">\n  <a href=\"https://github.com/meta-llama/synthetic-data-kit\"><img alt=\"Llama Tools Syntethic Data Kit\" src=\"https://img.shields.io/badge/Llama_Tools-synthetic--data--kit-orange?logo=meta\" /></a>\n  <a href=\"https://github.com/meta-llama/llama-prompt-ops\"><img alt=\"Llama Tools Syntethic Data Kit\" src=\"https://img.shields.io/badge/Llama_Tools-llama--prompt--ops-orange?logo=meta\" /></a>\n    <a href=\"https://github.com/meta-llama/llama-cookbook\"><img alt=\"Llama Cookbook\" src=\"https://img.shields.io/badge/Llama_Cookbook-llama--cookbook-orange?logo=meta\" /></a>\n</p>\n\n\n\nllama-prompt-ops is a Python package that **automatically optimizes prompts** for Llama models. It transforms prompts that work well with other LLMs into prompts that are optimized for Llama models, improving performance and reliability.\n\n**Key Benefits:**\n- **No More Trial and Error**: Stop manually tweaking prompts to get better results\n- **Fast Optimization**: Get Llama-optimized prompts in minutes with template-based optimization\n- **Data-Driven Improvements**: Use your own examples to create prompts that work for your specific use case\n- **Measurable Results**: Evaluate prompt performance with customizable metrics\n\n## Requirements\n\nTo get started with llama-prompt-ops, you'll need:\n\n- Existing System Prompt: Your existing system prompt that you want to optimize\n- Existing Query-Response Dataset: A JSON file containing query-response pairs (as few as 50 examples) for evaluation and optimization (see [prepare your dataset](#preparing-your-data) below)\n- Configuration File: A YAML configuration file (config.yaml) specifying model hyperparameters, and optimization details (see [example configuration](configs/facility-simple.yaml))\n\n## How It Works\n\n```\n┌──────────────────────────┐  ┌──────────────────────────┐  ┌────────────────────┐\n│  Existing System Prompt  │  │  set(query, responses)   │  │ YAML Configuration │\n└────────────┬─────────────┘  └─────────────┬────────────┘  └───────────┬────────┘\n             │                              │                           │\n             │                              │                           │\n             ▼                              ▼                           ▼\n         ┌────────────────────────────────────────────────────────────────────┐\n         │                     llama-prompt-ops migrate                       │\n         └────────────────────────────────────────────────────────────────────┘\n                                            │\n                                            │\n                                            ▼\n                                ┌──────────────────────┐\n                                │   Optimized Prompt   │\n                                └──────────────────────┘\n```\n\n### Simple Workflow\n\n1. **Start with your existing system prompt**: Take your existing system prompt that works with other LLMs (see [example prompt](use-cases/facility-support-analyzer/facility_prompt_sys.txt))\n2. [**Prepare your dataset**](#preparing-your-data): Create a JSON file with query-response pairs for evaluation and optimization\n3. **Configure optimization**: Set up a simple YAML file with your dataset and preferences (see [example configuration](configs/facility-simple.yaml))\n4. [**Run optimization**](#step-4-run-optimization): Execute a single command to transform your prompt\n5. [**Get results**](#prompt-transformation-example): Receive a Llama-optimized prompt with performance metrics\n\n\n## Real-world Results\n\n### HotpotQA\n<table>\n<tr>\n<td width=\"100%\"><img src=\"./docs/_static/output-hotpotqa.png\" onerror=\"this.onerror=null;this.src='https://github.com/user-attachments/assets/52080f54-d1ca-4d21-8263-a9b2ee1d3c10'\" alt=\"HotpotQA Benchmark Results\"></td>\n</tr>\n</table>\n\nThese results were measured on the [HotpotQA multi-hop reasoning benchmark](https://hotpotqa.github.io/), which tests a model's ability to answer complex questions requiring information from multiple sources. Our optimized prompts showed substantial improvements over baseline prompts across different model sizes.\n\n\n## Quick Start (5 minutes)\n\n### Step 1: Installation\n\n```bash\n# Create a virtual environment\nconda create -n prompt-ops python=3.10\nconda activate prompt-ops\n\n# Install from PyPI\npip install llama-prompt-ops\n\n# OR install from source\ngit clone https://github.com/meta-llama/llama-prompt-ops.git\ncd llama-prompt-ops\npip install -e .\n\n```\n\n### Step 2: Create a sample project\n\nThis will create a directory called my-project with a sample configuration and dataset in the current folder.\n\n```bash\nllama-prompt-ops create my-project\ncd my-project\n```\n\n### Step 3: Set Up Your API Key\n\nAdd your API key to the `.env` file:\n\n```bash\nOPENROUTER_API_KEY=your_key_here\n```\nYou can get an OpenRouter API key by creating an account at [OpenRouter](https://openrouter.ai/). For more inference provider options, see [Inference Providers](./docs/inference_providers.md).\n\n### Step 4: Run Optimization\nThe optimization will take about 5 minutes.\n\n```bash\nllama-prompt-ops migrate # defaults to config.yaml if --config not specified\n```\n\nDone! The optimized prompt will be saved to the `results` directory with performance metrics comparing the original and optimized versions.\n\nTo read more about this use case, we go into more detail in [Basic Tutorial](./docs/basic/readme.md).\n\n\n### Prompt Transformation Example\n\nBelow is an example of a transformed system prompt from proprietary LM to Llama:\n\n| Original Proprietary LM Prompt | Optimized Llama Prompt |\n| --- | --- |\n| You are a helpful assistant. Extract and return a JSON with the following keys and values:<br><br>1. \"urgency\": one of `high`, `medium`, `low`<br>2. \"sentiment\": one of `negative`, `neutral`, `positive`<br>3. \"categories\": Create a dictionary with categories as keys and boolean values (True/False), where the value indicates whether the category matches tags like `emergency_repair_services`, `routine_maintenance_requests`, etc.<br><br>Your complete message should be a valid JSON string that can be read directly. | You are an expert in analyzing customer service messages. Your task is to categorize the following message based on urgency, sentiment, and relevant categories.<br><br>Analyze the message and return a JSON object with these fields:<br><br>1. \"urgency\": Classify as \"high\", \"medium\", or \"low\" based on how quickly this needs attention<br>2. \"sentiment\": Classify as \"negative\", \"neutral\", or \"positive\" based on the customer's tone<br>3. \"categories\": Create a dictionary with facility management categories as keys and boolean values<br><br>Only include these exact keys in your response. Return a valid JSON object without code blocks, prefixes, or explanations. |\n\n\n## Preparing Your Data\n\nTo use llama-prompt-ops for prompt optimization, you'll need to prepare a dataset with your prompts and expected responses. The standard format is a JSON file structured like this:\n\n```json\n[\n    {\n        \"question\": \"Your input query here\",\n        \"answer\": \"Expected response here\"\n    },\n    {\n        \"question\": \"Another input query\",\n        \"answer\": \"Another expected response\"\n    }\n]\n```\n\nIf your data matches this format, you can use the built-in [`StandardJSONAdapter`](src/llama_prompt_ops/core/datasets.py) which will handle it automatically.\n\n### Custom Data Formats\n\nIf your data is formatted differently, and there isn't a built-in dataset adapter, you can create a custom dataset adapter by extending the `DatasetAdapter` class. See the [Dataset Adapter Selection Guide](docs/dataset_adapter_selection_guide.md) for more details.\n\n## Multiple Inference Provider Support\n\nllama-prompt-ops supports various inference providers and endpoints to fit your infrastructure needs. See our [detailed guide on inference providers](./docs/inference_providers.md) for configuration examples with:\n\n- OpenRouter (cloud-based API)\n- vLLM (local deployment)\n- NVIDIA NIMs (optimized containers)\n\n## Documentation and Examples\n\nFor more detailed information, check out these resources:\n\n- [Quick Start Guide](docs/basic/readme.md): Get up and running with llama-prompt-ops in 5 minutes\n- [Intermediate Configuration Guide](docs/intermediate/readme.md): Learn how to configure datasets, metrics, and optimization strategies\n- [Dataset Adapter Selection Guide](docs/dataset_adapter_selection_guide.md): Choose the right adapter for your dataset format\n- [Metric Selection Guide](docs/metric_selection_guide.md): Select appropriate evaluation metrics for your use case\n- [Inference Providers Guide](docs/inference_providers.md): Configure different model providers and endpoints\n\n## Acknowledgements\nThis project leverages some of awesome open source projects including [DSPy](https://github.com/stanfordnlp/dspy), thanks to the team for the inspiring work!\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
  "source_type": "meta_github_repository",
  "domain": "meta_ai",
  "keywords": [
    "llama"
  ],
  "retrieval_tags": [
    "meta",
    "ai",
    "github",
    "repository",
    "research"
  ]
}