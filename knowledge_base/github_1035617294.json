{
  "id": "github_1035617294",
  "title": "nano-agent",
  "description": "A MCP Server for a small scale engineering agents with multi-provider LLM support.",
  "url": "https://github.com/disler/nano-agent",
  "language": "Python",
  "stars": 167,
  "forks": 49,
  "created_at": "2025-08-10T19:21:06Z",
  "updated_at": "2025-09-26T19:48:13Z",
  "topics": [],
  "readme_content": "# Nano Agent\n> Watched how we used GPT-5 and Claude Code with nano-agents [here](https://youtu.be/tcZ3W8QYirQ).\n\n**What?** A MCP Server for experimental, small scale engineering agents with multi-provider LLM support.\n\n**Why?** To test and compare **Agentic** Capabilities of Cloud and Local LLMs across Performance, Speed, and Cost.\n\n> \"It's not about a single prompt call anymore. It's about how well your agent chains together multiple tools to accomplish real engineering results on your behalf.\" - From our evaluation\n\n<img src=\"images/nano-agent.png\" alt=\"Nano Agent\" style=\"max-width: 800px;\">\n\n### 🎬 See It In Action\n\n**Multi-Model Evaluation Flow** - Watch 9 models (GPT-5, Claude Opus, Local GPT-OSS) running in parallel on the same M4 Max:\n<img src=\"images/multi-model-eval-flow.gif\" alt=\"Multi-Model Evaluation Flow\" style=\"max-width: 800px;\">\n\n**Model Comparison: GPT-5 vs Local Models** - Surprising results: GPT-OSS 20B/120B running on-device with $0.00 cost:\n<img src=\"images/model-comparison-gpt5-oss.gif\" alt=\"Model Comparison GPT-5 vs OSS\" style=\"max-width: 800px;\">\n\n### 🔥 Key Findings from Our Testing\n\n- **Surprising Winners**: GPT-5 Nano/Mini often outperform larger models when factoring in speed and cost\n- **Local Revolution**: GPT-OSS 20B/120B models complete real agentic coding tasks on M4 Max (128GB RAM)\n- **Cost Reality Check**: Claude Opus 4.1 is extraordinarily expensive - performance isn't everything\n- **The Trade-off Triangle**: Performance vs Speed vs Cost - you don't always need the most expensive model\n\n## Installation\n\n### Quick Install (Recommended)\n\n- Install [Astral UV](https://docs.astral.sh/uv/getting-started/installation/)\n- Setup [Claude Code](https://docs.anthropic.com/en/docs/claude-code/overview)\n- Setup [Ollama](https://ollama.com/)\n- Get your OpenAI API key and Anthropic API key\n- Setup dotenv\n  - `cp ./.env.sample ./.env` and fill out variables\n  - `cp ./apps/nano_agent_mcp_server/.env.sample ./apps/nano_agent_mcp_server/.env` and fill out variables\n- Clone the repository\n  - `git clone https://github.com/disler/nano-agent`\n- Global Install `nano-agent` to expose it for Claude Code (any mcp client)\n  - `cd nano-agent/apps/nano_agent_mcp_server`\n  - `./scripts/install.sh`\n  - `uv tool install -e .`\n- cp `.mcp.json.sample` to `.mcp.json` to use `nano-agent`\n- You should end up with a `.mcp.json` file that looks like this:\n```json\n{\n  \"mcpServers\": {\n    \"nano-agent\": {\n      \"command\": \"nano-agent\",\n      \"args\": []\n    }\n  }\n}\n```\n- You can also test without installing `nano-agent` globally by running it this directory with\n```json\n{\n  \"mcpServers\": {\n    \"nano-agent\": {\n      \"command\": \"uv\",\n      \"args\": [\"--directory\", \"apps/nano_agent_mcp_server\", \"run\", \"nano-agent\"]\n    }\n  }\n}\n```\n\nNow you can follow the [Nano Agent Interaction section below](#nano-agent-interaction) to test out the nano agent.\n\n## Nano Agent Interaction\n\nThere are three ways to interact with the nano agent.\n1. Nano Agent **Through the CLI** (`uv run nano-cli run`)\n   - Great for understanding agent capabilities\n2. Nano Agent **Through Claude Code** or any MCP client (`.mcp.json` or equivalent configuration)\n   - Great for delegating work and scaling up compute in the field\n3. Nano Agent **Through the Higher Order Prompt** (HOP) and Lower Order Prompt (LOP) pattern to test and compare models across providers and models.\n\n### Through the CLI\n\nRemember, when running directly your current directory is where ever you run `uv run nano-cli run` from.\n\n```bash\ncd apps/nano_agent_mcp_server\n\n# Test tools without API\nuv run nano-cli test-tools\n\n# Run with different models (provider auto-detected from model name)\nuv run nano-cli run \"List all Python files in the current directory\"  # gpt-5-mini (default)\nuv run nano-cli run \"Create a hello world script in python\" --model gpt-5-nano\nuv run nano-cli run \"Summarize the README.md\" --model gpt-5\n\n# Test Anthropic models (requires ANTHROPIC_API_KEY)\nuv run nano-cli run \"Hello\" --model claude-3-haiku-20240307 --provider anthropic\nuv run nano-cli run \"Hello\" --model claude-sonnet-4-20250514 --provider anthropic\nuv run nano-cli run \"Hello\" --model claude-opus-4-20250514 --provider anthropic\nuv run nano-cli run \"Hello\" --model claude-opus-4-1-20250805 --provider anthropic\n\n# Test local Ollama models (requires ollama service) (be sure to install the model first with `ollama pull gpt-oss:20b`)\nuv run nano-cli run \"List files\" --model gpt-oss:20b --provider ollama\nuv run nano-cli run \"List files and count the total number of files and directories\" --model gpt-oss:120b --provider ollama\n\n# Verbose mode (shows token usage)\nuv run nano-cli run \"Create and edit a test file\" --verbose\n```\n\n### Through Claude Code\n\n#### Call the MCP server directly\n\n```prompt\nmcp nano-agent: prompt_nano_agent \"Create a hello world script in python\" --model gpt-5\nmcp nano-agent: prompt_nano_agent \"Summarize the README.md\" --model claude-opus-4-1-20250805 --provider anthropic\nmcp nano-agent: prompt_nano_agent \"Read the first 10 lines and last 10 lines of the README.md\" --verbose\netc...\n```\n\n#### Call the MCP server through a sub-agent\n\n```prompt\n@agent-nano-agent-gpt-5-mini \"Create a hello world script in python\"\n\n@agent-nano-agent-gpt-5 \"Summarize the <file name>\"\n\n@agent-nano-agent-claude-opus-4-1 \"<insert agentic prompt here>\"\n\n@agent-nano-agent-gpt-oss-20b \"<insert agentic prompt here>\"\n\n@agent-nano-agent-gpt-oss-120b \"<insert agentic prompt here>\"\n\n@agent-nano-agent-claude-sonnet-4 \"<insert agentic prompt here>\"\n\n@agent-nano-agent-claude-3-haiku \"<insert agentic prompt here>\"\n```\n\n### Through the Higher Order Prompt (HOP) and Lower Order Prompt (LOP) pattern\n\nIn Claude Code call\n\n```\n/perf:hop_evaluate_nano_agents .claude/commands/perf/lop_eval_1__dummy_test.md\n\n/perf:hop_evaluate_nano_agents .claude/commands/perf/lop_eval_2__basic_read_test.md\n\n/perf:hop_evaluate_nano_agents .claude/commands/perf/lop_eval_3__file_operations_test.md\n\n/perf:hop_evaluate_nano_agents .claude/commands/perf/lop_eval_4__code_analysis_test.md\n\n/perf:hop_evaluate_nano_agents .claude/commands/perf/lop_eval_5__complex_engineering_test.md\n```\n\n#### Understanding HOP/LOP: How It Works\n\nThe **HOP/LOP pattern** enables systematic parallel evaluation of multiple models:\n\n- **HOP (Higher Order Prompt)**: The orchestrator that reads test files, delegates to agents in parallel, and grades results\n- **LOP (Lower Order Prompt)**: Individual test definitions with prompts, expected outputs, and grading rubrics\n- **Execution Flow**: HOP → reads LOP → calls 9 agents simultaneously → collects results → generates comparison tables\n\n**Example**: When you run `/perf:hop_evaluate_nano_agents lop_eval_3__file_operations_test.md`:\n1. HOP reads the test specification from the LOP file\n2. Extracts the prompt and list of agents to test\n3. Executes all agents in parallel (GPT-5, Claude, Local models)\n4. Each agent runs in isolation via the nano-agent MCP server\n5. Results are graded on Performance, Speed, and Cost\n6. Output shows ranked comparison with surprising results (e.g., Claude-3-haiku often beats expensive models)\n\nThis architecture ensures fair comparison by using the same OpenAI Agent SDK for all providers, creating a true apples-to-apples benchmark.\n\n## Features\n\n- 🤖 **Multi-Provider Support**: Seamlessly switch between OpenAI (GPT-5), Anthropic (Claude), and Ollama (local models)\n- 🔧 **File System Operations**: Read, write, edit, and analyze files autonomously\n- 🏗️ **Nested Agent Architecture**: MCP server spawns internal agents for task execution\n- 🎯 **Unified Interface**: All providers use the same OpenAI SDK for consistency\n- 📦 **Experiment Ready**: Decent testing, error handling, and token tracking\n- 🚀 **Easy Integration**: Works with Claude Desktop, or as a CLI\n\n## Nano-Agent Tools\n> Feel free to add/remove/improve tools as you see fit.\n\nNano-Agent tools are stored in `nano_agent_tools.py`.\n\nTools are:\n- `read_file` - Read file contents\n- `list_directory` - List directory contents (defaults to current working directory)\n- `write_file` - Create or overwrite files\n- `get_file_info` - Get file metadata (size, dates, type)\n- `edit_file` - Edit files by replacing exact text matches\n\n## Project Structure\n\n```\nnano-agent/\n├── apps/                           # ⚠️ ALL APPLICATION CODE GOES HERE\n│   └── nano_agent_mcp_server/     # Main MCP server application\n│       ├── src/                    # Source code\n│       │   └── nano_agent/         # Main package\n│       │       ├── modules/        # Core modules\n│       │       │   ├── constants.py         # Model/provider constants & defaults\n│       │       │   ├── data_types.py        # Pydantic models & type definitions\n│       │       │   ├── files.py             # File system operations\n│       │       │   ├── nano_agent.py        # Main agent execution logic\n│       │       │   ├── nano_agent_tools.py  # Internal agent tool implementations\n│       │       │   ├── provider_config.py   # Multi-provider configuration\n│       │       │   ├── token_tracking.py    # Token usage & cost tracking\n│       │       │   └── typing_fix.py        # Type compatibility fixes\n│       │       ├── __main__.py     # MCP server entry point\n│       │       └── cli.py          # CLI interface (nano-cli)\n│       ├── tests/                  # Test suite\n│       │   ├── nano_agent/         # Unit tests\n│       │   └── isolated/           # Provider integration tests\n│       ├── scripts/                # Installation & utility scripts\n│       ├── pyproject.toml          # Project configuration & dependencies\n│       ├── uv.lock                 # Locked dependency versions\n│       └── .env.sample             # Environment variables template\n├── .claude/                        # Claude Code configuration\n│   ├── agents/                     # Sub-agent configurations (9 models)\n│   │   ├── nano-agent-gpt-5-nano.md         # OpenAI GPT-5 Nano\n│   │   ├── nano-agent-gpt-5-mini.md         # OpenAI GPT-5 Mini (default)\n│   │   ├── nano-agent-gpt-5.md              # OpenAI GPT-5\n│   │   ├── nano-agent-claude-opus-4-1.md    # Claude Opus 4.1\n│   │   ├── nano-agent-claude-opus-4.md      # Claude Opus 4\n│   │   ├── nano-agent-claude-sonnet-4.md    # Claude Sonnet 4\n│   │   ├── nano-agent-claude-3-haiku.md     # Claude 3 Haiku\n│   │   ├── nano-agent-gpt-oss-20b.md        # Ollama 20B model\n│   │   ├── nano-agent-gpt-oss-120b.md       # Ollama 120B model\n│   │   └── hello-world.md                   # Simple greeting agent\n│   ├── commands/                   # Claude Code commands\n│   │   ├── perf/                   # Performance evaluation commands\n│   │   │   ├── hop_evaluate_nano_agents.md  # Higher Order Prompt orchestrator\n│   │   │   ├── lop_eval_1__dummy_test.md    # Simple Q&A test\n│   │   │   ├── lop_eval_2__basic_read_test.md   # File reading test\n│   │   │   ├── lop_eval_3__file_operations_test.md  # Complex I/O test\n│   │   │   ├── lop_eval_4__code_analysis_test.md    # Code understanding\n│   │   │   └── lop_eval_5__complex_engineering_test.md  # Full project test\n│   │   ├── convert_paths_absolute.md   # Convert to absolute paths\n│   │   ├── convert_paths_relative.md   # Convert to relative paths\n│   │   ├── create_worktree.md          # Git worktree management\n│   │   ├── plan.md                     # Planning template\n│   │   ├── prime.md                    # Codebase understanding\n│   │   └── build.md                    # Build commands\n│   ├── hooks/                      # Development hooks\n│   ├── settings.json               # Portable settings (relative paths)\n│   └── settings.local.json         # Local settings (absolute paths)\n├── eval_results_1_dummy_test.md    # Q&A test benchmark results\n├── eval_results_2_basic_read_test.md   # File reading benchmark results\n├── eval_results_3_file_operations_test.md  # I/O benchmark results\n├── eval_results_4_code_analysis_test.md    # Code analysis benchmark results\n├── eval_results_5_complex_engineering_test.md  # Project creation benchmark results\n├── images/                         # Documentation images\n│   └── nano-agent.png             # Project logo/diagram\n├── app_docs/                       # Application-specific documentation\n├── ai_docs/                        # AI/LLM documentation & guides\n│   ├── python_uv_mcp_server_cookbook.md    # MCP server development guide\n│   ├── openai_agent_sdk_*.md      # OpenAI SDK documentation\n│   ├── anthropic_openai_compat.md # Anthropic compatibility guide\n│   ├── ollama_openai_compat.md    # Ollama compatibility guide\n│   └── new_openai_gpt_models.md   # GPT-5 model specifications\n└── specs/                          # Technical specifications\n```\n\n## Development Guidelines\n\n### Prerequisites\n- Python 3.12+ (required for proper typing support)\n- [uv](https://github.com/astral-sh/uv) package manager\n- OpenAI API key (for GPT-5 model tests)\n\n### Development Setup\n\n```bash\ncd apps/nano_agent_mcp_server\nuv sync --extra test  # Include test dependencies\n```\n\n### Claude Code Hook Configuration\n\nIf you're using Claude Code to work on this codebase, the project includes hooks for enhanced development experience. The hooks use relative paths by default for portability.\n\n**To activate hooks with absolute paths for your local environment:**\nConvert relative paths to absolute paths in .claude/settings.local.json\nRun this command in Claude Code:\nThis updates all hook paths to use your machine's absolute paths\nA backup is automatically created at .claude/settings.json.backup\n\n`/convert_paths_absolute.md`\n\n**Note:** The hooks are optional but provide useful features like:\n- Pre/post tool use notifications\n- Session tracking\n- Event logging for debugging\n\nFor production use, see [Installation](#installation) section above.\n\n#### UV Dependency Management\n\nWhen working with UV and optional dependencies:\n- `uv sync` - Installs only the main dependencies (mcp, typer, rich)\n- `uv sync --extra test` - Installs main + test dependencies (includes pytest, openai, etc.)\n- `uv sync --all-extras` - Installs main + all optional dependency groups\n- `uv pip list` - Shows all installed packages in the virtual environment\n\n**Important:** Always use `--extra test` when you need to run tests, as `uv sync` alone will remove test dependencies.\n\n### Configuration\n\n1. Copy the environment template:\n```bash\ncp .env.sample .env\n```\n\n2. Add your OpenAI API key:\n```bash\necho \"OPENAI_API_KEY=sk-your-key-here\" > .env\n```\n\n### Running the Server\n\n```bash\ncd apps/nano_agent_mcp_server\nuv run nano-agent --help\n```\n\nThe server communicates via stdin/stdout using the MCP protocol.\n\n## Nano Agent Architecture\n\n### Nested Agent Hierarchy\n\n**Key Concept:** This is a nested agent system with two distinct agent layers.\n\n```text\n┌─────────────────────────────────────────────────────────────┐\n│ OUTER AGENT (e.g., Claude Code, any MCP client)            │\n│   • Communicates via MCP protocol                          │\n│   • Sees ONE tool: prompt_nano_agent                       │\n│   • Sends natural language prompts to nano-agent           │\n└─────────────────────────────────────────────────────────────┘\n                            │\n                            │ MCP Protocol\n                            ▼\n┌─────────────────────────────────────────────────────────────┐\n│ NANO-AGENT MCP SERVER (apps/nano_agent_mcp_server)         │\n│   • Exposes SINGLE MCP tool: prompt_nano_agent             │\n│   • Receives prompts from outer agent                      │\n│   • Spawns internal OpenAI agent to handle request         │\n└─────────────────────────────────────────────────────────────┘\n                            │\n                            │ Creates & Manages\n                            ▼\n┌─────────────────────────────────────────────────────────────┐\n│ INNER AGENT (OpenAI GPT with function calling)             │\n│   • Created fresh for each prompt_nano_agent call          │\n│   • Has its OWN tools (not visible to outer agent):        │\n│     - read_file: Read file contents                        │\n│     - list_directory: List directory contents              │\n│     - write_file: Create/overwrite files                   │\n│     - get_file_info: Get file metadata                     │\n│   • Runs autonomous loop (max 20 turns)                    │\n│   • Returns final result to MCP server → outer agent       │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Validation & Testing\n\n### Unit Tests (Real API Calls)\n```bash\n# Run all integration tests\nuv run pytest tests/ -v\n\n# Test specific functionality\nuv run pytest tests/nano_agent/modules/test_nano_agent.py::TestExecuteNanoAgent -v\n\n# Quick validation\nuv run pytest -k \"test_execute_nano_agent_success\" -v\n```\n\n### CLI Validation\n```bash\n# Validate tools work (no API needed)\nuv run nano-cli test-tools\n\n# Quick agent test\nexport OPENAI_API_KEY=sk-your-key\nuv run nano-cli run \"What is 2+2?\"  # Uses DEFAULT_MODEL\n```\n\n## Multi-Provider Support\n\nThe nano agent supports multiple LLM providers through a unified interface using the OpenAI SDK. All providers are accessed through OpenAI-compatible endpoints, providing a consistent API.\n\n### Available Providers & Models\n> Feel free to add/remove providers and models as you see fit.\n\n#### OpenAI (Default)\n- **Models**: `gpt-5`, `gpt-5-mini` (default), `gpt-5-nano`, `gpt-4o`\n- **Requirements**: `OPENAI_API_KEY` environment variable\n- **Special Features**: \n  - GPT-5 models use `max_completion_tokens` instead of `max_tokens`\n  - GPT-5 models only support temperature=1\n  - Extended context windows (400K tokens)\n\n#### Anthropic\n- **Models**: `claude-opus-4-1-20250805`, `claude-opus-4-20250514`, `claude-sonnet-4-20250514`, `claude-3-haiku-20240307`\n- **Requirements**: `ANTHROPIC_API_KEY` environment variable\n- **Implementation**: Uses Anthropic's OpenAI-compatible endpoint\n- **Base URL**: `https://api.anthropic.com/v1/`\n\n#### Ollama (Local Models)\n- **Models**: `gpt-oss:20b`, `gpt-oss:120b`, or any model you've pulled locally\n- **Requirements**: Ollama service running locally\n- **Implementation**: Uses Ollama's OpenAI-compatible API\n- **Base URL**: `http://localhost:11434/v1`\n\n### Using Different Providers\n\n#### CLI Usage\n```bash\n# OpenAI (default)\nuv run nano-cli run \"Create a hello world script\"\n\n# Use specific OpenAI model\nuv run nano-cli run \"Analyze this code\" --model gpt-5 --provider openai\n\n# Anthropic\nuv run nano-cli run \"Write a test file\" --model claude-3-haiku-20240307 --provider anthropic\n\n# Ollama (local)\nuv run nano-cli run \"List files\" --model gpt-oss:20b --provider ollama\n```\n\n## Multi-Model Evaluation System\n\nThe nano-agent includes a sophisticated multi-layer evaluation system for comparing LLM performance across different providers and models. This creates a level playing field for benchmarking by using the same execution environment (OpenAI Agent SDK) regardless of the underlying provider.\n\n> \"Don't trust any individual benchmark. You need to crack open the hood of all these models and say, where is the true value?\" - Engineering is all about trade-offs.\n\n### 🎯 The Bread and Butter: HOP/LOP Pattern\n\nThe evaluation system's core innovation is the **HOP/LOP (Higher Order Prompt / Lower Order Prompt)** pattern, which creates a hierarchical orchestration system for parallel model testing:\n\n```text\n┌─────────────────────────────────────────────────────────────┐\n│ 1. HIGHER ORDER PROMPT (HOP)                                │\n│   File: .claude/commands/perf/hop_evaluate_nano_agents.md  │\n│   • Orchestrates entire evaluation process                  │\n│   • Accepts test case files as $ARGUMENTS                   │\n│   • Formats and grades results                              │\n│   • Generates performance comparison tables                 │\n└─────────────────────────────────────────────────────────────┘\n                            │\n                            │ Reads & Executes\n                            ▼\n┌─────────────────────────────────────────────────────────────┐\n│ 2. LOWER ORDER PROMPT (LOP)                                 │\n│   Files: .claude/commands/perf/lop_eval_*.md               │\n│   • Defines test cases (prompts to evaluate)               │\n│   • Lists agents to test (@agent-nano-agent-*)             │\n│   • Specifies expected outputs                             │\n│   • Provides grading rubrics                               │\n└─────────────────────────────────────────────────────────────┘\n                            │\n                            │ @agent References\n                            ▼\n┌─────────────────────────────────────────────────────────────┐\n│ 3. CLAUDE CODE SUB-AGENTS                                   │\n│   Files: .claude/agents/nano-agent-*.md                    │\n│   • Individual agent configurations                        │\n│   • Each specifies model + provider combination            │\n│   • Color-coded by model family:                          │\n│     - green: GPT-5 series (nano, mini, standard)          │\n│     - blue: GPT-OSS series (20b, 120b)                    │\n│     - purple: Claude 4 Opus models                        │\n│     - orange: Claude 4 Sonnet & Claude 3 Haiku            │\n└─────────────────────────────────────────────────────────────┘\n                            │\n                            │ Calls MCP Server\n                            ▼\n┌─────────────────────────────────────────────────────────────┐\n│ 4. NANO-AGENT MCP SERVER                                    │\n│   Function: prompt_nano_agent(prompt, model, provider)     │\n│   • Creates isolated agent instance per request            │\n│   • Uses OpenAI Agent SDK for ALL providers               │\n│   • Ensures consistent execution environment               │\n│   • Returns structured results with metrics                │\n└─────────────────────────────────────────────────────────────┘\n```\n\n### Why This Architecture?\n\n1. **Fair Comparison**: All models use the same OpenAI Agent SDK, eliminating implementation differences\n2. **Parallel Execution**: Agents run simultaneously, reducing temporal variations\n3. **Structured Metrics**: Consistent tracking of time, tokens, and costs across all providers\n4. **Extensibility**: Easy to add new models, providers, or test cases\n5. **Visual Hierarchy**: Color-coded agents make results easy to scan in Claude Code\n6. **Reproducibility**: Same prompts and execution environment ensure consistent benchmarks\n\n## License\n\nMIT\n\n## Master AI Coding\n> And prepare for Agentic Engineering\n\nLearn to code with AI with foundational [Principles of AI Coding](https://agenticengineer.com/principled-ai-coding?y=nanoagent)\n\nFollow the [IndyDevDan youtube channel](https://www.youtube.com/@indydevdan) for more AI coding tips and tricks.\n",
  "source_type": "github_repository",
  "domain": "software_development",
  "keywords": [],
  "retrieval_tags": [
    "github",
    "repository",
    "code",
    "development"
  ]
}