
import os
import torch
import json
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, 
    TrainingArguments, Trainer, DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load model and tokenizer
model_name = "Qwen/Qwen2.5-7B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True,
    low_cpu_mem_usage=True
)

# Setup LoRA
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    inference_mode=False,
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)

model = get_peft_model(model, lora_config)

# Prepare training data
training_examples = [{"instruction": "Summarize the following document from specs/problem.md:", "response": "Document summary from specs/problem.md:\n\n# Problem Specification: Agentic LLM Core\n\n## Overview\n**Title:** Agentic LLM Core  \n**Goal:** Local-first agent system using Qwen3-Omni, Pydantic-AI, LangChain, MCP tools  \n**Created:** 2024-09-24  \n**Status:** Draft\n\n## Problem Statement\n\n### Core Challenge\nBuild a sophisticated, local-first agentic LLM system that operates entirely offline while maintaining high performance, reproducibility, and security. The system must leverage cutting-edge technologies including Qwen3-Omni, Pydantic-AI, LangChain, and MCP (Model Context Protocol) tools to create a deterministic, testable agent framework.\n\n### Key Constraints\n1. **Offline-by-default**: System must operate without internet connectivity\n2. **Apple Silicon optimization**: Leverage M1/M2/M3 chip capabilities\n3. **Spec-kit flow**: Follow specification-driven development methodology\n4. **Deterministic tasks**: All operations must be reproducible and predictable\n\n### Non-Negotiable Requirements\n1. **Reproducible**: Every run must produce identical results given same inputs\n2. **Testable**: Comprehensive test coverage with >85% coverage requirement\n3. **No secret leakage**: Zero risk of credential or sensitive data exposure\n\n## Technical Requirements\n\n### Core Technologies\n- **Qwen3-Omni**: Primary LLM for agent reasoning and task execution\n- **Pydantic-AI**: Structured AI framework for type-safe agent operations\n- **LangChain**: Agent orchestration and tool integration\n- **MCP Tools**: Model Context Protocol for standardized tool interfaces\n\n### Performance Requirements\n- **Latency**: < 50ms response time for critical operations\n- **Memory**: Efficient memory management for Apple Silicon\n- **Throughput**: Support concurrent agent operations\n- **Storage**: Local-first data persistence\n\n### Security Requirements\n- **Data isolation**: Complete separation of agent contexts\n- **Credential management**: Secure handling of API keys and tokens\n- **Audit trails**: Immutable logging of all operations\n- **Sandboxing**: Isolated execution environments\n\n## Success Criteria\n\n### Functional Success\n- [ ] Agents can perform complex reasoning tasks offline\n- [ ] System handles multiple concurrent agent operations\n- [ ] MCP tools integrate seamlessly with agent workflows\n- [ ] All operations are deterministic and reproducible\n\n### Performance Success\n- [ ] Response times consistently under 50ms\n- [ ] Memory usage optimized for Apple Silicon\n- [ ] Zero network dependencies for core operations\n- [ ] Efficient resource utilization\n\n### Quality Success\n- [ ] >85% test coverage across all components\n- [ ] Zero security vulnerabilities\n- [ ] Complete documentation and specifications\n- [ ] Reproducible builds and deployments\n\n## Risk Assessment\n\n### High Risk\n- **Model compatibility**: Qwen3-Omni integration challenges\n- **Performance optimization**: Meeting latency requirements on Apple Silicon\n- **Offline operation**: Ensuring complete functionality without network\n\n### Medium Risk\n- **Tool integration**: MCP tools compatibility with LangChain\n- **Memory management**: Efficient handling of large language models\n- **Testing complexity**: Achieving comprehensive test coverage\n\n### Low Risk\n- **Documentation**: Well-established frameworks for documentation\n- **Security**: Standard security practices are well-defined\n\n## Stakeholder Impact\n\n### Primary Users\n- **AI Researchers**: Need reliable, offline agent experimentation\n- **Developers**: Require deterministic, testable agent frameworks\n- **Security Teams**: Demand zero-leakage credential management\n\n### Secondary Users\n- **DevOps Teams**: Need reproducible deployment processes\n- **QA Teams**: Require comprehensive testing frameworks\n- **Product Teams**: Need predictable agent behavior\n\n## Success Metrics\n\n### Quantitative Metrics\n- Response time: < 50ms (target), < 100ms (acceptable)\n- Test coverage: >85% (target), >90% (stretch goal)\n- Memory efficiency: <2GB per agent instance\n- Uptime: >99.9% for core services\n\n### Qualitative Metrics\n- Developer experience: Intuitive API design\n- Documentation quality: Complete and accurate\n- Security posture: Zero vulnerabilities\n- Reproducibility: 100% deterministic operations\n\n## Next Steps\n\n1. **Technical Architecture Design**: Define system architecture and component interactions\n2. **Technology Stack Validation**: Verify compatibility of chosen technologies\n3. **Performance Benchmarking**: Establish baseline performance metrics\n4. **Security Audit Plan**: Define security testing and validation procedures\n5. **Testing Strategy**: Develop comprehensive testing framework\n\n---\n\n**Document Version:** 1.0  \n**Last Updated:** 2024-09-24  \n**Next Review:** 2024-10-01\n", "source": "specs/problem.md"}, {"instruction": "Summarize the following document from specs/system.md:", "response": "Document summary from specs/system.md:\n\n# System Specification: Agentic LLM Core\n\n## Overview\n**System Name:** Agentic LLM Core  \n**Version:** 0.1.0 (MVP)  \n**Architecture:** Local-first, Pipeline-based  \n**Platform:** Apple Silicon (M1/M2/M3)  \n**Created:** 2024-09-24  \n**Status:** Draft  \n**Scope:** v0.1 MVP - Ingest\u2192Answer Pipeline with MCP Tools and Multimodal Input\n\n## v0.1 Scope Definition\n\n### Core MVP Features\n**Primary Goal:** Build a focused ingest\u2192answer pipeline that demonstrates core agentic capabilities with MCP tool integration and multimodal input support.\n\n#### 1. Ingest\u2192Answer Pipeline\n- **Input Processing**: Accept multimodal inputs (text, images, documents)\n- **Context Understanding**: Parse and understand input context\n- **Answer Generation**: Generate structured, actionable responses\n- **Output Formatting**: Deliver answers in multiple formats (text, structured data, visualizations)\n\n#### 2. MCP Tool Execution\n- **Tool Discovery**: Automatically identify relevant tools for tasks\n- **Tool Selection**: Choose appropriate MCP tools based on context\n- **Tool Execution**: Execute tools with proper parameter handling\n- **Result Integration**: Incorporate tool results into answer generation\n\n#### 3. Multimodal Input Support\n- **Text Input**: Natural language queries and instructions\n- **Image Input**: Visual content analysis and understanding\n- **Document Input**: PDF, markdown, and structured document processing\n- **Audio Input**: Speech-to-text and audio content analysis (future)\n\n### v0.1 Success Criteria\n- [ ] Process multimodal inputs (text + images) within 5 seconds\n- [ ] Execute MCP tools with <100ms latency\n- [ ] Generate coherent, actionable answers\n- [ ] Handle 10+ concurrent input streams\n- [ ] Maintain >90% accuracy on test dataset\n\n### Out of Scope for v0.1\n- Multi-agent orchestration\n- Complex workflow management\n- Advanced security features\n- Production deployment\n- Performance optimization beyond basic requirements\n\n## System Architecture\n\n### High-Level Architecture (v0.1 MVP)\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Agentic LLM Core v0.1 (MVP)                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502                Ingest\u2192Answer Pipeline                   \u2502 \u2502\n\u2502  \u2502                                                         \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502\n\u2502  \u2502  \u2502   Input     \u2502\u2500\u2500\u2500\u25b6\u2502  Context    \u2502\u2500\u2500\u2500\u25b6\u2502   Answer    \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502  Processor  \u2502    \u2502  Analyzer   \u2502    \u2502 Generator   \u2502 \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502\n\u2502  \u2502       \u2502                     \u2502                   \u2502       \u2502 \u2502\n\u2502  \u2502       \u25bc                     \u25bc                   \u25bc       \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502\n\u2502  \u2502  \u2502  Multimodal \u2502    \u2502  Qwen3-Omni \u2502    \u2502   Output    \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502   Handler   \u2502    \u2502   Engine    \u2502    \u2502 Formatter   \u2502 \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502                MCP Tools Execution                      \u2502 \u2502\n\u2502  \u2502                                                         \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502\n\u2502  \u2502  \u2502   Tool      \u2502\u2500\u2500\u2500\u25b6\u2502   Tool      \u2502\u2500\u2500\u2500\u25b6\u2502   Tool      \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502 Discovery   \u2502    \u2502 Selection   \u2502    \u2502 Execution   \u2502 \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502\n\u2502  \u2502       \u2502                     \u2502                   \u2502       \u2502 \u2502\n\u2502  \u2502       \u25bc                     \u25bc                   \u25bc       \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502\n\u2502  \u2502  \u2502   File      \u2502    \u2502  Database   \u2502    \u2502  Network    \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502   Tools     \u2502    \u2502   Tools     \u2502    \u2502   Tools     \u2502 \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502              Pydantic-AI Framework                      \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502 \u2502\n\u2502  \u2502  \u2502   Type      \u2502 \u2502   Data      \u2502 \u2502   Error     \u2502       \u2502 \u2502\n\u2502  \u2502  \u2502 Validation  \u2502 \u2502 Validation  \u2502 \u2502 Handling    \u2502       \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### v0.1 Pipeline Flow\n\n```\nInput \u2192 Processing \u2192 Context \u2192 Reasoning \u2192 Tools \u2192 Answer \u2192 Output\n  \u2502         \u2502          \u2502         \u2502         \u2502        \u2502        \u2502\n  \u25bc         \u25bc          \u25bc         \u25bc         \u25bc        \u25bc        \u25bc\nText    Multimodal  Qwen3-Omni  Pydantic  MCP    Structured  Format\nImage   Handler     Engine      AI        Tools   Response   Response\nDoc     Parser      Analysis    Validation Execution\n```\n\n### v0.1 Component Specifications\n\n#### 1. Input Processor\n**Purpose:** Handle multimodal input ingestion and preprocessing  \n**Technology:** Python 3.11+ with asyncio  \n**Responsibilities:**\n- Accept text, image, and document inputs\n- Validate input formats and sizes\n- Preprocess inputs for downstream processing\n- Queue inputs for processing pipeline\n\n**Key Interfaces:**\n```python\nclass InputProcessor:\n    async def process_text(self, text: str) -> ProcessedInput\n    async def process_image(self, image_data: bytes) -> ProcessedInput\n    async def process_document(self, doc_path: str) -> ProcessedInput\n    async def validate_input(self, input_data: Any) -> ValidationResult\n```\n\n#### 2. Multimodal Handler\n**Purpose:** Process and understand multimodal content  \n**Technology:** Custom multimodal processing pipeline  \n**Responsibilities:**\n- Extract features from images and documents\n- Combine multimodal information into unified context\n- Handle text extraction from images/documents\n- Generate embeddings for semantic understanding\n\n**Key Interfaces:**\n```python\nclass MultimodalHandler:\n    async def extract_text_from_image(self, image: bytes) -> str\n    async def extract_text_from_document(self, doc_path: str) -> str\n    async def generate_image_embedding(self, image: bytes) -> Embedding\n    async def combine_multimodal_context(self, inputs: list[Input]) -> UnifiedContext\n```\n\n#### 3. Qwen3-Omni Engine\n**Purpose:** Local LLM inference engine optimized for Apple Silicon  \n**Technology:** Native Metal Performance Shaders (MPS) integration  \n**Responsibilities:**\n- Model loading and memory management\n- Inference optimization for Apple Silicon\n- Context analysis and understanding\n- Answer generation with tool integration\n\n**v0.1 Performance Requirements:**\n- < 5 seconds total processing time for multimodal inputs\n- < 2GB memory footprint per model instance\n- Support for concurrent inference requests\n- Automatic memory cleanup and garbage collection\n\n**Key Interfaces:**\n```python\nclass Qwen3OmniEngine:\n    async def analyze_context(self, context: UnifiedContext) -> ContextAnalysis\n    async def generate_answer(self, analysis: ContextAnalysis, tools: list[Tool]) -> Answer\n    async def determine_tool_usage(self, analysis: ContextAnalysis) -> list[ToolCall]\n    async def process_multimodal_input(self, input: ProcessedInput) -> Understanding\n```\n\n#### 4. MCP Tool Execution Engine\n**Purpose:** Execute MCP tools based on context analysis  \n**Technology:** Custom MCP implementation with Pydantic-AI integration  \n**Responsibilities:**\n- Tool discovery and selection\n- Parameter validation and preparation\n- Tool execution with error handling\n- Result integration into answer generation\n\n**Key Interfaces:**\n```python\nclass MCPToolEngine:\n    async def discover_tools(self, context: ContextAnalysis) -> list[AvailableTool]\n    async def select_tools(self, available: list[AvailableTool], analysis: ContextAnalysis) -> list[ToolCall]\n    async def execute_tool(self, tool_call: ToolCall) -> ToolResult\n    async def integrate_results(self, results: list[ToolResult], answer: Answer) -> FinalAnswer\n```\n\n#### 5. Pydantic-AI Framework\n**Purpose:** Type-safe agent operations and data validation  \n**Technology:** Pydantic v2 with AI-specific extensions  \n**Responsibilities:**\n- Structured input/output validation for v0.1 pipeline\n- Type-safe tool integration and execution\n- Error handling and validation\n- Data flow type checking\n\n**v0.1 Key Components:**\n```python\nfrom pydantic_ai import Agent, Tool\nfrom pydantic import BaseModel\nfrom typing import Union, List, Any\n\nclass ProcessedInput(BaseModel):\n    input_type: str  # \"text\", \"image\", \"document\"\n    content: Union[str, bytes, dict]\n    metadata: dict[str, Any]\n    timestamp: datetime\n\nclass UnifiedContext(BaseModel):\n    text_content: str\n    image_features: List[float]\n    document_content: str\n    combined_embedding: List[float]\n    confidence_scores: dict[str, float]\n\nclass ContextAnalysis(BaseModel):\n    intent: str\n    entities: List[str]\n    required_tools: List[str]\n    confidence: float\n    reasoning: str\n\nclass ToolCall(BaseModel):\n    tool_name: str\n    parameters: dict[str, Any]\n    priority: int\n\nclass FinalAnswer(BaseModel):\n    answer: str\n    confidence: float\n    tools_used: List[str]\n    processing_time: float\n    metadata: dict[str, Any]\n```\n\n#### 6. Output Formatter\n**Purpose:** Format and deliver answers in multiple formats  \n**Technology:** Custom formatting engine  \n**Responsibilities:**\n- Format answers as text, JSON, or structured data\n- Generate visualizations when appropriate\n- Handle different output requirements\n- Ensure consistent response formatting\n\n**Key Interfaces:**\n```python\nclass OutputFormatter:\n    async def format_text_answer(self, answer: FinalAnswer) -> str\n    async def format_structured_answer(self, answer: FinalAnswer) -> dict\n    async def format_json_answer(self, answer: FinalAnswer) -> str\n    async def generate_visualization(self, answer: FinalAnswer, data: Any) -> bytes\n```\n\n### v0.1 MCP Tools Specification\n\n#### Core MCP Tools for v0.1\n\n**1. File System Tools**\n```python\nclass FileSystemTools:\n    async def read_file(self, path: str) -> str\n    async def write_file(self, path: str, content: str) -> bool\n    async def list_directory(self, path: str) -> List[str]\n    async def search_files(self, pattern: str, directory: str) -> List[str]\n    async def get_file_info(self, path: str) -> FileInfo\n```\n\n**2. Database Tools**\n```python\nclass DatabaseTools:\n    async def query_sqlite(self, query: str, db_path: str) -> List[dict]\n    async def insert_data(self, table: str, data: dict, db_path: str) -> bool\n    async def create_table(self, schema: str, db_path: str) -> bool\n    async def backup_database(self, db_path: str, backup_path: str) -> bool\n```\n\n**3. Text Processing Tools**\n```python\nclass TextProcessingTools:\n    async def extract_text_from_pdf(self, pdf_path: str) -> str\n    async def extract_text_from_image(self, image_path: str) -> str\n    async def summarize_text(self, text: str, max_length: int) -> str\n    async def extract_entities(self, text: str) -> List[str]\n```\n\n**4. Data Analysis Tools**\n```python\nclass DataAnalysisTools:\n    async def analyze_csv(self, csv_path: str, columns: List[str]) -> dict\n    async def create_chart(self, data: dict, chart_type: str) -> bytes\n    async def calculate_statistics(self, data: List[float]) -> dict\n    async def filter_data(self, data: List[dict], filters: dict) -> List[dict]\n```\n\n### v0.1 Data Flow Architecture\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   User      \u2502\u2500\u2500\u2500\u25b6\u2502   Input     \u2502\u2500\u2500\u2500\u25b6\u2502 Multimodal  \u2502\n\u2502  Request    \u2502    \u2502 Processor   \u2502    \u2502  Handler    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502                   \u2502\n                           \u25bc                   \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Output    \u2502\u25c0\u2500\u2500\u2500\u2502   Answer    \u2502\u25c0\u2500\u2500\u2500\u2502 Qwen3-Omni  \u2502\n\u2502 Formatter   \u2502    \u2502 Generator   \u2502    \u2502   Engine    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502                   \u2502                   \u2502\n       \u25bc                   \u25bc                   \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Formatted  \u2502    \u2502   Final     \u2502    \u2502   Context   \u2502\n\u2502  Response   \u2502    \u2502   Answer    \u2502    \u2502  Analysis   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                      \u2502\n                                                      \u25bc\n                                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                           \u2502    MCP      \u2502\n                                           \u2502 Tool Engine \u2502\n                                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                      \u2502\n                                                      \u25bc\n                                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                           \u2502   Tool      \u2502\n                                           \u2502 Execution   \u2502\n                                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### v0.1 Processing Pipeline\n\n```\nInput \u2192 Validation \u2192 Multimodal Processing \u2192 Context Analysis \u2192 Tool Selection \u2192 Answer Generation \u2192 Output Formatting \u2192 Response\n  \u2502         \u2502              \u2502                      \u2502                \u2502                    \u2502                    \u2502\n  \u25bc         \u25bc              \u25bc                      \u25bc                \u25bc                    \u25bc                    \u25bc\nText    Pydantic-AI    Image/Text         Qwen3-Omni        MCP Tools         Structured         Multiple\nImage   Validation     Extraction         Understanding      Execution         Answer            Formats\nDoc     Type Check     Embedding          Intent Analysis    Tool Results      Integration       (Text/JSON/Visual)\n```\n\n### Security Architecture\n\n#### 1. Credential Management\n**Implementation:** Hardware-backed keychain integration  \n**Features:**\n- Secure credential storage using macOS Keychain\n- Automatic credential rotation\n- Zero-memory credential handling\n- Audit logging of all credential access\n\n#### 2. Data Isolation\n**Implementation:** Process-level sandboxing  \n**Features:**\n- Each agent runs in isolated process\n- Memory space separation\n- File system access controls\n- Network access restrictions\n\n#### 3. Audit and Logging\n**Implementation:** Immutable audit trail  \n**Features:**\n- All operations logged with cryptographic signatures\n- Tamper-proof log storage\n- Real-time security monitoring\n- Automated threat detection\n\n### v0.1 Performance Specifications\n\n#### Latency Requirements (v0.1 MVP)\n- **Input Processing**: < 1 second\n- **Multimodal Processing**: < 3 seconds\n- **Context Analysis**: < 2 seconds\n- **Tool Execution**: < 100ms per tool\n- **Answer Generation**: < 3 seconds\n- **Total Pipeline**: < 5 seconds\n\n#### Throughput Requirements (v0.1 MVP)\n- **Concurrent Inputs**: 10+ simultaneous inputs\n- **Processing Rate**: 100+ inputs/minute\n- **Tool Operations**: 500+ operations/minute\n- **Data Processing**: 100MB/minute\n\n#### Resource Requirements (v0.1 MVP)\n- **Memory**: < 4GB total system memory\n- **CPU**: Optimized for Apple Silicon cores (M1/M2/M3)\n- **Storage**: < 500MB for models and data\n- **Network**: Zero bandwidth for offline operations\n\n### v0.1 Multimodal Input Handling\n\n#### Supported Input Types\n\n**1. Text Input**\n```python\nclass TextInput:\n    content: str\n    language: Optional[str] = \"en\"\n    encoding: str = \"utf-8\"\n    max_length: int = 10000\n```\n\n**2. Image Input**\n```python\nclass ImageInput:\n    data: bytes\n    format: str  # \"jpg\", \"png\", \"gif\", \"webp\"\n    max_size: tuple[int, int] = (2048, 2048)\n    quality: int = 85\n```\n\n**3. Document Input**\n```python\nclass DocumentInput:\n    file_path: str\n    file_type: str  # \"pdf\", \"docx\", \"txt\", \"md\"\n    max_size: int = 50 * 1024 * 1024  # 50MB\n    encoding: str = \"utf-8\"\n```\n\n#### Multimodal Processing Pipeline\n\n```\nInput Detection \u2192 Format Validation \u2192 Content Extraction \u2192 Feature Generation \u2192 Context Integration\n      \u2502                \u2502                     \u2502                    \u2502                     \u2502\n      \u25bc                \u25bc                     \u25bc                    \u25bc                     \u25bc\n   Type Check      Pydantic-AI          OCR/Text            Embedding          Unified Context\n   Size Check      Validation           Extraction          Generation         Object Creation\n   Format Check    Type Safety          Feature Analysis    Similarity Calc    Confidence Scoring\n```\n\n#### Input Validation Rules\n\n**Text Input:**\n- Maximum length: 10,000 characters\n- Encoding: UTF-8\n- Content filtering: Basic profanity/toxicity check\n- Language detection: Automatic with confidence scoring\n\n**Image Input:**\n- Maximum dimensions: 2048x2048 pixels\n- Supported formats: JPEG, PNG, GIF, WebP\n- Maximum file size: 10MB\n- Color space: RGB/RGBA\n\n**Document Input:**\n- Maximum file size: 50MB\n- Supported formats: PDF, DOCX, TXT, Markdown\n- Text extraction: OCR for images, direct extraction for text\n- Metadata preservation: Author, creation date, modification date\n\n### v0.1 Technology Stack\n\n#### Core Framework (v0.1 MVP)\n- **Python**: 3.11+ with asyncio support\n- **Pydantic**: v2 for data validation and type safety\n- **Qwen3-Omni**: Local inference engine integration\n- **FastAPI**: HTTP API for input/output handling\n- **SQLite**: Local data storage and caching\n\n#### Multimodal Processing\n- **Pillow (PIL)**: Image processing and manipulation\n- **PyPDF2/pdfplumber**: PDF text extraction\n- **python-docx**: Word document processing\n- **pytesseract**: OCR for image text extraction\n- **sentence-transformers**: Text embeddings generation\n\n#### Apple Silicon Optimization\n- **Metal Performance Shaders**: GPU acceleration for Qwen3-Omni\n- **Core ML**: Model optimization and quantization\n- **Accelerate Framework**: Mathematical operations\n- **asyncio**: Concurrent processing\n\n#### Development Tools (v0.1)\n- **Poetry**: Dependency management\n- **Pytest**: Testing framework with async support\n- **Black**: Code formatting\n- **Ruff**: Linting and import sorting\n- **Mypy**: Type checking\n\n#### v0.1 Monitoring\n- **Structured Logging**: JSON-formatted logs\n- **Performance Metrics**: Basic timing and memory usage\n- **Error Tracking**: Exception logging and handling\n- **Input/Output Logging**: Request/response tracking\n\n### v0.1 Deployment Architecture\n\n#### Local Development Setup\n```bash\n# Environment setup\npoetry install\npoetry run python -m agentic_llm_core --dev\n\n# Testing\npoetry run pytest --cov=agentic_llm_core tests/\n\n# Linting\npoetry run ruff check .\npoetry run black .\npoetry run mypy .\n\n# Run v0.1 MVP\npoetry run python -m agentic_llm_core --version 0.1\n```\n\n#### v0.1 MVP Deployment\n```bash\n# Simple local deployment\npoetry run uvicorn agentic_llm_core.main:app --host 0.0.0.0 --port 8000\n\n# With model loading\npoetry run python -m agentic_llm_core --model-path /models/qwen3-omni --port 8000\n```\n\n### Configuration Management\n\n#### Environment Configuration\n```yaml\n# config/default.yaml\nsystem:\n  max_agents: 100\n  max_concurrent_tasks: 1000\n  memory_limit_per_agent: \"2GB\"\n  \nqwen3_omni:\n  model_path: \"/models/qwen3-omni\"\n  device: \"mps\"  # Metal Performance Shaders\n  precision: \"float16\"\n  \nsecurity:\n  credential_store: \"keychain\"\n  audit_log_path: \"/var/log/agentic-llm-core\"\n  enable_sandboxing: true\n  \nperformance:\n  target_latency_ms: 50\n  max_memory_usage: \"8GB\"\n  enable_profiling: false\n```\n\n### Testing Strategy\n\n#### Unit Testing\n- **Coverage Target**: >85%\n- **Framework**: Pytest with async support\n- **Mocking**: unittest.mock for external dependencies\n- **Fixtures**: Pytest fixtures for test data\n\n#### Integration Testing\n- **Agent Integration**: End-to-end agent workflows\n- **Tool Integration**: MCP tool functionality\n- **Performance Testing**: Latency and throughput validation\n- **Security Testing**: Credential handling and isolation\n\n#### Load Testing\n- **Concurrent Agents**: 100+ simultaneous agents\n- **Task Load**: 1000+ tasks per minute\n- **Memory Stress**: Extended operation testing\n- **Recovery Testing**: Failure and recovery scenarios\n\n---\n\n**Document Version:** 1.0  \n**Last Updated:** 2024-09-24  \n**Next Review:** 2024-10-01  \n**Approved By:** [To be filled]  \n**Technical Lead:** [To be filled]\n", "source": "specs/system.md"}, {"instruction": "Explain the following code from src/core/system_checker.py:", "response": "This code is from src/core/system_checker.py:\n\n\"\"\"\nComprehensive System Checker for Agentic LLM Core v0.1\n\nThis module performs comprehensive checks including:\n- Test coverage analysis (minimum 80%)\n- Specification compliance validation\n- Critical issue detection\n- Fail-fast error reporting\n\nCreated: 2024-09-24\nStatus: Draft\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport os\nimport subprocess\nimport sys\nimport time\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Tuple, Union\nfrom uuid import uuid4\n\nimport yaml\nfrom pydantic import BaseModel, Field\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n\n# ============================================================================\n# Data Models\n# ============================================================================\n\nclass CheckStatus(str, Enum):\n    \"\"\"Check status.\"\"\"\n    PASS = \"pass\"\n    FAIL = \"fail\"\n    WARNING = \"warning\"\n    SKIP = \"skip\"\n\n\nclass CheckSeverity(str, Enum):\n    \"\"\"Check severity.\"\"\"\n    CRITICAL = \"critical\"\n    HIGH = \"high\"\n    MEDIUM = \"medium\"\n    LOW = \"low\"\n\n\nclass CheckResult(BaseModel):\n    \"\"\"Individual check result.\"\"\"\n    check_id: str = Field(default_factory=lambda: str(uuid4()), description=\"Check ID\")\n    name: str = Field(..., description=\"Check name\")\n    status: CheckStatus = Field(..., description=\"Check status\")\n    severity: CheckSeverity = Field(..., description=\"Check severity\")\n    message: str = Field(..., description=\"Check message\")\n    details: Dict[str, Any] = Field(default_factory=dict, description=\"Check details\")\n    execution_time: float = Field(0.0, description=\"Execution time in seconds\")\n    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description=\"Check timestamp\")\n\n\nclass CoverageReport(BaseModel):\n    \"\"\"Test coverage report.\"\"\"\n    total_coverage: float = Field(..., description=\"Total coverage percentage\")\n    file_coverage: Dict[str, float] = Field(default_factory=dict, description=\"Per-file coverage\")\n    missing_lines: Dict[str, List[int]] = Field(default_factory=dict, description=\"Missing lines per file\")\n    total_lines: int = Field(0, description=\"Total lines of code\")\n    covered_lines: int = Field(0, description=\"Covered lines of code\")\n\n\nclass SpecComplianceReport(BaseModel):\n    \"\"\"Specification compliance report.\"\"\"\n    spec_files: List[str] = Field(default_factory=list, description=\"Specification files found\")\n    implemented_features: List[str] = Field(default_factory=list, description=\"Implemented features\")\n    missing_features: List[str] = Field(default_factory=list, description=\"Missing features\")\n    compliance_score: float = Field(0.0, description=\"Compliance score (0-100)\")\n\n\nclass SystemCheckReport(BaseModel):\n    \"\"\"Comprehensive system check report.\"\"\"\n    report_id: str = Field(default_factory=lambda: str(uuid4()), description=\"Report ID\")\n    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description=\"Report timestamp\")\n    total_checks: int = Field(0, description=\"Total checks performed\")\n    passed_checks: int = Field(0, description=\"Passed checks\")\n    failed_checks: int = Field(0, description=\"Failed checks\")\n    warning_checks: int = Field(0, description=\"Warning checks\")\n    skipped_checks: int = Field(0, description=\"Skipped checks\")\n    coverage_report: Optional[CoverageReport] = Field(None, description=\"Coverage report\")\n    spec_compliance_report: Optional[SpecComplianceReport] = Field(None, description=\"Spec compliance report\")\n    check_results: List[CheckResult] = Field(default_factory=list, description=\"Individual check results\")\n    critical_issues: List[CheckResult] = Field(default_factory=list, description=\"Critical issues found\")\n    recommendations: List[str] = Field(default_factory=list, description=\"Recommendations\")\n    overall_status: CheckStatus = Field(CheckStatus.PASS, description=\"Overall system status\")\n\n\n# ============================================================================\n# System Checker\n# ============================================================================\n\nclass SystemChecker:\n    \"\"\"Comprehensive system checker.\"\"\"\n    \n    def __init__(self, config: Optional[Dict[str, Any]] = None):\n        self.logger = logging.getLogger(__name__)\n        self.config = config or {}\n        self.coverage_min = self.config.get('coverage_min', 80.0)\n        self.spec_compliance_required = self.config.get('spec_compliance', True)\n        self.fail_fast = self.config.get('fail_fast', True)\n        self.check_results: List[CheckResult] = []\n        self.critical_issues: List[CheckResult] = []\n        \n    async def run_comprehensive_check(self) -> SystemCheckReport:\n        \"\"\"Run comprehensive system check.\"\"\"\n        start_time = time.time()\n        self.logger.info(\"Starting comprehensive system check...\")\n        \n        # Initialize report\n        report = SystemCheckReport()\n        \n        try:\n            # Run all checks\n            checks = [\n                self._check_project_structure,\n                self._check_dependencies,\n                self._check_test_coverage,\n                self._check_spec_compliance,\n                self._check_security_policies,\n                self._check_model_policies,\n                self._check_code_quality,\n                self._check_documentation,\n                self._check_configuration,\n                self._check_performance_requirements\n            ]\n            \n            for check_func in checks:\n                try:\n                    result = await check_func()\n                    self.check_results.append(result)\n                    \n                    if result.status == CheckStatus.FAIL:\n                        if result.severity == CheckSeverity.CRITICAL:\n                            self.critical_issues.append(result)\n                            if self.fail_fast:\n                                self.logger.error(f\"Critical issue found: {result.message}\")\n                                break\n                        else:\n                            self.logger.warning(f\"Check failed: {result.message}\")\n                    elif result.status == CheckStatus.WARNING:\n                        self.logger.warning(f\"Check warning: {result.message}\")\n                    else:\n                        self.logger.info(f\"Check passed: {result.name}\")\n                        \n                except Exception as e:\n                    error_result = CheckResult(\n                        name=check_func.__name__,\n                        status=CheckStatus.FAIL,\n                        severity=CheckSeverity.CRITICAL,\n                        message=f\"Check execution failed: {str(e)}\",\n                        details={\"error\": str(e)}\n                    )\n                    self.check_results.append(error_result)\n                    self.critical_issues.append(error_result)\n                    \n                    if self.fail_fast:\n                        self.logger.error(f\"Critical error in {check_func.__name__}: {e}\")\n                        break\n            \n            # Compile report\n            report.check_results = self.check_results\n            report.critical_issues = self.critical_issues\n            report.total_checks = len(self.check_results)\n            report.passed_checks = len([r for r in self.check_results if r.status == CheckStatus.PASS])\n            report.failed_checks = len([r for r in self.check_results if r.status == CheckStatus.FAIL])\n            report.warning_checks = len([r for r in self.check_results if r.status == CheckStatus.WARNING])\n            report.skipped_checks = len([r for r in self.check_results if r.status == CheckStatus.SKIP])\n            \n            # Determine overall status\n            if self.critical_issues:\n                report.overall_status = CheckStatus.FAIL\n            elif report.failed_checks > 0:\n                report.overall_status = CheckStatus.WARNING\n            else:\n                report.overall_status = CheckStatus.PASS\n            \n            # Generate recommendations\n            report.recommendations = self._generate_recommendations(report)\n            \n            execution_time = time.time() - start_time\n            self.logger.info(f\"Comprehensive check completed in {execution_time:.2f}s\")\n            self.logger.info(f\"Overall status: {report.overall_status.value}\")\n            \n        except Exception as e:\n            self.logger.error(f\"System check failed: {e}\")\n            report.overall_status = CheckStatus.FAIL\n            report.recommendations = [f\"System check failed: {str(e)}\"]\n        \n        return report\n    \n    async def _check_project_structure(self) -> CheckResult:\n        \"\"\"Check project structure compliance.\"\"\"\n        start_time = time.time()\n        \n        required_dirs = [\n            \"src/core\",\n            \"src/core/models\",\n            \"src/core/providers\", \n            \"src/core/tools\",\n            \"src/core/memory\",\n            \"src/core/runtime\",\n            \"src/core/security\",\n            \"tests\",\n            \"configs\",\n            \"specs\",\n            \"plans\",\n            \"tasks\"\n        ]\n        \n        required_files = [\n            \"src/core/models/contracts.py\",\n            \"src/core/providers/llm_qwen3.py\",\n            \"src/core/tools/mcp_adapter.py\",\n            \"src/core/memory/vector_pg.py\",\n            \"src/core/runtime/planner.py\",\n            \"src/core/runtime/reviewer.py\",\n            \"src/core/runtime/runner.py\",\n            \"src/core/security/policy_manager.py\",\n            \"configs/policies.yaml\",\n            \"specs/problem.md\",\n            \"specs/system.md\"\n        ]\n        \n        missing_dirs = []\n        missing_files = []\n        \n        for dir_path in required_dirs:\n            if not Path(dir_path).exists():\n                missing_dirs.append(dir_path)\n        \n        for file_path in required_files:\n            if not Path(file_path).exists():\n                missing_files.append(file_path)\n        \n        execution_time = time.time() - start_time\n        \n        if missing_dirs or missing_files:\n            return CheckResult(\n                name=\"project_structure\",\n                status=CheckStatus.FAIL,\n                severity=CheckSeverity.HIGH,\n                message=f\"Missing {len(missing_dirs)} directories and {len(missing_files)} files\",\n                details={\n                    \"missing_dirs\": missing_dirs,\n                    \"missing_files\": missing_files\n                },\n                execution_time=execution_time\n            )\n        else:\n            return CheckResult(\n                name=\"project_structure\",\n                status=CheckStatus.PASS,\n                severity=CheckSeverity.LOW,\n                message=\"Project structure is compliant\",\n                details={\n                    \"checked_dirs\": len(required_dirs),\n                    \"checked_files\": len(required_files)\n                },\n                execution_time=execution_time\n            )\n    \n    async def _check_dependencies(self) -> CheckResult:\n        \"\"\"Check dependency requirements.\"\"\"\n        start_time = time.time()\n        \n        required_packages = [\n            \"pydantic\",\n            \"torch\",\n            \"psutil\",\n            \"yaml\",\n            \"asyncio\"\n        ]\n        \n        missing_packages = []\n        \n        for package in required_packages:\n            try:\n                __import__(package)\n            except ImportError:\n                missing_packages.append(package)\n        \n        execution_time = time.time() - start_time\n        \n        if missing_packages:\n            return CheckResult(\n                name=\"dependencies\",\n                status=CheckStatus.FAIL,\n                severity=CheckSeverity.CRITICAL,\n                message=f\"Missing required packages: {', '.join(missing_packages)}\",\n                details={\"missing_packages\": missing_packages},\n                execution_time=execution_time\n            )\n        else:\n            return CheckResult(\n                name=\"dependencies\",\n                status=CheckStatus.PASS,\n                severity=CheckSeverity.LOW,\n                message=\"All required dependencies are available\",\n                details={\"checked_packages\": len(required_packages)},\n                execution_time=execution_time\n            )\n    \n    async def _check_test_coverage(self) -> CheckResult:\n        \"\"\"Check test coverage.\"\"\"\n        start_time = time.time()\n        \n        try:\n            # Run pytest with coverage\n            result = subprocess.run([\n                \"python3\", \"-m\", \"pytest\", \n                \"--cov=src\", \n                \"--cov-report=json\",\n                \"--cov-report=term-missing\",\n                \"-q\"\n            ], capture_output=True, text=True, timeout=60)\n            \n            # Parse coverage report\n            coverage_file = Path(\".coverage.json\")\n            if coverage_file.exists():\n                with open(coverage_file, 'r') as f:\n                    coverage_data = json.load(f)\n                \n                total_coverage = coverage_data.get('totals', {}).get('percent_covered', 0.0)\n                \n                execution_time = time.time() - start_time\n                \n                if total_coverage >= self.coverage_min:\n                    return CheckResult(\n                        name=\"test_coverage\",\n                        status=CheckStatus.PASS,\n                        severity=CheckSeverity.LOW,\n                        message=f\"Test coverage {total_coverage:.1f}% meets minimum requirement {self.coverage_min}%\",\n                        details={\n                            \"total_coverage\": total_coverage,\n                            \"minimum_required\": self.coverage_min,\n                            \"coverage_data\": coverage_data\n                        },\n                        execution_time=execution_time\n                    )\n                else:\n                    return CheckResult(\n                        name=\"test_coverage\",\n                        status=CheckStatus.FAIL,\n                        severity=CheckSeverity.HIGH,\n                        message=f\"Test coverage {total_coverage:.1f}% below minimum requirement {self.coverage_min}%\",\n                        details={\n                            \"total_coverage\": total_coverage,\n                            \"minimum_required\": self.coverage_min,\n                            \"coverage_data\": coverage_data\n                        },\n                        execution_time=execution_time\n                    )\n            else:\n                return CheckResult(\n                    name=\"test_coverage\",\n                    status=CheckStatus.FAIL,\n                    severity=CheckSeverity.HIGH,\n                    message=\"Coverage report not generated\",\n                    details={\"error\": \"No .coverage.json file found\"},\n                    execution_time=time.time() - start_time\n                )\n                \n        except subprocess.TimeoutExpired:\n            return CheckResult(\n                name=\"test_coverage\",\n                status=CheckStatus.FAIL,\n                severity=CheckSeverity.HIGH,\n                message=\"Test execution timed out\",\n                details={\"error\": \"pytest execution exceeded 60 seconds\"},\n                execution_time=time.time() - start_time\n            )\n        except Exception as e:\n            return CheckResult(\n                name=\"test_coverage\",\n                status=CheckStatus.FAIL,\n                severity=CheckSeverity.HIGH,\n                message=f\"Test coverage check failed: {str(e)}\",\n                details={\"error\": str(e)},\n                execution_time=time.time() - start_time\n            )\n    \n    async def _check_spec_compliance(self) -> CheckResult:\n        \"\"\"Check specification compliance.\"\"\"\n        start_time = time.time()\n        \n        spec_files = [\"specs/problem.md\", \"specs/system.md\"]\n        missing_specs = []\n        implemented_features = []\n        missing_features = []\n        \n        for spec_file in spec_files:\n            if not Path(spec_file).exists():\n                missing_specs.append(spec_file)\n        \n        # Check for implemented features based on existing files\n        feature_checks = {\n            \"Core Models\": Path(\"src/core/models/contracts.py\").exists(),\n            \"Qwen3 Provider\": Path(\"src/core/providers/llm_qwen3.py\").exists(),\n            \"MCP Adapter\": Path(\"src/core/tools/mcp_adapter.py\").exists(),\n            \"Vector Store\": Path(\"src/core/memory/vector_pg.py\").exists(),\n            \"Agent Planner\": Path(\"src/core/runtime/planner.py\").exists(),\n            \"Agent Reviewer\": Path(\"src/core/runtime/reviewer.py\").exists(),\n            \"Task Runner\": Path(\"src/core/runtime/runner.py\").exists(),\n            \"Security Policies\": Path(\"src/core/security/policy_manager.py\").exists(),\n            \"Model Policies\": Path(\"src/core/models/policy_manager.py\").exists()\n        }\n        \n        for feature, implemented in feature_checks.items():\n            if implemented:\n                implemented_features.append(feature)\n            else:\n                missing_features.append(feature)\n        \n        execution_time = time.time() - start_time\n        \n        if missing_specs:\n            return CheckResult(\n                name=\"spec_compliance\",\n                status=CheckStatus.FAIL,\n                severity=CheckSeverity.CRITICAL,\n                message=f\"Missing specification files: {', '.join(missing_specs)}\",\n                details={\n                    \"missing_specs\": missing_specs,\n                    \"implemented_features\": implemented_features,\n                    \"missing_features\": missing_features\n                },\n                execution_time=execution_time\n            )\n        elif missing_features:\n            return CheckResult(\n                name=\"spec_compliance\",\n                status=CheckStatus.WARNING,\n                severity=CheckSeverity.MEDIUM,\n                message=f\"Missing {len(missing_features)} features from specifications\",\n                details={\n                    \"implemented_features\": implemented_features,\n                    \"missing_features\": missing_features\n                },\n                execution_time=execution_time\n            )\n        else:\n            return CheckResult(\n                name=\"spec_compliance\",\n                status=CheckStatus.PASS,\n                severity=CheckSeverity.LOW,\n                message=\"All specification requirements are met\",\n                details={\n                    \"implemented_features\": implemented_features,\n                    \"missing_features\": missing_features\n                },\n                execution_time=execution_time\n            )\n    \n    async def _check_security_policies(self) -> CheckResult:\n        \"\"\"Check security policy implementation.\"\"\"\n        start_time = time.time()\n        \n        try:\n            # Import and test security policy manager\n            sys.path.insert(0, \"src\")\n            from core.security.policy_manager import SecurityPolicyManager\n            \n            manager = SecurityPolicyManager(\"configs/policies.yaml\")\n            \n            # Test redaction\n            test_text = \"My API key is sk-1234567890abcdef and email is user@example.com\"\n            redaction_result = manager.redact_text(test_text)\n            \n            # Test tool access\n            tool_result = manager.check_tool_access(\"mcp:sql\")\n            \n            execution_time = time.time() - start_time\n            \n            if redaction_result.redaction_count > 0 and tool_result.allowed:\n                return CheckResult(\n                    name=\"security_policies\",\n                    status=CheckStatus.PASS,\n                    severity=CheckSeverity.LOW,\n                    message=\"Security policies are properly implemented\",\n                    details={\n                        \"redaction_patterns\": len(manager.redaction_patterns),\n                        \"allowed_tools\": len(manager.allowed_tools),\n                        \"blocked_tools\": len(manager.blocked_tools),\n                        \"test_redaction_count\": redaction_result.redaction_count,\n                        \"test_tool_access\": tool_result.allowed\n                    },\n                    execution_time=execution_time\n                )\n            else:\n                return CheckResult(\n                    name=\"security_policies\",\n                    status=CheckStatus.FAIL,\n                    severity=CheckSeverity.HIGH,\n                    message=\"Security policy tests failed\",\n                    details={\n                        \"redaction_test\": redaction_result.redaction_count > 0,\n                        \"tool_access_test\": tool_result.allowed\n                    },\n                    execution_time=execution_time\n                )\n                \n        except Exception as e:\n            return CheckResult(\n                name=\"security_policies\",\n                status=CheckStatus.FAIL,\n                severity=CheckSeverity.CRITICAL,\n                message=f\"Security policy check failed: {str(e)}\",\n                details={\"error\": str(e)},\n                execution_time=time.time() - start_time\n            )\n    \n    async def _check_model_policies(self) -> CheckResult:\n        \"\"\"Check model policy implementation.\"\"\"\n        start_time = time.time()\n        \n        try:\n            # Import and test model policy manager\n            sys.path.insert(0, \"src\")\n            from core.models.policy_manager import ModelPolicyManager\n            \n            manager = ModelPolicyManager(\"configs/policies.yaml\")\n            \n            # Test model selection\n            from core.models.policy_manager import ModelRequest\n            request = ModelRequest(\n                input_type=\"multimodal\",\n                input_tokens=1000,\n                capabilities_required=[]\n            )\n            \n            selection = await manager.select_model(request)\n            \n            execution_time = time.time() - start_time\n            \n            if selection.selected_model and selection.confidence > 0:\n                return CheckResult(\n                    name=\"model_policies\",\n                    status=CheckStatus.PASS,\n                    severity=CheckSeverity.LOW,\n                    message=\"Model policies are properly implemented\",\n                    details={\n                        \"available_models\": len(manager.models),\n                        \"routing_rules\": len(manager.routing_rules),\n                        \"test_selection\": selection.selected_model,\n                        \"test_confidence\": selection.confidence\n                    },\n                    execution_time=execution_time\n                )\n            else:\n                return CheckResult(\n                    name=\"model_policies\",\n                    status=CheckStatus.FAIL,\n                    severity=CheckSeverity.HIGH,\n                    message=\"Model policy tests failed\",\n                    details={\n                        \"test_selection\": selection.selected_model,\n                        \"test_confidence\": selection.confidence\n                    },\n                    execution_time=execution_time\n                )\n                \n        except Exception as e:\n            return CheckResult(\n                name=\"model_policies\",\n                status=CheckStatus.FAIL,\n                severity=CheckSeverity.CRITICAL,\n                message=f\"Model policy check failed: {str(e)}\",\n                details={\"error\": str(e)},\n                execution_time=time.time() - start_time\n            )\n    \n    async def _check_code_quality(self) -> CheckResult:\n        \"\"\"Check code quality.\"\"\"\n        start_time = time.time()\n        \n        # Check for common code quality issues\n        issues = []\n        \n        # Check for TODO comments\n        for py_file in Path(\"src\").rglob(\"*.py\"):\n            try:\n                with open(py_file, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                    if \"TODO\" in content or \"FIXME\" in content:\n                        issues.append(f\"{py_file}: Contains TODO/FIXME comments\")\n            except Exception:\n                pass\n        \n        # Check for missing docstrings\n        for py_file in Path(\"src\").rglob(\"*.py\"):\n            try:\n                with open(py_file, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                    if \"def \" in content and '\"\"\"' not in content:\n                        issues.append(f\"{py_file}: Missing docstrings\")\n            except Exception:\n                pass\n        \n        execution_time = time.time() - start_time\n        \n        if issues:\n            return CheckResult(\n                name=\"code_quality\",\n                status=CheckStatus.WARNING,\n                severity=CheckSeverity.MEDIUM,\n                message=f\"Found {len(issues)} code quality issues\",\n                details={\"issues\": issues},\n                execution_time=execution_time\n            )\n        else:\n            return CheckResult(\n                name=\"code_quality\",\n                status=CheckStatus.PASS,\n                severity=CheckSeverity.LOW,\n                message=\"Code quality is acceptable\",\n                details={\"checked_files\": len(list(Path(\"src\").rglob(\"*.py\")))},\n                execution_time=execution_time\n            )\n    \n    async def _check_documentation(self) -> CheckResult:\n        \"\"\"Check documentation completeness.\"\"\"\n        start_time = time.time()\n        \n        required_docs = [\n            \"specs/problem.md\",\n            \"specs/system.md\",\n            \"plans/architecture.md\",\n            \"plans/milestones.md\"\n        ]\n        \n        missing_docs = []\n        for doc in required_docs:\n            if not Path(doc).exists():\n                missing_docs.append(doc)\n        \n        execution_time = time.time() - start_time\n        \n        if missing_docs:\n            return CheckResult(\n                name=\"documentation\",\n                status=CheckStatus.FAIL,\n                severity=CheckSeverity.MEDIUM,\n                message=f\"Missing documentation: {', '.join(missing_docs)}\",\n                details={\"missing_docs\": missing_docs},\n                execution_time=execution_time\n            )\n        else:\n            return CheckResult(\n                name=\"documentation\",\n                status=CheckStatus.PASS,\n                severity=CheckSeverity.LOW,\n                message=\"Documentation is complete\",\n                details={\"checked_docs\": len(required_docs)},\n                execution_time=execution_time\n            )\n    \n    async def _check_configuration(self) -> CheckResult:\n        \"\"\"Check configuration files.\"\"\"\n        start_time = time.time()\n        \n        config_files = [\"configs/policies.yaml\"]\n        invalid_configs = []\n        \n        for config_file in config_files:\n            try:\n                with open(config_file, 'r') as f:\n                    yaml.safe_load(f)\n            except Exception as e:\n                invalid_configs.append(f\"{config_file}: {str(e)}\")\n        \n        execution_time = time.time() - start_time\n        \n        if invalid_configs:\n            return CheckResult(\n                name=\"configuration\",\n                status=CheckStatus.FAIL,\n                severity=CheckSeverity.HIGH,\n                message=f\"Invalid configuration files: {', '.join(invalid_configs)}\",\n                details={\"invalid_configs\": invalid_configs},\n                execution_time=execution_time\n            )\n        else:\n            return CheckResult(\n                name=\"configuration\",\n                status=CheckStatus.PASS,\n                severity=CheckSeverity.LOW,\n                message=\"Configuration files are valid\",\n                details={\"checked_configs\": len(config_files)},\n                execution_time=execution_time\n            )\n    \n    async def _check_performance_requirements(self) -> CheckResult:\n        \"\"\"Check performance requirements.\"\"\"\n        start_time = time.time()\n        \n        # Check if performance-critical files exist\n        performance_files = [\n            \"src/core/providers/llm_qwen3.py\",\n            \"src/core/runtime/runner.py\",\n            \"src/core/memory/vector_pg.py\"\n        ]\n        \n        missing_files = []\n        for file_path in performance_files:\n            if not Path(file_path).exists():\n                missing_files.append(file_path)\n        \n        execution_time = time.time() - start_time\n        \n        if missing_files:\n            return CheckResult(\n                name=\"performance_requirements\",\n                status=CheckStatus.FAIL,\n                severity=CheckSeverity.HIGH,\n                message=f\"Missing performance-critical files: {', '.join(missing_files)}\",\n                details={\"missing_files\": missing_files},\n                execution_time=execution_time\n            )\n        else:\n            return CheckResult(\n                name=\"performance_requirements\",\n                status=CheckStatus.PASS,\n                severity=CheckSeverity.LOW,\n                message=\"Performance-critical components are present\",\n                details={\"checked_files\": len(performance_files)},\n                execution_time=execution_time\n            )\n    \n    def _generate_recommendations(self, report: SystemCheckReport) -> List[str]:\n        \"\"\"Generate recommendations based on check results.\"\"\"\n        recommendations = []\n        \n        if report.failed_checks > 0:\n            recommendations.append(f\"Address {report.failed_checks} failed checks\")\n        \n        if report.warning_checks > 0:\n            recommendations.append(f\"Review {report.warning_checks} warning checks\")\n        \n        if report.coverage_report and report.coverage_report.total_coverage < self.coverage_min:\n            recommendations.append(f\"Increase test coverage from {report.coverage_report.total_coverage:.1f}% to {self.coverage_min}%\")\n        \n        if report.critical_issues:\n            recommendations.append(f\"Fix {len(report.critical_issues)} critical issues immediately\")\n        \n        if report.overall_status == CheckStatus.PASS:\n            recommendations.append(\"System is in good condition - consider adding more comprehensive tests\")\n        \n        return recommendations\n\n\n# ============================================================================\n# Main Function\n# ============================================================================\n\nasync def main():\n    \"\"\"Main function for command-line usage.\"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"Comprehensive System Checker\")\n    parser.add_argument(\"--coverage-min\", type=float, default=80.0, help=\"Minimum coverage percentage\")\n    parser.add_argument(\"--spec-compliance\", action=\"store_true\", help=\"Require spec compliance\")\n    parser.add_argument(\"--fail-fast\", action=\"store_true\", help=\"Fail fast on critical issues\")\n    parser.add_argument(\"--output\", help=\"Output file for report\")\n    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Verbose output\")\n    \n    args = parser.parse_args()\n    \n    if args.verbose:\n        logging.getLogger().setLevel(logging.DEBUG)\n    \n    config = {\n        \"coverage_min\": args.coverage_min,\n        \"spec_compliance\": args.spec_compliance,\n        \"fail_fast\": args.fail_fast\n    }\n    \n    try:\n        checker = SystemChecker(config)\n        report = await checker.run_comprehensive_check()\n        \n        # Print summary\n        print(\"\\n\" + \"=\"*80)\n        print(\"COMPREHENSIVE SYSTEM CHECK REPORT\")\n        print(\"=\"*80)\n        print(f\"Report ID: {report.report_id}\")\n        print(f\"Timestamp: {report.timestamp}\")\n        print(f\"Overall Status: {report.overall_status.value.upper()}\")\n        print(f\"Total Checks: {report.total_checks}\")\n        print(f\"Passed: {report.passed_checks} \u2705\")\n        print(f\"Failed: {report.failed_checks} \u274c\")\n        print(f\"Warnings: {report.warning_checks} \u26a0\ufe0f\")\n        print(f\"Skipped: {report.skipped_checks} \u23ed\ufe0f\")\n        \n        if report.critical_issues:\n            print(f\"\\n\ud83d\udea8 CRITICAL ISSUES ({len(report.critical_issues)}):\")\n            for issue in report.critical_issues:\n                print(f\"   \u274c {issue.name}: {issue.message}\")\n        \n        if report.recommendations:\n            print(f\"\\n\ud83d\udca1 RECOMMENDATIONS:\")\n            for rec in report.recommendations:\n                print(f\"   \u2022 {rec}\")\n        \n        # Save report if requested\n        if args.output:\n            with open(args.output, 'w') as f:\n                json.dump(report.model_dump(), f, indent=2, default=str)\n            print(f\"\\n\ud83d\udcc4 Report saved to: {args.output}\")\n        \n        return 0 if report.overall_status == CheckStatus.PASS else 1\n        \n    except Exception as e:\n        logger.error(f\"System check failed: {e}\")\n        return 1\n\n\nif __name__ == \"__main__\":\n    exit(asyncio.run(main()))\n", "source": "src/core/system_checker.py"}, {"instruction": "Explain the following code from src/core/integration_test.py:", "response": "This code is from src/core/integration_test.py:\n\n#!/usr/bin/env python3\n\"\"\"\nAgentic LLM Core Integration Test\n\nThis script tests the integration of all core components:\n- Input Ingestion Service\n- Qwen3-Omni Engine\n- Processing Pipeline\n\nComplies with:\n- System Specification: Agentic LLM Core v0.1 (specs/system.md)\n- Architecture Plan: Agentic LLM Core v0.1 (plans/architecture.md)\n- Milestone 1: Core Pipeline Foundation (tasks/milestone_1_core_pipeline.md)\n\"\"\"\n\nimport asyncio\nimport logging\nimport sys\nfrom pathlib import Path\nimport time\nimport json\n\n# Setup logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# Add src to path for imports\nsys.path.append(str(Path(__file__).parent / \"src\"))\n\nfrom core.services.input_ingestion_service import InputIngestionService, InputValidator\nfrom core.engines.qwen3_omni_engine import Qwen3OmniEngine, ProcessedInput\nfrom core.schemas.input_schemas import TextInput, ImageInput, DocumentInput\n\nasync def test_integration():\n    \"\"\"Test the complete integration pipeline\"\"\"\n    logger.info(\"\ud83d\ude80 Starting Agentic LLM Core Integration Test\")\n    logger.info(\"=\" * 60)\n    \n    # Initialize components\n    logger.info(\"\ud83d\udce5 Initializing components...\")\n    \n    # Input ingestion service\n    validator = InputValidator()\n    ingestion_service = InputIngestionService(validator)\n    logger.info(\"\u2705 Input ingestion service initialized\")\n    \n    # Qwen3-Omni engine\n    model_path = \"./Qwen3-Omni-30B-A3B-Instruct\"\n    engine = Qwen3OmniEngine(model_path)\n    \n    if not await engine.initialize():\n        logger.error(\"\u274c Failed to initialize Qwen3-Omni engine\")\n        return False\n    \n    logger.info(\"\u2705 Qwen3-Omni engine initialized\")\n    \n    try:\n        # Test 1: Text Processing Pipeline\n        logger.info(\"\\n\ud83e\uddea Test 1: Text Processing Pipeline\")\n        logger.info(\"-\" * 40)\n        \n        text_content = \"What is artificial intelligence and how does it work?\"\n        logger.info(f\"Input: {text_content}\")\n        \n        # Step 1: Ingest text\n        start_time = time.time()\n        processed_input = await ingestion_service.ingest_text(\n            text_content,\n            {\"source\": \"integration_test\", \"language\": \"en\"}\n        )\n        ingestion_time = time.time() - start_time\n        logger.info(f\"\u2705 Text ingested in {ingestion_time:.3f}s\")\n        \n        # Step 2: Process multimodal input\n        start_time = time.time()\n        context = await engine.process_multimodal_input(processed_input)\n        processing_time = time.time() - start_time\n        logger.info(f\"\u2705 Context processed in {processing_time:.3f}s\")\n        \n        # Step 3: Analyze context\n        start_time = time.time()\n        analysis = await engine.analyze_context(context)\n        analysis_time = time.time() - start_time\n        logger.info(f\"\u2705 Context analyzed in {analysis_time:.3f}s\")\n        logger.info(f\"   Intent: {analysis.intent}\")\n        logger.info(f\"   Entities: {analysis.entities}\")\n        logger.info(f\"   Confidence: {analysis.confidence}\")\n        \n        # Step 4: Generate answer\n        start_time = time.time()\n        answer = await engine.generate_answer(analysis)\n        generation_time = time.time() - start_time\n        logger.info(f\"\u2705 Answer generated in {generation_time:.3f}s\")\n        logger.info(f\"   Answer: {answer.answer[:100]}...\")\n        logger.info(f\"   Confidence: {answer.confidence}\")\n        \n        total_time = ingestion_time + processing_time + analysis_time + generation_time\n        logger.info(f\"\ud83d\udcca Total pipeline time: {total_time:.3f}s\")\n        \n        # Test 2: Performance Validation\n        logger.info(\"\\n\ud83e\uddea Test 2: Performance Validation\")\n        logger.info(\"-\" * 40)\n        \n        # Check performance requirements from system.md\n        requirements_met = True\n        \n        if total_time > 5.0:  # < 5 seconds requirement\n            logger.warning(f\"\u26a0\ufe0f Total time {total_time:.3f}s exceeds 5s requirement\")\n            requirements_met = False\n        else:\n            logger.info(f\"\u2705 Total time {total_time:.3f}s meets <5s requirement\")\n        \n        if ingestion_time > 1.0:  # < 1 second requirement\n            logger.warning(f\"\u26a0\ufe0f Ingestion time {ingestion_time:.3f}s exceeds 1s requirement\")\n            requirements_met = False\n        else:\n            logger.info(f\"\u2705 Ingestion time {ingestion_time:.3f}s meets <1s requirement\")\n        \n        if analysis_time > 2.0:  # < 2 seconds requirement\n            logger.warning(f\"\u26a0\ufe0f Analysis time {analysis_time:.3f}s exceeds 2s requirement\")\n            requirements_met = False\n        else:\n            logger.info(f\"\u2705 Analysis time {analysis_time:.3f}s meets <2s requirement\")\n        \n        if generation_time > 3.0:  # < 3 seconds requirement\n            logger.warning(f\"\u26a0\ufe0f Generation time {generation_time:.3f}s exceeds 3s requirement\")\n            requirements_met = False\n        else:\n            logger.info(f\"\u2705 Generation time {generation_time:.3f}s meets <3s requirement\")\n        \n        # Test 3: Multiple Input Types\n        logger.info(\"\\n\ud83e\uddea Test 3: Multiple Input Types\")\n        logger.info(\"-\" * 40)\n        \n        # Test image input\n        try:\n            image_data = b\"fake_image_data_for_testing\"\n            image_input = await ingestion_service.ingest_image(\n                image_data,\n                {\"source\": \"integration_test\", \"format\": \"png\"}\n            )\n            logger.info(f\"\u2705 Image input processed: {image_input.input_type}\")\n            \n            # Process through pipeline\n            context = await engine.process_multimodal_input(image_input)\n            analysis = await engine.analyze_context(context)\n            logger.info(f\"\u2705 Image context analyzed: {analysis.intent}\")\n            \n        except Exception as e:\n            logger.warning(f\"\u26a0\ufe0f Image processing test failed: {e}\")\n        \n        # Test document input\n        try:\n            # Create temporary test document\n            test_doc = Path(\"integration_test_doc.txt\")\n            test_doc.write_text(\"This is a test document for integration testing.\")\n            \n            doc_input = await ingestion_service.ingest_document(\n                str(test_doc),\n                {\"source\": \"integration_test\"}\n            )\n            logger.info(f\"\u2705 Document input processed: {doc_input.input_type}\")\n            \n            # Process through pipeline\n            context = await engine.process_multimodal_input(doc_input)\n            analysis = await engine.analyze_context(context)\n            logger.info(f\"\u2705 Document context analyzed: {analysis.intent}\")\n            \n            # Clean up\n            if test_doc.exists():\n                test_doc.unlink()\n                \n        except Exception as e:\n            logger.warning(f\"\u26a0\ufe0f Document processing test failed: {e}\")\n        \n        # Test 4: Statistics and Metrics\n        logger.info(\"\\n\ud83e\uddea Test 4: Statistics and Metrics\")\n        logger.info(\"-\" * 40)\n        \n        # Get ingestion service stats\n        ingestion_stats = ingestion_service.get_stats()\n        logger.info(f\"\ud83d\udcca Ingestion stats: {json.dumps(ingestion_stats, indent=2)}\")\n        \n        # Get engine metrics\n        engine_metrics = engine.get_performance_metrics()\n        logger.info(f\"\ud83d\udcca Engine metrics: {json.dumps(engine_metrics, indent=2)}\")\n        \n        # Test 5: Error Handling\n        logger.info(\"\\n\ud83e\uddea Test 5: Error Handling\")\n        logger.info(\"-\" * 40)\n        \n        try:\n            # Test invalid text input\n            await ingestion_service.ingest_text(\"\")  # Empty content\n            logger.warning(\"\u26a0\ufe0f Empty text input should have failed\")\n        except Exception as e:\n            logger.info(f\"\u2705 Empty text input properly rejected: {e}\")\n        \n        try:\n            # Test invalid image format\n            await ingestion_service.ingest_image(\n                b\"fake_data\",\n                {\"format\": \"invalid_format\"}\n            )\n            logger.warning(\"\u26a0\ufe0f Invalid image format should have failed\")\n        except Exception as e:\n            logger.info(f\"\u2705 Invalid image format properly rejected: {e}\")\n        \n        # Final Results\n        logger.info(\"\\n\ud83c\udfaf Integration Test Results\")\n        logger.info(\"=\" * 60)\n        \n        if requirements_met:\n            logger.info(\"\u2705 All performance requirements met\")\n        else:\n            logger.warning(\"\u26a0\ufe0f Some performance requirements not met\")\n        \n        logger.info(\"\u2705 Integration test completed successfully!\")\n        logger.info(\"\ud83d\ude80 Agentic LLM Core is ready for development!\")\n        \n        return True\n        \n    except Exception as e:\n        logger.error(f\"\u274c Integration test failed: {e}\")\n        import traceback\n        traceback.print_exc()\n        return False\n        \n    finally:\n        # Cleanup\n        engine.cleanup()\n        logger.info(\"\u2705 Cleanup completed\")\n\nasync def main():\n    \"\"\"Main integration test function\"\"\"\n    success = await test_integration()\n    return success\n\nif __name__ == \"__main__\":\n    success = asyncio.run(main())\n    sys.exit(0 if success else 1)\n", "source": "src/core/integration_test.py"}, {"instruction": "Explain the following code from src/core/tools/catalog.py:", "response": "This code is from src/core/tools/catalog.py:\n\n\"\"\"\nTool Catalog Parser and Validator for Agentic LLM Core v0.1\n\nThis module provides functionality to parse, validate, and manage tool catalogs\nfrom YAML configuration files.\n\nCreated: 2024-09-24\nStatus: Draft\n\"\"\"\n\nfrom __future__ import annotations\n\nimport yaml\nimport logging\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional, Union\nfrom enum import Enum\nfrom pathlib import Path\n\nfrom pydantic import BaseModel, Field, validator, ValidationError\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n\n# ============================================================================\n# Data Models\n# ============================================================================\n\nclass SafetyLevel(str, Enum):\n    \"\"\"Tool safety levels.\"\"\"\n    SAFE = \"safe\"\n    MODERATE = \"moderate\"\n    DANGEROUS = \"dangerous\"\n    CRITICAL = \"critical\"\n\n\nclass ValidationSeverity(str, Enum):\n    \"\"\"Validation severity levels.\"\"\"\n    INFO = \"info\"\n    WARNING = \"warning\"\n    ERROR = \"error\"\n\n\nclass ToolParameter(BaseModel):\n    \"\"\"Tool parameter specification.\"\"\"\n    name: str = Field(..., description=\"Parameter name\")\n    type: str = Field(..., description=\"Parameter type\")\n    required: bool = Field(default=False, description=\"Whether parameter is required\")\n    default: Optional[Any] = Field(None, description=\"Default value\")\n    description: str = Field(..., description=\"Parameter description\")\n    validation: Optional[Dict[str, Any]] = Field(None, description=\"Validation rules\")\n\n\nclass ToolSpec(BaseModel):\n    \"\"\"Tool specification.\"\"\"\n    name: str = Field(..., description=\"Tool name\")\n    category: str = Field(..., description=\"Tool category\")\n    description: str = Field(..., description=\"Tool description\")\n    parameters: List[ToolParameter] = Field(default_factory=list, description=\"Tool parameters\")\n    returns: Dict[str, str] = Field(default_factory=dict, description=\"Return value specifications\")\n    safety_level: SafetyLevel = Field(..., description=\"Tool safety level\")\n    requires_permissions: List[str] = Field(default_factory=list, description=\"Required permissions\")\n\n\nclass ToolCategory(BaseModel):\n    \"\"\"Tool category specification.\"\"\"\n    name: str = Field(..., description=\"Category name\")\n    description: str = Field(..., description=\"Category description\")\n    priority: str = Field(..., description=\"Category priority\")\n    offline_compatible: bool = Field(..., description=\"Offline compatibility\")\n\n\nclass ValidationRule(BaseModel):\n    \"\"\"Validation rule specification.\"\"\"\n    name: str = Field(..., description=\"Rule name\")\n    description: str = Field(..., description=\"Rule description\")\n    severity: ValidationSeverity = Field(..., description=\"Rule severity\")\n\n\nclass ToolCatalog(BaseModel):\n    \"\"\"Complete tool catalog.\"\"\"\n    version: str = Field(..., description=\"Catalog version\")\n    last_updated: str = Field(..., description=\"Last update timestamp\")\n    description: str = Field(..., description=\"Catalog description\")\n    categories: List[ToolCategory] = Field(default_factory=list, description=\"Tool categories\")\n    tools: List[ToolSpec] = Field(default_factory=list, description=\"Tool specifications\")\n    validation: Optional[Dict[str, Any]] = Field(None, description=\"Validation rules\")\n    policies: Optional[Dict[str, Any]] = Field(None, description=\"Tool policies\")\n\n\nclass CatalogValidationResult(BaseModel):\n    \"\"\"Tool catalog validation result.\"\"\"\n    is_valid: bool = Field(..., description=\"Whether catalog is valid\")\n    errors: List[str] = Field(default_factory=list, description=\"Validation errors\")\n    warnings: List[str] = Field(default_factory=list, description=\"Validation warnings\")\n    info: List[str] = Field(default_factory=list, description=\"Validation info\")\n    tool_count: int = Field(0, description=\"Number of tools\")\n    category_count: int = Field(0, description=\"Number of categories\")\n    validation_summary: Dict[str, Any] = Field(default_factory=dict, description=\"Validation summary\")\n\n\nclass ToolSummary(BaseModel):\n    \"\"\"Tool summary for reporting.\"\"\"\n    name: str = Field(..., description=\"Tool name\")\n    category: str = Field(..., description=\"Tool category\")\n    description: str = Field(..., description=\"Tool description\")\n    safety_level: SafetyLevel = Field(..., description=\"Safety level\")\n    parameter_count: int = Field(0, description=\"Number of parameters\")\n    required_permissions: List[str] = Field(default_factory=list, description=\"Required permissions\")\n    offline_compatible: bool = Field(True, description=\"Offline compatibility\")\n\n\n# ============================================================================\n# Tool Catalog Parser\n# ============================================================================\n\nclass ToolCatalogParser:\n    \"\"\"Parser for tool catalog YAML files.\"\"\"\n    \n    def __init__(self):\n        self.logger = logging.getLogger(__name__)\n    \n    def parse_file(self, file_path: Union[str, Path]) -> ToolCatalog:\n        \"\"\"Parse a tool catalog from YAML file.\"\"\"\n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                data = yaml.safe_load(f)\n            \n            self.logger.info(f\"Successfully loaded tool catalog from {file_path}\")\n            return self._parse_data(data)\n            \n        except FileNotFoundError:\n            raise FileNotFoundError(f\"Tool catalog file not found: {file_path}\")\n        except yaml.YAMLError as e:\n            raise ValueError(f\"Invalid YAML format in {file_path}: {e}\")\n        except Exception as e:\n            raise RuntimeError(f\"Failed to parse tool catalog from {file_path}: {e}\")\n    \n    def _parse_data(self, data: Dict[str, Any]) -> ToolCatalog:\n        \"\"\"Parse tool catalog data.\"\"\"\n        try:\n            # Parse tool catalog structure\n            catalog_data = data.get('tool_catalog', {})\n            tools_data = data.get('tools', [])\n            validation_data = data.get('validation', {})\n            policies_data = data.get('policies', {})\n            \n            # Parse categories\n            categories = []\n            for cat_data in catalog_data.get('categories', []):\n                categories.append(ToolCategory(**cat_data))\n            \n            # Parse tools\n            tools = []\n            for tool_data in tools_data:\n                # Parse parameters\n                parameters = []\n                for param_data in tool_data.get('parameters', []):\n                    parameters.append(ToolParameter(**param_data))\n                \n                tool_data['parameters'] = parameters\n                tools.append(ToolSpec(**tool_data))\n            \n            return ToolCatalog(\n                version=catalog_data.get('version', '0.1.0'),\n                last_updated=catalog_data.get('last_updated', datetime.now().isoformat()),\n                description=catalog_data.get('description', 'Tool catalog'),\n                categories=categories,\n                tools=tools,\n                validation=validation_data,\n                policies=policies_data\n            )\n            \n        except ValidationError as e:\n            raise ValueError(f\"Invalid tool catalog structure: {e}\")\n        except Exception as e:\n            raise RuntimeError(f\"Failed to parse tool catalog data: {e}\")\n\n\n# ============================================================================\n# Tool Catalog Validator\n# ============================================================================\n\nclass ToolCatalogValidator:\n    \"\"\"Validator for tool catalogs.\"\"\"\n    \n    def __init__(self):\n        self.logger = logging.getLogger(__name__)\n    \n    def validate(self, catalog: ToolCatalog) -> CatalogValidationResult:\n        \"\"\"Validate a tool catalog.\"\"\"\n        errors = []\n        warnings = []\n        info = []\n        \n        # Basic validation\n        self._validate_basic_structure(catalog, errors, warnings, info)\n        \n        # Tool validation\n        self._validate_tools(catalog, errors, warnings, info)\n        \n        # Category validation\n        self._validate_categories(catalog, errors, warnings, info)\n        \n        # Policy validation\n        self._validate_policies(catalog, errors, warnings, info)\n        \n        # Generate summary\n        validation_summary = self._generate_validation_summary(catalog, errors, warnings, info)\n        \n        return CatalogValidationResult(\n            is_valid=len(errors) == 0,\n            errors=errors,\n            warnings=warnings,\n            info=info,\n            tool_count=len(catalog.tools),\n            category_count=len(catalog.categories),\n            validation_summary=validation_summary\n        )\n    \n    def _validate_basic_structure(self, catalog: ToolCatalog, errors: List[str], warnings: List[str], info: List[str]):\n        \"\"\"Validate basic catalog structure.\"\"\"\n        if not catalog.tools:\n            errors.append(\"Tool catalog must contain at least one tool\")\n        \n        if not catalog.categories:\n            warnings.append(\"Tool catalog contains no categories\")\n        \n        if catalog.version == \"0.1.0\":\n            info.append(\"Using initial version of tool catalog\")\n    \n    def _validate_tools(self, catalog: ToolCatalog, errors: List[str], warnings: List[str], info: List[str]):\n        \"\"\"Validate tool specifications.\"\"\"\n        tool_names = set()\n        category_names = {cat.name for cat in catalog.categories}\n        \n        for tool in catalog.tools:\n            # Check for duplicate tool names\n            if tool.name in tool_names:\n                errors.append(f\"Duplicate tool name: {tool.name}\")\n            tool_names.add(tool.name)\n            \n            # Check category exists\n            if tool.category not in category_names:\n                errors.append(f\"Tool '{tool.name}' references unknown category: {tool.category}\")\n            \n            # Check parameter validation\n            for param in tool.parameters:\n                if param.required and param.default is not None:\n                    warnings.append(f\"Tool '{tool.name}' parameter '{param.name}' is required but has default value\")\n            \n            # Check safety level\n            if tool.safety_level in [SafetyLevel.DANGEROUS, SafetyLevel.CRITICAL]:\n                if not tool.requires_permissions:\n                    warnings.append(f\"Tool '{tool.name}' has {tool.safety_level} safety level but no required permissions\")\n    \n    def _validate_categories(self, catalog: ToolCatalog, errors: List[str], warnings: List[str], info: List[str]):\n        \"\"\"Validate category specifications.\"\"\"\n        category_names = set()\n        \n        for category in catalog.categories:\n            if category.name in category_names:\n                errors.append(f\"Duplicate category name: {category.name}\")\n            category_names.add(category.name)\n            \n            if category.priority not in [\"low\", \"medium\", \"high\", \"critical\"]:\n                warnings.append(f\"Category '{category.name}' has invalid priority: {category.priority}\")\n    \n    def _validate_policies(self, catalog: ToolCatalog, errors: List[str], warnings: List[str], info: List[str]):\n        \"\"\"Validate tool policies.\"\"\"\n        if catalog.policies:\n            policies = catalog.policies\n            \n            # Check execution policies\n            if 'execution' in policies:\n                exec_policies = policies['execution']\n                if exec_policies.get('max_concurrent_tools', 0) <= 0:\n                    errors.append(\"max_concurrent_tools must be positive\")\n                \n                if exec_policies.get('default_timeout', 0) <= 0:\n                    errors.append(\"default_timeout must be positive\")\n            \n            # Check security policies\n            if 'security' in policies:\n                sec_policies = policies['security']\n                if not sec_policies.get('require_authentication', False):\n                    warnings.append(\"Authentication is not required - consider enabling for production\")\n    \n    def _generate_validation_summary(self, catalog: ToolCatalog, errors: List[str], warnings: List[str], info: List[str]) -> Dict[str, Any]:\n        \"\"\"Generate validation summary.\"\"\"\n        return {\n            \"total_tools\": len(catalog.tools),\n            \"total_categories\": len(catalog.categories),\n            \"tools_by_category\": self._count_tools_by_category(catalog),\n            \"tools_by_safety_level\": self._count_tools_by_safety_level(catalog),\n            \"validation_issues\": {\n                \"errors\": len(errors),\n                \"warnings\": len(warnings),\n                \"info\": len(info)\n            },\n            \"offline_compatible_tools\": self._count_offline_compatible_tools(catalog),\n            \"tools_requiring_permissions\": self._count_tools_requiring_permissions(catalog)\n        }\n    \n    def _count_tools_by_category(self, catalog: ToolCatalog) -> Dict[str, int]:\n        \"\"\"Count tools by category.\"\"\"\n        counts = {}\n        for tool in catalog.tools:\n            counts[tool.category] = counts.get(tool.category, 0) + 1\n        return counts\n    \n    def _count_tools_by_safety_level(self, catalog: ToolCatalog) -> Dict[str, int]:\n        \"\"\"Count tools by safety level.\"\"\"\n        counts = {}\n        for tool in catalog.tools:\n            counts[tool.safety_level.value] = counts.get(tool.safety_level.value, 0) + 1\n        return counts\n    \n    def _count_offline_compatible_tools(self, catalog: ToolCatalog) -> int:\n        \"\"\"Count offline compatible tools.\"\"\"\n        compatible_categories = {cat.name for cat in catalog.categories if cat.offline_compatible}\n        return sum(1 for tool in catalog.tools if tool.category in compatible_categories)\n    \n    def _count_tools_requiring_permissions(self, catalog: ToolCatalog) -> int:\n        \"\"\"Count tools requiring permissions.\"\"\"\n        return sum(1 for tool in catalog.tools if tool.requires_permissions)\n\n\n# ============================================================================\n# Tool Catalog Manager\n# ============================================================================\n\nclass ToolCatalogManager:\n    \"\"\"Manager for tool catalogs.\"\"\"\n    \n    def __init__(self):\n        self.parser = ToolCatalogParser()\n        self.validator = ToolCatalogValidator()\n        self.logger = logging.getLogger(__name__)\n    \n    def load_and_validate(self, file_path: Union[str, Path], validate: bool = True) -> tuple[ToolCatalog, Optional[CatalogValidationResult]]:\n        \"\"\"Load and optionally validate a tool catalog.\"\"\"\n        catalog = self.parser.parse_file(file_path)\n        \n        validation_result = None\n        if validate:\n            validation_result = self.validator.validate(catalog)\n            \n            if not validation_result.is_valid:\n                self.logger.error(f\"Tool catalog validation failed with {len(validation_result.errors)} errors\")\n            else:\n                self.logger.info(f\"Tool catalog validation passed with {len(validation_result.warnings)} warnings\")\n        \n        return catalog, validation_result\n    \n    def generate_tool_summaries(self, catalog: ToolCatalog) -> List[ToolSummary]:\n        \"\"\"Generate tool summaries.\"\"\"\n        summaries = []\n        category_map = {cat.name: cat for cat in catalog.categories}\n        \n        for tool in catalog.tools:\n            category = category_map.get(tool.category)\n            summary = ToolSummary(\n                name=tool.name,\n                category=tool.category,\n                description=tool.description,\n                safety_level=tool.safety_level,\n                parameter_count=len(tool.parameters),\n                required_permissions=tool.requires_permissions,\n                offline_compatible=category.offline_compatible if category else True\n            )\n            summaries.append(summary)\n        \n        return summaries\n    \n    def print_summaries(self, summaries: List[ToolSummary]):\n        \"\"\"Print tool summaries in a formatted way.\"\"\"\n        print(\"\\n\" + \"=\"*80)\n        print(\"TOOL CATALOG SUMMARIES\")\n        print(\"=\"*80)\n        \n        # Group by category\n        by_category = {}\n        for summary in summaries:\n            if summary.category not in by_category:\n                by_category[summary.category] = []\n            by_category[summary.category].append(summary)\n        \n        for category, tools in by_category.items():\n            print(f\"\\n\ud83d\udcc1 {category.upper()} ({len(tools)} tools)\")\n            print(\"-\" * 50)\n            \n            for tool in tools:\n                safety_emoji = {\n                    SafetyLevel.SAFE: \"\ud83d\udfe2\",\n                    SafetyLevel.MODERATE: \"\ud83d\udfe1\", \n                    SafetyLevel.DANGEROUS: \"\ud83d\udfe0\",\n                    SafetyLevel.CRITICAL: \"\ud83d\udd34\"\n                }.get(tool.safety_level, \"\u26aa\")\n                \n                offline_emoji = \"\ud83c\udfe0\" if tool.offline_compatible else \"\ud83c\udf10\"\n                perm_emoji = \"\ud83d\udd10\" if tool.required_permissions else \"\ud83d\udd13\"\n                \n                print(f\"  {safety_emoji} {tool.name}\")\n                print(f\"    Description: {tool.description}\")\n                print(f\"    Parameters: {tool.parameter_count}\")\n                print(f\"    Safety: {tool.safety_level.value}\")\n                print(f\"    {offline_emoji} Offline: {'Yes' if tool.offline_compatible else 'No'}\")\n                print(f\"    {perm_emoji} Permissions: {', '.join(tool.required_permissions) if tool.required_permissions else 'None'}\")\n                print()\n\n\n# ============================================================================\n# Main Function\n# ============================================================================\n\ndef main():\n    \"\"\"Main function for command-line usage.\"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"Tool Catalog Parser and Validator\")\n    parser.add_argument(\"source\", help=\"Path to the tool catalog YAML file\")\n    parser.add_argument(\"--validate\", action=\"store_true\", help=\"Validate the catalog\")\n    parser.add_argument(\"--print-summaries\", action=\"store_true\", help=\"Print tool summaries\")\n    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Verbose output\")\n    \n    args = parser.parse_args()\n    \n    if args.verbose:\n        logging.getLogger().setLevel(logging.DEBUG)\n    \n    try:\n        manager = ToolCatalogManager()\n        catalog, validation_result = manager.load_and_validate(args.source, validate=args.validate)\n        \n        print(f\"\u2705 Successfully loaded tool catalog: {catalog.description}\")\n        print(f\"\ud83d\udcca Version: {catalog.version}\")\n        print(f\"\ud83d\udcc5 Last Updated: {catalog.last_updated}\")\n        print(f\"\ud83d\udd27 Tools: {len(catalog.tools)}\")\n        print(f\"\ud83d\udcc1 Categories: {len(catalog.categories)}\")\n        \n        if validation_result:\n            print(f\"\\n\ud83d\udd0d Validation Results:\")\n            print(f\"   Status: {'\u2705 Valid' if validation_result.is_valid else '\u274c Invalid'}\")\n            print(f\"   Errors: {len(validation_result.errors)}\")\n            print(f\"   Warnings: {len(validation_result.warnings)}\")\n            print(f\"   Info: {len(validation_result.info)}\")\n            \n            if validation_result.errors:\n                print(\"\\n\u274c Errors:\")\n                for error in validation_result.errors:\n                    print(f\"   - {error}\")\n            \n            if validation_result.warnings:\n                print(\"\\n\u26a0\ufe0f  Warnings:\")\n                for warning in validation_result.warnings:\n                    print(f\"   - {warning}\")\n        \n        if args.print_summaries:\n            summaries = manager.generate_tool_summaries(catalog)\n            manager.print_summaries(summaries)\n        \n    except Exception as e:\n        logger.error(f\"Failed to process tool catalog: {e}\")\n        return 1\n    \n    return 0\n\n\nif __name__ == \"__main__\":\n    exit(main())\n", "source": "src/core/tools/catalog.py"}, {"instruction": "Explain the following code from src/core/tools/mcp_adapter.py:", "response": "This code is from src/core/tools/mcp_adapter.py:\n\n\"\"\"\nMCP (Model Context Protocol) Adapter for Agentic LLM Core v0.1\n\nThis module provides an MCP adapter with support for:\n- stdin/stdout and socket communication\n- Input/output schema validation\n- Redaction for keys, secrets, and PII\n- Comprehensive error handling and security\n\nCreated: 2024-09-24\nStatus: Draft\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport re\nimport socket\nimport sys\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Union, AsyncGenerator, Callable\nfrom uuid import uuid4\n\nimport pydantic\nfrom pydantic import BaseModel, Field, validator\n\nfrom ..models.contracts import ToolCall, ToolResult, ToolSchema\n\n\n# ============================================================================\n# MCP Protocol Definitions\n# ============================================================================\n\nclass MCPTransportType(str, Enum):\n    \"\"\"Supported MCP transport types.\"\"\"\n    STDIN_STDOUT = \"stdin_stdout\"\n    SOCKET = \"socket\"\n    WEBSOCKET = \"websocket\"  # Future support\n\n\nclass MCPMessageType(str, Enum):\n    \"\"\"MCP message types.\"\"\"\n    REQUEST = \"request\"\n    RESPONSE = \"response\"\n    NOTIFICATION = \"notification\"\n    ERROR = \"error\"\n\n\nclass MCPMethod(str, Enum):\n    \"\"\"MCP methods.\"\"\"\n    # Tool methods\n    TOOLS_LIST = \"tools/list\"\n    TOOLS_CALL = \"tools/call\"\n    \n    # Resource methods\n    RESOURCES_LIST = \"resources/list\"\n    RESOURCES_READ = \"resources/read\"\n    \n    # Prompt methods\n    PROMPTS_LIST = \"prompts/list\"\n    PROMPTS_GET = \"prompts/get\"\n    \n    # Completion methods\n    COMPLETION = \"completion\"\n\n\n# ============================================================================\n# MCP Message Models\n# ============================================================================\n\nclass MCPMessage(BaseModel):\n    \"\"\"Base MCP message model.\"\"\"\n    jsonrpc: str = Field(default=\"2.0\", description=\"JSON-RPC version\")\n    id: Optional[Union[str, int]] = Field(None, description=\"Message ID\")\n    method: Optional[str] = Field(None, description=\"Method name\")\n    params: Optional[Dict[str, Any]] = Field(None, description=\"Method parameters\")\n    result: Optional[Any] = Field(None, description=\"Response result\")\n    error: Optional[Dict[str, Any]] = Field(None, description=\"Error information\")\n\n\nclass MCPRequest(BaseModel):\n    \"\"\"MCP request message.\"\"\"\n    jsonrpc: str = Field(default=\"2.0\")\n    id: Union[str, int] = Field(..., description=\"Request ID\")\n    method: str = Field(..., description=\"Method name\")\n    params: Dict[str, Any] = Field(default_factory=dict, description=\"Method parameters\")\n\n\nclass MCPResponse(BaseModel):\n    \"\"\"MCP response message.\"\"\"\n    jsonrpc: str = Field(default=\"2.0\")\n    id: Union[str, int] = Field(..., description=\"Request ID\")\n    result: Optional[Any] = Field(None, description=\"Response result\")\n    error: Optional[MCPError] = Field(None, description=\"Error information\")\n\n\nclass MCPNotification(BaseModel):\n    \"\"\"MCP notification message.\"\"\"\n    jsonrpc: str = Field(default=\"2.0\")\n    method: str = Field(..., description=\"Method name\")\n    params: Dict[str, Any] = Field(default_factory=dict, description=\"Method parameters\")\n\n\nclass MCPError(BaseModel):\n    \"\"\"MCP error information.\"\"\"\n    code: int = Field(..., description=\"Error code\")\n    message: str = Field(..., description=\"Error message\")\n    data: Optional[Any] = Field(None, description=\"Additional error data\")\n\n\n# ============================================================================\n# Schema Validation Models\n# ============================================================================\n\nclass MCPSchemaValidator:\n    \"\"\"Schema validator for MCP messages.\"\"\"\n    \n    def __init__(self):\n        self.input_schemas: Dict[str, pydantic.BaseModel] = {}\n        self.output_schemas: Dict[str, pydantic.BaseModel] = {}\n    \n    def register_input_schema(self, method: str, schema: pydantic.BaseModel):\n        \"\"\"Register input schema for a method.\"\"\"\n        self.input_schemas[method] = schema\n    \n    def register_output_schema(self, method: str, schema: pydantic.BaseModel):\n        \"\"\"Register output schema for a method.\"\"\"\n        self.output_schemas[method] = schema\n    \n    def validate_input(self, method: str, params: Dict[str, Any]) -> pydantic.BaseModel:\n        \"\"\"Validate input parameters against schema.\"\"\"\n        if method not in self.input_schemas:\n            raise ValueError(f\"No input schema registered for method: {method}\")\n        \n        try:\n            return self.input_schemas[method](**params)\n        except pydantic.ValidationError as e:\n            raise ValueError(f\"Input validation failed for {method}: {e}\")\n    \n    def validate_output(self, method: str, result: Any) -> pydantic.BaseModel:\n        \"\"\"Validate output result against schema.\"\"\"\n        if method not in self.output_schemas:\n            raise ValueError(f\"No output schema registered for method: {method}\")\n        \n        try:\n            if isinstance(result, dict):\n                return self.output_schemas[method](**result)\n            else:\n                return self.output_schemas[method](result=result)\n        except pydantic.ValidationError as e:\n            raise ValueError(f\"Output validation failed for {method}: {e}\")\n\n\n# ============================================================================\n# Redaction System\n# ============================================================================\n\nclass RedactionPattern(BaseModel):\n    \"\"\"Pattern for redaction.\"\"\"\n    name: str = Field(..., description=\"Pattern name\")\n    regex: str = Field(..., description=\"Regex pattern\")\n    replacement: str = Field(default=\"[REDACTED]\", description=\"Replacement text\")\n    case_sensitive: bool = Field(default=False, description=\"Case sensitive matching\")\n\n\nclass RedactionConfig(BaseModel):\n    \"\"\"Configuration for redaction system.\"\"\"\n    enable_redaction: bool = Field(default=True, description=\"Enable redaction\")\n    redact_keys: bool = Field(default=True, description=\"Redact API keys\")\n    redact_secrets: bool = Field(default=True, description=\"Redact secrets\")\n    redact_pii: bool = Field(default=True, description=\"Redact PII\")\n    custom_patterns: List[RedactionPattern] = Field(default_factory=list, description=\"Custom patterns\")\n\n\nclass MCPRedactor:\n    \"\"\"Redaction system for MCP messages.\"\"\"\n    \n    def __init__(self, config: RedactionConfig):\n        self.config = config\n        self.patterns = self._build_patterns()\n    \n    def _build_patterns(self) -> List[RedactionPattern]:\n        \"\"\"Build redaction patterns.\"\"\"\n        patterns = []\n        \n        if self.config.redact_keys:\n            patterns.extend([\n                RedactionPattern(\n                    name=\"api_key\",\n                    regex=r'(?i)(api[_-]?key|apikey)\\s*[:=]\\s*[\"\\']?([a-zA-Z0-9_\\-]{20,})[\"\\']?',\n                    replacement='\\\\1: [REDACTED_API_KEY]'\n                ),\n                RedactionPattern(\n                    name=\"bearer_token\",\n                    regex=r'(?i)(bearer|authorization)\\s*[:=]\\s*[\"\\']?([a-zA-Z0-9_\\-\\.]{20,})[\"\\']?',\n                    replacement='\\\\1: [REDACTED_TOKEN]'\n                ),\n                RedactionPattern(\n                    name=\"access_key\",\n                    regex=r'(?i)(access[_-]?key|accesskey)\\s*[:=]\\s*[\"\\']?([a-zA-Z0-9_\\-]{20,})[\"\\']?',\n                    replacement='\\\\1: [REDACTED_ACCESS_KEY]'\n                )\n            ])\n        \n        if self.config.redact_secrets:\n            patterns.extend([\n                RedactionPattern(\n                    name=\"password\",\n                    regex=r'(?i)(password|passwd|pwd)\\s*[:=]\\s*[\"\\']?([^\"\\'\\s]+)[\"\\']?',\n                    replacement='\\\\1: [REDACTED_PASSWORD]'\n                ),\n                RedactionPattern(\n                    name=\"secret\",\n                    regex=r'(?i)(secret|secret[_-]?key)\\s*[:=]\\s*[\"\\']?([a-zA-Z0-9_\\-]{16,})[\"\\']?',\n                    replacement='\\\\1: [REDACTED_SECRET]'\n                ),\n                RedactionPattern(\n                    name=\"private_key\",\n                    regex=r'-----BEGIN\\s+(?:RSA\\s+)?PRIVATE\\s+KEY-----.*?-----END\\s+(?:RSA\\s+)?PRIVATE\\s+KEY-----',\n                    replacement='[REDACTED_PRIVATE_KEY]'\n                )\n            ])\n        \n        if self.config.redact_pii:\n            patterns.extend([\n                RedactionPattern(\n                    name=\"email\",\n                    regex=r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n                    replacement='[REDACTED_EMAIL]'\n                ),\n                RedactionPattern(\n                    name=\"phone\",\n                    regex=r'\\b(?:\\+?1[-.\\s]?)?\\(?([0-9]{3})\\)?[-.\\s]?([0-9]{3})[-.\\s]?([0-9]{4})\\b',\n                    replacement='[REDACTED_PHONE]'\n                ),\n                RedactionPattern(\n                    name=\"ssn\",\n                    regex=r'\\b\\d{3}-?\\d{2}-?\\d{4}\\b',\n                    replacement='[REDACTED_SSN]'\n                ),\n                RedactionPattern(\n                    name=\"credit_card\",\n                    regex=r'\\b(?:\\d{4}[-\\s]?){3}\\d{4}\\b',\n                    replacement='[REDACTED_CREDIT_CARD]'\n                )\n            ])\n        \n        # Add custom patterns\n        patterns.extend(self.config.custom_patterns)\n        \n        return patterns\n    \n    def redact_text(self, text: str) -> str:\n        \"\"\"Redact sensitive information from text.\"\"\"\n        if not self.config.enable_redaction:\n            return text\n        \n        redacted = text\n        for pattern in self.patterns:\n            flags = 0 if pattern.case_sensitive else re.IGNORECASE\n            redacted = re.sub(pattern.regex, pattern.replacement, redacted, flags=flags)\n        \n        return redacted\n    \n    def redact_dict(self, data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Redact sensitive information from dictionary.\"\"\"\n        if not self.config.enable_redaction:\n            return data\n        \n        redacted = {}\n        for key, value in data.items():\n            if isinstance(value, str):\n                redacted[key] = self.redact_text(value)\n            elif isinstance(value, dict):\n                redacted[key] = self.redact_dict(value)\n            elif isinstance(value, list):\n                redacted[key] = [self.redact_dict(item) if isinstance(item, dict) else self.redact_text(item) if isinstance(item, str) else item for item in value]\n            else:\n                redacted[key] = value\n        \n        return redacted\n\n\n# ============================================================================\n# Transport Layer\n# ============================================================================\n\nclass MCPTransport(ABC):\n    \"\"\"Abstract base class for MCP transports.\"\"\"\n    \n    @abstractmethod\n    async def send_message(self, message: MCPMessage) -> None:\n        \"\"\"Send a message.\"\"\"\n        pass\n    \n    @abstractmethod\n    async def receive_message(self) -> MCPMessage:\n        \"\"\"Receive a message.\"\"\"\n        pass\n    \n    @abstractmethod\n    async def close(self) -> None:\n        \"\"\"Close the transport.\"\"\"\n        pass\n\n\nclass MCPStdioTransport(MCPTransport):\n    \"\"\"MCP transport using stdin/stdout.\"\"\"\n    \n    def __init__(self):\n        self.stdin = sys.stdin\n        self.stdout = sys.stdout\n        self.stderr = sys.stderr\n        self.is_closed = False\n    \n    async def send_message(self, message: MCPMessage) -> None:\n        \"\"\"Send message via stdout.\"\"\"\n        if self.is_closed:\n            raise ConnectionError(\"Transport is closed\")\n        \n        try:\n            message_str = message.json() + \"\\n\"\n            self.stdout.write(message_str)\n            self.stdout.flush()\n        except Exception as e:\n            raise ConnectionError(f\"Failed to send message: {e}\")\n    \n    async def receive_message(self) -> MCPMessage:\n        \"\"\"Receive message via stdin.\"\"\"\n        if self.is_closed:\n            raise ConnectionError(\"Transport is closed\")\n        \n        try:\n            line = await asyncio.get_event_loop().run_in_executor(None, self.stdin.readline)\n            if not line:\n                raise ConnectionError(\"End of input\")\n            \n            message_data = json.loads(line.strip())\n            return MCPMessage(**message_data)\n        except json.JSONDecodeError as e:\n            raise ValueError(f\"Invalid JSON message: {e}\")\n        except Exception as e:\n            raise ConnectionError(f\"Failed to receive message: {e}\")\n    \n    async def close(self) -> None:\n        \"\"\"Close the transport.\"\"\"\n        self.is_closed = True\n\n\nclass MCPSocketTransport(MCPTransport):\n    \"\"\"MCP transport using socket connection.\"\"\"\n    \n    def __init__(self, host: str = \"localhost\", port: int = 8080):\n        self.host = host\n        self.port = port\n        self.reader: Optional[asyncio.StreamReader] = None\n        self.writer: Optional[asyncio.StreamWriter] = None\n        self.is_connected = False\n    \n    async def connect(self) -> None:\n        \"\"\"Connect to the socket.\"\"\"\n        try:\n            self.reader, self.writer = await asyncio.open_connection(self.host, self.port)\n            self.is_connected = True\n        except Exception as e:\n            raise ConnectionError(f\"Failed to connect to {self.host}:{self.port}: {e}\")\n    \n    async def send_message(self, message: MCPMessage) -> None:\n        \"\"\"Send message via socket.\"\"\"\n        if not self.is_connected or not self.writer:\n            raise ConnectionError(\"Not connected\")\n        \n        try:\n            message_str = message.json() + \"\\n\"\n            self.writer.write(message_str.encode())\n            await self.writer.drain()\n        except Exception as e:\n            raise ConnectionError(f\"Failed to send message: {e}\")\n    \n    async def receive_message(self) -> MCPMessage:\n        \"\"\"Receive message via socket.\"\"\"\n        if not self.is_connected or not self.reader:\n            raise ConnectionError(\"Not connected\")\n        \n        try:\n            line = await self.reader.readline()\n            if not line:\n                raise ConnectionError(\"Connection closed\")\n            \n            message_data = json.loads(line.decode().strip())\n            return MCPMessage(**message_data)\n        except json.JSONDecodeError as e:\n            raise ValueError(f\"Invalid JSON message: {e}\")\n        except Exception as e:\n            raise ConnectionError(f\"Failed to receive message: {e}\")\n    \n    async def close(self) -> None:\n        \"\"\"Close the socket connection.\"\"\"\n        if self.writer:\n            self.writer.close()\n            await self.writer.wait_closed()\n        self.is_connected = False\n\n\n# ============================================================================\n# Main MCP Adapter\n# ============================================================================\n\nclass MCPAdapter:\n    \"\"\"Main MCP adapter with validation and redaction.\"\"\"\n    \n    def __init__(\n        self,\n        transport: MCPTransport,\n        schema_validator: Optional[MCPSchemaValidator] = None,\n        redaction_config: Optional[RedactionConfig] = None\n    ):\n        self.transport = transport\n        self.schema_validator = schema_validator or MCPSchemaValidator()\n        self.redactor = MCPRedactor(redaction_config or RedactionConfig())\n        \n        # Message handlers\n        self.message_handlers: Dict[str, Callable] = {}\n        self.request_handlers: Dict[str, Callable] = {}\n        self.notification_handlers: Dict[str, Callable] = {}\n        \n        # Statistics\n        self.total_messages = 0\n        self.successful_messages = 0\n        self.failed_messages = 0\n        self.redacted_messages = 0\n        \n        # Logging\n        self.logger = logging.getLogger(__name__)\n    \n    def register_request_handler(self, method: str, handler: Callable):\n        \"\"\"Register a request handler.\"\"\"\n        self.request_handlers[method] = handler\n    \n    def register_notification_handler(self, method: str, handler: Callable):\n        \"\"\"Register a notification handler.\"\"\"\n        self.notification_handlers[method] = handler\n    \n    async def send_request(self, method: str, params: Dict[str, Any], request_id: Optional[Union[str, int]] = None) -> MCPResponse:\n        \"\"\"Send a request and wait for response.\"\"\"\n        if request_id is None:\n            request_id = str(uuid4())\n        \n        # Validate input schema\n        try:\n            validated_params = self.schema_validator.validate_input(method, params)\n            params = validated_params.dict() if hasattr(validated_params, 'dict') else params\n        except ValueError as e:\n            self.logger.warning(f\"Input validation failed for {method}: {e}\")\n        \n        # Redact sensitive information\n        redacted_params = self.redactor.redact_dict(params)\n        if redacted_params != params:\n            self.redacted_messages += 1\n        \n        # Create and send request\n        request = MCPRequest(\n            id=request_id,\n            method=method,\n            params=redacted_params\n        )\n        \n        await self.transport.send_message(request)\n        self.total_messages += 1\n        \n        # Wait for response\n        response = await self.transport.receive_message()\n        \n        if not isinstance(response, MCPResponse):\n            raise ValueError(\"Expected response message\")\n        \n        # Validate output schema\n        if response.result is not None:\n            try:\n                validated_result = self.schema_validator.validate_output(method, response.result)\n                response.result = validated_result.dict() if hasattr(validated_result, 'dict') else response.result\n            except ValueError as e:\n                self.logger.warning(f\"Output validation failed for {method}: {e}\")\n        \n        if response.error:\n            self.failed_messages += 1\n        else:\n            self.successful_messages += 1\n        \n        return response\n    \n    async def send_notification(self, method: str, params: Dict[str, Any]) -> None:\n        \"\"\"Send a notification.\"\"\"\n        # Validate input schema\n        try:\n            validated_params = self.schema_validator.validate_input(method, params)\n            params = validated_params.dict() if hasattr(validated_params, 'dict') else params\n        except ValueError as e:\n            self.logger.warning(f\"Input validation failed for {method}: {e}\")\n        \n        # Redact sensitive information\n        redacted_params = self.redactor.redact_dict(params)\n        if redacted_params != params:\n            self.redacted_messages += 1\n        \n        # Create and send notification\n        notification = MCPNotification(\n            method=method,\n            params=redacted_params\n        )\n        \n        await self.transport.send_message(notification)\n        self.total_messages += 1\n    \n    async def send_response(self, request_id: Union[str, int], result: Any = None, error: Optional[MCPError] = None) -> None:\n        \"\"\"Send a response to a request.\"\"\"\n        # Validate output schema if result provided\n        if result is not None:\n            # We need the method to validate output, but we don't have it in the response\n            # This is a limitation of the current design\n            pass\n        \n        # Redact sensitive information\n        redacted_result = self.redactor.redact_dict(result) if isinstance(result, dict) else result\n        if redacted_result != result:\n            self.redacted_messages += 1\n        \n        # Create and send response\n        response = MCPResponse(\n            id=request_id,\n            result=redacted_result,\n            error=error\n        )\n        \n        await self.transport.send_message(response)\n        self.total_messages += 1\n    \n    async def handle_message(self, message: MCPMessage) -> None:\n        \"\"\"Handle an incoming message.\"\"\"\n        try:\n            if message.method and message.id is not None:\n                # This is a request\n                await self._handle_request(message)\n            elif message.method:\n                # This is a notification\n                await self._handle_notification(message)\n            elif message.result is not None or message.error is not None:\n                # This is a response\n                await self._handle_response(message)\n            else:\n                raise ValueError(\"Invalid message format\")\n        except Exception as e:\n            self.logger.error(f\"Failed to handle message: {e}\")\n            self.failed_messages += 1\n    \n    async def _handle_request(self, message: MCPRequest) -> None:\n        \"\"\"Handle a request message.\"\"\"\n        method = message.method\n        if method not in self.request_handlers:\n            error = MCPError(\n                code=-32601,\n                message=f\"Method not found: {method}\"\n            )\n            await self.send_response(message.id, error=error)\n            return\n        \n        try:\n            # Validate input schema\n            validated_params = self.schema_validator.validate_input(method, message.params)\n            \n            # Call handler\n            handler = self.request_handlers[method]\n            result = await handler(validated_params.dict() if hasattr(validated_params, 'dict') else message.params)\n            \n            # Send response\n            await self.send_response(message.id, result=result)\n            \n        except ValueError as e:\n            error = MCPError(\n                code=-32602,\n                message=f\"Invalid params: {str(e)}\"\n            )\n            await self.send_response(message.id, error=error)\n        except Exception as e:\n            error = MCPError(\n                code=-32603,\n                message=f\"Internal error: {str(e)}\"\n            )\n            await self.send_response(message.id, error=error)\n    \n    async def _handle_notification(self, message: MCPNotification) -> None:\n        \"\"\"Handle a notification message.\"\"\"\n        method = message.method\n        if method in self.notification_handlers:\n            try:\n                # Validate input schema\n                validated_params = self.schema_validator.validate_input(method, message.params)\n                \n                # Call handler\n                handler = self.notification_handlers[method]\n                await handler(validated_params.dict() if hasattr(validated_params, 'dict') else message.params)\n                \n            except ValueError as e:\n                self.logger.warning(f\"Invalid notification params for {method}: {e}\")\n            except Exception as e:\n                self.logger.error(f\"Failed to handle notification {method}: {e}\")\n    \n    async def _handle_response(self, message: MCPResponse) -> None:\n        \"\"\"Handle a response message.\"\"\"\n        # Responses are typically handled by the caller\n        # This is a placeholder for future response handling\n        pass\n    \n    async def run_server(self) -> None:\n        \"\"\"Run the MCP server.\"\"\"\n        self.logger.info(\"Starting MCP server\")\n        \n        try:\n            while True:\n                try:\n                    message = await self.transport.receive_message()\n                    await self.handle_message(message)\n                except ConnectionError:\n                    self.logger.info(\"Connection closed\")\n                    break\n                except Exception as e:\n                    self.logger.error(f\"Error in server loop: {e}\")\n                    continue\n        finally:\n            await self.transport.close()\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get adapter statistics.\"\"\"\n        success_rate = (self.successful_messages / self.total_messages * 100) if self.total_messages > 0 else 0\n        \n        return {\n            \"total_messages\": self.total_messages,\n            \"successful_messages\": self.successful_messages,\n            \"failed_messages\": self.failed_messages,\n            \"redacted_messages\": self.redacted_messages,\n            \"success_rate\": success_rate,\n            \"registered_request_handlers\": len(self.request_handlers),\n            \"registered_notification_handlers\": len(self.notification_handlers)\n        }\n    \n    async def close(self) -> None:\n        \"\"\"Close the adapter.\"\"\"\n        await self.transport.close()\n\n\n# ============================================================================\n# Factory Functions\n# ============================================================================\n\ndef create_stdio_adapter(\n    redaction_config: Optional[RedactionConfig] = None,\n    schema_validator: Optional[MCPSchemaValidator] = None\n) -> MCPAdapter:\n    \"\"\"Create an MCP adapter using stdin/stdout transport.\"\"\"\n    transport = MCPStdioTransport()\n    return MCPAdapter(transport, schema_validator, redaction_config)\n\n\ndef create_socket_adapter(\n    host: str = \"localhost\",\n    port: int = 8080,\n    redaction_config: Optional[RedactionConfig] = None,\n    schema_validator: Optional[MCPSchemaValidator] = None\n) -> MCPAdapter:\n    \"\"\"Create an MCP adapter using socket transport.\"\"\"\n    transport = MCPSocketTransport(host, port)\n    return MCPAdapter(transport, schema_validator, redaction_config)\n\n\nasync def create_connected_socket_adapter(\n    host: str = \"localhost\",\n    port: int = 8080,\n    redaction_config: Optional[RedactionConfig] = None,\n    schema_validator: Optional[MCPSchemaValidator] = None\n) -> MCPAdapter:\n    \"\"\"Create and connect an MCP adapter using socket transport.\"\"\"\n    adapter = create_socket_adapter(host, port, redaction_config, schema_validator)\n    await adapter.transport.connect()\n    return adapter\n\n\n# ============================================================================\n# Export all classes and functions\n# ============================================================================\n\n__all__ = [\n    # Enums\n    \"MCPTransportType\",\n    \"MCPMessageType\",\n    \"MCPMethod\",\n    \n    # Message models\n    \"MCPMessage\",\n    \"MCPRequest\",\n    \"MCPResponse\",\n    \"MCPNotification\",\n    \"MCPError\",\n    \n    # Schema validation\n    \"MCPSchemaValidator\",\n    \n    # Redaction\n    \"RedactionPattern\",\n    \"RedactionConfig\",\n    \"MCPRedactor\",\n    \n    # Transport\n    \"MCPTransport\",\n    \"MCPStdioTransport\",\n    \"MCPSocketTransport\",\n    \n    # Main adapter\n    \"MCPAdapter\",\n    \n    # Factory functions\n    \"create_stdio_adapter\",\n    \"create_socket_adapter\",\n    \"create_connected_socket_adapter\",\n]\n", "source": "src/core/tools/mcp_adapter.py"}, {"instruction": "Explain the following code from src/core/tools/pydantic_ai_mcp.py:", "response": "This code is from src/core/tools/pydantic_ai_mcp.py:\n\n\"\"\"\nPydantic AI MCP Integration for Agentic LLM Core v0.1\n\nThis module provides enhanced MCP integration using Pydantic AI for better\ntool execution, validation, and knowledge base integration.\n\nCreated: 2024-09-24\nStatus: Enhanced\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional, Union, Type, Callable, Awaitable\nfrom uuid import uuid4\n\nimport pydantic\nfrom pydantic import BaseModel, Field, field_validator\nfrom pydantic_ai import Agent, RunContext, Tool\n\nfrom .mcp_adapter import MCPAdapter, MCPRequest, MCPResponse, MCPNotification\nfrom ..models.contracts import ToolCall as ContractToolCall, ToolResult as ContractToolResult\nfrom ..memory.vector_pg import VectorStore, Document\n\n\n# ============================================================================\n# Pydantic AI Enhanced Models\n# ============================================================================\n\nclass PydanticAIToolSpec(BaseModel):\n    \"\"\"Enhanced tool specification for Pydantic AI integration.\"\"\"\n    name: str = Field(..., description=\"Tool name\")\n    description: str = Field(..., description=\"Tool description\")\n    parameters: Dict[str, Any] = Field(default_factory=dict, description=\"Tool parameters schema\")\n    returns: Dict[str, Any] = Field(default_factory=dict, description=\"Return value schema\")\n    safety_level: str = Field(default=\"safe\", description=\"Tool safety level\")\n    requires_permissions: List[str] = Field(default_factory=list, description=\"Required permissions\")\n    knowledge_base_enabled: bool = Field(default=True, description=\"Enable knowledge base integration\")\n    context_aware: bool = Field(default=True, description=\"Enable context awareness\")\n    \n    @field_validator('safety_level')\n    @classmethod\n    def validate_safety_level(cls, v):\n        allowed_levels = ['safe', 'moderate', 'dangerous', 'critical']\n        if v not in allowed_levels:\n            raise ValueError(f\"Safety level must be one of: {allowed_levels}\")\n        return v\n\n\nclass KnowledgeBaseContext(BaseModel):\n    \"\"\"Context for knowledge base integration.\"\"\"\n    query: str = Field(..., description=\"Knowledge base query\")\n    context_type: str = Field(default=\"general\", description=\"Type of context\")\n    max_results: int = Field(default=5, description=\"Maximum results to return\")\n    similarity_threshold: float = Field(default=0.7, description=\"Similarity threshold\")\n    include_metadata: bool = Field(default=True, description=\"Include metadata in results\")\n    \n    @field_validator('similarity_threshold')\n    @classmethod\n    def validate_threshold(cls, v):\n        if not 0.0 <= v <= 1.0:\n            raise ValueError(\"Similarity threshold must be between 0.0 and 1.0\")\n        return v\n\n\nclass EnhancedToolCall(BaseModel):\n    \"\"\"Enhanced tool call with knowledge base integration.\"\"\"\n    tool_call_id: str = Field(default_factory=lambda: str(uuid4()), description=\"Unique tool call ID\")\n    tool_name: str = Field(..., description=\"Name of the tool to call\")\n    parameters: Dict[str, Any] = Field(default_factory=dict, description=\"Tool parameters\")\n    context: Optional[KnowledgeBaseContext] = Field(None, description=\"Knowledge base context\")\n    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Additional metadata\")\n    timestamp: datetime = Field(default_factory=datetime.utcnow, description=\"Call timestamp\")\n    \n    @field_validator('tool_name')\n    @classmethod\n    def validate_tool_name(cls, v):\n        if not v or not v.strip():\n            raise ValueError(\"Tool name cannot be empty\")\n        return v.strip().lower()\n\n\nclass EnhancedToolResult(BaseModel):\n    \"\"\"Enhanced tool result with knowledge base integration.\"\"\"\n    tool_call_id: str = Field(..., description=\"Corresponding tool call ID\")\n    success: bool = Field(..., description=\"Whether the call was successful\")\n    result: Any = Field(..., description=\"Tool execution result\")\n    knowledge_base_results: Optional[List[Dict[str, Any]]] = Field(None, description=\"Knowledge base results\")\n    execution_time: float = Field(..., description=\"Execution time in seconds\")\n    error_message: Optional[str] = Field(None, description=\"Error message if failed\")\n    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Additional metadata\")\n    timestamp: datetime = Field(default_factory=datetime.utcnow, description=\"Result timestamp\")\n\n\n# ============================================================================\n# Pydantic AI MCP Agent\n# ============================================================================\n\nclass PydanticAIMCPAgent:\n    \"\"\"Enhanced MCP agent using Pydantic AI for better tool execution.\"\"\"\n    \n    def __init__(\n        self,\n        mcp_adapter: MCPAdapter,\n        vector_store: Optional[VectorStore] = None,\n        agent_name: str = \"pydantic_ai_mcp_agent\"\n    ):\n        self.mcp_adapter = mcp_adapter\n        self.vector_store = vector_store\n        self.agent_name = agent_name\n        self.logger = logging.getLogger(__name__)\n        \n        # Initialize Pydantic AI agent\n        self.agent = Agent(\n            'pydantic-ai-mcp-agent',\n            result_type=str,\n            tools=self._get_available_tools()\n        )\n        \n        # Tool registry\n        self.tool_registry: Dict[str, PydanticAIToolSpec] = {}\n        self._register_default_tools()\n    \n    def _get_available_tools(self) -> List[Callable]:\n        \"\"\"Get available tools for Pydantic AI agent.\"\"\"\n        return [\n            self._execute_tool_call,\n            self._query_knowledge_base,\n            self._validate_tool_parameters,\n            self._get_tool_specifications\n        ]\n    \n    def _register_default_tools(self):\n        \"\"\"Register default tools from the tool catalog.\"\"\"\n        default_tools = [\n            PydanticAIToolSpec(\n                name=\"file_read\",\n                description=\"Read contents of a file\",\n                parameters={\n                    \"file_path\": {\"type\": \"string\", \"required\": True},\n                    \"encoding\": {\"type\": \"string\", \"default\": \"utf-8\"}\n                },\n                returns={\"content\": \"string\", \"size\": \"integer\"},\n                safety_level=\"safe\",\n                requires_permissions=[\"read\"]\n            ),\n            PydanticAIToolSpec(\n                name=\"file_write\",\n                description=\"Write content to a file\",\n                parameters={\n                    \"file_path\": {\"type\": \"string\", \"required\": True},\n                    \"content\": {\"type\": \"string\", \"required\": True},\n                    \"encoding\": {\"type\": \"string\", \"default\": \"utf-8\"}\n                },\n                returns={\"success\": \"boolean\", \"bytes_written\": \"integer\"},\n                safety_level=\"moderate\",\n                requires_permissions=[\"write\"]\n            ),\n            PydanticAIToolSpec(\n                name=\"text_summarize\",\n                description=\"Generate a summary of text\",\n                parameters={\n                    \"text\": {\"type\": \"string\", \"required\": True},\n                    \"max_length\": {\"type\": \"integer\", \"default\": 200},\n                    \"style\": {\"type\": \"string\", \"default\": \"extractive\"}\n                },\n                returns={\"summary\": \"string\", \"compression_ratio\": \"float\"},\n                safety_level=\"safe\",\n                requires_permissions=[]\n            ),\n            PydanticAIToolSpec(\n                name=\"data_analyze\",\n                description=\"Analyze structured data\",\n                parameters={\n                    \"data\": {\"type\": \"array\", \"required\": True},\n                    \"analysis_type\": {\"type\": \"string\", \"required\": True},\n                    \"columns\": {\"type\": \"array\", \"required\": False}\n                },\n                returns={\"results\": \"object\", \"insights\": \"array\"},\n                safety_level=\"safe\",\n                requires_permissions=[]\n            )\n        ]\n        \n        for tool in default_tools:\n            self.tool_registry[tool.name] = tool\n    \n    async def _execute_tool_call(\n        self,\n        tool_call: EnhancedToolCall,\n        context: RunContext[str]\n    ) -> EnhancedToolResult:\n        \"\"\"Execute a tool call with enhanced capabilities.\"\"\"\n        start_time = datetime.utcnow()\n        \n        try:\n            # Validate tool exists\n            if tool_call.tool_name not in self.tool_registry:\n                raise ValueError(f\"Unknown tool: {tool_call.tool_name}\")\n            \n            tool_spec = self.tool_registry[tool_call.tool_name]\n            \n            # Query knowledge base if enabled\n            knowledge_base_results = None\n            if tool_call.context and tool_spec.knowledge_base_enabled:\n                knowledge_base_results = await self._query_knowledge_base_internal(\n                    tool_call.context\n                )\n            \n            # Execute tool via MCP\n            mcp_request = MCPRequest(\n                method=\"tools/call\",\n                params={\n                    \"name\": tool_call.tool_name,\n                    \"arguments\": tool_call.parameters\n                }\n            )\n            \n            mcp_response = await self.mcp_adapter.send_request(mcp_request)\n            \n            execution_time = (datetime.utcnow() - start_time).total_seconds()\n            \n            return EnhancedToolResult(\n                tool_call_id=tool_call.tool_call_id,\n                success=mcp_response.error is None,\n                result=mcp_response.result if mcp_response.error is None else None,\n                knowledge_base_results=knowledge_base_results,\n                execution_time=execution_time,\n                error_message=mcp_response.error.get(\"message\") if mcp_response.error else None,\n                metadata={\n                    \"tool_spec\": tool_spec.model_dump(),\n                    \"mcp_response_id\": mcp_response.id\n                }\n            )\n            \n        except Exception as e:\n            execution_time = (datetime.utcnow() - start_time).total_seconds()\n            self.logger.error(f\"Tool execution failed: {e}\")\n            \n            return EnhancedToolResult(\n                tool_call_id=tool_call.tool_call_id,\n                success=False,\n                result=None,\n                knowledge_base_results=None,\n                execution_time=execution_time,\n                error_message=str(e),\n                metadata={\"error_type\": type(e).__name__}\n            )\n    \n    async def _query_knowledge_base(\n        self,\n        query: str,\n        context_type: str = \"general\",\n        max_results: int = 5,\n        context: RunContext[str] = None\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Query the knowledge base for relevant information.\"\"\"\n        if not self.vector_store:\n            return []\n        \n        try:\n            # Search for relevant documents\n            results = await self.vector_store.search(\n                query=query,\n                limit=max_results,\n                similarity_threshold=0.7\n            )\n            \n            return [\n                {\n                    \"content\": result.document.content,\n                    \"metadata\": result.document.metadata,\n                    \"similarity\": result.similarity,\n                    \"source\": result.document.source\n                }\n                for result in results\n            ]\n            \n        except Exception as e:\n            self.logger.error(f\"Knowledge base query failed: {e}\")\n            return []\n    \n    async def _query_knowledge_base_internal(\n        self,\n        kb_context: KnowledgeBaseContext\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Internal knowledge base query method.\"\"\"\n        return await self._query_knowledge_base(\n            query=kb_context.query,\n            context_type=kb_context.context_type,\n            max_results=kb_context.max_results\n        )\n    \n    async def _validate_tool_parameters(\n        self,\n        tool_name: str,\n        parameters: Dict[str, Any],\n        context: RunContext[str] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Validate tool parameters against the tool specification.\"\"\"\n        if tool_name not in self.tool_registry:\n            return {\"valid\": False, \"error\": f\"Unknown tool: {tool_name}\"}\n        \n        tool_spec = self.tool_registry[tool_name]\n        \n        try:\n            # Validate required parameters\n            for param_name, param_spec in tool_spec.parameters.items():\n                if param_spec.get(\"required\", False) and param_name not in parameters:\n                    return {\n                        \"valid\": False,\n                        \"error\": f\"Missing required parameter: {param_name}\"\n                    }\n            \n            # Validate parameter types\n            for param_name, param_value in parameters.items():\n                if param_name in tool_spec.parameters:\n                    expected_type = tool_spec.parameters[param_name].get(\"type\")\n                    if expected_type and not self._validate_type(param_value, expected_type):\n                        return {\n                            \"valid\": False,\n                            \"error\": f\"Invalid type for parameter {param_name}: expected {expected_type}\"\n                        }\n            \n            return {\"valid\": True, \"validated_parameters\": parameters}\n            \n        except Exception as e:\n            return {\"valid\": False, \"error\": str(e)}\n    \n    def _validate_type(self, value: Any, expected_type: str) -> bool:\n        \"\"\"Validate a value against an expected type.\"\"\"\n        type_mapping = {\n            \"string\": str,\n            \"integer\": int,\n            \"float\": float,\n            \"boolean\": bool,\n            \"array\": list,\n            \"object\": dict\n        }\n        \n        if expected_type not in type_mapping:\n            return True  # Unknown type, assume valid\n        \n        return isinstance(value, type_mapping[expected_type])\n    \n    async def _get_tool_specifications(\n        self,\n        tool_name: Optional[str] = None,\n        context: RunContext[str] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Get tool specifications.\"\"\"\n        if tool_name:\n            if tool_name in self.tool_registry:\n                return {tool_name: self.tool_registry[tool_name].model_dump()}\n            else:\n                return {\"error\": f\"Tool not found: {tool_name}\"}\n        else:\n            return {\n                name: spec.model_dump()\n                for name, spec in self.tool_registry.items()\n            }\n    \n    async def execute_with_knowledge_base(\n        self,\n        tool_call: EnhancedToolCall,\n        user_context: Optional[str] = None\n    ) -> EnhancedToolResult:\n        \"\"\"Execute a tool call with knowledge base integration.\"\"\"\n        # Add user context to knowledge base query if provided\n        if user_context and tool_call.context:\n            tool_call.context.query = f\"{tool_call.context.query} Context: {user_context}\"\n        \n        # Execute the tool call\n        result = await self._execute_tool_call(tool_call, None)\n        \n        # Log the execution\n        self.logger.info(\n            f\"Tool {tool_call.tool_name} executed in {result.execution_time:.3f}s \"\n            f\"with {len(result.knowledge_base_results or [])} knowledge base results\"\n        )\n        \n        return result\n    \n    async def batch_execute_tools(\n        self,\n        tool_calls: List[EnhancedToolCall],\n        parallel: bool = True\n    ) -> List[EnhancedToolResult]:\n        \"\"\"Execute multiple tool calls in batch.\"\"\"\n        if parallel:\n            # Execute tools in parallel\n            tasks = [self._execute_tool_call(call, None) for call in tool_calls]\n            return await asyncio.gather(*tasks)\n        else:\n            # Execute tools sequentially\n            results = []\n            for call in tool_calls:\n                result = await self._execute_tool_call(call, None)\n                results.append(result)\n            return results\n    \n    def register_tool(self, tool_spec: PydanticAIToolSpec):\n        \"\"\"Register a new tool specification.\"\"\"\n        self.tool_registry[tool_spec.name] = tool_spec\n        self.logger.info(f\"Registered tool: {tool_spec.name}\")\n    \n    def get_tool_catalog(self) -> Dict[str, PydanticAIToolSpec]:\n        \"\"\"Get the complete tool catalog.\"\"\"\n        return self.tool_registry.copy()\n\n\n# ============================================================================\n# Factory Functions\n# ============================================================================\n\nasync def create_pydantic_ai_mcp_agent(\n    mcp_adapter: MCPAdapter,\n    vector_store: Optional[VectorStore] = None,\n    agent_name: str = \"pydantic_ai_mcp_agent\"\n) -> PydanticAIMCPAgent:\n    \"\"\"Create a new Pydantic AI MCP agent.\"\"\"\n    return PydanticAIMCPAgent(mcp_adapter, vector_store, agent_name)\n\n\ndef create_enhanced_tool_call(\n    tool_name: str,\n    parameters: Dict[str, Any],\n    knowledge_base_query: Optional[str] = None,\n    context_type: str = \"general\"\n) -> EnhancedToolCall:\n    \"\"\"Create an enhanced tool call with knowledge base integration.\"\"\"\n    context = None\n    if knowledge_base_query:\n        context = KnowledgeBaseContext(\n            query=knowledge_base_query,\n            context_type=context_type\n        )\n    \n    return EnhancedToolCall(\n        tool_name=tool_name,\n        parameters=parameters,\n        context=context\n    )\n\n\n# ============================================================================\n# Example Usage\n# ============================================================================\n\nasync def example_usage():\n    \"\"\"Example usage of the Pydantic AI MCP integration.\"\"\"\n    # This would be used in actual implementation\n    pass\n", "source": "src/core/tools/pydantic_ai_mcp.py"}, {"instruction": "Explain the following code from src/core/memory/ingest.py:", "response": "This code is from src/core/memory/ingest.py:\n\n\"\"\"\nDocument Ingestion Pipeline for Agentic LLM Core v0.1\n\nThis module provides a comprehensive document ingestion pipeline with support for:\n- Multiple document formats (text, PDF, images, audio, video)\n- Chunking and preprocessing\n- Vector embedding generation\n- Batch processing and parallel ingestion\n- Progress tracking and error handling\n\nCreated: 2024-09-24\nStatus: Draft\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hashlib\nimport logging\nimport mimetypes\nimport os\nimport tempfile\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Union, AsyncGenerator, Tuple\nfrom urllib.parse import urlparse\n\nimport aiofiles\nimport aiohttp\nfrom pydantic import BaseModel, Field, field_validator\n\nfrom .vector_pg import Document, DocumentType, DocumentStatus, VectorStore\nfrom ..providers.llm_qwen3 import Qwen3Provider, Qwen3EmbedRequest\n\n\n# ============================================================================\n# Ingestion Models\n# ============================================================================\n\nclass IngestionStatus(str, Enum):\n    \"\"\"Ingestion process status.\"\"\"\n    PENDING = \"pending\"\n    PROCESSING = \"processing\"\n    CHUNKING = \"chunking\"\n    EMBEDDING = \"embedding\"\n    INDEXING = \"indexing\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    CANCELLED = \"cancelled\"\n\n\nclass IngestionSource(BaseModel):\n    \"\"\"Source information for ingestion.\"\"\"\n    source_type: str = Field(..., description=\"Type of source (file, url, text)\")\n    source_path: Optional[str] = Field(None, description=\"Path or URL to source\")\n    source_content: Optional[str] = Field(None, description=\"Direct content for text sources\")\n    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Source metadata\")\n    \n    @field_validator('source_type')\n    @classmethod    def validate_source_type(cls, v):\n        allowed_types = ['file', 'url', 'text', 'stream']\n        if v not in allowed_types:\n            raise ValueError(f\"Source type must be one of: {allowed_types}\")\n        return v\n\n\nclass ChunkingConfig(BaseModel):\n    \"\"\"Configuration for document chunking.\"\"\"\n    chunk_size: int = Field(default=1000, ge=100, le=10000, description=\"Maximum chunk size in characters\")\n    chunk_overlap: int = Field(default=200, ge=0, le=1000, description=\"Overlap between chunks\")\n    preserve_sentences: bool = Field(default=True, description=\"Preserve sentence boundaries\")\n    preserve_paragraphs: bool = Field(default=True, description=\"Preserve paragraph boundaries\")\n    min_chunk_size: int = Field(default=100, ge=10, description=\"Minimum chunk size\")\n    \n    @field_validator('chunk_overlap')\n    @classmethod    def validate_chunk_overlap(cls, v, values):\n        if 'chunk_size' in values and v >= values['chunk_size']:\n            raise ValueError(\"Chunk overlap must be less than chunk size\")\n        return v\n\n\nclass EmbeddingConfig(BaseModel):\n    \"\"\"Configuration for embedding generation.\"\"\"\n    model_name: str = Field(default=\"qwen3-omni\", description=\"Embedding model name\")\n    batch_size: int = Field(default=32, ge=1, le=128, description=\"Batch size for embedding generation\")\n    normalize_embeddings: bool = Field(default=True, description=\"Normalize embeddings\")\n    cache_embeddings: bool = Field(default=True, description=\"Cache embeddings for reuse\")\n\n\nclass IngestionJob(BaseModel):\n    \"\"\"Ingestion job model.\"\"\"\n    job_id: str = Field(default_factory=lambda: str(uuid.uuid4()), description=\"Unique job ID\")\n    sources: List[IngestionSource] = Field(..., description=\"Sources to ingest\")\n    chunking_config: ChunkingConfig = Field(default_factory=ChunkingConfig, description=\"Chunking configuration\")\n    embedding_config: EmbeddingConfig = Field(default_factory=EmbeddingConfig, description=\"Embedding configuration\")\n    status: IngestionStatus = Field(default=IngestionStatus.PENDING, description=\"Job status\")\n    progress: float = Field(default=0.0, ge=0.0, le=1.0, description=\"Progress percentage\")\n    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description=\"Creation timestamp\")\n    started_at: Optional[datetime] = Field(None, description=\"Start timestamp\")\n    completed_at: Optional[datetime] = Field(None, description=\"Completion timestamp\")\n    error_message: Optional[str] = Field(None, description=\"Error message if failed\")\n    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Job metadata\")\n    \n    # Results\n    total_documents: int = Field(default=0, description=\"Total documents processed\")\n    total_chunks: int = Field(default=0, description=\"Total chunks created\")\n    successful_documents: int = Field(default=0, description=\"Successfully processed documents\")\n    failed_documents: int = Field(default=0, description=\"Failed documents\")\n\n\n# ============================================================================\n# Document Processors\n# ============================================================================\n\nclass DocumentProcessor(ABC):\n    \"\"\"Abstract base class for document processors.\"\"\"\n    \n    @abstractmethod\n    async def can_process(self, source: IngestionSource) -> bool:\n        \"\"\"Check if this processor can handle the source.\"\"\"\n        pass\n    \n    @abstractmethod\n    async def process(self, source: IngestionSource) -> List[Document]:\n        \"\"\"Process the source and return documents.\"\"\"\n        pass\n\n\nclass TextProcessor(DocumentProcessor):\n    \"\"\"Processor for plain text documents.\"\"\"\n    \n    async def can_process(self, source: IngestionSource) -> bool:\n        \"\"\"Check if this is a text source.\"\"\"\n        if source.source_type == \"text\":\n            return True\n        \n        if source.source_type == \"file\":\n            path = Path(source.source_path)\n            return path.suffix.lower() in ['.txt', '.md', '.rst']\n        \n        return False\n    \n    async def process(self, source: IngestionSource) -> List[Document]:\n        \"\"\"Process text source.\"\"\"\n        if source.source_type == \"text\":\n            content = source.source_content\n        else:\n            async with aiofiles.open(source.source_path, 'r', encoding='utf-8') as f:\n                content = await f.read()\n        \n        # Create document\n        doc = Document(\n            content=content,\n            title=source.metadata.get('title', 'Untitled Text Document'),\n            doc_type=DocumentType.TEXT,\n            metadata=source.metadata,\n            source=source.source_path or \"direct_text\",\n            status=DocumentStatus.PENDING\n        )\n        \n        return [doc]\n\n\nclass PDFProcessor(DocumentProcessor):\n    \"\"\"Processor for PDF documents.\"\"\"\n    \n    async def can_process(self, source: IngestionSource) -> bool:\n        \"\"\"Check if this is a PDF source.\"\"\"\n        if source.source_type == \"file\":\n            path = Path(source.source_path)\n            return path.suffix.lower() == '.pdf'\n        return False\n    \n    async def process(self, source: IngestionSource) -> List[Document]:\n        \"\"\"Process PDF source.\"\"\"\n        try:\n            import PyPDF2\n        except ImportError:\n            raise ImportError(\"PyPDF2 is required for PDF processing. Install with: pip install PyPDF2\")\n        \n        documents = []\n        \n        async with aiofiles.open(source.source_path, 'rb') as f:\n            pdf_content = await f.read()\n            \n            # Create PDF reader\n            pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_content))\n            \n            # Extract text from each page\n            for page_num, page in enumerate(pdf_reader.pages):\n                try:\n                    text = page.extract_text()\n                    if text.strip():\n                        doc = Document(\n                            content=text,\n                            title=f\"{source.metadata.get('title', 'PDF Document')} - Page {page_num + 1}\",\n                            doc_type=DocumentType.PDF,\n                            metadata={\n                                **source.metadata,\n                                'page_number': page_num + 1,\n                                'total_pages': len(pdf_reader.pages)\n                            },\n                            source=source.source_path,\n                            status=DocumentStatus.PENDING\n                        )\n                        documents.append(doc)\n                except Exception as e:\n                    logging.warning(f\"Failed to extract text from page {page_num + 1}: {e}\")\n                    continue\n        \n        return documents\n\n\nclass ImageProcessor(DocumentProcessor):\n    \"\"\"Processor for image documents.\"\"\"\n    \n    async def can_process(self, source: IngestionSource) -> bool:\n        \"\"\"Check if this is an image source.\"\"\"\n        if source.source_type == \"file\":\n            path = Path(source.source_path)\n            return path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff']\n        return False\n    \n    async def process(self, source: IngestionSource) -> List[Document]:\n        \"\"\"Process image source.\"\"\"\n        try:\n            from PIL import Image\n            import pytesseract\n        except ImportError:\n            raise ImportError(\"PIL and pytesseract are required for image processing. Install with: pip install Pillow pytesseract\")\n        \n        # Load image\n        image = Image.open(source.source_path)\n        \n        # Extract text using OCR\n        try:\n            text = pytesseract.image_to_string(image)\n        except Exception as e:\n            logging.warning(f\"OCR failed for {source.source_path}: {e}\")\n            text = f\"[Image: {source.source_path}] - OCR extraction failed\"\n        \n        # Create document\n        doc = Document(\n            content=text,\n            title=source.metadata.get('title', f\"Image Document: {Path(source.source_path).name}\"),\n            doc_type=DocumentType.IMAGE,\n            metadata={\n                **source.metadata,\n                'image_size': image.size,\n                'image_mode': image.mode,\n                'ocr_extracted': True\n            },\n            source=source.source_path,\n            status=DocumentStatus.PENDING\n        )\n        \n        return [doc]\n\n\nclass URLProcessor(DocumentProcessor):\n    \"\"\"Processor for URL sources.\"\"\"\n    \n    async def can_process(self, source: IngestionSource) -> bool:\n        \"\"\"Check if this is a URL source.\"\"\"\n        return source.source_type == \"url\"\n    \n    async def process(self, source: IngestionSource) -> List[Document]:\n        \"\"\"Process URL source.\"\"\"\n        async with aiohttp.ClientSession() as session:\n            try:\n                async with session.get(source.source_path) as response:\n                    if response.status == 200:\n                        content = await response.text()\n                        \n                        # Extract title from HTML if possible\n                        title = source.metadata.get('title')\n                        if not title:\n                            import re\n                            title_match = re.search(r'<title>(.*?)</title>', content, re.IGNORECASE)\n                            if title_match:\n                                title = title_match.group(1).strip()\n                            else:\n                                title = f\"Web Page: {urlparse(source.source_path).netloc}\"\n                        \n                        # Clean HTML content (basic)\n                        import re\n                        clean_content = re.sub(r'<[^>]+>', '', content)\n                        clean_content = re.sub(r'\\s+', ' ', clean_content).strip()\n                        \n                        doc = Document(\n                            content=clean_content,\n                            title=title,\n                            doc_type=DocumentType.TEXT,\n                            metadata={\n                                **source.metadata,\n                                'url': source.source_path,\n                                'response_status': response.status,\n                                'content_type': response.headers.get('content-type', '')\n                            },\n                            source=source.source_path,\n                            status=DocumentStatus.PENDING\n                        )\n                        \n                        return [doc]\n                    else:\n                        raise Exception(f\"HTTP {response.status}: {response.reason}\")\n                        \n            except Exception as e:\n                raise Exception(f\"Failed to fetch URL {source.source_path}: {e}\")\n\n\n# ============================================================================\n# Chunking Engine\n# ============================================================================\n\nclass ChunkingEngine:\n    \"\"\"Engine for chunking documents.\"\"\"\n    \n    def __init__(self, config: ChunkingConfig):\n        self.config = config\n    \n    def chunk_document(self, document: Document) -> List[Document]:\n        \"\"\"Chunk a document into smaller pieces.\"\"\"\n        content = document.content\n        \n        if len(content) <= self.config.chunk_size:\n            return [document]\n        \n        chunks = []\n        \n        if self.config.preserve_paragraphs:\n            chunks = self._chunk_by_paragraphs(content)\n        elif self.config.preserve_sentences:\n            chunks = self._chunk_by_sentences(content)\n        else:\n            chunks = self._chunk_by_characters(content)\n        \n        # Create chunk documents\n        chunk_docs = []\n        for i, chunk_content in enumerate(chunks):\n            if len(chunk_content.strip()) >= self.config.min_chunk_size:\n                chunk_doc = Document(\n                    content=chunk_content,\n                    title=f\"{document.title} - Chunk {i + 1}\",\n                    doc_type=document.doc_type,\n                    metadata={\n                        **document.metadata,\n                        'chunk_index': i,\n                        'total_chunks': len(chunks),\n                        'original_document_id': document.id\n                    },\n                    source=document.source,\n                    chunk_index=i,\n                    parent_id=document.id,\n                    status=DocumentStatus.PENDING\n                )\n                chunk_docs.append(chunk_doc)\n        \n        return chunk_docs\n    \n    def _chunk_by_paragraphs(self, content: str) -> List[str]:\n        \"\"\"Chunk content by paragraphs.\"\"\"\n        paragraphs = content.split('\\n\\n')\n        chunks = []\n        current_chunk = \"\"\n        \n        for paragraph in paragraphs:\n            if len(current_chunk) + len(paragraph) <= self.config.chunk_size:\n                current_chunk += paragraph + \"\\n\\n\"\n            else:\n                if current_chunk:\n                    chunks.append(current_chunk.strip())\n                current_chunk = paragraph + \"\\n\\n\"\n        \n        if current_chunk:\n            chunks.append(current_chunk.strip())\n        \n        return chunks\n    \n    def _chunk_by_sentences(self, content: str) -> List[str]:\n        \"\"\"Chunk content by sentences.\"\"\"\n        import re\n        \n        # Simple sentence splitting\n        sentences = re.split(r'[.!?]+', content)\n        chunks = []\n        current_chunk = \"\"\n        \n        for sentence in sentences:\n            sentence = sentence.strip()\n            if not sentence:\n                continue\n                \n            if len(current_chunk) + len(sentence) <= self.config.chunk_size:\n                current_chunk += sentence + \". \"\n            else:\n                if current_chunk:\n                    chunks.append(current_chunk.strip())\n                current_chunk = sentence + \". \"\n        \n        if current_chunk:\n            chunks.append(current_chunk.strip())\n        \n        return chunks\n    \n    def _chunk_by_characters(self, content: str) -> List[str]:\n        \"\"\"Chunk content by character count.\"\"\"\n        chunks = []\n        start = 0\n        \n        while start < len(content):\n            end = start + self.config.chunk_size\n            \n            # Add overlap if not at the beginning\n            if start > 0:\n                start = max(0, start - self.config.chunk_overlap)\n            \n            chunk = content[start:end]\n            chunks.append(chunk)\n            start = end\n        \n        return chunks\n\n\n# ============================================================================\n# Embedding Generator\n# ============================================================================\n\nclass EmbeddingGenerator:\n    \"\"\"Generator for document embeddings.\"\"\"\n    \n    def __init__(self, provider: Qwen3Provider, config: EmbeddingConfig):\n        self.provider = provider\n        self.config = config\n        self.cache: Dict[str, List[float]] = {}\n    \n    async def generate_embeddings(self, documents: List[Document]) -> List[List[float]]:\n        \"\"\"Generate embeddings for documents.\"\"\"\n        embeddings = []\n        \n        # Process in batches\n        for i in range(0, len(documents), self.config.batch_size):\n            batch = documents[i:i + self.config.batch_size]\n            batch_embeddings = await self._generate_batch_embeddings(batch)\n            embeddings.extend(batch_embeddings)\n        \n        return embeddings\n    \n    async def _generate_batch_embeddings(self, documents: List[Document]) -> List[List[float]]:\n        \"\"\"Generate embeddings for a batch of documents.\"\"\"\n        # Check cache first\n        cached_embeddings = []\n        uncached_docs = []\n        \n        for doc in documents:\n            content_hash = hashlib.md5(doc.content.encode()).hexdigest()\n            if self.config.cache_embeddings and content_hash in self.cache:\n                cached_embeddings.append(self.cache[content_hash])\n            else:\n                uncached_docs.append((doc, content_hash))\n        \n        # Generate embeddings for uncached documents\n        if uncached_docs:\n            texts = [doc.content for doc, _ in uncached_docs]\n            \n            request = Qwen3EmbedRequest(\n                texts=texts,\n                normalize=self.config.normalize_embeddings\n            )\n            \n            response = await self.provider.embed(request)\n            \n            if response.success and response.embeddings:\n                # Cache embeddings\n                for (doc, content_hash), embedding in zip(uncached_docs, response.embeddings):\n                    if self.config.cache_embeddings:\n                        self.cache[content_hash] = embedding\n                    cached_embeddings.append(embedding)\n            else:\n                raise Exception(f\"Failed to generate embeddings: {response.error_message}\")\n        \n        return cached_embeddings\n\n\n# ============================================================================\n# Main Ingestion Pipeline\n# ============================================================================\n\nclass IngestionPipeline:\n    \"\"\"Main ingestion pipeline.\"\"\"\n    \n    def __init__(\n        self,\n        vector_store: VectorStore,\n        embedding_provider: Qwen3Provider,\n        chunking_config: Optional[ChunkingConfig] = None,\n        embedding_config: Optional[EmbeddingConfig] = None\n    ):\n        self.vector_store = vector_store\n        self.embedding_provider = embedding_provider\n        self.chunking_config = chunking_config or ChunkingConfig()\n        self.embedding_config = embedding_config or EmbeddingConfig()\n        \n        # Initialize components\n        self.chunking_engine = ChunkingEngine(self.chunking_config)\n        self.embedding_generator = EmbeddingGenerator(self.embedding_provider, self.embedding_config)\n        \n        # Document processors\n        self.processors = [\n            TextProcessor(),\n            PDFProcessor(),\n            ImageProcessor(),\n            URLProcessor()\n        ]\n        \n        self.logger = logging.getLogger(__name__)\n    \n    async def ingest_job(self, job: IngestionJob) -> IngestionJob:\n        \"\"\"Process an ingestion job.\"\"\"\n        job.status = IngestionStatus.PROCESSING\n        job.started_at = datetime.now(timezone.utc)\n        \n        try:\n            # Process sources\n            all_documents = []\n            for source in job.sources:\n                documents = await self._process_source(source)\n                all_documents.extend(documents)\n            \n            job.total_documents = len(all_documents)\n            job.progress = 0.1\n            \n            # Chunk documents\n            job.status = IngestionStatus.CHUNKING\n            chunked_documents = []\n            for doc in all_documents:\n                chunks = self.chunking_engine.chunk_document(doc)\n                chunked_documents.extend(chunks)\n            \n            job.total_chunks = len(chunked_documents)\n            job.progress = 0.3\n            \n            # Generate embeddings\n            job.status = IngestionStatus.EMBEDDING\n            embeddings = await self.embedding_generator.generate_embeddings(chunked_documents)\n            \n            job.progress = 0.7\n            \n            # Index documents\n            job.status = IngestionStatus.INDEXING\n            indexed_ids = await self.vector_store.index(chunked_documents, embeddings)\n            \n            job.successful_documents = len(indexed_ids)\n            job.progress = 1.0\n            job.status = IngestionStatus.COMPLETED\n            job.completed_at = datetime.now(timezone.utc)\n            \n            self.logger.info(f\"Ingestion job {job.job_id} completed successfully\")\n            \n        except Exception as e:\n            job.status = IngestionStatus.FAILED\n            job.error_message = str(e)\n            job.completed_at = datetime.now(timezone.utc)\n            self.logger.error(f\"Ingestion job {job.job_id} failed: {e}\")\n        \n        return job\n    \n    async def _process_source(self, source: IngestionSource) -> List[Document]:\n        \"\"\"Process a single source.\"\"\"\n        # Find appropriate processor\n        processor = None\n        for p in self.processors:\n            if await p.can_process(source):\n                processor = p\n                break\n        \n        if not processor:\n            raise ValueError(f\"No processor found for source type: {source.source_type}\")\n        \n        # Process source\n        documents = await processor.process(source)\n        \n        # Update document metadata\n        for doc in documents:\n            doc.metadata.update(source.metadata)\n            doc.status = DocumentStatus.PENDING\n        \n        return documents\n    \n    async def ingest_sources(\n        self,\n        sources: List[IngestionSource],\n        chunking_config: Optional[ChunkingConfig] = None,\n        embedding_config: Optional[EmbeddingConfig] = None\n    ) -> IngestionJob:\n        \"\"\"Ingest multiple sources.\"\"\"\n        # Create job\n        job = IngestionJob(\n            sources=sources,\n            chunking_config=chunking_config or self.chunking_config,\n            embedding_config=embedding_config or self.embedding_config\n        )\n        \n        # Process job\n        return await self.ingest_job(job)\n    \n    async def ingest_file(\n        self,\n        file_path: str,\n        title: Optional[str] = None,\n        metadata: Optional[Dict[str, Any]] = None\n    ) -> IngestionJob:\n        \"\"\"Ingest a single file.\"\"\"\n        source = IngestionSource(\n            source_type=\"file\",\n            source_path=file_path,\n            metadata={\n                **(metadata or {}),\n                'title': title or Path(file_path).name\n            }\n        )\n        \n        return await self.ingest_sources([source])\n    \n    async def ingest_text(\n        self,\n        content: str,\n        title: str = \"Text Document\",\n        metadata: Optional[Dict[str, Any]] = None\n    ) -> IngestionJob:\n        \"\"\"Ingest text content.\"\"\"\n        source = IngestionSource(\n            source_type=\"text\",\n            source_content=content,\n            metadata={\n                **(metadata or {}),\n                'title': title\n            }\n        )\n        \n        return await self.ingest_sources([source])\n    \n    async def ingest_url(\n        self,\n        url: str,\n        title: Optional[str] = None,\n        metadata: Optional[Dict[str, Any]] = None\n    ) -> IngestionJob:\n        \"\"\"Ingest content from URL.\"\"\"\n        source = IngestionSource(\n            source_type=\"url\",\n            source_path=url,\n            metadata={\n                **(metadata or {}),\n                'title': title or f\"URL: {urlparse(url).netloc}\"\n            }\n        )\n        \n        return await self.ingest_sources([source])\n\n\n# ============================================================================\n# Factory Functions\n# ============================================================================\n\ndef create_ingestion_pipeline(\n    vector_store: VectorStore,\n    embedding_provider: Qwen3Provider,\n    chunk_size: int = 1000,\n    chunk_overlap: int = 200,\n    batch_size: int = 32\n) -> IngestionPipeline:\n    \"\"\"Create an ingestion pipeline with default configuration.\"\"\"\n    chunking_config = ChunkingConfig(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap\n    )\n    \n    embedding_config = EmbeddingConfig(\n        batch_size=batch_size\n    )\n    \n    return IngestionPipeline(\n        vector_store=vector_store,\n        embedding_provider=embedding_provider,\n        chunking_config=chunking_config,\n        embedding_config=embedding_config\n    )\n\n\n# ============================================================================\n# Export all classes and functions\n# ============================================================================\n\n__all__ = [\n    # Enums\n    \"IngestionStatus\",\n    \n    # Models\n    \"IngestionSource\",\n    \"ChunkingConfig\",\n    \"EmbeddingConfig\",\n    \"IngestionJob\",\n    \n    # Processors\n    \"DocumentProcessor\",\n    \"TextProcessor\",\n    \"PDFProcessor\",\n    \"ImageProcessor\",\n    \"URLProcessor\",\n    \n    # Engines\n    \"ChunkingEngine\",\n    \"EmbeddingGenerator\",\n    \n    # Main pipeline\n    \"IngestionPipeline\",\n    \n    # Factory functions\n    \"create_ingestion_pipeline\",\n]\n", "source": "src/core/memory/ingest.py"}, {"instruction": "Explain the following code from src/core/memory/vector_weaviate.py:", "response": "This code is from src/core/memory/vector_weaviate.py:\n\n\"\"\"\nWeaviate Vector Store Implementation for Agentic LLM Core v0.1\n\nThis module provides a Weaviate-based vector store using the existing Docker container.\nLeverages the universal-ai-tools-weaviate container running on localhost:8090.\n\nCreated: 2024-09-24\nStatus: Draft\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport uuid\nfrom datetime import datetime, timezone\nfrom typing import Any, Dict, List, Optional, Union\nfrom uuid import UUID\n\nimport aiohttp\nfrom pydantic import BaseModel, Field\n\nfrom .vector_pg import VectorStore, Document, DocumentType, DocumentStatus, SearchResult\n\n\n# ============================================================================\n# Weaviate Configuration\n# ============================================================================\n\nclass WeaviateConfig(BaseModel):\n    \"\"\"Weaviate configuration.\"\"\"\n    host: str = Field(default=\"localhost\", description=\"Weaviate host\")\n    port: int = Field(default=8090, ge=1, le=65535, description=\"Weaviate port\")\n    scheme: str = Field(default=\"http\", description=\"Connection scheme\")\n    api_key: Optional[str] = Field(None, description=\"API key for authentication\")\n    \n    # Class settings\n    class_name: str = Field(default=\"Document\", description=\"Weaviate class name\")\n    vector_dimension: int = Field(default=1536, ge=1, description=\"Vector dimension\")\n    \n    # Performance settings\n    batch_size: int = Field(default=100, ge=1, le=1000, description=\"Batch size for operations\")\n    timeout: float = Field(default=30.0, ge=1.0, description=\"Request timeout in seconds\")\n    \n    @property\n    def base_url(self) -> str:\n        \"\"\"Get base URL for Weaviate.\"\"\"\n        return f\"{self.scheme}://{self.host}:{self.port}\"\n\n\n# ============================================================================\n# Weaviate Vector Store Implementation\n# ============================================================================\n\nclass WeaviateVectorStore(VectorStore):\n    \"\"\"Weaviate-based vector store.\"\"\"\n    \n    def __init__(self, config: WeaviateConfig):\n        self.config = config\n        self.session: Optional[aiohttp.ClientSession] = None\n        self.logger = logging.getLogger(__name__)\n        \n        # Statistics\n        self.total_documents = 0\n        self.total_queries = 0\n        self.total_indexes = 0\n    \n    async def initialize(self) -> None:\n        \"\"\"Initialize the Weaviate vector store.\"\"\"\n        try:\n            # Create HTTP session\n            timeout = aiohttp.ClientTimeout(total=self.config.timeout)\n            self.session = aiohttp.ClientSession(timeout=timeout)\n            \n            # Test connection\n            await self._test_connection()\n            \n            # Create schema if it doesn't exist\n            await self._create_schema()\n            \n            self.logger.info(f\"Weaviate vector store initialized at {self.config.base_url}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to initialize Weaviate vector store: {e}\")\n            raise\n    \n    async def _test_connection(self) -> None:\n        \"\"\"Test connection to Weaviate.\"\"\"\n        if not self.session:\n            raise RuntimeError(\"Session not initialized\")\n        \n        async with self.session.get(f\"{self.config.base_url}/v1/meta\") as response:\n            if response.status != 200:\n                raise ConnectionError(f\"Weaviate connection failed: {response.status}\")\n            \n            meta = await response.json()\n            self.logger.info(f\"Connected to Weaviate version: {meta.get('version', 'unknown')}\")\n    \n    async def _create_schema(self) -> None:\n        \"\"\"Create Weaviate schema if it doesn't exist.\"\"\"\n        if not self.session:\n            raise RuntimeError(\"Session not initialized\")\n        \n        # Check if class exists\n        async with self.session.get(f\"{self.config.base_url}/v1/schema/{self.config.class_name}\") as response:\n            if response.status == 200:\n                self.logger.info(f\"Class {self.config.class_name} already exists\")\n                return\n        \n        # Create class schema\n        schema = {\n            \"class\": self.config.class_name,\n            \"description\": \"Document storage for Agentic LLM Core\",\n            \"vectorizer\": \"none\",  # We provide our own vectors\n            \"properties\": [\n                {\n                    \"name\": \"content\",\n                    \"dataType\": [\"text\"],\n                    \"description\": \"Document content\"\n                },\n                {\n                    \"name\": \"title\",\n                    \"dataType\": [\"text\"],\n                    \"description\": \"Document title\"\n                },\n                {\n                    \"name\": \"doc_type\",\n                    \"dataType\": [\"text\"],\n                    \"description\": \"Document type\"\n                },\n                {\n                    \"name\": \"metadata\",\n                    \"dataType\": [\"object\"],\n                    \"description\": \"Document metadata\"\n                },\n                {\n                    \"name\": \"status\",\n                    \"dataType\": [\"text\"],\n                    \"description\": \"Document status\"\n                },\n                {\n                    \"name\": \"source\",\n                    \"dataType\": [\"text\"],\n                    \"description\": \"Document source\"\n                },\n                {\n                    \"name\": \"chunk_index\",\n                    \"dataType\": [\"int\"],\n                    \"description\": \"Chunk index\"\n                },\n                {\n                    \"name\": \"parent_id\",\n                    \"dataType\": [\"text\"],\n                    \"description\": \"Parent document ID\"\n                },\n                {\n                    \"name\": \"created_at\",\n                    \"dataType\": [\"date\"],\n                    \"description\": \"Creation timestamp\"\n                },\n                {\n                    \"name\": \"updated_at\",\n                    \"dataType\": [\"date\"],\n                    \"description\": \"Last update timestamp\"\n                }\n            ]\n        }\n        \n        async with self.session.post(\n            f\"{self.config.base_url}/v1/schema\",\n            json=schema\n        ) as response:\n            if response.status == 200:\n                self.logger.info(f\"Created class {self.config.class_name}\")\n            else:\n                error_text = await response.text()\n                raise RuntimeError(f\"Failed to create schema: {error_text}\")\n    \n    async def index(self, documents: List[Document], vectors: List[List[float]]) -> List[str]:\n        \"\"\"Index documents with their vectors.\"\"\"\n        if len(documents) != len(vectors):\n            raise ValueError(\"Number of documents must match number of vectors\")\n        \n        if not documents:\n            return []\n        \n        # Validate vector dimensions\n        for i, vector in enumerate(vectors):\n            if len(vector) != self.config.vector_dimension:\n                raise ValueError(f\"Vector {i} has dimension {len(vector)}, expected {self.config.vector_dimension}\")\n        \n        indexed_ids = []\n        \n        try:\n            # Process in batches\n            for i in range(0, len(documents), self.config.batch_size):\n                batch_docs = documents[i:i + self.config.batch_size]\n                batch_vectors = vectors[i:i + self.config.batch_size]\n                \n                batch_ids = await self._index_batch(batch_docs, batch_vectors)\n                indexed_ids.extend(batch_ids)\n            \n            self.total_indexes += len(indexed_ids)\n            self.logger.info(f\"Indexed {len(indexed_ids)} documents in Weaviate\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to index documents: {e}\")\n            raise\n        \n        return indexed_ids\n    \n    async def _index_batch(self, documents: List[Document], vectors: List[List[float]]) -> List[str]:\n        \"\"\"Index a batch of documents.\"\"\"\n        if not self.session:\n            raise RuntimeError(\"Session not initialized\")\n        \n        # Prepare batch objects\n        objects = []\n        for doc, vector in zip(documents, vectors):\n            obj = {\n                \"id\": doc.id,\n                \"properties\": {\n                    \"content\": doc.content,\n                    \"title\": doc.title or \"\",\n                    \"doc_type\": doc.doc_type.value,\n                    \"metadata\": doc.metadata,\n                    \"status\": doc.status.value,\n                    \"source\": doc.source or \"\",\n                    \"chunk_index\": doc.chunk_index,\n                    \"parent_id\": doc.parent_id or \"\",\n                    \"created_at\": doc.created_at.isoformat(),\n                    \"updated_at\": doc.updated_at.isoformat()\n                },\n                \"vector\": vector\n            }\n            objects.append(obj)\n        \n        # Send batch to Weaviate\n        batch_data = {\n            \"objects\": objects\n        }\n        \n        async with self.session.post(\n            f\"{self.config.base_url}/v1/batch/objects\",\n            json=batch_data\n        ) as response:\n            if response.status == 200:\n                result = await response.json()\n                indexed_ids = [obj[\"id\"] for obj in result.get(\"objects\", [])]\n                return indexed_ids\n            else:\n                error_text = await response.text()\n                raise RuntimeError(f\"Failed to index batch: {error_text}\")\n    \n    async def query(self, vector: List[float], limit: int = 10, filter_metadata: Optional[Dict[str, Any]] = None) -> List[SearchResult]:\n        \"\"\"Query similar vectors.\"\"\"\n        if len(vector) != self.config.vector_dimension:\n            raise ValueError(f\"Query vector has dimension {len(vector)}, expected {self.config.vector_dimension}\")\n        \n        if limit <= 0:\n            raise ValueError(\"Limit must be positive\")\n        \n        try:\n            if not self.session:\n                raise RuntimeError(\"Session not initialized\")\n            \n            # Build where clause for metadata filtering\n            where_clause = {\n                \"path\": [\"status\"],\n                \"operator\": \"Equal\",\n                \"valueText\": \"indexed\"\n            }\n            \n            if filter_metadata:\n                # Add metadata filters\n                metadata_filters = []\n                for key, value in filter_metadata.items():\n                    if isinstance(value, (list, tuple)):\n                        # Array contains\n                        metadata_filters.append({\n                            \"path\": [\"metadata\", key],\n                            \"operator\": \"ContainsAny\",\n                            \"valueTextArray\": value\n                        })\n                    else:\n                        # Exact match\n                        metadata_filters.append({\n                            \"path\": [\"metadata\", key],\n                            \"operator\": \"Equal\",\n                            \"valueText\": str(value)\n                        })\n                \n                if metadata_filters:\n                    where_clause = {\n                        \"operator\": \"And\",\n                        \"operands\": [where_clause] + metadata_filters\n                    }\n            \n            # Build query\n            query_data = {\n                \"query\": {\n                    \"nearVector\": {\n                        \"vector\": vector,\n                        \"certainty\": 0.0  # We'll calculate similarity ourselves\n                    },\n                    \"where\": where_clause,\n                    \"limit\": limit,\n                    \"withMeta\": True\n                }\n            }\n            \n            async with self.session.post(\n                f\"{self.config.base_url}/v1/graphql\",\n                json={\"query\": self._build_graphql_query(query_data)}\n            ) as response:\n                if response.status == 200:\n                    result = await response.json()\n                    return self._parse_search_results(result)\n                else:\n                    error_text = await response.text()\n                    raise RuntimeError(f\"Failed to query: {error_text}\")\n                    \n        except Exception as e:\n            self.logger.error(f\"Failed to query vectors: {e}\")\n            raise\n    \n    def _build_graphql_query(self, query_data: Dict[str, Any]) -> str:\n        \"\"\"Build GraphQL query for Weaviate.\"\"\"\n        # This is a simplified GraphQL query builder\n        # In a production system, you'd want a more robust GraphQL client\n        return f\"\"\"\n        {{\n            Get {{\n                {self.config.class_name}(\n                    nearVector: {{\n                        vector: {query_data['query']['nearVector']['vector']}\n                        certainty: {query_data['query']['nearVector']['certainty']}\n                    }}\n                    where: {json.dumps(query_data['query']['where'])}\n                    limit: {query_data['query']['limit']}\n                ) {{\n                    _additional {{\n                        id\n                        distance\n                    }}\n                    content\n                    title\n                    doc_type\n                    metadata\n                    status\n                    source\n                    chunk_index\n                    parent_id\n                    created_at\n                    updated_at\n                }}\n            }}\n        }}\n        \"\"\"\n    \n    def _parse_search_results(self, result: Dict[str, Any]) -> List[SearchResult]:\n        \"\"\"Parse Weaviate search results.\"\"\"\n        results = []\n        \n        data = result.get(\"data\", {}).get(\"Get\", {}).get(self.config.class_name, [])\n        \n        for rank, item in enumerate(data, 1):\n            additional = item.get(\"_additional\", {})\n            \n            # Create document\n            doc = Document(\n                id=additional.get(\"id\", \"\"),\n                content=item.get(\"content\", \"\"),\n                title=item.get(\"title\"),\n                doc_type=DocumentType(item.get(\"doc_type\", \"text\")),\n                metadata=item.get(\"metadata\", {}),\n                status=DocumentStatus(item.get(\"status\", \"pending\")),\n                source=item.get(\"source\"),\n                chunk_index=item.get(\"chunk_index\"),\n                parent_id=item.get(\"parent_id\"),\n                created_at=datetime.fromisoformat(item.get(\"created_at\", datetime.now(timezone.utc).isoformat())),\n                updated_at=datetime.fromisoformat(item.get(\"updated_at\", datetime.now(timezone.utc).isoformat()))\n            )\n            \n            # Calculate similarity score from distance\n            distance = additional.get(\"distance\", 1.0)\n            similarity_score = max(0.0, 1.0 - distance)\n            \n            result_obj = SearchResult(\n                document=doc,\n                similarity_score=similarity_score,\n                distance=distance,\n                rank=rank\n            )\n            \n            results.append(result_obj)\n        \n        self.total_queries += 1\n        return results\n    \n    async def get(self, document_ids: List[str]) -> List[Document]:\n        \"\"\"Get documents by IDs.\"\"\"\n        if not document_ids:\n            return []\n        \n        try:\n            if not self.session:\n                raise RuntimeError(\"Session not initialized\")\n            \n            # Build GraphQL query for multiple IDs\n            ids_str = json.dumps(document_ids)\n            query = f\"\"\"\n            {{\n                Get {{\n                    {self.config.class_name}(\n                        where: {{\n                            path: [\"id\"]\n                            operator: ContainsAny\n                            valueTextArray: {ids_str}\n                        }}\n                    ) {{\n                        _additional {{\n                            id\n                        }}\n                        content\n                        title\n                        doc_type\n                        metadata\n                        status\n                        source\n                        chunk_index\n                        parent_id\n                        created_at\n                        updated_at\n                    }}\n                }}\n            }}\n            \"\"\"\n            \n            async with self.session.post(\n                f\"{self.config.base_url}/v1/graphql\",\n                json={\"query\": query}\n            ) as response:\n                if response.status == 200:\n                    result = await response.json()\n                    return self._parse_documents(result)\n                else:\n                    error_text = await response.text()\n                    raise RuntimeError(f\"Failed to get documents: {error_text}\")\n                    \n        except Exception as e:\n            self.logger.error(f\"Failed to get documents: {e}\")\n            raise\n    \n    def _parse_documents(self, result: Dict[str, Any]) -> List[Document]:\n        \"\"\"Parse Weaviate documents.\"\"\"\n        documents = []\n        \n        data = result.get(\"data\", {}).get(\"Get\", {}).get(self.config.class_name, [])\n        \n        for item in data:\n            additional = item.get(\"_additional\", {})\n            \n            doc = Document(\n                id=additional.get(\"id\", \"\"),\n                content=item.get(\"content\", \"\"),\n                title=item.get(\"title\"),\n                doc_type=DocumentType(item.get(\"doc_type\", \"text\")),\n                metadata=item.get(\"metadata\", {}),\n                status=DocumentStatus(item.get(\"status\", \"pending\")),\n                source=item.get(\"source\"),\n                chunk_index=item.get(\"chunk_index\"),\n                parent_id=item.get(\"parent_id\"),\n                created_at=datetime.fromisoformat(item.get(\"created_at\", datetime.now(timezone.utc).isoformat())),\n                updated_at=datetime.fromisoformat(item.get(\"updated_at\", datetime.now(timezone.utc).isoformat()))\n            )\n            \n            documents.append(doc)\n        \n        return documents\n    \n    async def delete(self, document_ids: List[str]) -> bool:\n        \"\"\"Delete documents by IDs.\"\"\"\n        if not document_ids:\n            return True\n        \n        try:\n            if not self.session:\n                raise RuntimeError(\"Session not initialized\")\n            \n            # Weaviate doesn't have soft delete, so we'll update status\n            for doc_id in document_ids:\n                mutation = f\"\"\"\n                {{\n                    Update {{\n                        {self.config.class_name}(\n                            where: {{\n                                path: [\"id\"]\n                                operator: Equal\n                                valueText: \"{doc_id}\"\n                            }}\n                            data: {{\n                                status: \"deleted\"\n                                updated_at: \"{datetime.now(timezone.utc).isoformat()}\"\n                            }}\n                        ) {{\n                            _additional {{\n                                id\n                            }}\n                        }}\n                    }}\n                }}\n                \"\"\"\n                \n                async with self.session.post(\n                    f\"{self.config.base_url}/v1/graphql\",\n                    json={\"query\": mutation}\n                ) as response:\n                    if response.status != 200:\n                        error_text = await response.text()\n                        self.logger.warning(f\"Failed to delete document {doc_id}: {error_text}\")\n            \n            self.logger.info(f\"Deleted {len(document_ids)} documents from Weaviate\")\n            return True\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to delete documents: {e}\")\n            raise\n    \n    async def update(self, document_id: str, document: Document, vector: Optional[List[float]] = None) -> bool:\n        \"\"\"Update a document and optionally its vector.\"\"\"\n        try:\n            if not self.session:\n                raise RuntimeError(\"Session not initialized\")\n            \n            # Build update data\n            update_data = {\n                \"content\": document.content,\n                \"title\": document.title or \"\",\n                \"doc_type\": document.doc_type.value,\n                \"metadata\": document.metadata,\n                \"status\": document.status.value,\n                \"source\": document.source or \"\",\n                \"chunk_index\": document.chunk_index,\n                \"parent_id\": document.parent_id or \"\",\n                \"updated_at\": datetime.now(timezone.utc).isoformat()\n            }\n            \n            mutation = f\"\"\"\n            {{\n                Update {{\n                    {self.config.class_name}(\n                        where: {{\n                            path: [\"id\"]\n                            operator: Equal\n                            valueText: \"{document_id}\"\n                        }}\n                        data: {json.dumps(update_data)}\n                    ) {{\n                        _additional {{\n                            id\n                        }}\n                    }}\n                }}\n            }}\n            \"\"\"\n            \n            async with self.session.post(\n                f\"{self.config.base_url}/v1/graphql\",\n                json={\"query\": mutation}\n            ) as response:\n                if response.status == 200:\n                    return True\n                else:\n                    error_text = await response.text()\n                    raise RuntimeError(f\"Failed to update document: {error_text}\")\n                    \n        except Exception as e:\n            self.logger.error(f\"Failed to update document {document_id}: {e}\")\n            raise\n    \n    async def search_by_metadata(self, metadata: Dict[str, Any], limit: int = 10) -> List[Document]:\n        \"\"\"Search documents by metadata.\"\"\"\n        if not metadata:\n            return []\n        \n        try:\n            if not self.session:\n                raise RuntimeError(\"Session not initialized\")\n            \n            # Build where clause\n            metadata_filters = []\n            for key, value in metadata.items():\n                if isinstance(value, (list, tuple)):\n                    metadata_filters.append({\n                        \"path\": [\"metadata\", key],\n                        \"operator\": \"ContainsAny\",\n                        \"valueTextArray\": value\n                    })\n                else:\n                    metadata_filters.append({\n                        \"path\": [\"metadata\", key],\n                        \"operator\": \"Equal\",\n                        \"valueText\": str(value)\n                    })\n            \n            where_clause = {\n                \"operator\": \"And\",\n                \"operands\": [\n                    {\n                        \"path\": [\"status\"],\n                        \"operator\": \"Equal\",\n                        \"valueText\": \"indexed\"\n                    }\n                ] + metadata_filters\n            }\n            \n            query = f\"\"\"\n            {{\n                Get {{\n                    {self.config.class_name}(\n                        where: {json.dumps(where_clause)}\n                        limit: {limit}\n                        sort: [{{\n                            path: [\"created_at\"]\n                            order: desc\n                        }}]\n                    ) {{\n                        _additional {{\n                            id\n                        }}\n                        content\n                        title\n                        doc_type\n                        metadata\n                        status\n                        source\n                        chunk_index\n                        parent_id\n                        created_at\n                        updated_at\n                    }}\n                }}\n            }}\n            \"\"\"\n            \n            async with self.session.post(\n                f\"{self.config.base_url}/v1/graphql\",\n                json={\"query\": query}\n            ) as response:\n                if response.status == 200:\n                    result = await response.json()\n                    return self._parse_documents(result)\n                else:\n                    error_text = await response.text()\n                    raise RuntimeError(f\"Failed to search by metadata: {error_text}\")\n                    \n        except Exception as e:\n            self.logger.error(f\"Failed to search by metadata: {e}\")\n            raise\n    \n    async def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get store statistics.\"\"\"\n        try:\n            if not self.session:\n                raise RuntimeError(\"Session not initialized\")\n            \n            # Get class statistics\n            query = f\"\"\"\n            {{\n                Aggregate {{\n                    {self.config.class_name} {{\n                        meta {{\n                            count\n                        }}\n                        groupedBy {{\n                            path: [\"status\"]\n                            groups {{\n                                value\n                                count\n                            }}\n                        }}\n                    }}\n                }}\n            }}\n            \"\"\"\n            \n            async with self.session.post(\n                f\"{self.config.base_url}/v1/graphql\",\n                json={\"query\": query}\n            ) as response:\n                if response.status == 200:\n                    result = await response.json()\n                    data = result.get(\"data\", {}).get(\"Aggregate\", {}).get(self.config.class_name, {})\n                    \n                    total_count = data.get(\"meta\", {}).get(\"count\", 0)\n                    status_groups = data.get(\"groupedBy\", [{}])[0].get(\"groups\", [])\n                    \n                    status_counts = {group[\"value\"]: group[\"count\"] for group in status_groups}\n                    \n                    return {\n                        \"total_documents\": total_count,\n                        \"status_counts\": status_counts,\n                        \"vector_dimension\": self.config.vector_dimension,\n                        \"total_queries\": self.total_queries,\n                        \"total_indexes\": self.total_indexes,\n                        \"weaviate_url\": self.config.base_url,\n                        \"class_name\": self.config.class_name\n                    }\n                else:\n                    error_text = await response.text()\n                    raise RuntimeError(f\"Failed to get stats: {error_text}\")\n                    \n        except Exception as e:\n            self.logger.error(f\"Failed to get stats: {e}\")\n            raise\n    \n    async def health_check(self) -> Dict[str, Any]:\n        \"\"\"Perform health check.\"\"\"\n        try:\n            if not self.session:\n                return {\n                    \"status\": \"unhealthy\",\n                    \"error\": \"Session not initialized\",\n                    \"timestamp\": datetime.now(timezone.utc).isoformat()\n                }\n            \n            # Test basic connectivity\n            async with self.session.get(f\"{self.config.base_url}/v1/meta\") as response:\n                if response.status == 200:\n                    meta = await response.json()\n                    return {\n                        \"status\": \"healthy\",\n                        \"weaviate_version\": meta.get(\"version\", \"unknown\"),\n                        \"weaviate_url\": self.config.base_url,\n                        \"timestamp\": datetime.now(timezone.utc).isoformat()\n                    }\n                else:\n                    return {\n                        \"status\": \"unhealthy\",\n                        \"error\": f\"HTTP {response.status}\",\n                        \"timestamp\": datetime.now(timezone.utc).isoformat()\n                    }\n                    \n        except Exception as e:\n            return {\n                \"status\": \"unhealthy\",\n                \"error\": str(e),\n                \"timestamp\": datetime.now(timezone.utc).isoformat()\n            }\n    \n    async def close(self) -> None:\n        \"\"\"Close the vector store.\"\"\"\n        if self.session:\n            await self.session.close()\n            self.logger.info(\"Weaviate vector store closed\")\n\n\n# ============================================================================\n# Factory Functions\n# ============================================================================\n\ndef create_weaviate_vector_store(\n    host: str = \"localhost\",\n    port: int = 8090,\n    class_name: str = \"Document\",\n    vector_dimension: int = 1536,\n    **kwargs\n) -> WeaviateVectorStore:\n    \"\"\"Create a Weaviate vector store with default configuration.\"\"\"\n    config = WeaviateConfig(\n        host=host,\n        port=port,\n        class_name=class_name,\n        vector_dimension=vector_dimension,\n        **kwargs\n    )\n    return WeaviateVectorStore(config)\n\n\nasync def create_and_initialize_weaviate_store(\n    host: str = \"localhost\",\n    port: int = 8090,\n    class_name: str = \"Document\",\n    vector_dimension: int = 1536,\n    **kwargs\n) -> WeaviateVectorStore:\n    \"\"\"Create and initialize a Weaviate vector store.\"\"\"\n    store = create_weaviate_vector_store(\n        host=host,\n        port=port,\n        class_name=class_name,\n        vector_dimension=vector_dimension,\n        **kwargs\n    )\n    await store.initialize()\n    return store\n\n\n# ============================================================================\n# Export all classes and functions\n# ============================================================================\n\n__all__ = [\n    # Configuration\n    \"WeaviateConfig\",\n    \n    # Implementation\n    \"WeaviateVectorStore\",\n    \n    # Factory functions\n    \"create_weaviate_vector_store\",\n    \"create_and_initialize_weaviate_store\",\n]\n", "source": "src/core/memory/vector_weaviate.py"}, {"instruction": "Explain the following code from src/core/memory/vector_pg.py:", "response": "This code is from src/core/memory/vector_pg.py:\n\n\"\"\"\nPostgreSQL Vector Store Implementation for Agentic LLM Core v0.1\n\nThis module provides a PostgreSQL-based vector store with support for:\n- Vector indexing and similarity search\n- Document storage and retrieval\n- Metadata filtering and querying\n- Batch operations and performance optimization\n- Apple Silicon optimization with pgvector\n\nCreated: 2024-09-24\nStatus: Draft\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Union, Tuple, AsyncGenerator\nfrom uuid import UUID\n\nimport asyncpg\nimport numpy as np\nfrom pydantic import BaseModel, Field, field_validator\n\nfrom ..models.contracts import ProcessedInput, UnifiedContext\n\n\n# ============================================================================\n# Vector Store Interface\n# ============================================================================\n\nclass VectorStore(ABC):\n    \"\"\"Abstract base class for vector stores.\"\"\"\n    \n    @abstractmethod\n    async def index(self, documents: List[Document], vectors: List[List[float]]) -> List[str]:\n        \"\"\"Index documents with their vectors.\"\"\"\n        pass\n    \n    @abstractmethod\n    async def query(self, vector: List[float], limit: int = 10, filter_metadata: Optional[Dict[str, Any]] = None) -> List[SearchResult]:\n        \"\"\"Query similar vectors.\"\"\"\n        pass\n    \n    @abstractmethod\n    async def get(self, document_ids: List[str]) -> List[Document]:\n        \"\"\"Get documents by IDs.\"\"\"\n        pass\n    \n    @abstractmethod\n    async def delete(self, document_ids: List[str]) -> bool:\n        \"\"\"Delete documents by IDs.\"\"\"\n        pass\n    \n    @abstractmethod\n    async def update(self, document_id: str, document: Document, vector: Optional[List[float]] = None) -> bool:\n        \"\"\"Update a document and optionally its vector.\"\"\"\n        pass\n    \n    @abstractmethod\n    async def search_by_metadata(self, metadata: Dict[str, Any], limit: int = 10) -> List[Document]:\n        \"\"\"Search documents by metadata.\"\"\"\n        pass\n    \n    @abstractmethod\n    async def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get store statistics.\"\"\"\n        pass\n\n\n# ============================================================================\n# Data Models\n# ============================================================================\n\nclass DocumentType(str, Enum):\n    \"\"\"Types of documents.\"\"\"\n    TEXT = \"text\"\n    IMAGE = \"image\"\n    AUDIO = \"audio\"\n    VIDEO = \"video\"\n    PDF = \"pdf\"\n    CODE = \"code\"\n    STRUCTURED = \"structured\"\n\n\nclass DocumentStatus(str, Enum):\n    \"\"\"Document processing status.\"\"\"\n    PENDING = \"pending\"\n    PROCESSING = \"processing\"\n    INDEXED = \"indexed\"\n    FAILED = \"failed\"\n    DELETED = \"deleted\"\n\n\nclass Document(BaseModel):\n    \"\"\"Document model for vector store.\"\"\"\n    id: str = Field(default_factory=lambda: str(uuid.uuid4()), description=\"Unique document ID\")\n    content: str = Field(..., description=\"Document content\")\n    title: Optional[str] = Field(None, description=\"Document title\")\n    doc_type: DocumentType = Field(default=DocumentType.TEXT, description=\"Document type\")\n    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Document metadata\")\n    status: DocumentStatus = Field(default=DocumentStatus.PENDING, description=\"Processing status\")\n    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description=\"Creation timestamp\")\n    updated_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description=\"Last update timestamp\")\n    source: Optional[str] = Field(None, description=\"Source of the document\")\n    chunk_index: Optional[int] = Field(None, description=\"Chunk index for chunked documents\")\n    parent_id: Optional[str] = Field(None, description=\"Parent document ID for chunks\")\n    \n    @field_validator('content')\n    @classmethod\n    def validate_content(cls, v):\n        if not v or len(v.strip()) == 0:\n            raise ValueError(\"Document content cannot be empty\")\n        return v.strip()\n    \n    @field_validator('metadata')\n    @classmethod\n    def validate_metadata(cls, v):\n        # Ensure metadata is JSON serializable\n        try:\n            json.dumps(v)\n        except (TypeError, ValueError):\n            raise ValueError(\"Metadata must be JSON serializable\")\n        return v\n\n\nclass SearchResult(BaseModel):\n    \"\"\"Search result model.\"\"\"\n    document: Document = Field(..., description=\"Matched document\")\n    similarity_score: float = Field(..., ge=0.0, le=1.0, description=\"Similarity score\")\n    distance: float = Field(..., ge=0.0, description=\"Vector distance\")\n    rank: int = Field(..., ge=1, description=\"Result rank\")\n\n\nclass VectorIndex(BaseModel):\n    \"\"\"Vector index information.\"\"\"\n    id: str = Field(..., description=\"Index ID\")\n    name: str = Field(..., description=\"Index name\")\n    dimension: int = Field(..., ge=1, description=\"Vector dimension\")\n    document_count: int = Field(default=0, ge=0, description=\"Number of documents in index\")\n    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description=\"Creation timestamp\")\n    updated_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description=\"Last update timestamp\")\n    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Index metadata\")\n\n\n# ============================================================================\n# PostgreSQL Configuration\n# ============================================================================\n\nclass PostgreSQLConfig(BaseModel):\n    \"\"\"PostgreSQL configuration.\"\"\"\n    host: str = Field(default=\"localhost\", description=\"Database host\")\n    port: int = Field(default=5432, ge=1, le=65535, description=\"Database port\")\n    database: str = Field(default=\"agi_agents\", description=\"Database name\")\n    username: str = Field(default=\"agi_user\", description=\"Database username\")\n    password: str = Field(default=\"default_db_password\", description=\"Database password\")\n    \n    # Connection pool settings\n    min_connections: int = Field(default=5, ge=1, description=\"Minimum connections\")\n    max_connections: int = Field(default=20, ge=1, description=\"Maximum connections\")\n    connection_timeout: float = Field(default=30.0, ge=1.0, description=\"Connection timeout in seconds\")\n    \n    # Vector settings\n    vector_dimension: int = Field(default=1536, ge=1, description=\"Vector dimension\")\n    index_name: str = Field(default=\"documents_vector_idx\", description=\"Vector index name\")\n    \n    # Performance settings\n    batch_size: int = Field(default=1000, ge=1, le=10000, description=\"Batch size for operations\")\n    enable_parallel_indexing: bool = Field(default=True, description=\"Enable parallel indexing\")\n    \n    # Apple Silicon optimization\n    use_native_compilation: bool = Field(default=True, description=\"Use native compilation on Apple Silicon\")\n    optimize_for_m1: bool = Field(default=True, description=\"Optimize for M1/M2/M3 chips\")\n\n\n# ============================================================================\n# PostgreSQL Vector Store Implementation\n# ============================================================================\n\nclass PostgreSQLVectorStore(VectorStore):\n    \"\"\"PostgreSQL-based vector store using pgvector extension.\"\"\"\n    \n    def __init__(self, config: PostgreSQLConfig):\n        self.config = config\n        self.pool: Optional[asyncpg.Pool] = None\n        self.logger = logging.getLogger(__name__)\n        \n        # Statistics\n        self.total_documents = 0\n        self.total_queries = 0\n        self.total_indexes = 0\n        self.cache_hits = 0\n        self.cache_misses = 0\n    \n    async def initialize(self) -> None:\n        \"\"\"Initialize the vector store.\"\"\"\n        try:\n            # Create connection pool\n            self.pool = await asyncpg.create_pool(\n                host=self.config.host,\n                port=self.config.port,\n                database=self.config.database,\n                user=self.config.username,\n                password=self.config.password,\n                min_size=self.config.min_connections,\n                max_size=self.config.max_connections,\n                command_timeout=self.config.connection_timeout\n            )\n            \n            # Initialize database schema\n            await self._create_schema()\n            \n            self.logger.info(f\"PostgreSQL vector store initialized on {self.config.host}:{self.config.port}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to initialize PostgreSQL vector store: {e}\")\n            raise\n    \n    async def _create_schema(self) -> None:\n        \"\"\"Create database schema and indexes.\"\"\"\n        async with self.pool.acquire() as conn:\n            # Enable pgvector extension\n            await conn.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n            \n            # Create documents table\n            await conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS documents (\n                    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n                    content TEXT NOT NULL,\n                    title TEXT,\n                    doc_type VARCHAR(50) NOT NULL DEFAULT 'text',\n                    metadata JSONB DEFAULT '{}',\n                    status VARCHAR(50) NOT NULL DEFAULT 'pending',\n                    created_at TIMESTAMPTZ DEFAULT NOW(),\n                    updated_at TIMESTAMPTZ DEFAULT NOW(),\n                    source TEXT,\n                    chunk_index INTEGER,\n                    parent_id UUID,\n                    embedding VECTOR(%d)\n                )\n            \"\"\" % self.config.vector_dimension)\n            \n            # Create indexes\n            await conn.execute(f\"\"\"\n                CREATE INDEX IF NOT EXISTS {self.config.index_name}\n                ON documents USING ivfflat (embedding vector_cosine_ops)\n                WITH (lists = 100)\n            \"\"\")\n            \n            await conn.execute(\"\"\"\n                CREATE INDEX IF NOT EXISTS documents_status_idx ON documents (status)\n            \"\"\")\n            \n            await conn.execute(\"\"\"\n                CREATE INDEX IF NOT EXISTS documents_type_idx ON documents (doc_type)\n            \"\"\")\n            \n            await conn.execute(\"\"\"\n                CREATE INDEX IF NOT EXISTS documents_created_at_idx ON documents (created_at)\n            \"\"\")\n            \n            await conn.execute(\"\"\"\n                CREATE INDEX IF NOT EXISTS documents_metadata_idx ON documents USING GIN (metadata)\n            \"\"\")\n            \n            # Create updated_at trigger\n            await conn.execute(\"\"\"\n                CREATE OR REPLACE FUNCTION update_updated_at_column()\n                RETURNS TRIGGER AS $$\n                BEGIN\n                    NEW.updated_at = NOW();\n                    RETURN NEW;\n                END;\n                $$ language 'plpgsql';\n            \"\"\")\n            \n            await conn.execute(\"\"\"\n                DROP TRIGGER IF EXISTS update_documents_updated_at ON documents;\n                CREATE TRIGGER update_documents_updated_at\n                    BEFORE UPDATE ON documents\n                    FOR EACH ROW\n                    EXECUTE FUNCTION update_updated_at_column();\n            \"\"\")\n    \n    async def index(self, documents: List[Document], vectors: List[List[float]]) -> List[str]:\n        \"\"\"Index documents with their vectors.\"\"\"\n        if len(documents) != len(vectors):\n            raise ValueError(\"Number of documents must match number of vectors\")\n        \n        if not documents:\n            return []\n        \n        # Validate vector dimensions\n        for i, vector in enumerate(vectors):\n            if len(vector) != self.config.vector_dimension:\n                raise ValueError(f\"Vector {i} has dimension {len(vector)}, expected {self.config.vector_dimension}\")\n        \n        indexed_ids = []\n        \n        try:\n            if self.config.enable_parallel_indexing and len(documents) > self.config.batch_size:\n                # Process in parallel batches\n                indexed_ids = await self._index_parallel(documents, vectors)\n            else:\n                # Process in single batch\n                indexed_ids = await self._index_batch(documents, vectors)\n            \n            self.total_indexes += len(indexed_ids)\n            self.logger.info(f\"Indexed {len(indexed_ids)} documents\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to index documents: {e}\")\n            raise\n        \n        return indexed_ids\n    \n    async def _index_batch(self, documents: List[Document], vectors: List[List[float]]) -> List[str]:\n        \"\"\"Index documents in a single batch.\"\"\"\n        async with self.pool.acquire() as conn:\n            indexed_ids = []\n            \n            for doc, vector in zip(documents, vectors):\n                # Update document status\n                doc.status = DocumentStatus.PROCESSING\n                doc.updated_at = datetime.now(timezone.utc)\n                \n                # Insert document with vector\n                result = await conn.fetchrow(\"\"\"\n                    INSERT INTO documents (\n                        id, content, title, doc_type, metadata, status,\n                        created_at, updated_at, source, chunk_index, parent_id, embedding\n                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12)\n                    RETURNING id\n                \"\"\", \n                    doc.id, doc.content, doc.title, doc.doc_type.value,\n                    json.dumps(doc.metadata), doc.status.value,\n                    doc.created_at, doc.updated_at, doc.source,\n                    doc.chunk_index, doc.parent_id, vector\n                )\n                \n                indexed_ids.append(str(result['id']))\n            \n            # Update status to indexed\n            await conn.execute(\"\"\"\n                UPDATE documents \n                SET status = 'indexed', updated_at = NOW()\n                WHERE id = ANY($1)\n            \"\"\", indexed_ids)\n            \n            return indexed_ids\n    \n    async def _index_parallel(self, documents: List[Document], vectors: List[List[float]]) -> List[str]:\n        \"\"\"Index documents in parallel batches.\"\"\"\n        batch_size = self.config.batch_size\n        batches = [\n            (documents[i:i + batch_size], vectors[i:i + batch_size])\n            for i in range(0, len(documents), batch_size)\n        ]\n        \n        # Process batches in parallel\n        tasks = [self._index_batch(docs, vecs) for docs, vecs in batches]\n        results = await asyncio.gather(*tasks)\n        \n        # Flatten results\n        indexed_ids = []\n        for batch_ids in results:\n            indexed_ids.extend(batch_ids)\n        \n        return indexed_ids\n    \n    async def query(self, vector: List[float], limit: int = 10, filter_metadata: Optional[Dict[str, Any]] = None) -> List[SearchResult]:\n        \"\"\"Query similar vectors.\"\"\"\n        if len(vector) != self.config.vector_dimension:\n            raise ValueError(f\"Query vector has dimension {len(vector)}, expected {self.config.vector_dimension}\")\n        \n        if limit <= 0:\n            raise ValueError(\"Limit must be positive\")\n        \n        try:\n            async with self.pool.acquire() as conn:\n                # Build query with optional metadata filtering\n                if filter_metadata:\n                    metadata_conditions = []\n                    params = [vector, limit]\n                    param_count = 2\n                    \n                    for key, value in filter_metadata.items():\n                        param_count += 1\n                        if isinstance(value, (list, tuple)):\n                            # Array contains\n                            metadata_conditions.append(f\"metadata->>'{key}' = ANY(${param_count})\")\n                            params.append(value)\n                        else:\n                            # Exact match\n                            metadata_conditions.append(f\"metadata->>'{key}' = ${param_count}\")\n                            params.append(value)\n                    \n                    where_clause = \"WHERE \" + \" AND \".join(metadata_conditions)\n                else:\n                    where_clause = \"\"\n                    params = [vector, limit]\n                \n                # Execute similarity search\n                query = f\"\"\"\n                    SELECT \n                        id, content, title, doc_type, metadata, status,\n                        created_at, updated_at, source, chunk_index, parent_id,\n                        1 - (embedding <=> $1) as similarity_score,\n                        embedding <-> $1 as distance\n                    FROM documents\n                    {where_clause}\n                    AND status = 'indexed'\n                    ORDER BY embedding <-> $1\n                    LIMIT $2\n                \"\"\"\n                \n                rows = await conn.fetch(query, *params)\n                \n                # Convert to SearchResult objects\n                results = []\n                for rank, row in enumerate(rows, 1):\n                    doc = Document(\n                        id=str(row['id']),\n                        content=row['content'],\n                        title=row['title'],\n                        doc_type=DocumentType(row['doc_type']),\n                        metadata=row['metadata'],\n                        status=DocumentStatus(row['status']),\n                        created_at=row['created_at'],\n                        updated_at=row['updated_at'],\n                        source=row['source'],\n                        chunk_index=row['chunk_index'],\n                        parent_id=str(row['parent_id']) if row['parent_id'] else None\n                    )\n                    \n                    result = SearchResult(\n                        document=doc,\n                        similarity_score=float(row['similarity_score']),\n                        distance=float(row['distance']),\n                        rank=rank\n                    )\n                    \n                    results.append(result)\n                \n                self.total_queries += 1\n                self.logger.debug(f\"Query returned {len(results)} results\")\n                \n                return results\n                \n        except Exception as e:\n            self.logger.error(f\"Failed to query vectors: {e}\")\n            raise\n    \n    async def get(self, document_ids: List[str]) -> List[Document]:\n        \"\"\"Get documents by IDs.\"\"\"\n        if not document_ids:\n            return []\n        \n        try:\n            async with self.pool.acquire() as conn:\n                rows = await conn.fetch(\"\"\"\n                    SELECT id, content, title, doc_type, metadata, status,\n                           created_at, updated_at, source, chunk_index, parent_id\n                    FROM documents\n                    WHERE id = ANY($1)\n                \"\"\", document_ids)\n                \n                documents = []\n                for row in rows:\n                    doc = Document(\n                        id=str(row['id']),\n                        content=row['content'],\n                        title=row['title'],\n                        doc_type=DocumentType(row['doc_type']),\n                        metadata=row['metadata'],\n                        status=DocumentStatus(row['status']),\n                        created_at=row['created_at'],\n                        updated_at=row['updated_at'],\n                        source=row['source'],\n                        chunk_index=row['chunk_index'],\n                        parent_id=str(row['parent_id']) if row['parent_id'] else None\n                    )\n                    documents.append(doc)\n                \n                return documents\n                \n        except Exception as e:\n            self.logger.error(f\"Failed to get documents: {e}\")\n            raise\n    \n    async def delete(self, document_ids: List[str]) -> bool:\n        \"\"\"Delete documents by IDs.\"\"\"\n        if not document_ids:\n            return True\n        \n        try:\n            async with self.pool.acquire() as conn:\n                # Soft delete by updating status\n                result = await conn.execute(\"\"\"\n                    UPDATE documents \n                    SET status = 'deleted', updated_at = NOW()\n                    WHERE id = ANY($1) AND status != 'deleted'\n                \"\"\", document_ids)\n                \n                deleted_count = int(result.split()[-1])\n                self.logger.info(f\"Deleted {deleted_count} documents\")\n                \n                return deleted_count > 0\n                \n        except Exception as e:\n            self.logger.error(f\"Failed to delete documents: {e}\")\n            raise\n    \n    async def update(self, document_id: str, document: Document, vector: Optional[List[float]] = None) -> bool:\n        \"\"\"Update a document and optionally its vector.\"\"\"\n        try:\n            async with self.pool.acquire() as conn:\n                if vector is not None:\n                    if len(vector) != self.config.vector_dimension:\n                        raise ValueError(f\"Vector has dimension {len(vector)}, expected {self.config.vector_dimension}\")\n                    \n                    # Update document with vector\n                    result = await conn.execute(\"\"\"\n                        UPDATE documents \n                        SET content = $2, title = $3, doc_type = $4, metadata = $5,\n                            status = $6, source = $7, chunk_index = $8, parent_id = $9,\n                            embedding = $10, updated_at = NOW()\n                        WHERE id = $1\n                    \"\"\", \n                        document_id, document.content, document.title, document.doc_type.value,\n                        json.dumps(document.metadata), document.status.value,\n                        document.source, document.chunk_index, document.parent_id, vector\n                    )\n                else:\n                    # Update document without vector\n                    result = await conn.execute(\"\"\"\n                        UPDATE documents \n                        SET content = $2, title = $3, doc_type = $4, metadata = $5,\n                            status = $6, source = $7, chunk_index = $8, parent_id = $9,\n                            updated_at = NOW()\n                        WHERE id = $1\n                    \"\"\", \n                        document_id, document.content, document.title, document.doc_type.value,\n                        json.dumps(document.metadata), document.status.value,\n                        document.source, document.chunk_index, document.parent_id\n                    )\n                \n                updated_count = int(result.split()[-1])\n                return updated_count > 0\n                \n        except Exception as e:\n            self.logger.error(f\"Failed to update document {document_id}: {e}\")\n            raise\n    \n    async def search_by_metadata(self, metadata: Dict[str, Any], limit: int = 10) -> List[Document]:\n        \"\"\"Search documents by metadata.\"\"\"\n        if not metadata:\n            return []\n        \n        try:\n            async with self.pool.acquire() as conn:\n                conditions = []\n                params = [limit]\n                param_count = 1\n                \n                for key, value in metadata.items():\n                    param_count += 1\n                    if isinstance(value, (list, tuple)):\n                        # Array contains\n                        conditions.append(f\"metadata->>'{key}' = ANY(${param_count})\")\n                        params.append(value)\n                    else:\n                        # Exact match\n                        conditions.append(f\"metadata->>'{key}' = ${param_count}\")\n                        params.append(value)\n                \n                where_clause = \"WHERE \" + \" AND \".join(conditions)\n                \n                query = f\"\"\"\n                    SELECT id, content, title, doc_type, metadata, status,\n                           created_at, updated_at, source, chunk_index, parent_id\n                    FROM documents\n                    {where_clause}\n                    AND status = 'indexed'\n                    ORDER BY created_at DESC\n                    LIMIT $1\n                \"\"\"\n                \n                rows = await conn.fetch(query, *params)\n                \n                documents = []\n                for row in rows:\n                    doc = Document(\n                        id=str(row['id']),\n                        content=row['content'],\n                        title=row['title'],\n                        doc_type=DocumentType(row['doc_type']),\n                        metadata=row['metadata'],\n                        status=DocumentStatus(row['status']),\n                        created_at=row['created_at'],\n                        updated_at=row['updated_at'],\n                        source=row['source'],\n                        chunk_index=row['chunk_index'],\n                        parent_id=str(row['parent_id']) if row['parent_id'] else None\n                    )\n                    documents.append(doc)\n                \n                return documents\n                \n        except Exception as e:\n            self.logger.error(f\"Failed to search by metadata: {e}\")\n            raise\n    \n    async def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get store statistics.\"\"\"\n        try:\n            async with self.pool.acquire() as conn:\n                # Get document counts by status\n                status_counts = await conn.fetch(\"\"\"\n                    SELECT status, COUNT(*) as count\n                    FROM documents\n                    GROUP BY status\n                \"\"\")\n                \n                # Get document counts by type\n                type_counts = await conn.fetch(\"\"\"\n                    SELECT doc_type, COUNT(*) as count\n                    FROM documents\n                    WHERE status = 'indexed'\n                    GROUP BY doc_type\n                \"\"\")\n                \n                # Get total document count\n                total_docs = await conn.fetchval(\"SELECT COUNT(*) FROM documents\")\n                \n                # Get index size\n                index_size = await conn.fetchval(\"\"\"\n                    SELECT pg_size_pretty(pg_relation_size('documents'))\n                \"\"\")\n                \n                stats = {\n                    \"total_documents\": total_docs,\n                    \"status_counts\": {row['status']: row['count'] for row in status_counts},\n                    \"type_counts\": {row['doc_type']: row['count'] for row in type_counts},\n                    \"index_size\": index_size,\n                    \"vector_dimension\": self.config.vector_dimension,\n                    \"total_queries\": self.total_queries,\n                    \"total_indexes\": self.total_indexes,\n                    \"cache_hits\": self.cache_hits,\n                    \"cache_misses\": self.cache_misses,\n                    \"connection_pool_size\": self.pool.get_size() if self.pool else 0,\n                    \"connection_pool_idle\": self.pool.get_idle_size() if self.pool else 0\n                }\n                \n                return stats\n                \n        except Exception as e:\n            self.logger.error(f\"Failed to get stats: {e}\")\n            raise\n    \n    async def health_check(self) -> Dict[str, Any]:\n        \"\"\"Perform health check.\"\"\"\n        try:\n            async with self.pool.acquire() as conn:\n                # Test basic connectivity\n                await conn.fetchval(\"SELECT 1\")\n                \n                # Test vector operations\n                test_vector = [0.1] * self.config.vector_dimension\n                await conn.fetchval(\"SELECT $1::vector\", test_vector)\n                \n                return {\n                    \"status\": \"healthy\",\n                    \"database_connected\": True,\n                    \"vector_extension_available\": True,\n                    \"timestamp\": datetime.now(timezone.utc).isoformat()\n                }\n                \n        except Exception as e:\n            return {\n                \"status\": \"unhealthy\",\n                \"database_connected\": False,\n                \"vector_extension_available\": False,\n                \"error\": str(e),\n                \"timestamp\": datetime.now(timezone.utc).isoformat()\n            }\n    \n    async def close(self) -> None:\n        \"\"\"Close the vector store.\"\"\"\n        if self.pool:\n            await self.pool.close()\n            self.logger.info(\"PostgreSQL vector store closed\")\n\n\n# ============================================================================\n# Factory Functions\n# ============================================================================\n\ndef create_postgresql_vector_store(\n    database: str,\n    username: str,\n    password: str,\n    host: str = \"localhost\",\n    port: int = 5432,\n    vector_dimension: int = 1536,\n    **kwargs\n) -> PostgreSQLVectorStore:\n    \"\"\"Create a PostgreSQL vector store with default configuration.\"\"\"\n    config = PostgreSQLConfig(\n        host=host,\n        port=port,\n        database=database,\n        username=username,\n        password=password,\n        vector_dimension=vector_dimension,\n        **kwargs\n    )\n    return PostgreSQLVectorStore(config)\n\n\nasync def create_and_initialize_vector_store(\n    database: str,\n    username: str,\n    password: str,\n    host: str = \"localhost\",\n    port: int = 5432,\n    vector_dimension: int = 1536,\n    **kwargs\n) -> PostgreSQLVectorStore:\n    \"\"\"Create and initialize a PostgreSQL vector store.\"\"\"\n    store = create_postgresql_vector_store(\n        database=database,\n        username=username,\n        password=password,\n        host=host,\n        port=port,\n        vector_dimension=vector_dimension,\n        **kwargs\n    )\n    await store.initialize()\n    return store\n\n\n# ============================================================================\n# Export all classes and functions\n# ============================================================================\n\n__all__ = [\n    # Interface\n    \"VectorStore\",\n    \n    # Enums\n    \"DocumentType\",\n    \"DocumentStatus\",\n    \n    # Models\n    \"Document\",\n    \"SearchResult\",\n    \"VectorIndex\",\n    \"PostgreSQLConfig\",\n    \n    # Implementation\n    \"PostgreSQLVectorStore\",\n    \n    # Factory functions\n    \"create_postgresql_vector_store\",\n    \"create_and_initialize_vector_store\",\n]\n", "source": "src/core/memory/vector_pg.py"}, {"instruction": "Explain the following code from src/core/memory/docker_config.py:", "response": "This code is from src/core/memory/docker_config.py:\n\n\"\"\"\nDocker-aware Configuration for Knowledge Base Adapter\n\nThis module provides configuration that leverages existing Docker infrastructure:\n- PostgreSQL: agi-postgres-consolidated (port 5432)\n- Weaviate: universal-ai-tools-weaviate (port 8090)\n- Redis: agi-redis (for caching)\n- Supabase: Multiple services for additional functionality\n\nCreated: 2024-09-24\nStatus: Draft\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Dict, Any, Optional\nfrom pydantic import BaseModel, Field\n\nfrom .vector_pg import PostgreSQLConfig\nfrom .vector_weaviate import WeaviateConfig\n\n\n# ============================================================================\n# Docker Infrastructure Configuration\n# ============================================================================\n\nclass DockerInfrastructureConfig(BaseModel):\n    \"\"\"Configuration for Docker infrastructure services.\"\"\"\n    \n    # PostgreSQL Configuration (agi-postgres-consolidated)\n    postgresql: PostgreSQLConfig = Field(\n        default_factory=lambda: PostgreSQLConfig(\n            host=\"localhost\",\n            port=5432,\n            database=\"agi_agents\",\n            username=\"agi_user\",\n            password=\"default_db_password\",\n            vector_dimension=1536,\n            min_connections=5,\n            max_connections=20,\n            enable_parallel_indexing=True,\n            batch_size=1000\n        ),\n        description=\"PostgreSQL vector store configuration\"\n    )\n    \n    # Weaviate Configuration (universal-ai-tools-weaviate)\n    weaviate: WeaviateConfig = Field(\n        default_factory=lambda: WeaviateConfig(\n            host=\"localhost\",\n            port=8090,\n            scheme=\"http\",\n            class_name=\"Document\",\n            vector_dimension=1536,\n            batch_size=100,\n            timeout=30.0\n        ),\n        description=\"Weaviate vector store configuration\"\n    )\n    \n    # Redis Configuration (agi-redis)\n    redis: Dict[str, Any] = Field(\n        default_factory=lambda: {\n            \"host\": \"localhost\",\n            \"port\": 6379,\n            \"db\": 0,\n            \"password\": None,\n            \"max_connections\": 20,\n            \"socket_timeout\": 5.0,\n            \"socket_connect_timeout\": 5.0,\n            \"retry_on_timeout\": True\n        },\n        description=\"Redis cache configuration\"\n    )\n    \n    # Supabase Configuration\n    supabase: Dict[str, Any] = Field(\n        default_factory=lambda: {\n            \"url\": \"http://localhost:54321\",\n            \"anon_key\": \"your_anon_key_here\",\n            \"service_role_key\": \"your_service_role_key_here\",\n            \"database_url\": \"postgresql://agi_user:default_db_password@localhost:54322/agi_agents\"\n        },\n        description=\"Supabase configuration\"\n    )\n    \n    # Monitoring Configuration\n    monitoring: Dict[str, Any] = Field(\n        default_factory=lambda: {\n            \"prometheus_url\": \"http://localhost:9090\",\n            \"grafana_url\": \"http://localhost:3000\",\n            \"loki_url\": \"http://localhost:3100\",\n            \"cadvisor_url\": \"http://localhost:8080\"\n        },\n        description=\"Monitoring services configuration\"\n    )\n    \n    # Search Configuration (searxng-search)\n    search: Dict[str, Any] = Field(\n        default_factory=lambda: {\n            \"url\": \"http://localhost:8888\",\n            \"timeout\": 10.0,\n            \"max_results\": 50\n        },\n        description=\"Search service configuration\"\n    )\n\n\n# ============================================================================\n# Service Discovery\n# ============================================================================\n\nclass ServiceDiscovery:\n    \"\"\"Service discovery for Docker infrastructure.\"\"\"\n    \n    def __init__(self, config: DockerInfrastructureConfig):\n        self.config = config\n    \n    def get_postgresql_config(self) -> PostgreSQLConfig:\n        \"\"\"Get PostgreSQL configuration.\"\"\"\n        return self.config.postgresql\n    \n    def get_weaviate_config(self) -> WeaviateConfig:\n        \"\"\"Get Weaviate configuration.\"\"\"\n        return self.config.weaviate\n    \n    def get_redis_config(self) -> Dict[str, Any]:\n        \"\"\"Get Redis configuration.\"\"\"\n        return self.config.redis\n    \n    def get_supabase_config(self) -> Dict[str, Any]:\n        \"\"\"Get Supabase configuration.\"\"\"\n        return self.config.supabase\n    \n    def get_monitoring_config(self) -> Dict[str, Any]:\n        \"\"\"Get monitoring configuration.\"\"\"\n        return self.config.monitoring\n    \n    def get_search_config(self) -> Dict[str, Any]:\n        \"\"\"Get search configuration.\"\"\"\n        return self.config.search\n    \n    def get_all_services(self) -> Dict[str, Dict[str, Any]]:\n        \"\"\"Get all service configurations.\"\"\"\n        return {\n            \"postgresql\": self.config.postgresql.dict(),\n            \"weaviate\": self.config.weaviate.dict(),\n            \"redis\": self.config.redis,\n            \"supabase\": self.config.supabase,\n            \"monitoring\": self.config.monitoring,\n            \"search\": self.config.search\n        }\n\n\n# ============================================================================\n# Docker Health Check\n# ============================================================================\n\nclass DockerHealthChecker:\n    \"\"\"Health checker for Docker services.\"\"\"\n    \n    def __init__(self, config: DockerInfrastructureConfig):\n        self.config = config\n    \n    async def check_postgresql_health(self) -> Dict[str, Any]:\n        \"\"\"Check PostgreSQL health.\"\"\"\n        try:\n            import asyncpg\n            \n            conn = await asyncpg.connect(\n                host=self.config.postgresql.host,\n                port=self.config.postgresql.port,\n                database=self.config.postgresql.database,\n                user=self.config.postgresql.username,\n                password=self.config.postgresql.password\n            )\n            \n            # Test basic connectivity\n            result = await conn.fetchval(\"SELECT 1\")\n            await conn.close()\n            \n            return {\n                \"service\": \"postgresql\",\n                \"status\": \"healthy\",\n                \"host\": self.config.postgresql.host,\n                \"port\": self.config.postgresql.port,\n                \"database\": self.config.postgresql.database,\n                \"test_result\": result\n            }\n            \n        except Exception as e:\n            return {\n                \"service\": \"postgresql\",\n                \"status\": \"unhealthy\",\n                \"error\": str(e),\n                \"host\": self.config.postgresql.host,\n                \"port\": self.config.postgresql.port\n            }\n    \n    async def check_weaviate_health(self) -> Dict[str, Any]:\n        \"\"\"Check Weaviate health.\"\"\"\n        try:\n            import aiohttp\n            \n            async with aiohttp.ClientSession() as session:\n                async with session.get(\n                    f\"{self.config.weaviate.base_url}/v1/meta\",\n                    timeout=aiohttp.ClientTimeout(total=5.0)\n                ) as response:\n                    if response.status == 200:\n                        meta = await response.json()\n                        return {\n                            \"service\": \"weaviate\",\n                            \"status\": \"healthy\",\n                            \"url\": self.config.weaviate.base_url,\n                            \"version\": meta.get(\"version\", \"unknown\")\n                        }\n                    else:\n                        return {\n                            \"service\": \"weaviate\",\n                            \"status\": \"unhealthy\",\n                            \"error\": f\"HTTP {response.status}\",\n                            \"url\": self.config.weaviate.base_url\n                        }\n                        \n        except Exception as e:\n            return {\n                \"service\": \"weaviate\",\n                \"status\": \"unhealthy\",\n                \"error\": str(e),\n                \"url\": self.config.weaviate.base_url\n            }\n    \n    async def check_redis_health(self) -> Dict[str, Any]:\n        \"\"\"Check Redis health.\"\"\"\n        try:\n            import redis.asyncio as redis\n            \n            r = redis.Redis(\n                host=self.config.redis[\"host\"],\n                port=self.config.redis[\"port\"],\n                db=self.config.redis[\"db\"],\n                password=self.config.redis[\"password\"],\n                socket_timeout=self.config.redis[\"socket_timeout\"],\n                socket_connect_timeout=self.config.redis[\"socket_connect_timeout\"]\n            )\n            \n            # Test basic connectivity\n            result = await r.ping()\n            await r.close()\n            \n            return {\n                \"service\": \"redis\",\n                \"status\": \"healthy\",\n                \"host\": self.config.redis[\"host\"],\n                \"port\": self.config.redis[\"port\"],\n                \"test_result\": result\n            }\n            \n        except Exception as e:\n            return {\n                \"service\": \"redis\",\n                \"status\": \"unhealthy\",\n                \"error\": str(e),\n                \"host\": self.config.redis[\"host\"],\n                \"port\": self.config.redis[\"port\"]\n            }\n    \n    async def check_all_services(self) -> Dict[str, Dict[str, Any]]:\n        \"\"\"Check health of all services.\"\"\"\n        import asyncio\n        \n        # Run all health checks concurrently\n        results = await asyncio.gather(\n            self.check_postgresql_health(),\n            self.check_weaviate_health(),\n            self.check_redis_health(),\n            return_exceptions=True\n        )\n        \n        return {\n            \"postgresql\": results[0] if not isinstance(results[0], Exception) else {\"service\": \"postgresql\", \"status\": \"unhealthy\", \"error\": str(results[0])},\n            \"weaviate\": results[1] if not isinstance(results[1], Exception) else {\"service\": \"weaviate\", \"status\": \"unhealthy\", \"error\": str(results[1])},\n            \"redis\": results[2] if not isinstance(results[2], Exception) else {\"service\": \"redis\", \"status\": \"unhealthy\", \"error\": str(results[2])}\n        }\n\n\n# ============================================================================\n# Factory Functions\n# ============================================================================\n\ndef create_docker_config(\n    postgresql_overrides: Optional[Dict[str, Any]] = None,\n    weaviate_overrides: Optional[Dict[str, Any]] = None,\n    redis_overrides: Optional[Dict[str, Any]] = None\n) -> DockerInfrastructureConfig:\n    \"\"\"Create Docker infrastructure configuration with optional overrides.\"\"\"\n    \n    config = DockerInfrastructureConfig()\n    \n    # Apply PostgreSQL overrides\n    if postgresql_overrides:\n        for key, value in postgresql_overrides.items():\n            if hasattr(config.postgresql, key):\n                setattr(config.postgresql, key, value)\n    \n    # Apply Weaviate overrides\n    if weaviate_overrides:\n        for key, value in weaviate_overrides.items():\n            if hasattr(config.weaviate, key):\n                setattr(config.weaviate, key, value)\n    \n    # Apply Redis overrides\n    if redis_overrides:\n        config.redis.update(redis_overrides)\n    \n    return config\n\n\ndef create_service_discovery(config: Optional[DockerInfrastructureConfig] = None) -> ServiceDiscovery:\n    \"\"\"Create service discovery instance.\"\"\"\n    if config is None:\n        config = DockerInfrastructureConfig()\n    return ServiceDiscovery(config)\n\n\ndef create_health_checker(config: Optional[DockerInfrastructureConfig] = None) -> DockerHealthChecker:\n    \"\"\"Create health checker instance.\"\"\"\n    if config is None:\n        config = DockerInfrastructureConfig()\n    return DockerHealthChecker(config)\n\n\n# ============================================================================\n# Export all classes and functions\n# ============================================================================\n\n__all__ = [\n    # Configuration\n    \"DockerInfrastructureConfig\",\n    \n    # Service discovery\n    \"ServiceDiscovery\",\n    \n    # Health checking\n    \"DockerHealthChecker\",\n    \n    # Factory functions\n    \"create_docker_config\",\n    \"create_service_discovery\",\n    \"create_health_checker\",\n]\n", "source": "src/core/memory/docker_config.py"}, {"instruction": "Explain the following code from src/core/chat/multimodal_chat.py:", "response": "This code is from src/core/chat/multimodal_chat.py:\n\n\"\"\"\nMultimodal Chat System for Agentic LLM Core v0.1\n\nThis module provides turn-based multimodal conversation capabilities\nwith image analysis, tool integration, and context management.\n\nCreated: 2024-09-24\nStatus: Draft\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport base64\nimport json\nimport logging\nimport uuid\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Union\nfrom uuid import uuid4\n\nimport torch\nfrom PIL import Image\nfrom pydantic import BaseModel, Field, validator\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n\n# ============================================================================\n# Data Models\n# ============================================================================\n\nclass MessageType(str, Enum):\n    \"\"\"Message types in conversation.\"\"\"\n    USER_TEXT = \"user_text\"\n    USER_IMAGE = \"user_image\"\n    USER_MULTIMODAL = \"user_multimodal\"\n    ASSISTANT_TEXT = \"assistant_text\"\n    ASSISTANT_MULTIMODAL = \"assistant_multimodal\"\n    SYSTEM = \"system\"\n    TOOL_CALL = \"tool_call\"\n    TOOL_RESULT = \"tool_result\"\n\n\nclass ToolType(str, Enum):\n    \"\"\"Available tool types.\"\"\"\n    LC_RETRIEVER = \"lc:retriever\"\n    VISION_ANALYZER = \"vision:analyzer\"\n    KNOWLEDGE_BASE = \"kb:query\"\n    MCP_TOOL = \"mcp:tool\"\n\n\nclass ConversationTurn(BaseModel):\n    \"\"\"A single turn in the conversation.\"\"\"\n    turn_id: str = Field(default_factory=lambda: str(uuid4()), description=\"Unique turn ID\")\n    message_type: MessageType = Field(..., description=\"Type of message\")\n    content: str = Field(..., description=\"Text content\")\n    image_path: Optional[str] = Field(None, description=\"Path to image file\")\n    image_analysis: Optional[Dict[str, Any]] = Field(None, description=\"Image analysis results\")\n    tools_used: List[str] = Field(default_factory=list, description=\"Tools used in this turn\")\n    tool_results: Dict[str, Any] = Field(default_factory=dict, description=\"Results from tool execution\")\n    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description=\"Turn timestamp\")\n    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Additional metadata\")\n\n\nclass ChatRequest(BaseModel):\n    \"\"\"Multimodal chat request.\"\"\"\n    turn_id: str = Field(default_factory=lambda: str(uuid4()), description=\"Turn ID\")\n    text: str = Field(..., description=\"User text input\")\n    image: Optional[str] = Field(None, description=\"Path to image file\")\n    tools: List[str] = Field(default_factory=list, description=\"Requested tools\")\n    save_context: bool = Field(True, description=\"Save conversation context\")\n    max_tokens: int = Field(1024, ge=100, le=4096, description=\"Maximum response tokens\")\n    temperature: float = Field(0.7, ge=0.0, le=2.0, description=\"Response temperature\")\n\n\nclass ChatResponse(BaseModel):\n    \"\"\"Multimodal chat response.\"\"\"\n    response_id: str = Field(default_factory=lambda: str(uuid4()), description=\"Response ID\")\n    turn_id: str = Field(..., description=\"Original turn ID\")\n    response_text: str = Field(..., description=\"Assistant response text\")\n    image_analysis: Optional[Dict[str, Any]] = Field(None, description=\"Image analysis results\")\n    tools_executed: List[str] = Field(default_factory=list, description=\"Tools that were executed\")\n    tool_results: Dict[str, Any] = Field(default_factory=dict, description=\"Tool execution results\")\n    context_saved: bool = Field(False, description=\"Whether context was saved\")\n    confidence: float = Field(0.8, ge=0.0, le=1.0, description=\"Response confidence\")\n    processing_time_ms: float = Field(..., description=\"Processing time in milliseconds\")\n    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description=\"Response timestamp\")\n\n\nclass ConversationContext(BaseModel):\n    \"\"\"Conversation context for persistence.\"\"\"\n    session_id: str = Field(default_factory=lambda: str(uuid4()), description=\"Session ID\")\n    turns: List[ConversationTurn] = Field(default_factory=list, description=\"Conversation turns\")\n    context_summary: str = Field(\"\", description=\"Summary of conversation context\")\n    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description=\"Context creation time\")\n    updated_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description=\"Last update time\")\n    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Additional context metadata\")\n\n\n# ============================================================================\n# Tool Implementations\n# ============================================================================\n\nclass LangChainRetriever:\n    \"\"\"LangChain retriever tool implementation.\"\"\"\n    \n    def __init__(self):\n        self.logger = logging.getLogger(__name__)\n        self.knowledge_base = {\n            \"circuit_defects\": [\n                \"Solder bridges are critical defects that can cause short circuits\",\n                \"Cold solder joints appear dull and may cause intermittent connections\",\n                \"Excess solder can cause bridging and component damage\",\n                \"Missing components will cause circuit malfunction\",\n                \"Cracked traces can cause open circuits and signal loss\",\n                \"Contamination can cause corrosion and electrical leakage\"\n            ],\n            \"ipc_standards\": [\n                \"IPC-A-610 Class 3 is the highest quality standard for electronic assemblies\",\n                \"Class 3 requires zero defects for critical applications\",\n                \"Solder joints must be smooth, shiny, and properly formed\",\n                \"Component placement must be within specified tolerances\",\n                \"No foreign material or contamination is allowed\",\n                \"All connections must be secure and properly terminated\"\n            ],\n            \"troubleshooting\": [\n                \"Visual inspection is the first step in defect detection\",\n                \"Use magnification to identify small defects\",\n                \"Check for proper solder joint formation and wetting\",\n                \"Verify component orientation and placement\",\n                \"Look for signs of thermal damage or stress\",\n                \"Document all findings for quality control\"\n            ]\n        }\n    \n    async def retrieve(self, query: str, context: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"Retrieve relevant information based on query.\"\"\"\n        try:\n            query_lower = query.lower()\n            relevant_info = []\n            \n            # Search through knowledge base\n            for category, items in self.knowledge_base.items():\n                for item in items:\n                    if any(keyword in query_lower for keyword in item.lower().split()):\n                        relevant_info.append({\n                            \"category\": category,\n                            \"content\": item,\n                            \"relevance_score\": 0.8\n                        })\n            \n            # If no specific matches, return general troubleshooting info\n            if not relevant_info:\n                relevant_info = [\n                    {\n                        \"category\": \"troubleshooting\",\n                        \"content\": \"Visual inspection is the first step in defect detection\",\n                        \"relevance_score\": 0.6\n                    },\n                    {\n                        \"category\": \"troubleshooting\", \n                        \"content\": \"Use magnification to identify small defects\",\n                        \"relevance_score\": 0.6\n                    }\n                ]\n            \n            return {\n                \"query\": query,\n                \"results\": relevant_info,\n                \"total_results\": len(relevant_info),\n                \"context\": context\n            }\n            \n        except Exception as e:\n            self.logger.error(f\"LangChain retriever failed: {e}\")\n            return {\n                \"query\": query,\n                \"results\": [],\n                \"error\": str(e),\n                \"context\": context\n            }\n\n\n# ============================================================================\n# Multimodal Chat System\n# ============================================================================\n\nclass MultimodalChatSystem:\n    \"\"\"Multimodal chat system with image analysis and tool integration.\"\"\"\n    \n    def __init__(self, vision_system=None, context_storage_path: str = \"contexts\"):\n        self.logger = logging.getLogger(__name__)\n        self.vision_system = vision_system\n        self.context_storage_path = Path(context_storage_path)\n        self.context_storage_path.mkdir(exist_ok=True)\n        \n        # Initialize tools\n        self.tools = {\n            ToolType.LC_RETRIEVER: LangChainRetriever(),\n        }\n        \n        # Active conversation contexts\n        self.active_contexts: Dict[str, ConversationContext] = {}\n        \n        # Import vision system if not provided\n        if not self.vision_system:\n            try:\n                from src.core.vision.description_system import VisionDescriptionSystem\n                self.vision_system = VisionDescriptionSystem()\n            except ImportError:\n                self.logger.warning(\"Vision system not available\")\n    \n    async def chat(self, request: ChatRequest) -> ChatResponse:\n        \"\"\"Process a multimodal chat request.\"\"\"\n        start_time = datetime.now()\n        \n        try:\n            # Load or create conversation context\n            context = await self._load_context(request.turn_id)\n            \n            # Analyze image if provided\n            image_analysis = None\n            if request.image:\n                image_analysis = await self._analyze_image(request.image, request.text)\n            \n            # Execute requested tools\n            tools_executed = []\n            tool_results = {}\n            \n            for tool_name in request.tools:\n                if tool_name in self.tools:\n                    try:\n                        result = await self._execute_tool(tool_name, request.text, image_analysis)\n                        tools_executed.append(tool_name)\n                        tool_results[tool_name] = result\n                    except Exception as e:\n                        self.logger.error(f\"Tool execution failed for {tool_name}: {e}\")\n                        tool_results[tool_name] = {\"error\": str(e)}\n            \n            # Generate response\n            response_text = await self._generate_response(\n                request.text, \n                image_analysis, \n                tool_results,\n                context\n            )\n            \n            # Save conversation turn\n            turn = ConversationTurn(\n                turn_id=request.turn_id,\n                message_type=MessageType.USER_MULTIMODAL if request.image else MessageType.USER_TEXT,\n                content=request.text,\n                image_path=request.image,\n                image_analysis=image_analysis,\n                tools_used=tools_executed,\n                tool_results=tool_results\n            )\n            \n            context.turns.append(turn)\n            \n            # Save context if requested\n            context_saved = False\n            if request.save_context:\n                context_saved = await self._save_context(context)\n            \n            processing_time = (datetime.now() - start_time).total_seconds() * 1000\n            \n            return ChatResponse(\n                turn_id=request.turn_id,\n                response_text=response_text,\n                image_analysis=image_analysis,\n                tools_executed=tools_executed,\n                tool_results=tool_results,\n                context_saved=context_saved,\n                confidence=0.8,\n                processing_time_ms=processing_time\n            )\n            \n        except Exception as e:\n            self.logger.error(f\"Chat processing failed: {e}\")\n            processing_time = (datetime.now() - start_time).total_seconds() * 1000\n            \n            return ChatResponse(\n                turn_id=request.turn_id,\n                response_text=f\"I encountered an error processing your request: {str(e)}\",\n                confidence=0.0,\n                processing_time_ms=processing_time\n            )\n    \n    async def _load_context(self, turn_id: str) -> ConversationContext:\n        \"\"\"Load conversation context.\"\"\"\n        # For now, create a new context for each turn_id\n        # In a real implementation, this would load from persistent storage\n        if turn_id not in self.active_contexts:\n            self.active_contexts[turn_id] = ConversationContext()\n        return self.active_contexts[turn_id]\n    \n    async def _save_context(self, context: ConversationContext) -> bool:\n        \"\"\"Save conversation context.\"\"\"\n        try:\n            context.updated_at = datetime.now(timezone.utc)\n            context_file = self.context_storage_path / f\"{context.session_id}.json\"\n            \n            with open(context_file, 'w') as f:\n                json.dump(context.model_dump(), f, indent=2, default=str)\n            \n            return True\n        except Exception as e:\n            self.logger.error(f\"Failed to save context: {e}\")\n            return False\n    \n    async def _analyze_image(self, image_path: str, text_context: str) -> Dict[str, Any]:\n        \"\"\"Analyze image using vision system.\"\"\"\n        try:\n            if not self.vision_system:\n                return {\"error\": \"Vision system not available\"}\n            \n            # Determine analysis mode based on text context\n            analysis_mode = \"defect_detection\"\n            hints = []\n            \n            if \"defect\" in text_context.lower() or \"wrong\" in text_context.lower():\n                analysis_mode = \"defect_detection\"\n                hints = [\"IPC-A-610 Class 3 defects\", \"solder bridge\", \"excess solder\", \"cold solder joint\"]\n            elif \"component\" in text_context.lower():\n                analysis_mode = \"component_identification\"\n                hints = [\"component identification\", \"resistor\", \"capacitor\", \"IC\"]\n            elif \"technical\" in text_context.lower():\n                analysis_mode = \"technical\"\n                hints = [\"technical analysis\", \"wiring\"]\n            else:\n                hints = [\"general analysis\"]\n            \n            # Create vision request\n            from src.core.vision.description_system import VisionDescriptionRequest, AnalysisMode\n            \n            vision_request = VisionDescriptionRequest(\n                image_path=image_path,\n                hints=hints,\n                max_tokens=512,\n                normalize=True,\n                analysis_mode=AnalysisMode(analysis_mode)\n            )\n            \n            # Get vision analysis\n            vision_response = await self.vision_system.describe_image(vision_request)\n            \n            return {\n                \"analysis_mode\": analysis_mode,\n                \"description\": vision_response.description,\n                \"confidence\": vision_response.confidence,\n                \"defects\": [defect.model_dump() for defect in vision_response.defects],\n                \"components\": [comp.model_dump() for comp in vision_response.components],\n                \"technical_details\": vision_response.technical_details,\n                \"metadata\": vision_response.metadata.model_dump() if vision_response.metadata else None\n            }\n            \n        except Exception as e:\n            self.logger.error(f\"Image analysis failed: {e}\")\n            return {\"error\": str(e)}\n    \n    async def _execute_tool(self, tool_name: str, text: str, image_analysis: Optional[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"Execute a specific tool.\"\"\"\n        try:\n            tool = self.tools.get(tool_name)\n            if not tool:\n                return {\"error\": f\"Tool {tool_name} not found\"}\n            \n            # Prepare context for tool\n            context = \"\"\n            if image_analysis:\n                context = f\"Image analysis: {image_analysis.get('description', '')}\"\n            \n            # Execute tool based on type\n            if tool_name == ToolType.LC_RETRIEVER:\n                return await tool.retrieve(text, context)\n            else:\n                return {\"error\": f\"Tool {tool_name} execution not implemented\"}\n                \n        except Exception as e:\n            self.logger.error(f\"Tool execution failed for {tool_name}: {e}\")\n            return {\"error\": str(e)}\n    \n    async def _generate_response(self, text: str, image_analysis: Optional[Dict[str, Any]], \n                               tool_results: Dict[str, Any], context: ConversationContext) -> str:\n        \"\"\"Generate response based on input and analysis.\"\"\"\n        response_parts = []\n        \n        # Analyze the question\n        if \"what's wrong\" in text.lower() or \"what is wrong\" in text.lower():\n            response_parts.append(\"Based on my analysis of the image, here are the issues I've identified:\")\n            \n            if image_analysis and \"defects\" in image_analysis:\n                defects = image_analysis.get(\"defects\", [])\n                if defects:\n                    for defect in defects:\n                        response_parts.append(f\"\u2022 **{defect.get('defect_type', 'Unknown')}**: {defect.get('severity', 'Unknown')} severity\")\n                        response_parts.append(f\"  - {defect.get('recommendation', 'No recommendation available')}\")\n                else:\n                    response_parts.append(\"\u2022 No obvious defects detected in the image\")\n            \n            # Add tool-based insights\n            if ToolType.LC_RETRIEVER in tool_results:\n                retriever_results = tool_results[ToolType.LC_RETRIEVER]\n                if \"results\" in retriever_results:\n                    response_parts.append(\"\\n**Additional Information:**\")\n                    for result in retriever_results[\"results\"][:3]:  # Limit to top 3\n                        response_parts.append(f\"\u2022 {result.get('content', '')}\")\n        \n        elif \"analyze\" in text.lower() or \"inspect\" in text.lower():\n            response_parts.append(\"Here's my detailed analysis:\")\n            \n            if image_analysis:\n                response_parts.append(f\"**Image Analysis**: {image_analysis.get('description', 'No description available')}\")\n                \n                if image_analysis.get(\"technical_details\"):\n                    response_parts.append(\"**Technical Details:**\")\n                    for key, value in image_analysis[\"technical_details\"].items():\n                        response_parts.append(f\"\u2022 {key}: {value}\")\n        \n        else:\n            # General response\n            response_parts.append(\"I've analyzed your input and here's what I found:\")\n            \n            if image_analysis:\n                response_parts.append(f\"**Image**: {image_analysis.get('description', 'Image analyzed')}\")\n            \n            if tool_results:\n                response_parts.append(\"**Additional Information:**\")\n                for tool_name, result in tool_results.items():\n                    if \"results\" in result:\n                        response_parts.append(f\"\u2022 {tool_name}: {len(result['results'])} relevant items found\")\n        \n        return \"\\n\".join(response_parts)\n    \n    def get_conversation_history(self, session_id: str) -> List[ConversationTurn]:\n        \"\"\"Get conversation history for a session.\"\"\"\n        context = self.active_contexts.get(session_id)\n        return context.turns if context else []\n\n\n# ============================================================================\n# Main Function\n# ============================================================================\n\nasync def main():\n    \"\"\"Main function for command-line usage.\"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"Multimodal Chat System\")\n    parser.add_argument(\"--turn-id\", required=True, help=\"Turn ID\")\n    parser.add_argument(\"--text\", required=True, help=\"User text input\")\n    parser.add_argument(\"--image\", help=\"Path to image file\")\n    parser.add_argument(\"--tools\", nargs=\"*\", help=\"Tools to use\")\n    parser.add_argument(\"--save-context\", action=\"store_true\", help=\"Save conversation context\")\n    parser.add_argument(\"--max-tokens\", type=int, default=1024, help=\"Maximum tokens\")\n    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Verbose output\")\n    \n    args = parser.parse_args()\n    \n    if args.verbose:\n        logging.getLogger().setLevel(logging.DEBUG)\n    \n    try:\n        # Create chat system\n        chat_system = MultimodalChatSystem()\n        \n        # Create request\n        request = ChatRequest(\n            turn_id=args.turn_id,\n            text=args.text,\n            image=args.image,\n            tools=args.tools or [],\n            save_context=args.save_context,\n            max_tokens=args.max_tokens\n        )\n        \n        # Process chat\n        response = await chat_system.chat(request)\n        \n        # Print results\n        print(\"\\n\" + \"=\"*80)\n        print(\"MULTIMODAL CHAT RESULT\")\n        print(\"=\"*80)\n        print(f\"Turn ID: {request.turn_id}\")\n        print(f\"Response ID: {response.response_id}\")\n        print(f\"Confidence: {response.confidence:.2f}\")\n        print(f\"Processing Time: {response.processing_time_ms:.2f}ms\")\n        print(f\"Context Saved: {response.context_saved}\")\n        \n        if response.image_analysis:\n            print(f\"\\n\ud83d\uddbc\ufe0f Image Analysis:\")\n            print(f\"   Mode: {response.image_analysis.get('analysis_mode', 'unknown')}\")\n            print(f\"   Description: {response.image_analysis.get('description', 'No description')}\")\n            print(f\"   Confidence: {response.image_analysis.get('confidence', 0):.2f}\")\n            \n            if response.image_analysis.get('defects'):\n                print(f\"   Defects Found: {len(response.image_analysis['defects'])}\")\n                for defect in response.image_analysis['defects']:\n                    print(f\"     - {defect.get('defect_type', 'Unknown')}: {defect.get('severity', 'Unknown')}\")\n        \n        if response.tools_executed:\n            print(f\"\\n\ud83d\udd27 Tools Executed:\")\n            for tool in response.tools_executed:\n                print(f\"   - {tool}\")\n        \n        print(f\"\\n\ud83d\udcac Response:\")\n        print(f\"   {response.response_text}\")\n        \n        return 0\n        \n    except Exception as e:\n        logger.error(f\"Multimodal chat failed: {e}\")\n        return 1\n\n\nif __name__ == \"__main__\":\n    exit(asyncio.run(main()))\n", "source": "src/core/chat/multimodal_chat.py"}, {"instruction": "Explain the following code from src/core/security/policy_manager.py:", "response": "This code is from src/core/security/policy_manager.py:\n\n\"\"\"\nSecurity Policy Manager for Agentic LLM Core v0.1\n\nThis module implements comprehensive security policies including data redaction,\ntool allowlisting, and side effects logging.\n\nCreated: 2024-09-24\nStatus: Draft\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport re\nimport time\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Union, Tuple\nfrom uuid import uuid4\n\nimport yaml\nfrom pydantic import BaseModel, Field, validator\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n\n# ============================================================================\n# Data Models\n# ============================================================================\n\nclass RedactionMode(str, Enum):\n    \"\"\"Redaction modes.\"\"\"\n    STRICT = \"strict\"\n    MODERATE = \"moderate\"\n    LENIENT = \"lenient\"\n\n\nclass SideEffectMode(str, Enum):\n    \"\"\"Side effect modes.\"\"\"\n    LOG_ONLY = \"log-only\"\n    BLOCK = \"block\"\n    ALLOW_WITH_LOG = \"allow-with-log\"\n\n\nclass SecurityLevel(str, Enum):\n    \"\"\"Security levels.\"\"\"\n    PUBLIC = \"public\"\n    INTERNAL = \"internal\"\n    CONFIDENTIAL = \"confidential\"\n    RESTRICTED = \"restricted\"\n\n\nclass RedactionPattern(BaseModel):\n    \"\"\"Redaction pattern definition.\"\"\"\n    enabled: bool = Field(True, description=\"Whether pattern is enabled\")\n    patterns: List[str] = Field(..., description=\"Regex patterns to match\")\n    replacement: str = Field(..., description=\"Replacement text\")\n    case_sensitive: bool = Field(False, description=\"Case sensitivity\")\n\n\nclass ToolPermission(BaseModel):\n    \"\"\"Tool permission definition.\"\"\"\n    name: str = Field(..., description=\"Tool name\")\n    category: str = Field(..., description=\"Tool category\")\n    description: str = Field(..., description=\"Tool description\")\n    permissions: List[str] = Field(..., description=\"Allowed permissions\")\n    restrictions: List[str] = Field(default_factory=list, description=\"Restrictions\")\n\n\nclass SecurityEvent(BaseModel):\n    \"\"\"Security event record.\"\"\"\n    event_id: str = Field(default_factory=lambda: str(uuid4()), description=\"Event ID\")\n    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description=\"Event timestamp\")\n    event_type: str = Field(..., description=\"Type of security event\")\n    severity: str = Field(..., description=\"Event severity\")\n    user_id: Optional[str] = Field(None, description=\"User ID\")\n    session_id: Optional[str] = Field(None, description=\"Session ID\")\n    tool_name: Optional[str] = Field(None, description=\"Tool name\")\n    details: Dict[str, Any] = Field(default_factory=dict, description=\"Event details\")\n    redacted_data: Optional[str] = Field(None, description=\"Redacted data if applicable\")\n\n\nclass RedactionResult(BaseModel):\n    \"\"\"Redaction result.\"\"\"\n    original_text: str = Field(..., description=\"Original text\")\n    redacted_text: str = Field(..., description=\"Redacted text\")\n    redaction_count: int = Field(0, description=\"Number of redactions made\")\n    redaction_types: List[str] = Field(default_factory=list, description=\"Types of redactions\")\n    redaction_details: List[Dict[str, Any]] = Field(default_factory=list, description=\"Redaction details\")\n\n\nclass ToolAccessResult(BaseModel):\n    \"\"\"Tool access result.\"\"\"\n    allowed: bool = Field(..., description=\"Whether access is allowed\")\n    reason: str = Field(..., description=\"Reason for decision\")\n    restrictions: List[str] = Field(default_factory=list, description=\"Applied restrictions\")\n    permissions: List[str] = Field(default_factory=list, description=\"Granted permissions\")\n\n\n# ============================================================================\n# Security Policy Manager\n# ============================================================================\n\nclass SecurityPolicyManager:\n    \"\"\"Manages security policies and enforcement.\"\"\"\n    \n    def __init__(self, config_path: Optional[Union[str, Path]] = None):\n        self.logger = logging.getLogger(__name__)\n        self.redaction_patterns: Dict[str, RedactionPattern] = {}\n        self.allowed_tools: Dict[str, ToolPermission] = {}\n        self.blocked_tools: List[str] = []\n        self.side_effect_mode = SideEffectMode.LOG_ONLY\n        self.security_events: List[SecurityEvent] = []\n        \n        if config_path:\n            self.load_config(config_path)\n        else:\n            self._load_default_config()\n    \n    def load_config(self, config_path: Union[str, Path]):\n        \"\"\"Load security policy configuration.\"\"\"\n        try:\n            with open(config_path, 'r', encoding='utf-8') as f:\n                if str(config_path).endswith('.yaml') or str(config_path).endswith('.yml'):\n                    config_data = yaml.safe_load(f)\n                else:\n                    config_data = json.load(f)\n            \n            security_policy = config_data.get('security_policy', {})\n            \n            # Load redaction patterns\n            redaction_config = security_policy.get('redaction', {})\n            patterns_config = redaction_config.get('patterns', {})\n            for pattern_name, pattern_data in patterns_config.items():\n                pattern = RedactionPattern(**pattern_data)\n                self.redaction_patterns[pattern_name] = pattern\n            \n            # Load tool permissions\n            tool_security = security_policy.get('tool_security', {})\n            allowed_tools_config = tool_security.get('allowed_tools', [])\n            for tool_data in allowed_tools_config:\n                tool_permission = ToolPermission(**tool_data)\n                self.allowed_tools[tool_permission.name] = tool_permission\n            \n            # Load blocked tools\n            blocked_tools_config = tool_security.get('blocked_tools', [])\n            self.blocked_tools = [tool['name'] for tool in blocked_tools_config]\n            \n            # Load side effect mode\n            side_effects = security_policy.get('side_effects', {})\n            self.side_effect_mode = SideEffectMode(side_effects.get('mode', 'log-only'))\n            \n            self.logger.info(f\"Loaded security policy configuration from {config_path}\")\n            self.logger.info(f\"Loaded {len(self.redaction_patterns)} redaction patterns\")\n            self.logger.info(f\"Loaded {len(self.allowed_tools)} allowed tools\")\n            self.logger.info(f\"Loaded {len(self.blocked_tools)} blocked tools\")\n            \n        except FileNotFoundError:\n            raise FileNotFoundError(f\"Configuration file not found: {config_path}\")\n        except (json.JSONDecodeError, yaml.YAMLError) as e:\n            raise ValueError(f\"Invalid configuration format: {e}\")\n        except Exception as e:\n            raise RuntimeError(f\"Failed to load configuration: {e}\")\n    \n    def _load_default_config(self):\n        \"\"\"Load default security configuration.\"\"\"\n        # Default redaction patterns\n        self.redaction_patterns = {\n            \"api_keys\": RedactionPattern(\n                patterns=[\"api[_-]?key\", \"apikey\", \"access[_-]?token\"],\n                replacement=\"[REDACTED_API_KEY]\"\n            ),\n            \"secrets\": RedactionPattern(\n                patterns=[\"password\", \"secret\", \"private[_-]?key\"],\n                replacement=\"[REDACTED_SECRET]\"\n            ),\n            \"emails\": RedactionPattern(\n                patterns=[\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}\"],\n                replacement=\"[REDACTED_EMAIL]\"\n            )\n        }\n        \n        # Default allowed tools\n        self.allowed_tools = {\n            \"mcp:sql\": ToolPermission(\n                name=\"mcp:sql\",\n                category=\"mcp\",\n                description=\"SQL database operations\",\n                permissions=[\"read\", \"write\", \"schema_query\"],\n                restrictions=[\"no_drop_table\", \"no_delete_all\"]\n            ),\n            \"mcp:fs\": ToolPermission(\n                name=\"mcp:fs\",\n                category=\"mcp\",\n                description=\"File system operations\",\n                permissions=[\"read\", \"write\", \"list_directory\"],\n                restrictions=[\"no_system_files\", \"no_executable_files\"]\n            ),\n            \"lc:retriever\": ToolPermission(\n                name=\"lc:retriever\",\n                category=\"langchain\",\n                description=\"Document retrieval\",\n                permissions=[\"search\", \"retrieve\", \"embed\"],\n                restrictions=[\"no_personal_data\", \"max_results: 100\"]\n            )\n        }\n        \n        # Default blocked tools\n        self.blocked_tools = [\"system:shell\", \"network:curl\", \"crypto:encrypt\"]\n        \n        self.side_effect_mode = SideEffectMode.LOG_ONLY\n    \n    def redact_text(self, text: str, context: Optional[Dict[str, Any]] = None) -> RedactionResult:\n        \"\"\"Redact sensitive information from text.\"\"\"\n        if not text:\n            return RedactionResult(original_text=text, redacted_text=text)\n        \n        redacted_text = text\n        redaction_count = 0\n        redaction_types = []\n        redaction_details = []\n        \n        # Collect all matches first to avoid double-redaction\n        all_matches = []\n        \n        for pattern_name, pattern in self.redaction_patterns.items():\n            if not pattern.enabled:\n                continue\n            \n            for regex_pattern in pattern.patterns:\n                try:\n                    flags = 0 if pattern.case_sensitive else re.IGNORECASE\n                    matches = list(re.finditer(regex_pattern, text, flags))  # Use original text\n                    \n                    for match in matches:\n                        all_matches.append({\n                            \"pattern_name\": pattern_name,\n                            \"matched_text\": match.group(),\n                            \"start\": match.start(),\n                            \"end\": match.end(),\n                            \"replacement\": pattern.replacement,\n                            \"regex_pattern\": regex_pattern,\n                            \"flags\": flags\n                        })\n                        \n                except re.error as e:\n                    self.logger.warning(f\"Invalid regex pattern '{regex_pattern}' for {pattern_name}: {e}\")\n                    continue\n        \n        # Sort matches by start position (descending) to redact from end to beginning\n        all_matches.sort(key=lambda x: x[\"start\"], reverse=True)\n        \n        # Apply redactions from end to beginning to avoid position shifts\n        for match_info in all_matches:\n            redaction_count += 1\n            if match_info[\"pattern_name\"] not in redaction_types:\n                redaction_types.append(match_info[\"pattern_name\"])\n            \n            redaction_details.append({\n                \"pattern_name\": match_info[\"pattern_name\"],\n                \"matched_text\": match_info[\"matched_text\"],\n                \"start\": match_info[\"start\"],\n                \"end\": match_info[\"end\"],\n                \"replacement\": match_info[\"replacement\"]\n            })\n            \n            # Replace the matched text\n            redacted_text = (redacted_text[:match_info[\"start\"]] + \n                           match_info[\"replacement\"] + \n                           redacted_text[match_info[\"end\"]:])\n        \n        # Log redaction event if any redactions were made\n        if redaction_count > 0:\n            self._log_security_event(\n                event_type=\"data_redaction\",\n                severity=\"INFO\",\n                details={\n                    \"redaction_count\": redaction_count,\n                    \"redaction_types\": redaction_types,\n                    \"context\": context\n                }\n            )\n        \n        return RedactionResult(\n            original_text=text,\n            redacted_text=redacted_text,\n            redaction_count=redaction_count,\n            redaction_types=redaction_types,\n            redaction_details=redaction_details\n        )\n    \n    def check_tool_access(self, tool_name: str, user_context: Optional[Dict[str, Any]] = None) -> ToolAccessResult:\n        \"\"\"Check if tool access is allowed.\"\"\"\n        # Check if tool is explicitly blocked\n        if tool_name in self.blocked_tools:\n            self._log_security_event(\n                event_type=\"tool_access_denied\",\n                severity=\"WARNING\",\n                tool_name=tool_name,\n                details={\"reason\": \"Tool is in blocked list\", \"user_context\": user_context}\n            )\n            return ToolAccessResult(\n                allowed=False,\n                reason=f\"Tool '{tool_name}' is blocked by security policy\",\n                restrictions=[\"blocked_tool\"]\n            )\n        \n        # Check if tool is in allowlist\n        if tool_name not in self.allowed_tools:\n            self._log_security_event(\n                event_type=\"tool_access_denied\",\n                severity=\"WARNING\",\n                tool_name=tool_name,\n                details={\"reason\": \"Tool not in allowlist\", \"user_context\": user_context}\n            )\n            return ToolAccessResult(\n                allowed=False,\n                reason=f\"Tool '{tool_name}' is not in the allowed tools list\",\n                restrictions=[\"not_allowlisted\"]\n            )\n        \n        # Tool is allowed\n        tool_permission = self.allowed_tools[tool_name]\n        self._log_security_event(\n            event_type=\"tool_access_granted\",\n            severity=\"INFO\",\n            tool_name=tool_name,\n            details={\n                \"permissions\": tool_permission.permissions,\n                \"restrictions\": tool_permission.restrictions,\n                \"user_context\": user_context\n            }\n        )\n        \n        return ToolAccessResult(\n            allowed=True,\n            reason=f\"Tool '{tool_name}' is allowed\",\n            permissions=tool_permission.permissions,\n            restrictions=tool_permission.restrictions\n        )\n    \n    def log_side_effect(self, effect_type: str, details: Dict[str, Any], user_context: Optional[Dict[str, Any]] = None):\n        \"\"\"Log side effects based on policy.\"\"\"\n        severity_map = {\n            \"data_access\": \"INFO\",\n            \"data_modification\": \"WARNING\",\n            \"external_communication\": \"WARNING\",\n            \"system_changes\": \"ERROR\",\n            \"security_events\": \"ERROR\"\n        }\n        \n        severity = severity_map.get(effect_type, \"INFO\")\n        \n        # Redact sensitive data in details\n        redacted_details = {}\n        for key, value in details.items():\n            if isinstance(value, str):\n                redaction_result = self.redact_text(value)\n                redacted_details[key] = redaction_result.redacted_text\n            else:\n                redacted_details[key] = value\n        \n        self._log_security_event(\n            event_type=f\"side_effect_{effect_type}\",\n            severity=severity,\n            details={\n                \"effect_type\": effect_type,\n                \"original_details\": details,\n                \"redacted_details\": redacted_details,\n                \"user_context\": user_context\n            }\n        )\n    \n    def _log_security_event(self, event_type: str, severity: str, user_id: Optional[str] = None,\n                           session_id: Optional[str] = None, tool_name: Optional[str] = None,\n                           details: Optional[Dict[str, Any]] = None):\n        \"\"\"Log a security event.\"\"\"\n        event = SecurityEvent(\n            event_type=event_type,\n            severity=severity,\n            user_id=user_id,\n            session_id=session_id,\n            tool_name=tool_name,\n            details=details or {}\n        )\n        \n        self.security_events.append(event)\n        \n        # Log to console/file based on severity\n        if severity == \"ERROR\":\n            self.logger.error(f\"Security Event [{event_type}]: {details}\")\n        elif severity == \"WARNING\":\n            self.logger.warning(f\"Security Event [{event_type}]: {details}\")\n        else:\n            self.logger.info(f\"Security Event [{event_type}]: {details}\")\n    \n    def get_security_summary(self) -> Dict[str, Any]:\n        \"\"\"Get security policy summary.\"\"\"\n        return {\n            \"redaction_patterns\": len(self.redaction_patterns),\n            \"allowed_tools\": len(self.allowed_tools),\n            \"blocked_tools\": len(self.blocked_tools),\n            \"side_effect_mode\": self.side_effect_mode.value,\n            \"security_events_count\": len(self.security_events),\n            \"recent_events\": self.security_events[-10:] if self.security_events else []\n        }\n    \n    def print_security_summary(self):\n        \"\"\"Print security policy summary.\"\"\"\n        print(\"\\n\" + \"=\"*80)\n        print(\"SECURITY POLICY SUMMARY\")\n        print(\"=\"*80)\n        \n        print(f\"\\n\ud83d\udd12 Redaction Patterns ({len(self.redaction_patterns)}):\")\n        for pattern_name, pattern in self.redaction_patterns.items():\n            status_emoji = \"\u2705\" if pattern.enabled else \"\u274c\"\n            print(f\"   {status_emoji} {pattern_name}\")\n            print(f\"     Patterns: {len(pattern.patterns)} regex patterns\")\n            print(f\"     Replacement: {pattern.replacement}\")\n            print(f\"     Case Sensitive: {'Yes' if pattern.case_sensitive else 'No'}\")\n        \n        print(f\"\\n\ud83d\udee0\ufe0f Allowed Tools ({len(self.allowed_tools)}):\")\n        for tool_name, tool_permission in self.allowed_tools.items():\n            category_emoji = {\n                \"mcp\": \"\ud83d\udd0c\",\n                \"langchain\": \"\ud83d\udd17\",\n                \"utility\": \"\u2699\ufe0f\",\n                \"file_system\": \"\ud83d\udcc1\",\n                \"database\": \"\ud83d\uddc4\ufe0f\"\n            }.get(tool_permission.category, \"\ud83d\udd27\")\n            \n            print(f\"   {category_emoji} {tool_name}\")\n            print(f\"     Category: {tool_permission.category}\")\n            print(f\"     Permissions: {', '.join(tool_permission.permissions)}\")\n            if tool_permission.restrictions:\n                print(f\"     Restrictions: {', '.join(tool_permission.restrictions)}\")\n        \n        print(f\"\\n\ud83d\udeab Blocked Tools ({len(self.blocked_tools)}):\")\n        for tool_name in self.blocked_tools:\n            print(f\"   \u274c {tool_name}\")\n        \n        print(f\"\\n\ud83d\udcca Side Effects Mode: {self.side_effect_mode.value}\")\n        \n        print(f\"\\n\ud83d\udcc8 Security Events: {len(self.security_events)} total\")\n        if self.security_events:\n            recent_events = self.security_events[-5:]\n            print(\"   Recent events:\")\n            for event in recent_events:\n                severity_emoji = {\n                    \"ERROR\": \"\ud83d\udd34\",\n                    \"WARNING\": \"\ud83d\udfe1\",\n                    \"INFO\": \"\ud83d\udd35\"\n                }.get(event.severity, \"\u26aa\")\n                print(f\"     {severity_emoji} {event.event_type} ({event.severity})\")\n\n\n# ============================================================================\n# Main Function\n# ============================================================================\n\nasync def main():\n    \"\"\"Main function for command-line usage.\"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"Security Policy Manager\")\n    parser.add_argument(\"--config\", help=\"Path to security policy configuration file\")\n    parser.add_argument(\"--test-redaction\", help=\"Test redaction with sample text\")\n    parser.add_argument(\"--test-tool\", help=\"Test tool access for specific tool\")\n    parser.add_argument(\"--show-policy\", action=\"store_true\", help=\"Show security policy summary\")\n    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Verbose output\")\n    \n    args = parser.parse_args()\n    \n    if args.verbose:\n        logging.getLogger().setLevel(logging.DEBUG)\n    \n    try:\n        # Create security policy manager\n        manager = SecurityPolicyManager(args.config)\n        \n        if args.show_policy:\n            manager.print_security_summary()\n        \n        if args.test_redaction:\n            # Test redaction\n            result = manager.redact_text(args.test_redaction)\n            print(f\"\\n\ud83d\udd0d Redaction Test:\")\n            print(f\"   Original: {result.original_text}\")\n            print(f\"   Redacted: {result.redacted_text}\")\n            print(f\"   Redactions: {result.redaction_count}\")\n            print(f\"   Types: {', '.join(result.redaction_types)}\")\n        \n        if args.test_tool:\n            # Test tool access\n            result = manager.check_tool_access(args.test_tool)\n            print(f\"\\n\ud83d\udee0\ufe0f Tool Access Test:\")\n            print(f\"   Tool: {args.test_tool}\")\n            print(f\"   Allowed: {'\u2705' if result.allowed else '\u274c'}\")\n            print(f\"   Reason: {result.reason}\")\n            if result.permissions:\n                print(f\"   Permissions: {', '.join(result.permissions)}\")\n            if result.restrictions:\n                print(f\"   Restrictions: {', '.join(result.restrictions)}\")\n        \n    except Exception as e:\n        logger.error(f\"Security policy manager error: {e}\")\n        return 1\n    \n    return 0\n\n\nif __name__ == \"__main__\":\n    exit(asyncio.run(main()))\n", "source": "src/core/security/policy_manager.py"}, {"instruction": "Explain the following code from src/core/providers/llm_qwen3.py:", "response": "This code is from src/core/providers/llm_qwen3.py:\n\n\"\"\"\nQwen3 LLM Provider for Agentic LLM Core v0.1\n\nThis module provides the Qwen3 LLM provider with support for:\n- Complete: Text completion and generation\n- Embed: Text embedding generation\n- Vision_to_text: Image to text conversion\n- Context and rate limiting\n- Local-small fallback support\n\nCreated: 2024-09-24\nStatus: Draft\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport time\nfrom datetime import datetime, timedelta\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Union, AsyncGenerator\nfrom uuid import uuid4\n\nimport torch\nfrom pydantic import BaseModel, Field, field_validator\n\nfrom ..models.contracts import ProcessedInput, UnifiedContext, FinalAnswer\n\n\n# ============================================================================\n# Provider Configuration\n# ============================================================================\n\nclass Qwen3Mode(str, Enum):\n    \"\"\"Supported Qwen3 operation modes.\"\"\"\n    COMPLETE = \"complete\"\n    EMBED = \"embed\"\n    VISION_TO_TEXT = \"vision_to_text\"\n\n\nclass Qwen3Device(str, Enum):\n    \"\"\"Supported device types for Qwen3.\"\"\"\n    CPU = \"cpu\"\n    MPS = \"mps\"  # Apple Silicon Metal Performance Shaders\n    CUDA = \"cuda\"\n\n\nclass Qwen3Precision(str, Enum):\n    \"\"\"Supported precision types.\"\"\"\n    FLOAT16 = \"float16\"\n    FLOAT32 = \"float32\"\n    INT8 = \"int8\"\n    INT4 = \"int4\"\n\n\n# ============================================================================\n# Configuration Models\n# ============================================================================\n\nclass ContextLimits(BaseModel):\n    \"\"\"Context window limits for Qwen3.\"\"\"\n    max_tokens: int = Field(default=8192, ge=1, le=32768, description=\"Maximum context tokens\")\n    max_prompt_tokens: int = Field(default=6144, ge=1, le=24576, description=\"Maximum prompt tokens\")\n    max_completion_tokens: int = Field(default=2048, ge=1, le=8192, description=\"Maximum completion tokens\")\n    reserve_tokens: int = Field(default=100, ge=0, description=\"Reserved tokens for overhead\")\n\n\nclass RateLimits(BaseModel):\n    \"\"\"Rate limiting configuration.\"\"\"\n    requests_per_minute: int = Field(default=60, ge=1, description=\"Max requests per minute\")\n    tokens_per_minute: int = Field(default=100000, ge=1, description=\"Max tokens per minute\")\n    concurrent_requests: int = Field(default=5, ge=1, description=\"Max concurrent requests\")\n    \n    @field_validator('requests_per_minute')\n    @classmethod\n    def validate_requests_per_minute(cls, v):\n        if v > 1000:\n            raise ValueError(\"Requests per minute cannot exceed 1000\")\n        return v\n\n\nclass Qwen3Config(BaseModel):\n    \"\"\"Qwen3 provider configuration.\"\"\"\n    # Model configuration\n    model_path: str = Field(..., description=\"Path to Qwen3 model\")\n    model_name: str = Field(default=\"qwen3-omni\", description=\"Model name\")\n    device: Qwen3Device = Field(default=Qwen3Device.MPS, description=\"Device to run on\")\n    precision: Qwen3Precision = Field(default=Qwen3Precision.FLOAT16, description=\"Model precision\")\n    \n    # Performance configuration\n    batch_size: int = Field(default=1, ge=1, le=8, description=\"Batch size for processing\")\n    max_memory_usage: int = Field(default=2 * 1024 * 1024 * 1024, description=\"Max memory usage in bytes\")\n    \n    # Limiting configuration\n    context_limits: ContextLimits = Field(default_factory=ContextLimits, description=\"Context limits\")\n    rate_limits: RateLimits = Field(default_factory=RateLimits, description=\"Rate limits\")\n    \n    # Fallback configuration\n    enable_fallback: bool = Field(default=True, description=\"Enable local-small fallback\")\n    fallback_model_path: Optional[str] = Field(None, description=\"Path to fallback model\")\n    \n    # Apple Silicon optimization\n    use_mps_acceleration: bool = Field(default=True, description=\"Use MPS acceleration on Apple Silicon\")\n    optimize_for_inference: bool = Field(default=True, description=\"Optimize model for inference\")\n\n\n# ============================================================================\n# Request/Response Models\n# ============================================================================\n\nclass Qwen3Request(BaseModel):\n    \"\"\"Base request model for Qwen3 operations.\"\"\"\n    request_id: str = Field(default_factory=lambda: str(uuid4()), description=\"Unique request ID\")\n    mode: Qwen3Mode = Field(..., description=\"Operation mode\")\n    timestamp: datetime = Field(default_factory=datetime.utcnow, description=\"Request timestamp\")\n    \n    # Common parameters\n    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description=\"Sampling temperature\")\n    top_p: float = Field(default=0.9, ge=0.0, le=1.0, description=\"Top-p sampling\")\n    max_tokens: Optional[int] = Field(None, description=\"Maximum tokens to generate\")\n    stream: bool = Field(default=False, description=\"Enable streaming response\")\n\n\nclass Qwen3CompleteRequest(Qwen3Request):\n    \"\"\"Request model for text completion.\"\"\"\n    mode: Qwen3Mode = Field(default=Qwen3Mode.COMPLETE)\n    prompt: str = Field(..., description=\"Input prompt\")\n    system_message: Optional[str] = Field(None, description=\"System message\")\n    stop_sequences: List[str] = Field(default_factory=list, description=\"Stop sequences\")\n    frequency_penalty: float = Field(default=0.0, ge=-2.0, le=2.0, description=\"Frequency penalty\")\n    presence_penalty: float = Field(default=0.0, ge=-2.0, le=2.0, description=\"Presence penalty\")\n\n\nclass Qwen3EmbedRequest(Qwen3Request):\n    \"\"\"Request model for text embedding.\"\"\"\n    mode: Qwen3Mode = Field(default=Qwen3Mode.EMBED)\n    texts: List[str] = Field(..., description=\"Texts to embed\")\n    normalize: bool = Field(default=True, description=\"Normalize embeddings\")\n    pooling_strategy: str = Field(default=\"mean\", description=\"Pooling strategy\")\n\n\nclass Qwen3VisionRequest(Qwen3Request):\n    \"\"\"Request model for vision-to-text.\"\"\"\n    mode: Qwen3Mode = Field(default=Qwen3Mode.VISION_TO_TEXT)\n    image_data: bytes = Field(..., description=\"Image data\")\n    prompt: Optional[str] = Field(None, description=\"Optional prompt for image\")\n    max_tokens: int = Field(default=1024, ge=1, le=4096, description=\"Maximum tokens for response\")\n\n\nclass Qwen3Response(BaseModel):\n    \"\"\"Base response model for Qwen3 operations.\"\"\"\n    request_id: str = Field(..., description=\"Request ID\")\n    success: bool = Field(..., description=\"Whether request was successful\")\n    mode: Qwen3Mode = Field(..., description=\"Operation mode\")\n    timestamp: datetime = Field(default_factory=datetime.utcnow, description=\"Response timestamp\")\n    processing_time: float = Field(..., description=\"Processing time in seconds\")\n    tokens_used: int = Field(default=0, description=\"Tokens used\")\n    \n    # Error information\n    error_message: Optional[str] = Field(None, description=\"Error message if failed\")\n    error_code: Optional[str] = Field(None, description=\"Error code if failed\")\n\n\nclass Qwen3CompleteResponse(Qwen3Response):\n    \"\"\"Response model for text completion.\"\"\"\n    mode: Qwen3Mode = Field(default=Qwen3Mode.COMPLETE)\n    text: Optional[str] = Field(None, description=\"Generated text\")\n    finish_reason: Optional[str] = Field(None, description=\"Reason for completion\")\n    usage: Dict[str, int] = Field(default_factory=dict, description=\"Token usage information\")\n\n\nclass Qwen3EmbedResponse(Qwen3Response):\n    \"\"\"Response model for text embedding.\"\"\"\n    mode: Qwen3Mode = Field(default=Qwen3Mode.EMBED)\n    embeddings: Optional[List[List[float]]] = Field(None, description=\"Generated embeddings\")\n    embedding_dimension: Optional[int] = Field(None, description=\"Embedding dimension\")\n\n\nclass Qwen3VisionResponse(Qwen3Response):\n    \"\"\"Response model for vision-to-text.\"\"\"\n    mode: Qwen3Mode = Field(default=Qwen3Mode.VISION_TO_TEXT)\n    text: Optional[str] = Field(None, description=\"Extracted text from image\")\n    confidence_score: Optional[float] = Field(None, description=\"Confidence score\")\n\n\n# ============================================================================\n# Rate Limiting and Context Management\n# ============================================================================\n\nclass RateLimiter:\n    \"\"\"Rate limiter for Qwen3 requests.\"\"\"\n    \n    def __init__(self, rate_limits: RateLimits):\n        self.rate_limits = rate_limits\n        self.request_times: List[datetime] = []\n        self.token_counts: List[tuple[datetime, int]] = []\n        self.active_requests = 0\n        self.lock = asyncio.Lock()\n    \n    async def acquire(self, estimated_tokens: int = 100) -> bool:\n        \"\"\"Acquire permission for a request.\"\"\"\n        async with self.lock:\n            now = datetime.utcnow()\n            \n            # Clean old entries\n            self._clean_old_entries(now)\n            \n            # Check concurrent requests\n            if self.active_requests >= self.rate_limits.concurrent_requests:\n                return False\n            \n            # Check requests per minute\n            if len(self.request_times) >= self.rate_limits.requests_per_minute:\n                return False\n            \n            # Check tokens per minute\n            recent_tokens = sum(tokens for _, tokens in self.token_counts)\n            if recent_tokens + estimated_tokens > self.rate_limits.tokens_per_minute:\n                return False\n            \n            # Record request\n            self.request_times.append(now)\n            self.token_counts.append((now, estimated_tokens))\n            self.active_requests += 1\n            \n            return True\n    \n    async def release(self, actual_tokens: int):\n        \"\"\"Release a request.\"\"\"\n        async with self.lock:\n            self.active_requests = max(0, self.active_requests - 1)\n    \n    def _clean_old_entries(self, now: datetime):\n        \"\"\"Clean old entries from tracking lists.\"\"\"\n        cutoff = now - timedelta(minutes=1)\n        self.request_times = [t for t in self.request_times if t > cutoff]\n        self.token_counts = [(t, tokens) for t, tokens in self.token_counts if t > cutoff]\n\n\nclass ContextManager:\n    \"\"\"Context window manager for Qwen3.\"\"\"\n    \n    def __init__(self, context_limits: ContextLimits):\n        self.context_limits = context_limits\n    \n    def validate_context(self, prompt: str, system_message: Optional[str] = None, max_completion: Optional[int] = None) -> bool:\n        \"\"\"Validate context window limits.\"\"\"\n        # Estimate token count (rough approximation: 1 token \u2248 4 characters)\n        prompt_tokens = len(prompt) // 4\n        system_tokens = len(system_message) // 4 if system_message else 0\n        completion_tokens = max_completion or self.context_limits.max_completion_tokens\n        \n        total_tokens = prompt_tokens + system_tokens + completion_tokens + self.context_limits.reserve_tokens\n        \n        return total_tokens <= self.context_limits.max_tokens\n    \n    def truncate_prompt(self, prompt: str, system_message: Optional[str] = None) -> str:\n        \"\"\"Truncate prompt to fit within context limits.\"\"\"\n        system_tokens = len(system_message) // 4 if system_message else 0\n        reserve_tokens = self.context_limits.reserve_tokens + 500  # Buffer for completion\n        \n        available_tokens = self.context_limits.max_prompt_tokens - system_tokens - reserve_tokens\n        available_chars = available_tokens * 4\n        \n        if len(prompt) <= available_chars:\n            return prompt\n        \n        # Truncate and add ellipsis\n        truncated = prompt[:available_chars - 3] + \"...\"\n        return truncated\n\n\n# ============================================================================\n# Local Small Fallback\n# ============================================================================\n\nclass LocalSmallFallback:\n    \"\"\"Local small model fallback for when Qwen3 is unavailable.\"\"\"\n    \n    def __init__(self, model_path: Optional[str] = None):\n        self.model_path = model_path\n        self.model = None\n        self.tokenizer = None\n        self.is_loaded = False\n    \n    async def load_model(self):\n        \"\"\"Load the local small model.\"\"\"\n        if self.model_path and not self.is_loaded:\n            try:\n                # Load a small local model (e.g., DistilBERT, TinyBERT)\n                # This is a placeholder implementation\n                self.is_loaded = True\n            except Exception as e:\n                print(f\"Failed to load fallback model: {e}\")\n    \n    async def complete(self, request: Qwen3CompleteRequest) -> Qwen3CompleteResponse:\n        \"\"\"Provide text completion using local small model.\"\"\"\n        if not self.is_loaded:\n            await self.load_model()\n        \n        # Placeholder implementation\n        return Qwen3CompleteResponse(\n            request_id=request.request_id,\n            success=True,\n            mode=Qwen3Mode.COMPLETE,\n            processing_time=0.1,\n            text=\"[Fallback response - local small model]\",\n            finish_reason=\"length\"\n        )\n    \n    async def embed(self, request: Qwen3EmbedRequest) -> Qwen3EmbedResponse:\n        \"\"\"Provide text embedding using local small model.\"\"\"\n        if not self.is_loaded:\n            await self.load_model()\n        \n        # Placeholder implementation\n        embeddings = [[0.1] * 384 for _ in request.texts]  # 384-dim embeddings\n        \n        return Qwen3EmbedResponse(\n            request_id=request.request_id,\n            success=True,\n            mode=Qwen3Mode.EMBED,\n            processing_time=0.1,\n            embeddings=embeddings,\n            embedding_dimension=384\n        )\n\n\n# ============================================================================\n# Main Qwen3 Provider\n# ============================================================================\n\nclass Qwen3Provider:\n    \"\"\"Qwen3 LLM provider with complete, embed, and vision-to-text capabilities.\"\"\"\n    \n    def __init__(self, config: Qwen3Config):\n        self.config = config\n        self.model = None\n        self.tokenizer = None\n        self.is_loaded = False\n        self.rate_limiter = RateLimiter(config.rate_limits)\n        self.context_manager = ContextManager(config.context_limits)\n        self.fallback = LocalSmallFallback(config.fallback_model_path) if config.enable_fallback else None\n        \n        # Statistics\n        self.total_requests = 0\n        self.successful_requests = 0\n        self.fallback_requests = 0\n        self.total_tokens = 0\n    \n    async def initialize(self):\n        \"\"\"Initialize the Qwen3 model.\"\"\"\n        try:\n            await self._load_model()\n            self.is_loaded = True\n            print(f\"Qwen3 model loaded successfully on {self.config.device}\")\n        except Exception as e:\n            print(f\"Failed to load Qwen3 model: {e}\")\n            if self.fallback:\n                await self.fallback.load_model()\n                print(\"Fallback model loaded\")\n    \n    async def _load_model(self):\n        \"\"\"Load the Qwen3 model and tokenizer.\"\"\"\n        # This is a placeholder implementation\n        # In reality, you would load the actual Qwen3 model here\n        print(f\"Loading Qwen3 model from {self.config.model_path}\")\n        \n        # Simulate model loading\n        await asyncio.sleep(1.0)\n        \n        # Initialize model and tokenizer (placeholder)\n        self.model = \"qwen3_model_placeholder\"\n        self.tokenizer = \"qwen3_tokenizer_placeholder\"\n    \n    async def complete(self, request: Qwen3CompleteRequest) -> Qwen3CompleteResponse:\n        \"\"\"Generate text completion.\"\"\"\n        start_time = time.time()\n        \n        try:\n            # Validate context\n            if not self.context_manager.validate_context(\n                request.prompt, \n                request.system_message, \n                request.max_tokens\n            ):\n                # Truncate prompt if necessary\n                request.prompt = self.context_manager.truncate_prompt(\n                    request.prompt, \n                    request.system_message\n                )\n            \n            # Check rate limits\n            estimated_tokens = len(request.prompt) // 4 + (request.max_tokens or 100)\n            if not await self.rate_limiter.acquire(estimated_tokens):\n                if self.fallback:\n                    self.fallback_requests += 1\n                    return await self.fallback.complete(request)\n                else:\n                    raise Exception(\"Rate limit exceeded and no fallback available\")\n            \n            # Generate completion\n            if not self.is_loaded:\n                if self.fallback:\n                    self.fallback_requests += 1\n                    return await self.fallback.complete(request)\n                else:\n                    raise Exception(\"Model not loaded and no fallback available\")\n            \n            # Placeholder implementation\n            generated_text = f\"[Qwen3 completion for: {request.prompt[:50]}...]\"\n            \n            processing_time = time.time() - start_time\n            tokens_used = estimated_tokens\n            \n            await self.rate_limiter.release(tokens_used)\n            self._update_stats(True, tokens_used)\n            \n            return Qwen3CompleteResponse(\n                request_id=request.request_id,\n                success=True,\n                mode=Qwen3Mode.COMPLETE,\n                processing_time=processing_time,\n                tokens_used=tokens_used,\n                text=generated_text,\n                finish_reason=\"stop\",\n                usage={\n                    \"prompt_tokens\": len(request.prompt) // 4,\n                    \"completion_tokens\": tokens_used - len(request.prompt) // 4,\n                    \"total_tokens\": tokens_used\n                }\n            )\n            \n        except Exception as e:\n            processing_time = time.time() - start_time\n            await self.rate_limiter.release(0)\n            self._update_stats(False, 0)\n            \n            return Qwen3CompleteResponse(\n                request_id=request.request_id,\n                success=False,\n                mode=Qwen3Mode.COMPLETE,\n                processing_time=processing_time,\n                error_message=str(e),\n                error_code=\"COMPLETION_ERROR\"\n            )\n    \n    async def embed(self, request: Qwen3EmbedRequest) -> Qwen3EmbedResponse:\n        \"\"\"Generate text embeddings.\"\"\"\n        start_time = time.time()\n        \n        try:\n            # Check rate limits\n            estimated_tokens = sum(len(text) // 4 for text in request.texts)\n            if not await self.rate_limiter.acquire(estimated_tokens):\n                if self.fallback:\n                    self.fallback_requests += 1\n                    return await self.fallback.embed(request)\n                else:\n                    raise Exception(\"Rate limit exceeded and no fallback available\")\n            \n            # Generate embeddings\n            if not self.is_loaded:\n                if self.fallback:\n                    self.fallback_requests += 1\n                    return await self.fallback.embed(request)\n                else:\n                    raise Exception(\"Model not loaded and no fallback available\")\n            \n            # Placeholder implementation\n            embeddings = []\n            embedding_dim = 1024  # Qwen3 embedding dimension\n            \n            for text in request.texts:\n                # Generate embedding (placeholder)\n                embedding = [0.1] * embedding_dim\n                if request.normalize:\n                    # Normalize embedding\n                    norm = sum(x * x for x in embedding) ** 0.5\n                    embedding = [x / norm for x in embedding]\n                embeddings.append(embedding)\n            \n            processing_time = time.time() - start_time\n            tokens_used = estimated_tokens\n            \n            await self.rate_limiter.release(tokens_used)\n            self._update_stats(True, tokens_used)\n            \n            return Qwen3EmbedResponse(\n                request_id=request.request_id,\n                success=True,\n                mode=Qwen3Mode.EMBED,\n                processing_time=processing_time,\n                tokens_used=tokens_used,\n                embeddings=embeddings,\n                embedding_dimension=embedding_dim\n            )\n            \n        except Exception as e:\n            processing_time = time.time() - start_time\n            await self.rate_limiter.release(0)\n            self._update_stats(False, 0)\n            \n            return Qwen3EmbedResponse(\n                request_id=request.request_id,\n                success=False,\n                mode=Qwen3Mode.EMBED,\n                processing_time=processing_time,\n                error_message=str(e),\n                error_code=\"EMBEDDING_ERROR\"\n            )\n    \n    async def vision_to_text(self, request: Qwen3VisionRequest) -> Qwen3VisionResponse:\n        \"\"\"Convert image to text.\"\"\"\n        start_time = time.time()\n        \n        try:\n            # Check rate limits\n            estimated_tokens = request.max_tokens + 100  # Image processing overhead\n            if not await self.rate_limiter.acquire(estimated_tokens):\n                raise Exception(\"Rate limit exceeded for vision processing\")\n            \n            # Process image\n            if not self.is_loaded:\n                raise Exception(\"Model not loaded for vision processing\")\n            \n            # Placeholder implementation\n            extracted_text = f\"[Vision-to-text result for image of size {len(request.image_data)} bytes]\"\n            confidence_score = 0.85\n            \n            processing_time = time.time() - start_time\n            tokens_used = estimated_tokens\n            \n            await self.rate_limiter.release(tokens_used)\n            self._update_stats(True, tokens_used)\n            \n            return Qwen3VisionResponse(\n                request_id=request.request_id,\n                success=True,\n                mode=Qwen3Mode.VISION_TO_TEXT,\n                processing_time=processing_time,\n                tokens_used=tokens_used,\n                text=extracted_text,\n                confidence_score=confidence_score\n            )\n            \n        except Exception as e:\n            processing_time = time.time() - start_time\n            await self.rate_limiter.release(0)\n            self._update_stats(False, 0)\n            \n            return Qwen3VisionResponse(\n                request_id=request.request_id,\n                success=False,\n                mode=Qwen3Mode.VISION_TO_TEXT,\n                processing_time=processing_time,\n                error_message=str(e),\n                error_code=\"VISION_ERROR\"\n            )\n    \n    async def complete_stream(self, request: Qwen3CompleteRequest) -> AsyncGenerator[str, None]:\n        \"\"\"Generate streaming text completion.\"\"\"\n        try:\n            # For now, return a single chunk (placeholder for streaming)\n            response = await self.complete(request)\n            if response.success and response.text:\n                yield response.text\n            else:\n                yield f\"[Error: {response.error_message}]\"\n        except Exception as e:\n            yield f\"[Streaming Error: {str(e)}]\"\n    \n    def _update_stats(self, success: bool, tokens: int):\n        \"\"\"Update provider statistics.\"\"\"\n        self.total_requests += 1\n        if success:\n            self.successful_requests += 1\n        self.total_tokens += tokens\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get provider statistics.\"\"\"\n        success_rate = (self.successful_requests / self.total_requests * 100) if self.total_requests > 0 else 0\n        \n        return {\n            \"total_requests\": self.total_requests,\n            \"successful_requests\": self.successful_requests,\n            \"fallback_requests\": self.fallback_requests,\n            \"success_rate\": success_rate,\n            \"total_tokens\": self.total_tokens,\n            \"is_loaded\": self.is_loaded,\n            \"fallback_available\": self.fallback is not None\n        }\n    \n    async def health_check(self) -> Dict[str, Any]:\n        \"\"\"Perform health check.\"\"\"\n        return {\n            \"status\": \"healthy\" if self.is_loaded else \"degraded\",\n            \"model_loaded\": self.is_loaded,\n            \"fallback_available\": self.fallback is not None and self.fallback.is_loaded,\n            \"rate_limiter_active\": self.rate_limiter.active_requests < self.config.rate_limits.concurrent_requests,\n            \"memory_usage\": \"placeholder\",  # Would implement actual memory monitoring\n            \"timestamp\": datetime.utcnow().isoformat()\n        }\n\n\n# ============================================================================\n# Factory Functions\n# ============================================================================\n\ndef create_qwen3_provider(\n    model_path: str,\n    device: Qwen3Device = Qwen3Device.MPS,\n    enable_fallback: bool = True,\n    **kwargs\n) -> Qwen3Provider:\n    \"\"\"Create a Qwen3 provider with default configuration.\"\"\"\n    config = Qwen3Config(\n        model_path=model_path,\n        device=device,\n        enable_fallback=enable_fallback,\n        **kwargs\n    )\n    return Qwen3Provider(config)\n\n\ndef create_qwen3_provider_for_apple_silicon(\n    model_path: str,\n    enable_fallback: bool = True\n) -> Qwen3Provider:\n    \"\"\"Create a Qwen3 provider optimized for Apple Silicon.\"\"\"\n    config = Qwen3Config(\n        model_path=model_path,\n        device=Qwen3Device.MPS,\n        precision=Qwen3Precision.FLOAT16,\n        use_mps_acceleration=True,\n        optimize_for_inference=True,\n        enable_fallback=enable_fallback\n    )\n    return Qwen3Provider(config)\n\n\n# ============================================================================\n# Export all classes and functions\n# ============================================================================\n\n__all__ = [\n    # Enums\n    \"Qwen3Mode\",\n    \"Qwen3Device\", \n    \"Qwen3Precision\",\n    \n    # Configuration models\n    \"ContextLimits\",\n    \"RateLimits\",\n    \"Qwen3Config\",\n    \n    # Request models\n    \"Qwen3Request\",\n    \"Qwen3CompleteRequest\",\n    \"Qwen3EmbedRequest\",\n    \"Qwen3VisionRequest\",\n    \n    # Response models\n    \"Qwen3Response\",\n    \"Qwen3CompleteResponse\",\n    \"Qwen3EmbedResponse\",\n    \"Qwen3VisionResponse\",\n    \n    # Core classes\n    \"RateLimiter\",\n    \"ContextManager\",\n    \"LocalSmallFallback\",\n    \"Qwen3Provider\",\n    \n    # Factory functions\n    \"create_qwen3_provider\",\n    \"create_qwen3_provider_for_apple_silicon\",\n]\n", "source": "src/core/providers/llm_qwen3.py"}, {"instruction": "Explain the following code from src/core/runtime/planner.py:", "response": "This code is from src/core/runtime/planner.py:\n\n\"\"\"\nAgent Planner for Agentic LLM Core v0.1\n\nThis module provides an intelligent agent planner that takes specifications, features,\nand user intent to generate optimized TaskGraphs with budget and latency policies.\n\nCreated: 2024-09-24\nStatus: Draft\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime, timezone, timedelta\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Union, Tuple, Set\nfrom uuid import UUID\n\nfrom pydantic import BaseModel, Field, field_validator\n\nfrom ..models.contracts import Task, TaskGraph, TaskStatus, ToolSpec\n\n\n# ============================================================================\n# Planning Input Models\n# ============================================================================\n\nclass PlanningInput(BaseModel):\n    \"\"\"Input for the agent planner.\"\"\"\n    specifications: Dict[str, Any] = Field(..., description=\"System specifications and requirements\")\n    features: List[str] = Field(..., description=\"Available features and capabilities\")\n    user_intent: str = Field(..., description=\"User's intent and goals\")\n    context: Optional[Dict[str, Any]] = Field(None, description=\"Additional context information\")\n    constraints: Optional[Dict[str, Any]] = Field(None, description=\"Planning constraints\")\n    \n    @field_validator('features')\n    @classmethod\n    def validate_features(cls, v):\n        if not v:\n            raise ValueError(\"Features list cannot be empty\")\n        return v\n    \n    @field_validator('user_intent')\n    @classmethod\n    def validate_user_intent(cls, v):\n        if not v or len(v.strip()) == 0:\n            raise ValueError(\"User intent cannot be empty\")\n        return v.strip()\n\n\nclass PlanningPolicy(BaseModel):\n    \"\"\"Planning policy configuration.\"\"\"\n    budget_limit: Optional[float] = Field(None, ge=0, description=\"Maximum budget in currency units\")\n    latency_limit: Optional[float] = Field(None, ge=0, description=\"Maximum latency in seconds\")\n    priority: str = Field(default=\"balanced\", description=\"Planning priority: speed, quality, balanced\")\n    optimization_target: str = Field(default=\"efficiency\", description=\"Optimization target: cost, speed, quality, efficiency\")\n    parallel_execution: bool = Field(default=True, description=\"Allow parallel task execution\")\n    resource_constraints: Dict[str, Any] = Field(default_factory=dict, description=\"Resource constraints\")\n    \n    @field_validator('priority')\n    @classmethod\n    def validate_priority(cls, v):\n        allowed_priorities = ['speed', 'quality', 'balanced']\n        if v not in allowed_priorities:\n            raise ValueError(f\"Priority must be one of: {allowed_priorities}\")\n        return v\n    \n    @field_validator('optimization_target')\n    @classmethod\n    def validate_optimization_target(cls, v):\n        allowed_targets = ['cost', 'speed', 'quality', 'efficiency']\n        if v not in allowed_targets:\n            raise ValueError(f\"Optimization target must be one of: {allowed_targets}\")\n        return v\n\n\nclass PlanningContext(BaseModel):\n    \"\"\"Context for planning decisions.\"\"\"\n    available_tools: List[ToolSpec] = Field(default_factory=list, description=\"Available tools\")\n    system_capabilities: Dict[str, Any] = Field(default_factory=dict, description=\"System capabilities\")\n    historical_performance: Dict[str, float] = Field(default_factory=dict, description=\"Historical performance metrics\")\n    current_load: Dict[str, float] = Field(default_factory=dict, description=\"Current system load\")\n    user_preferences: Dict[str, Any] = Field(default_factory=dict, description=\"User preferences\")\n    \n    @field_validator('historical_performance')\n    @classmethod\n    def validate_performance_metrics(cls, v):\n        for key, value in v.items():\n            if not isinstance(value, (int, float)) or value < 0:\n                raise ValueError(f\"Performance metric {key} must be a non-negative number\")\n        return v\n\n\n# ============================================================================\n# Planning Output Models\n# ============================================================================\n\nclass PlanningResult(BaseModel):\n    \"\"\"Result of the planning process.\"\"\"\n    task_graph: TaskGraph = Field(..., description=\"Generated task graph\")\n    planning_metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Planning metadata\")\n    estimated_cost: float = Field(default=0.0, ge=0, description=\"Estimated total cost\")\n    estimated_latency: float = Field(default=0.0, ge=0, description=\"Estimated total latency\")\n    confidence_score: float = Field(default=0.0, ge=0.0, le=1.0, description=\"Planning confidence\")\n    alternatives: List[TaskGraph] = Field(default_factory=list, description=\"Alternative plans\")\n    reasoning: str = Field(default=\"\", description=\"Planning reasoning and rationale\")\n    \n    @field_validator('confidence_score')\n    @classmethod\n    def validate_confidence(cls, v):\n        if not 0.0 <= v <= 1.0:\n            raise ValueError(\"Confidence score must be between 0.0 and 1.0\")\n        return v\n\n\nclass TaskEstimate(BaseModel):\n    \"\"\"Estimate for a single task.\"\"\"\n    task_id: str = Field(..., description=\"Task ID\")\n    estimated_cost: float = Field(default=0.0, ge=0, description=\"Estimated cost\")\n    estimated_latency: float = Field(default=0.0, ge=0, description=\"Estimated latency\")\n    resource_requirements: Dict[str, Any] = Field(default_factory=dict, description=\"Resource requirements\")\n    dependencies: List[str] = Field(default_factory=list, description=\"Task dependencies\")\n    priority: int = Field(default=1, ge=1, le=10, description=\"Task priority (1-10)\")\n    risk_level: str = Field(default=\"low\", description=\"Risk level: low, medium, high\")\n    \n    @field_validator('risk_level')\n    @classmethod\n    def validate_risk_level(cls, v):\n        allowed_levels = ['low', 'medium', 'high']\n        if v not in allowed_levels:\n            raise ValueError(f\"Risk level must be one of: {allowed_levels}\")\n        return v\n\n\n# ============================================================================\n# Planning Strategies\n# ============================================================================\n\nclass PlanningStrategy(ABC):\n    \"\"\"Abstract base class for planning strategies.\"\"\"\n    \n    @abstractmethod\n    async def plan(self, input_data: PlanningInput, policy: PlanningPolicy, context: PlanningContext) -> PlanningResult:\n        \"\"\"Generate a plan based on input, policy, and context.\"\"\"\n        pass\n    \n    @abstractmethod\n    def get_strategy_name(self) -> str:\n        \"\"\"Get the name of this planning strategy.\"\"\"\n        pass\n\n\nclass SequentialPlanningStrategy(PlanningStrategy):\n    \"\"\"Sequential planning strategy - tasks executed one after another.\"\"\"\n    \n    def get_strategy_name(self) -> str:\n        return \"sequential\"\n    \n    async def plan(self, input_data: PlanningInput, policy: PlanningPolicy, context: PlanningContext) -> PlanningResult:\n        \"\"\"Generate a sequential plan.\"\"\"\n        tasks = await self._generate_tasks(input_data, context)\n        \n        # Create sequential dependencies\n        for i in range(1, len(tasks)):\n            tasks[i].dependencies = [tasks[i-1].task_id]\n        \n        # Create task graph\n        task_graph = TaskGraph(\n            graph_id=str(uuid.uuid4()),\n            name=f\"Sequential Plan: {input_data.user_intent[:50]}\",\n            description=f\"Sequential execution plan for: {input_data.user_intent}\",\n            tasks=tasks,\n            entry_points=[tasks[0].task_id] if tasks else [],\n            exit_points=[tasks[-1].task_id] if tasks else []\n        )\n        \n        # Calculate estimates\n        estimates = await self._calculate_estimates(tasks, context)\n        total_cost = sum(est.estimated_cost for est in estimates)\n        total_latency = sum(est.estimated_latency for est in estimates)\n        \n        return PlanningResult(\n            task_graph=task_graph,\n            estimated_cost=total_cost,\n            estimated_latency=total_latency,\n            confidence_score=0.8,\n            reasoning=\"Sequential execution ensures proper dependency handling but may be slower\"\n        )\n    \n    async def _generate_tasks(self, input_data: PlanningInput, context: PlanningContext) -> List[Task]:\n        \"\"\"Generate tasks based on input and context.\"\"\"\n        tasks = []\n        \n        # Analyze user intent to determine required tasks\n        intent_tasks = await self._analyze_intent(input_data.user_intent, input_data.features)\n        \n        for i, task_info in enumerate(intent_tasks):\n            task = Task(\n                task_id=f\"task_{i+1}_{uuid.uuid4().hex[:8]}\",\n                name=task_info[\"name\"],\n                description=task_info[\"description\"],\n                input_data=task_info.get(\"input_data\", {}),\n                required_tools=task_info.get(\"required_tools\", []),\n                metadata=task_info.get(\"metadata\", {})\n            )\n            tasks.append(task)\n        \n        return tasks\n    \n    async def _analyze_intent(self, user_intent: str, features: List[str]) -> List[Dict[str, Any]]:\n        \"\"\"Analyze user intent to determine required tasks.\"\"\"\n        # This is a simplified intent analysis\n        # In a real implementation, this would use NLP/AI to understand intent\n        \n        intent_lower = user_intent.lower()\n        tasks = []\n        \n        if \"search\" in intent_lower or \"find\" in intent_lower:\n            tasks.append({\n                \"name\": \"Search Information\",\n                \"description\": \"Search for relevant information based on user query\",\n                \"required_tools\": [\"search_tool\", \"knowledge_base\"],\n                \"input_data\": {\"query\": user_intent},\n                \"metadata\": {\"type\": \"search\", \"priority\": 1}\n            })\n        \n        if \"analyze\" in intent_lower or \"process\" in intent_lower:\n            tasks.append({\n                \"name\": \"Analyze Data\",\n                \"description\": \"Analyze and process data according to user requirements\",\n                \"required_tools\": [\"analysis_tool\", \"data_processor\"],\n                \"input_data\": {\"analysis_type\": \"general\"},\n                \"metadata\": {\"type\": \"analysis\", \"priority\": 2}\n            })\n        \n        if \"generate\" in intent_lower or \"create\" in intent_lower:\n            tasks.append({\n                \"name\": \"Generate Content\",\n                \"description\": \"Generate content based on user specifications\",\n                \"required_tools\": [\"generation_tool\", \"llm_provider\"],\n                \"input_data\": {\"content_type\": \"general\"},\n                \"metadata\": {\"type\": \"generation\", \"priority\": 3}\n            })\n        \n        if \"summarize\" in intent_lower or \"summarise\" in intent_lower:\n            tasks.append({\n                \"name\": \"Summarize Information\",\n                \"description\": \"Create a summary of relevant information\",\n                \"required_tools\": [\"summarization_tool\", \"llm_provider\"],\n                \"input_data\": {\"summary_type\": \"general\"},\n                \"metadata\": {\"type\": \"summarization\", \"priority\": 4}\n            })\n        \n        # Default task if no specific intent detected\n        if not tasks:\n            tasks.append({\n                \"name\": \"Process Request\",\n                \"description\": \"Process user request using available capabilities\",\n                \"required_tools\": [\"general_processor\"],\n                \"input_data\": {\"request\": user_intent},\n                \"metadata\": {\"type\": \"general\", \"priority\": 1}\n            })\n        \n        return tasks\n    \n    async def _calculate_estimates(self, tasks: List[Task], context: PlanningContext) -> List[TaskEstimate]:\n        \"\"\"Calculate estimates for tasks.\"\"\"\n        estimates = []\n        \n        for task in tasks:\n            # Base estimates\n            base_cost = 0.1  # Base cost per task\n            base_latency = 1.0  # Base latency per task\n            \n            # Adjust based on task complexity\n            complexity_multiplier = len(task.required_tools) * 0.2 + 1.0\n            \n            # Adjust based on historical performance\n            historical_adjustment = context.historical_performance.get(task.name, 1.0)\n            \n            estimate = TaskEstimate(\n                task_id=task.task_id,\n                estimated_cost=base_cost * complexity_multiplier,\n                estimated_latency=base_latency * complexity_multiplier * historical_adjustment,\n                resource_requirements={\"cpu\": 1.0, \"memory\": 0.5},\n                priority=task.metadata.get(\"priority\", 1),\n                risk_level=\"medium\" if complexity_multiplier > 1.5 else \"low\"\n            )\n            estimates.append(estimate)\n        \n        return estimates\n\n\nclass ParallelPlanningStrategy(PlanningStrategy):\n    \"\"\"Parallel planning strategy - tasks executed in parallel where possible.\"\"\"\n    \n    def get_strategy_name(self) -> str:\n        return \"parallel\"\n    \n    async def plan(self, input_data: PlanningInput, policy: PlanningPolicy, context: PlanningContext) -> PlanningResult:\n        \"\"\"Generate a parallel plan.\"\"\"\n        tasks = await self._generate_tasks(input_data, context)\n        \n        # Analyze dependencies and create parallel groups\n        task_groups = await self._create_parallel_groups(tasks, context)\n        \n        # Create dependencies between groups\n        all_tasks = []\n        entry_points = []\n        exit_points = []\n        \n        for i, group in enumerate(task_groups):\n            for task in group:\n                all_tasks.append(task)\n            \n            # First group has no dependencies\n            if i == 0:\n                entry_points.extend([task.task_id for task in group])\n            else:\n                # Depend on previous group\n                prev_group_ids = [task.task_id for task in task_groups[i-1]]\n                for task in group:\n                    task.dependencies = prev_group_ids\n            \n            # Last group has no dependents\n            if i == len(task_groups) - 1:\n                exit_points.extend([task.task_id for task in group])\n        \n        # Create task graph\n        task_graph = TaskGraph(\n            graph_id=str(uuid.uuid4()),\n            name=f\"Parallel Plan: {input_data.user_intent[:50]}\",\n            description=f\"Parallel execution plan for: {input_data.user_intent}\",\n            tasks=all_tasks,\n            entry_points=entry_points,\n            exit_points=exit_points\n        )\n        \n        # Calculate estimates\n        estimates = await self._calculate_estimates(all_tasks, context)\n        total_cost = sum(est.estimated_cost for est in estimates)\n        \n        # Parallel latency is the maximum latency in each group\n        group_latencies = []\n        for group in task_groups:\n            group_estimates = [est for est in estimates if est.task_id in [t.task_id for t in group]]\n            group_latency = max(est.estimated_latency for est in group_estimates) if group_estimates else 0\n            group_latencies.append(group_latency)\n        \n        total_latency = sum(group_latencies)\n        \n        return PlanningResult(\n            task_graph=task_graph,\n            estimated_cost=total_cost,\n            estimated_latency=total_latency,\n            confidence_score=0.7,\n            reasoning=\"Parallel execution maximizes efficiency but requires careful dependency management\"\n        )\n    \n    async def _generate_tasks(self, input_data: PlanningInput, context: PlanningContext) -> List[Task]:\n        \"\"\"Generate tasks based on input and context.\"\"\"\n        # Similar to sequential strategy but with parallel considerations\n        tasks = []\n        \n        intent_tasks = await self._analyze_intent(input_data.user_intent, input_data.features)\n        \n        for i, task_info in enumerate(intent_tasks):\n            task = Task(\n                task_id=f\"task_{i+1}_{uuid.uuid4().hex[:8]}\",\n                name=task_info[\"name\"],\n                description=task_info[\"description\"],\n                input_data=task_info.get(\"input_data\", {}),\n                required_tools=task_info.get(\"required_tools\", []),\n                metadata=task_info.get(\"metadata\", {})\n            )\n            tasks.append(task)\n        \n        return tasks\n    \n    async def _analyze_intent(self, user_intent: str, features: List[str]) -> List[Dict[str, Any]]:\n        \"\"\"Analyze user intent to determine required tasks.\"\"\"\n        # Similar to sequential strategy\n        return await SequentialPlanningStrategy()._analyze_intent(user_intent, features)\n    \n    async def _create_parallel_groups(self, tasks: List[Task], context: PlanningContext) -> List[List[Task]]:\n        \"\"\"Create groups of tasks that can run in parallel.\"\"\"\n        groups = []\n        current_group = []\n        \n        for task in tasks:\n            # Check if task can run in parallel with current group\n            if self._can_run_parallel(task, current_group, context):\n                current_group.append(task)\n            else:\n                # Start new group\n                if current_group:\n                    groups.append(current_group)\n                current_group = [task]\n        \n        if current_group:\n            groups.append(current_group)\n        \n        return groups\n    \n    def _can_run_parallel(self, task: Task, group: List[Task], context: PlanningContext) -> bool:\n        \"\"\"Check if a task can run in parallel with a group of tasks.\"\"\"\n        if not group:\n            return True\n        \n        # Check for resource conflicts\n        task_resources = self._get_task_resources(task)\n        for group_task in group:\n            group_resources = self._get_task_resources(group_task)\n            if self._has_resource_conflict(task_resources, group_resources):\n                return False\n        \n        # Check for tool conflicts\n        task_tools = set(task.required_tools)\n        for group_task in group:\n            group_tools = set(group_task.required_tools)\n            if task_tools & group_tools:  # Intersection\n                return False\n        \n        return True\n    \n    def _get_task_resources(self, task: Task) -> Dict[str, Any]:\n        \"\"\"Get resource requirements for a task.\"\"\"\n        return task.metadata.get(\"resource_requirements\", {\"cpu\": 1.0, \"memory\": 0.5})\n    \n    def _has_resource_conflict(self, resources1: Dict[str, Any], resources2: Dict[str, Any]) -> bool:\n        \"\"\"Check if two resource requirements conflict.\"\"\"\n        # Simple conflict detection - in practice, this would be more sophisticated\n        total_cpu = resources1.get(\"cpu\", 0) + resources2.get(\"cpu\", 0)\n        total_memory = resources1.get(\"memory\", 0) + resources2.get(\"memory\", 0)\n        \n        return total_cpu > 4.0 or total_memory > 2.0  # Arbitrary limits\n    \n    async def _calculate_estimates(self, tasks: List[Task], context: PlanningContext) -> List[TaskEstimate]:\n        \"\"\"Calculate estimates for tasks.\"\"\"\n        return await SequentialPlanningStrategy()._calculate_estimates(tasks, context)\n\n\nclass OptimizedPlanningStrategy(PlanningStrategy):\n    \"\"\"Optimized planning strategy - balances cost, speed, and quality.\"\"\"\n    \n    def get_strategy_name(self) -> str:\n        return \"optimized\"\n    \n    async def plan(self, input_data: PlanningInput, policy: PlanningPolicy, context: PlanningContext) -> PlanningResult:\n        \"\"\"Generate an optimized plan.\"\"\"\n        # Generate multiple candidate plans\n        sequential_plan = await SequentialPlanningStrategy().plan(input_data, policy, context)\n        parallel_plan = await ParallelPlanningStrategy().plan(input_data, policy, context)\n        \n        # Evaluate plans against policy\n        candidates = [\n            (sequential_plan, \"sequential\"),\n            (parallel_plan, \"parallel\")\n        ]\n        \n        # Select best plan based on policy\n        best_plan = await self._select_best_plan(candidates, policy)\n        \n        # Optimize the selected plan\n        optimized_plan = await self._optimize_plan(best_plan, policy, context)\n        \n        return optimized_plan\n    \n    async def _select_best_plan(self, candidates: List[Tuple[PlanningResult, str]], policy: PlanningPolicy) -> PlanningResult:\n        \"\"\"Select the best plan based on policy.\"\"\"\n        best_score = -1\n        best_plan = None\n        \n        for plan, strategy_name in candidates:\n            score = await self._evaluate_plan(plan, policy)\n            if score > best_score:\n                best_score = score\n                best_plan = plan\n        \n        return best_plan or candidates[0][0]\n    \n    async def _evaluate_plan(self, plan: PlanningResult, policy: PlanningPolicy) -> float:\n        \"\"\"Evaluate a plan against the policy.\"\"\"\n        score = 0.0\n        \n        # Cost evaluation\n        if policy.budget_limit:\n            cost_ratio = plan.estimated_cost / policy.budget_limit\n            score += (1.0 - min(cost_ratio, 1.0)) * 0.4\n        \n        # Latency evaluation\n        if policy.latency_limit:\n            latency_ratio = plan.estimated_latency / policy.latency_limit\n            score += (1.0 - min(latency_ratio, 1.0)) * 0.4\n        \n        # Confidence evaluation\n        score += plan.confidence_score * 0.2\n        \n        return score\n    \n    async def _optimize_plan(self, plan: PlanningResult, policy: PlanningPolicy, context: PlanningContext) -> PlanningResult:\n        \"\"\"Optimize a plan based on policy.\"\"\"\n        # Apply optimizations based on policy\n        if policy.optimization_target == \"cost\":\n            plan = await self._optimize_for_cost(plan, context)\n        elif policy.optimization_target == \"speed\":\n            plan = await self._optimize_for_speed(plan, context)\n        elif policy.optimization_target == \"quality\":\n            plan = await self._optimize_for_quality(plan, context)\n        else:  # efficiency\n            plan = await self._optimize_for_efficiency(plan, context)\n        \n        return plan\n    \n    async def _optimize_for_cost(self, plan: PlanningResult, context: PlanningContext) -> PlanningResult:\n        \"\"\"Optimize plan for cost.\"\"\"\n        # Reduce task complexity where possible\n        optimized_tasks = []\n        for task in plan.task_graph.tasks:\n            if len(task.required_tools) > 1:\n                # Try to reduce tool requirements\n                simplified_task = Task(\n                    task_id=task.task_id,\n                    name=task.name,\n                    description=task.description,\n                    input_data=task.input_data,\n                    required_tools=task.required_tools[:1],  # Use only first tool\n                    metadata=task.metadata\n                )\n                optimized_tasks.append(simplified_task)\n            else:\n                optimized_tasks.append(task)\n        \n        # Update task graph\n        plan.task_graph.tasks = optimized_tasks\n        plan.estimated_cost *= 0.8  # Assume 20% cost reduction\n        plan.reasoning += \" Optimized for cost reduction.\"\n        \n        return plan\n    \n    async def _optimize_for_speed(self, plan: PlanningResult, context: PlanningContext) -> PlanningResult:\n        \"\"\"Optimize plan for speed.\"\"\"\n        # Increase parallelism where possible\n        plan.estimated_latency *= 0.7  # Assume 30% speed improvement\n        plan.reasoning += \" Optimized for speed improvement.\"\n        \n        return plan\n    \n    async def _optimize_for_quality(self, plan: PlanningResult, context: PlanningContext) -> PlanningResult:\n        \"\"\"Optimize plan for quality.\"\"\"\n        # Add quality assurance tasks\n        plan.confidence_score = min(plan.confidence_score + 0.1, 1.0)\n        plan.reasoning += \" Optimized for quality assurance.\"\n        \n        return plan\n    \n    async def _optimize_for_efficiency(self, plan: PlanningResult, context: PlanningContext) -> PlanningResult:\n        \"\"\"Optimize plan for efficiency.\"\"\"\n        # Balance cost and speed\n        plan.estimated_cost *= 0.9\n        plan.estimated_latency *= 0.8\n        plan.reasoning += \" Optimized for efficiency.\"\n        \n        return plan\n\n\n# ============================================================================\n# Main Agent Planner\n# ============================================================================\n\nclass AgentPlanner:\n    \"\"\"Main agent planner that orchestrates planning strategies.\"\"\"\n    \n    def __init__(self):\n        self.strategies: Dict[str, PlanningStrategy] = {\n            \"sequential\": SequentialPlanningStrategy(),\n            \"parallel\": ParallelPlanningStrategy(),\n            \"optimized\": OptimizedPlanningStrategy()\n        }\n        self.logger = logging.getLogger(__name__)\n        \n        # Statistics\n        self.total_plans_generated = 0\n        self.successful_plans = 0\n        self.failed_plans = 0\n    \n    async def plan(\n        self,\n        input_data: PlanningInput,\n        policy: PlanningPolicy,\n        context: PlanningContext,\n        strategy: Optional[str] = None\n    ) -> PlanningResult:\n        \"\"\"Generate a plan based on input, policy, and context.\"\"\"\n        try:\n            # Select strategy\n            if strategy is None:\n                strategy = self._select_strategy(policy)\n            \n            if strategy not in self.strategies:\n                raise ValueError(f\"Unknown strategy: {strategy}\")\n            \n            # Generate plan\n            self.logger.info(f\"Generating plan using {strategy} strategy\")\n            result = await self.strategies[strategy].plan(input_data, policy, context)\n            \n            # Update statistics\n            self.total_plans_generated += 1\n            self.successful_plans += 1\n            \n            # Validate against policy constraints\n            result = await self._validate_against_policy(result, policy)\n            \n            self.logger.info(f\"Plan generated successfully. Cost: {result.estimated_cost}, Latency: {result.estimated_latency}\")\n            \n            return result\n            \n        except Exception as e:\n            self.failed_plans += 1\n            self.logger.error(f\"Planning failed: {e}\")\n            raise\n    \n    def _select_strategy(self, policy: PlanningPolicy) -> str:\n        \"\"\"Select the best strategy based on policy.\"\"\"\n        if policy.priority == \"speed\":\n            return \"parallel\"\n        elif policy.priority == \"quality\":\n            return \"optimized\"\n        else:  # balanced\n            return \"optimized\"\n    \n    async def _validate_against_policy(self, result: PlanningResult, policy: PlanningPolicy) -> PlanningResult:\n        \"\"\"Validate plan against policy constraints.\"\"\"\n        # Check budget constraint\n        if policy.budget_limit and result.estimated_cost > policy.budget_limit:\n            self.logger.warning(f\"Plan exceeds budget limit: {result.estimated_cost} > {policy.budget_limit}\")\n            # Could implement cost reduction here\n        \n        # Check latency constraint\n        if policy.latency_limit and result.estimated_latency > policy.latency_limit:\n            self.logger.warning(f\"Plan exceeds latency limit: {result.estimated_latency} > {policy.latency_limit}\")\n            # Could implement latency reduction here\n        \n        return result\n    \n    async def generate_alternatives(\n        self,\n        input_data: PlanningInput,\n        policy: PlanningPolicy,\n        context: PlanningContext,\n        num_alternatives: int = 3\n    ) -> List[PlanningResult]:\n        \"\"\"Generate alternative plans.\"\"\"\n        alternatives = []\n        \n        for strategy_name in self.strategies.keys():\n            try:\n                result = await self.strategies[strategy_name].plan(input_data, policy, context)\n                alternatives.append(result)\n                \n                if len(alternatives) >= num_alternatives:\n                    break\n                    \n            except Exception as e:\n                self.logger.warning(f\"Failed to generate alternative with {strategy_name}: {e}\")\n                continue\n        \n        return alternatives\n    \n    def get_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get planner statistics.\"\"\"\n        success_rate = (self.successful_plans / self.total_plans_generated * 100) if self.total_plans_generated > 0 else 0\n        \n        return {\n            \"total_plans_generated\": self.total_plans_generated,\n            \"successful_plans\": self.successful_plans,\n            \"failed_plans\": self.failed_plans,\n            \"success_rate\": success_rate,\n            \"available_strategies\": list(self.strategies.keys())\n        }\n    \n    def register_strategy(self, name: str, strategy: PlanningStrategy) -> None:\n        \"\"\"Register a custom planning strategy.\"\"\"\n        self.strategies[name] = strategy\n        self.logger.info(f\"Registered custom strategy: {name}\")\n\n\n# ============================================================================\n# Factory Functions\n# ============================================================================\n\ndef create_agent_planner() -> AgentPlanner:\n    \"\"\"Create an agent planner with default strategies.\"\"\"\n    return AgentPlanner()\n\n\ndef create_planning_input(\n    specifications: Dict[str, Any],\n    features: List[str],\n    user_intent: str,\n    context: Optional[Dict[str, Any]] = None,\n    constraints: Optional[Dict[str, Any]] = None\n) -> PlanningInput:\n    \"\"\"Create a planning input with the given parameters.\"\"\"\n    return PlanningInput(\n        specifications=specifications,\n        features=features,\n        user_intent=user_intent,\n        context=context,\n        constraints=constraints\n    )\n\n\ndef create_planning_policy(\n    budget_limit: Optional[float] = None,\n    latency_limit: Optional[float] = None,\n    priority: str = \"balanced\",\n    optimization_target: str = \"efficiency\",\n    parallel_execution: bool = True\n) -> PlanningPolicy:\n    \"\"\"Create a planning policy with the given parameters.\"\"\"\n    return PlanningPolicy(\n        budget_limit=budget_limit,\n        latency_limit=latency_limit,\n        priority=priority,\n        optimization_target=optimization_target,\n        parallel_execution=parallel_execution\n    )\n\n\ndef create_planning_context(\n    available_tools: Optional[List[ToolSpec]] = None,\n    system_capabilities: Optional[Dict[str, Any]] = None,\n    historical_performance: Optional[Dict[str, float]] = None,\n    current_load: Optional[Dict[str, float]] = None,\n    user_preferences: Optional[Dict[str, Any]] = None\n) -> PlanningContext:\n    \"\"\"Create a planning context with the given parameters.\"\"\"\n    return PlanningContext(\n        available_tools=available_tools or [],\n        system_capabilities=system_capabilities or {},\n        historical_performance=historical_performance or {},\n        current_load=current_load or {},\n        user_preferences=user_preferences or {}\n    )\n\n\n# ============================================================================\n# Export all classes and functions\n# ============================================================================\n\n__all__ = [\n    # Input models\n    \"PlanningInput\",\n    \"PlanningPolicy\",\n    \"PlanningContext\",\n    \n    # Output models\n    \"PlanningResult\",\n    \"TaskEstimate\",\n    \n    # Strategies\n    \"PlanningStrategy\",\n    \"SequentialPlanningStrategy\",\n    \"ParallelPlanningStrategy\",\n    \"OptimizedPlanningStrategy\",\n    \n    # Main planner\n    \"AgentPlanner\",\n    \n    # Factory functions\n    \"create_agent_planner\",\n    \"create_planning_input\",\n    \"create_planning_policy\",\n    \"create_planning_context\",\n]\n", "source": "src/core/runtime/planner.py"}, {"instruction": "Explain the following code from src/core/runtime/runner.py:", "response": "This code is from src/core/runtime/runner.py:\n\n\"\"\"\nTask Runner for Agentic LLM Core v0.1\n\nThis module provides a comprehensive task runner that executes TaskGraphs with\ntool selection, retry mechanisms, comprehensive logging, and evidence capture.\n\nCreated: 2024-09-24\nStatus: Draft\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport time\nimport traceback\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime, timezone, timedelta\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Union, Callable, Awaitable\nfrom uuid import uuid4\n\nfrom pydantic import BaseModel, Field, field_validator\n\nfrom ..models.contracts import Task, TaskGraph, TaskResult, TaskStatus, ToolSpec\n\n\n# ============================================================================\n# Runner Models\n# ============================================================================\n\nclass ExecutionStatus(str, Enum):\n    \"\"\"Execution status enumeration.\"\"\"\n    PENDING = \"pending\"\n    RUNNING = \"running\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    CANCELLED = \"cancelled\"\n    RETRYING = \"retrying\"\n\n\nclass RetryStrategy(str, Enum):\n    \"\"\"Retry strategy enumeration.\"\"\"\n    IMMEDIATE = \"immediate\"\n    EXPONENTIAL_BACKOFF = \"exponential_backoff\"\n    LINEAR_BACKOFF = \"linear_backoff\"\n    FIXED_DELAY = \"fixed_delay\"\n\n\nclass LogLevel(str, Enum):\n    \"\"\"Log level enumeration.\"\"\"\n    DEBUG = \"DEBUG\"\n    INFO = \"INFO\"\n    WARNING = \"WARNING\"\n    ERROR = \"ERROR\"\n    CRITICAL = \"CRITICAL\"\n\n\nclass ExecutionContext(BaseModel):\n    \"\"\"Context for task execution.\"\"\"\n    execution_id: str = Field(default_factory=lambda: str(uuid4()), description=\"Unique execution ID\")\n    task_graph_id: str = Field(..., description=\"Task graph ID\")\n    start_time: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description=\"Execution start time\")\n    end_time: Optional[datetime] = Field(None, description=\"Execution end time\")\n    status: ExecutionStatus = Field(default=ExecutionStatus.PENDING, description=\"Current execution status\")\n    current_task: Optional[str] = Field(None, description=\"Currently executing task ID\")\n    completed_tasks: List[str] = Field(default_factory=list, description=\"Completed task IDs\")\n    failed_tasks: List[str] = Field(default_factory=list, description=\"Failed task IDs\")\n    retry_count: int = Field(default=0, description=\"Number of retries\")\n    max_retries: int = Field(default=3, description=\"Maximum retries allowed\")\n    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Additional metadata\")\n    \n    @field_validator('retry_count')\n    @classmethod\n    def validate_retry_count(cls, v):\n        if v < 0:\n            raise ValueError(\"Retry count cannot be negative\")\n        return v\n    \n    @field_validator('max_retries')\n    @classmethod\n    def validate_max_retries(cls, v):\n        if v < 0:\n            raise ValueError(\"Max retries cannot be negative\")\n        return v\n\n\nclass ExecutionLog(BaseModel):\n    \"\"\"Execution log entry.\"\"\"\n    log_id: str = Field(default_factory=lambda: str(uuid4()), description=\"Unique log ID\")\n    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description=\"Log timestamp\")\n    level: LogLevel = Field(..., description=\"Log level\")\n    task_id: Optional[str] = Field(None, description=\"Related task ID\")\n    message: str = Field(..., description=\"Log message\")\n    details: Dict[str, Any] = Field(default_factory=dict, description=\"Additional log details\")\n    execution_id: str = Field(..., description=\"Execution ID\")\n    \n    @field_validator('message')\n    @classmethod\n    def validate_message(cls, v):\n        if not v or len(v.strip()) == 0:\n            raise ValueError(\"Log message cannot be empty\")\n        return v.strip()\n\n\nclass ExecutionEvidence(BaseModel):\n    \"\"\"Evidence captured during execution.\"\"\"\n    evidence_id: str = Field(default_factory=lambda: str(uuid4()), description=\"Unique evidence ID\")\n    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description=\"Evidence timestamp\")\n    task_id: str = Field(..., description=\"Related task ID\")\n    evidence_type: str = Field(..., description=\"Type of evidence\")\n    data: Dict[str, Any] = Field(..., description=\"Evidence data\")\n    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Additional metadata\")\n    execution_id: str = Field(..., description=\"Execution ID\")\n    \n    @field_validator('evidence_type')\n    @classmethod\n    def validate_evidence_type(cls, v):\n        allowed_types = ['input', 'output', 'error', 'performance', 'state', 'custom']\n        if v not in allowed_types:\n            raise ValueError(f\"Evidence type must be one of: {allowed_types}\")\n        return v\n\n\nclass RetryConfig(BaseModel):\n    \"\"\"Retry configuration.\"\"\"\n    strategy: RetryStrategy = Field(default=RetryStrategy.EXPONENTIAL_BACKOFF, description=\"Retry strategy\")\n    max_retries: int = Field(default=3, ge=0, le=10, description=\"Maximum retries\")\n    base_delay: float = Field(default=1.0, ge=0.1, description=\"Base delay in seconds\")\n    max_delay: float = Field(default=60.0, ge=1.0, description=\"Maximum delay in seconds\")\n    backoff_multiplier: float = Field(default=2.0, ge=1.0, description=\"Backoff multiplier\")\n    jitter: bool = Field(default=True, description=\"Add jitter to delays\")\n    \n    @field_validator('max_delay')\n    @classmethod\n    def validate_max_delay(cls, v, info):\n        if info.data.get('base_delay') is not None and v < info.data['base_delay']:\n            raise ValueError(\"Max delay must be greater than base delay\")\n        return v\n\n\nclass LoggingConfig(BaseModel):\n    \"\"\"Logging configuration.\"\"\"\n    level: LogLevel = Field(default=LogLevel.INFO, description=\"Log level\")\n    format: str = Field(default=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", description=\"Log format\")\n    include_details: bool = Field(default=True, description=\"Include detailed information\")\n    include_timestamps: bool = Field(default=True, description=\"Include timestamps\")\n    max_log_entries: int = Field(default=1000, ge=100, description=\"Maximum log entries to keep\")\n    log_to_file: bool = Field(default=False, description=\"Log to file\")\n    log_file_path: Optional[str] = Field(None, description=\"Log file path\")\n\n\nclass EvidenceConfig(BaseModel):\n    \"\"\"Evidence capture configuration.\"\"\"\n    capture_input: bool = Field(default=True, description=\"Capture task inputs\")\n    capture_output: bool = Field(default=True, description=\"Capture task outputs\")\n    capture_errors: bool = Field(default=True, description=\"Capture errors\")\n    capture_performance: bool = Field(default=True, description=\"Capture performance metrics\")\n    capture_state: bool = Field(default=False, description=\"Capture state changes\")\n    max_evidence_entries: int = Field(default=500, ge=50, description=\"Maximum evidence entries\")\n    compress_evidence: bool = Field(default=False, description=\"Compress evidence data\")\n\n\nclass RunnerConfig(BaseModel):\n    \"\"\"Runner configuration.\"\"\"\n    retry_config: RetryConfig = Field(default_factory=RetryConfig, description=\"Retry configuration\")\n    logging_config: LoggingConfig = Field(default_factory=LoggingConfig, description=\"Logging configuration\")\n    evidence_config: EvidenceConfig = Field(default_factory=EvidenceConfig, description=\"Evidence configuration\")\n    max_concurrent_tasks: int = Field(default=5, ge=1, le=20, description=\"Maximum concurrent tasks\")\n    task_timeout: float = Field(default=300.0, ge=1.0, description=\"Task timeout in seconds\")\n    enable_parallel_execution: bool = Field(default=True, description=\"Enable parallel task execution\")\n    enable_evidence_capture: bool = Field(default=True, description=\"Enable evidence capture\")\n    enable_detailed_logging: bool = Field(default=True, description=\"Enable detailed logging\")\n\n\n# ============================================================================\n# Tool Interface\n# ============================================================================\n\nclass ToolExecutor(ABC):\n    \"\"\"Abstract base class for tool executors.\"\"\"\n    \n    @abstractmethod\n    async def execute(self, tool_name: str, parameters: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Execute a tool with given parameters.\"\"\"\n        pass\n    \n    @abstractmethod\n    def get_available_tools(self) -> List[ToolSpec]:\n        \"\"\"Get list of available tools.\"\"\"\n        pass\n    \n    @abstractmethod\n    def validate_tool_call(self, tool_name: str, parameters: Dict[str, Any]) -> bool:\n        \"\"\"Validate tool call parameters.\"\"\"\n        pass\n\n\nclass MockToolExecutor(ToolExecutor):\n    \"\"\"Mock tool executor for testing.\"\"\"\n    \n    def __init__(self):\n        self.tools = [\n            ToolSpec(\n                name=\"mock_tool\",\n                description=\"Mock tool for testing\",\n                parameters=[],\n                returns={\"result\": \"str\"},\n                category=\"test\"\n            ),\n            ToolSpec(\n                name=\"slow_tool\",\n                description=\"Slow mock tool\",\n                parameters=[],\n                returns={\"result\": \"str\"},\n                category=\"test\"\n            ),\n            ToolSpec(\n                name=\"failing_tool\",\n                description=\"Failing mock tool\",\n                parameters=[],\n                returns={\"result\": \"str\"},\n                category=\"test\"\n            )\n        ]\n    \n    async def execute(self, tool_name: str, parameters: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Execute mock tool.\"\"\"\n        if tool_name == \"mock_tool\":\n            await asyncio.sleep(0.1)  # Simulate work\n            return {\"result\": f\"Mock result for {parameters}\"}\n        elif tool_name == \"slow_tool\":\n            await asyncio.sleep(2.0)  # Simulate slow work\n            return {\"result\": f\"Slow result for {parameters}\"}\n        elif tool_name == \"failing_tool\":\n            raise Exception(\"Mock tool failure\")\n        else:\n            raise ValueError(f\"Unknown tool: {tool_name}\")\n    \n    def get_available_tools(self) -> List[ToolSpec]:\n        \"\"\"Get available tools.\"\"\"\n        return self.tools\n    \n    def validate_tool_call(self, tool_name: str, parameters: Dict[str, Any]) -> bool:\n        \"\"\"Validate tool call.\"\"\"\n        return any(tool.name == tool_name for tool in self.tools)\n\n\n# ============================================================================\n# Retry Mechanisms\n# ============================================================================\n\nclass RetryManager:\n    \"\"\"Manages retry logic for failed tasks.\"\"\"\n    \n    def __init__(self, config: RetryConfig):\n        self.config = config\n        self.logger = logging.getLogger(__name__)\n    \n    async def should_retry(self, task_id: str, error: Exception, retry_count: int) -> bool:\n        \"\"\"Determine if a task should be retried.\"\"\"\n        if retry_count >= self.config.max_retries:\n            self.logger.warning(f\"Task {task_id} exceeded max retries ({self.config.max_retries})\")\n            return False\n        \n        # Check if error is retryable\n        if not self._is_retryable_error(error):\n            self.logger.warning(f\"Task {task_id} failed with non-retryable error: {error}\")\n            return False\n        \n        return True\n    \n    def _is_retryable_error(self, error: Exception) -> bool:\n        \"\"\"Check if an error is retryable.\"\"\"\n        # Non-retryable errors\n        non_retryable_errors = [\n            ValueError,\n            TypeError,\n            AttributeError,\n            KeyError,\n            NotImplementedError\n        ]\n        \n        return not any(isinstance(error, error_type) for error_type in non_retryable_errors)\n    \n    async def get_retry_delay(self, retry_count: int) -> float:\n        \"\"\"Calculate retry delay based on strategy.\"\"\"\n        if self.config.strategy == RetryStrategy.IMMEDIATE:\n            return 0.0\n        elif self.config.strategy == RetryStrategy.FIXED_DELAY:\n            delay = self.config.base_delay\n        elif self.config.strategy == RetryStrategy.LINEAR_BACKOFF:\n            delay = self.config.base_delay * (retry_count + 1)\n        elif self.config.strategy == RetryStrategy.EXPONENTIAL_BACKOFF:\n            delay = self.config.base_delay * (self.config.backoff_multiplier ** retry_count)\n        else:\n            delay = self.config.base_delay\n        \n        # Apply max delay limit\n        delay = min(delay, self.config.max_delay)\n        \n        # Add jitter if enabled\n        if self.config.jitter:\n            import random\n            jitter_factor = random.uniform(0.5, 1.5)\n            delay *= jitter_factor\n        \n        return delay\n\n\n# ============================================================================\n# Logging System\n# ============================================================================\n\nclass ExecutionLogger:\n    \"\"\"Comprehensive logging system for task execution.\"\"\"\n    \n    def __init__(self, config: LoggingConfig, execution_id: str):\n        self.config = config\n        self.execution_id = execution_id\n        self.logs: List[ExecutionLog] = []\n        self.logger = logging.getLogger(f\"TaskRunner-{execution_id}\")\n        \n        # Configure logger\n        self._configure_logger()\n    \n    def _configure_logger(self):\n        \"\"\"Configure the logger.\"\"\"\n        handler = logging.StreamHandler()\n        formatter = logging.Formatter(self.config.format)\n        handler.setFormatter(formatter)\n        \n        self.logger.addHandler(handler)\n        self.logger.setLevel(getattr(logging, self.config.level.value))\n        \n        # Prevent duplicate logs\n        self.logger.propagate = False\n    \n    def log(self, level: LogLevel, message: str, task_id: Optional[str] = None, details: Optional[Dict[str, Any]] = None):\n        \"\"\"Add a log entry.\"\"\"\n        log_entry = ExecutionLog(\n            level=level,\n            task_id=task_id,\n            message=message,\n            details=details or {},\n            execution_id=self.execution_id\n        )\n        \n        self.logs.append(log_entry)\n        \n        # Limit log entries\n        if len(self.logs) > self.config.max_log_entries:\n            self.logs = self.logs[-self.config.max_log_entries:]\n        \n        # Log to standard logger\n        log_method = getattr(self.logger, level.value.lower())\n        log_message = f\"[{task_id or 'GLOBAL'}] {message}\"\n        if details and self.config.include_details:\n            log_message += f\" | Details: {details}\"\n        \n        log_method(log_message)\n    \n    def debug(self, message: str, task_id: Optional[str] = None, details: Optional[Dict[str, Any]] = None):\n        \"\"\"Log debug message.\"\"\"\n        self.log(LogLevel.DEBUG, message, task_id, details)\n    \n    def info(self, message: str, task_id: Optional[str] = None, details: Optional[Dict[str, Any]] = None):\n        \"\"\"Log info message.\"\"\"\n        self.log(LogLevel.INFO, message, task_id, details)\n    \n    def warning(self, message: str, task_id: Optional[str] = None, details: Optional[Dict[str, Any]] = None):\n        \"\"\"Log warning message.\"\"\"\n        self.log(LogLevel.WARNING, message, task_id, details)\n    \n    def error(self, message: str, task_id: Optional[str] = None, details: Optional[Dict[str, Any]] = None):\n        \"\"\"Log error message.\"\"\"\n        self.log(LogLevel.ERROR, message, task_id, details)\n    \n    def critical(self, message: str, task_id: Optional[str] = None, details: Optional[Dict[str, Any]] = None):\n        \"\"\"Log critical message.\"\"\"\n        self.log(LogLevel.CRITICAL, message, task_id, details)\n    \n    def get_logs(self, level: Optional[LogLevel] = None, task_id: Optional[str] = None) -> List[ExecutionLog]:\n        \"\"\"Get filtered logs.\"\"\"\n        filtered_logs = self.logs\n        \n        if level:\n            filtered_logs = [log for log in filtered_logs if log.level == level]\n        \n        if task_id:\n            filtered_logs = [log for log in filtered_logs if log.task_id == task_id]\n        \n        return filtered_logs\n\n\n# ============================================================================\n# Evidence Capture\n# ============================================================================\n\nclass EvidenceCapture:\n    \"\"\"Captures evidence during task execution.\"\"\"\n    \n    def __init__(self, config: EvidenceConfig, execution_id: str):\n        self.config = config\n        self.execution_id = execution_id\n        self.evidence: List[ExecutionEvidence] = []\n    \n    def capture_input(self, task_id: str, input_data: Dict[str, Any], metadata: Optional[Dict[str, Any]] = None):\n        \"\"\"Capture task input.\"\"\"\n        if not self.config.capture_input:\n            return\n        \n        evidence = ExecutionEvidence(\n            task_id=task_id,\n            evidence_type=\"input\",\n            data=input_data,\n            metadata=metadata or {},\n            execution_id=self.execution_id\n        )\n        \n        self._add_evidence(evidence)\n    \n    def capture_output(self, task_id: str, output_data: Dict[str, Any], metadata: Optional[Dict[str, Any]] = None):\n        \"\"\"Capture task output.\"\"\"\n        if not self.config.capture_output:\n            return\n        \n        evidence = ExecutionEvidence(\n            task_id=task_id,\n            evidence_type=\"output\",\n            data=output_data,\n            metadata=metadata or {},\n            execution_id=self.execution_id\n        )\n        \n        self._add_evidence(evidence)\n    \n    def capture_error(self, task_id: str, error: Exception, metadata: Optional[Dict[str, Any]] = None):\n        \"\"\"Capture task error.\"\"\"\n        if not self.config.capture_errors:\n            return\n        \n        error_data = {\n            \"error_type\": type(error).__name__,\n            \"error_message\": str(error),\n            \"traceback\": traceback.format_exc()\n        }\n        \n        evidence = ExecutionEvidence(\n            task_id=task_id,\n            evidence_type=\"error\",\n            data=error_data,\n            metadata=metadata or {},\n            execution_id=self.execution_id\n        )\n        \n        self._add_evidence(evidence)\n    \n    def capture_performance(self, task_id: str, performance_data: Dict[str, Any], metadata: Optional[Dict[str, Any]] = None):\n        \"\"\"Capture performance metrics.\"\"\"\n        if not self.config.capture_performance:\n            return\n        \n        evidence = ExecutionEvidence(\n            task_id=task_id,\n            evidence_type=\"performance\",\n            data=performance_data,\n            metadata=metadata or {},\n            execution_id=self.execution_id\n        )\n        \n        self._add_evidence(evidence)\n    \n    def capture_state(self, task_id: str, state_data: Dict[str, Any], metadata: Optional[Dict[str, Any]] = None):\n        \"\"\"Capture state changes.\"\"\"\n        if not self.config.capture_state:\n            return\n        \n        evidence = ExecutionEvidence(\n            task_id=task_id,\n            evidence_type=\"state\",\n            data=state_data,\n            metadata=metadata or {},\n            execution_id=self.execution_id\n        )\n        \n        self._add_evidence(evidence)\n    \n    def capture_custom(self, task_id: str, evidence_type: str, data: Dict[str, Any], metadata: Optional[Dict[str, Any]] = None):\n        \"\"\"Capture custom evidence.\"\"\"\n        evidence = ExecutionEvidence(\n            task_id=task_id,\n            evidence_type=\"custom\",\n            data=data,\n            metadata=metadata or {},\n            execution_id=self.execution_id\n        )\n        \n        self._add_evidence(evidence)\n    \n    def _add_evidence(self, evidence: ExecutionEvidence):\n        \"\"\"Add evidence to collection.\"\"\"\n        self.evidence.append(evidence)\n        \n        # Limit evidence entries\n        if len(self.evidence) > self.config.max_evidence_entries:\n            self.evidence = self.evidence[-self.config.max_evidence_entries:]\n    \n    def get_evidence(self, evidence_type: Optional[str] = None, task_id: Optional[str] = None) -> List[ExecutionEvidence]:\n        \"\"\"Get filtered evidence.\"\"\"\n        filtered_evidence = self.evidence\n        \n        if evidence_type:\n            filtered_evidence = [ev for ev in filtered_evidence if ev.evidence_type == evidence_type]\n        \n        if task_id:\n            filtered_evidence = [ev for ev in filtered_evidence if ev.task_id == task_id]\n        \n        return filtered_evidence\n\n\n# ============================================================================\n# Main Task Runner\n# ============================================================================\n\nclass TaskRunner:\n    \"\"\"Main task runner that executes TaskGraphs.\"\"\"\n    \n    def __init__(self, config: Optional[RunnerConfig] = None, tool_executor: Optional[ToolExecutor] = None):\n        self.config = config or RunnerConfig()\n        self.tool_executor = tool_executor or MockToolExecutor()\n        self.logger = logging.getLogger(__name__)\n        \n        # Components\n        self.retry_manager = RetryManager(self.config.retry_config)\n        \n        # Statistics\n        self.total_executions = 0\n        self.successful_executions = 0\n        self.failed_executions = 0\n        self.total_tasks_executed = 0\n        self.total_retries = 0\n    \n    async def run(self, task_graph: TaskGraph, context: Optional[Dict[str, Any]] = None) -> ExecutionContext:\n        \"\"\"Run a task graph.\"\"\"\n        context = context or {}\n        \n        # Create execution context\n        execution_context = ExecutionContext(\n            task_graph_id=task_graph.graph_id,\n            status=ExecutionStatus.RUNNING,\n            max_retries=self.config.retry_config.max_retries,\n            metadata=context\n        )\n        \n        # Initialize components\n        execution_logger = ExecutionLogger(self.config.logging_config, execution_context.execution_id)\n        evidence_capture = EvidenceCapture(self.config.evidence_config, execution_context.execution_id)\n        \n        execution_logger.info(f\"Starting execution of task graph: {task_graph.name}\")\n        execution_logger.info(f\"Tasks to execute: {len(task_graph.tasks)}\")\n        \n        try:\n            # Execute tasks\n            if self.config.enable_parallel_execution:\n                await self._execute_parallel(task_graph, execution_context, execution_logger, evidence_capture)\n            else:\n                await self._execute_sequential(task_graph, execution_context, execution_logger, evidence_capture)\n            \n            # Update execution status\n            if execution_context.failed_tasks:\n                execution_context.status = ExecutionStatus.FAILED\n                execution_logger.error(f\"Execution failed with {len(execution_context.failed_tasks)} failed tasks\")\n            else:\n                execution_context.status = ExecutionStatus.COMPLETED\n                execution_logger.info(\"Execution completed successfully\")\n            \n            # Update statistics\n            self.total_executions += 1\n            if execution_context.status == ExecutionStatus.COMPLETED:\n                self.successful_executions += 1\n            else:\n                self.failed_executions += 1\n            \n            self.total_tasks_executed += len(execution_context.completed_tasks)\n            self.total_retries += execution_context.retry_count\n            \n        except Exception as e:\n            execution_context.status = ExecutionStatus.FAILED\n            execution_logger.critical(f\"Execution failed with critical error: {e}\")\n            evidence_capture.capture_error(\"GLOBAL\", e)\n        \n        finally:\n            execution_context.end_time = datetime.now(timezone.utc)\n            execution_logger.info(f\"Execution completed in {(execution_context.end_time - execution_context.start_time).total_seconds():.2f}s\")\n        \n        return execution_context\n    \n    async def _execute_sequential(self, task_graph: TaskGraph, execution_context: ExecutionContext, \n                                 execution_logger: ExecutionLogger, evidence_capture: EvidenceCapture):\n        \"\"\"Execute tasks sequentially.\"\"\"\n        execution_logger.info(\"Executing tasks sequentially\")\n        \n        # Create task dependency map\n        task_map = {task.task_id: task for task in task_graph.tasks}\n        completed_tasks = set()\n        \n        # Execute tasks in dependency order\n        while len(completed_tasks) < len(task_graph.tasks):\n            # Find tasks ready to execute\n            ready_tasks = []\n            for task in task_graph.tasks:\n                if task.task_id not in completed_tasks:\n                    dependencies_met = all(dep_id in completed_tasks for dep_id in task.dependencies)\n                    if dependencies_met:\n                        ready_tasks.append(task)\n            \n            if not ready_tasks:\n                # No tasks ready - check for circular dependencies\n                remaining_tasks = [t for t in task_graph.tasks if t.task_id not in completed_tasks]\n                execution_logger.error(f\"No tasks ready to execute. Remaining: {[t.task_id for t in remaining_tasks]}\")\n                break\n            \n            # Execute ready tasks\n            for task in ready_tasks:\n                await self._execute_task(task, execution_context, execution_logger, evidence_capture)\n                completed_tasks.add(task.task_id)\n    \n    async def _execute_parallel(self, task_graph: TaskGraph, execution_context: ExecutionContext,\n                               execution_logger: ExecutionLogger, evidence_capture: EvidenceCapture):\n        \"\"\"Execute tasks in parallel where possible.\"\"\"\n        execution_logger.info(\"Executing tasks in parallel\")\n        \n        # Create task dependency map\n        task_map = {task.task_id: task for task in task_graph.tasks}\n        completed_tasks = set()\n        running_tasks = set()\n        \n        # Create semaphore for concurrency control\n        semaphore = asyncio.Semaphore(self.config.max_concurrent_tasks)\n        \n        async def execute_task_with_semaphore(task: Task):\n            async with semaphore:\n                await self._execute_task(task, execution_context, execution_logger, evidence_capture)\n                return task.task_id\n        \n        # Execute tasks in waves\n        while len(completed_tasks) < len(task_graph.tasks):\n            # Find tasks ready to execute\n            ready_tasks = []\n            for task in task_graph.tasks:\n                if task.task_id not in completed_tasks and task.task_id not in running_tasks:\n                    dependencies_met = all(dep_id in completed_tasks for dep_id in task.dependencies)\n                    if dependencies_met:\n                        ready_tasks.append(task)\n            \n            if not ready_tasks:\n                if running_tasks:\n                    # Wait for running tasks to complete\n                    await asyncio.sleep(0.1)\n                    continue\n                else:\n                    # No tasks ready - check for issues\n                    remaining_tasks = [t for t in task_graph.tasks if t.task_id not in completed_tasks]\n                    execution_logger.error(f\"No tasks ready to execute. Remaining: {[t.task_id for t in remaining_tasks]}\")\n                    break\n            \n            # Execute ready tasks in parallel\n            execution_logger.info(f\"Executing {len(ready_tasks)} tasks in parallel\")\n            running_tasks.update(task.task_id for task in ready_tasks)\n            \n            # Wait for tasks to complete\n            completed_task_ids = await asyncio.gather(*[\n                execute_task_with_semaphore(task) for task in ready_tasks\n            ])\n            \n            completed_tasks.update(completed_task_ids)\n            running_tasks.difference_update(completed_task_ids)\n    \n    async def _execute_task(self, task: Task, execution_context: ExecutionContext,\n                           execution_logger: ExecutionLogger, evidence_capture: EvidenceCapture):\n        \"\"\"Execute a single task.\"\"\"\n        execution_logger.info(f\"Executing task: {task.name}\", task.task_id)\n        execution_context.current_task = task.task_id\n        \n        # Capture input evidence\n        evidence_capture.capture_input(task.task_id, task.input_data, {\n            \"required_tools\": task.required_tools,\n            \"dependencies\": task.dependencies\n        })\n        \n        start_time = time.time()\n        retry_count = 0\n        \n        while retry_count <= execution_context.max_retries:\n            try:\n                # Execute task\n                result = await self._execute_task_with_tools(task, execution_logger, evidence_capture)\n                \n                # Update task status\n                task.status = TaskStatus.COMPLETED\n                execution_context.completed_tasks.append(task.task_id)\n                \n                # Capture output evidence\n                evidence_capture.capture_output(task.task_id, result.output_data, {\n                    \"execution_time_ms\": result.execution_time_ms,\n                    \"status\": result.status.value\n                })\n                \n                # Capture performance evidence\n                execution_time = time.time() - start_time\n                evidence_capture.capture_performance(task.task_id, {\n                    \"execution_time\": execution_time,\n                    \"retry_count\": retry_count,\n                    \"tools_used\": task.required_tools\n                })\n                \n                execution_logger.info(f\"Task completed successfully in {execution_time:.2f}s\", task.task_id)\n                break\n                \n            except Exception as e:\n                retry_count += 1\n                execution_context.retry_count += 1\n                \n                # Capture error evidence\n                evidence_capture.capture_error(task.task_id, e, {\n                    \"retry_count\": retry_count,\n                    \"max_retries\": execution_context.max_retries\n                })\n                \n                # Check if should retry\n                if await self.retry_manager.should_retry(task.task_id, e, retry_count):\n                    delay = await self.retry_manager.get_retry_delay(retry_count)\n                    execution_logger.warning(f\"Task failed, retrying in {delay:.2f}s (attempt {retry_count + 1})\", task.task_id, {\n                        \"error\": str(e),\n                        \"retry_count\": retry_count\n                    })\n                    \n                    if delay > 0:\n                        await asyncio.sleep(delay)\n                    continue\n                else:\n                    # Task failed permanently\n                    task.status = TaskStatus.FAILED\n                    execution_context.failed_tasks.append(task.task_id)\n                    execution_logger.error(f\"Task failed permanently after {retry_count} retries\", task.task_id, {\n                        \"error\": str(e),\n                        \"retry_count\": retry_count\n                    })\n                    break\n        \n        execution_context.current_task = None\n    \n    async def _execute_task_with_tools(self, task: Task, execution_logger: ExecutionLogger, \n                                      evidence_capture: EvidenceCapture) -> TaskResult:\n        \"\"\"Execute task using available tools.\"\"\"\n        execution_logger.debug(f\"Executing task with tools: {task.required_tools}\", task.task_id)\n        \n        # Validate tool availability\n        available_tools = self.tool_executor.get_available_tools()\n        available_tool_names = [tool.name for tool in available_tools]\n        \n        missing_tools = [tool for tool in task.required_tools if tool not in available_tool_names]\n        if missing_tools:\n            raise ValueError(f\"Missing required tools: {missing_tools}\")\n        \n        # Execute tools\n        tool_results = {}\n        for tool_name in task.required_tools:\n            try:\n                execution_logger.debug(f\"Executing tool: {tool_name}\", task.task_id)\n                \n                # Validate tool call\n                if not self.tool_executor.validate_tool_call(tool_name, task.input_data):\n                    raise ValueError(f\"Invalid parameters for tool: {tool_name}\")\n                \n                # Execute tool with timeout\n                tool_result = await asyncio.wait_for(\n                    self.tool_executor.execute(tool_name, task.input_data, task.metadata),\n                    timeout=self.config.task_timeout\n                )\n                \n                tool_results[tool_name] = tool_result\n                execution_logger.debug(f\"Tool {tool_name} completed successfully\", task.task_id)\n                \n            except asyncio.TimeoutError:\n                raise TimeoutError(f\"Tool {tool_name} timed out after {self.config.task_timeout}s\")\n            except Exception as e:\n                raise Exception(f\"Tool {tool_name} failed: {e}\")\n        \n        # Create task result\n        execution_time = time.time()\n        result = TaskResult(\n            task_id=task.task_id,\n            status=TaskStatus.COMPLETED,\n            output_data=tool_results,\n            execution_time_ms=(execution_time - time.time()) * 1000,\n            metadata={\n                \"tools_used\": task.required_tools,\n                \"tool_results\": tool_results\n            }\n        )\n        \n        return result\n    \n    def get_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get runner statistics.\"\"\"\n        success_rate = (self.successful_executions / self.total_executions * 100) if self.total_executions > 0 else 0\n        \n        return {\n            \"total_executions\": self.total_executions,\n            \"successful_executions\": self.successful_executions,\n            \"failed_executions\": self.failed_executions,\n            \"success_rate\": success_rate,\n            \"total_tasks_executed\": self.total_tasks_executed,\n            \"total_retries\": self.total_retries,\n            \"average_retries_per_execution\": self.total_retries / self.total_executions if self.total_executions > 0 else 0\n        }\n\n\n# ============================================================================\n# Factory Functions\n# ============================================================================\n\ndef create_task_runner(\n    max_concurrent_tasks: int = 5,\n    task_timeout: float = 300.0,\n    max_retries: int = 3,\n    retry_strategy: RetryStrategy = RetryStrategy.EXPONENTIAL_BACKOFF,\n    log_level: LogLevel = LogLevel.INFO,\n    enable_evidence_capture: bool = True,\n    tool_executor: Optional[ToolExecutor] = None\n) -> TaskRunner:\n    \"\"\"Create a task runner with specified configuration.\"\"\"\n    config = RunnerConfig(\n        max_concurrent_tasks=max_concurrent_tasks,\n        task_timeout=task_timeout,\n        retry_config=RetryConfig(\n            strategy=retry_strategy,\n            max_retries=max_retries\n        ),\n        logging_config=LoggingConfig(level=log_level),\n        evidence_config=EvidenceConfig() if enable_evidence_capture else EvidenceConfig(\n            capture_input=False,\n            capture_output=False,\n            capture_errors=False,\n            capture_performance=False\n        )\n    )\n    \n    return TaskRunner(config, tool_executor)\n\n\ndef create_runner_config(\n    max_concurrent_tasks: int = 5,\n    task_timeout: float = 300.0,\n    max_retries: int = 3,\n    retry_strategy: RetryStrategy = RetryStrategy.EXPONENTIAL_BACKOFF,\n    log_level: LogLevel = LogLevel.INFO,\n    enable_evidence_capture: bool = True\n) -> RunnerConfig:\n    \"\"\"Create a runner configuration.\"\"\"\n    return RunnerConfig(\n        max_concurrent_tasks=max_concurrent_tasks,\n        task_timeout=task_timeout,\n        retry_config=RetryConfig(\n            strategy=retry_strategy,\n            max_retries=max_retries\n        ),\n        logging_config=LoggingConfig(level=log_level),\n        evidence_config=EvidenceConfig() if enable_evidence_capture else EvidenceConfig(\n            capture_input=False,\n            capture_output=False,\n            capture_errors=False,\n            capture_performance=False\n        )\n    )\n\n\n# ============================================================================\n# Export all classes and functions\n# ============================================================================\n\n__all__ = [\n    # Enums\n    \"ExecutionStatus\",\n    \"RetryStrategy\", \n    \"LogLevel\",\n    \n    # Models\n    \"ExecutionContext\",\n    \"ExecutionLog\",\n    \"ExecutionEvidence\",\n    \"RetryConfig\",\n    \"LoggingConfig\",\n    \"EvidenceConfig\",\n    \"RunnerConfig\",\n    \n    # Tool interface\n    \"ToolExecutor\",\n    \"MockToolExecutor\",\n    \n    # Components\n    \"RetryManager\",\n    \"ExecutionLogger\",\n    \"EvidenceCapture\",\n    \n    # Main runner\n    \"TaskRunner\",\n    \n    # Factory functions\n    \"create_task_runner\",\n    \"create_runner_config\",\n]\n", "source": "src/core/runtime/runner.py"}, {"instruction": "Explain the following code from src/core/runtime/reviewer.py:", "response": "This code is from src/core/runtime/reviewer.py:\n\n\"\"\"\nAgent Reviewer for Agentic LLM Core v0.1\n\nThis module provides an intelligent agent reviewer that performs schema validation,\nunit testing, and acceptance criteria checks, generating comprehensive review reports.\n\nCreated: 2024-09-24\nStatus: Draft\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport os\nimport subprocess\nimport tempfile\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Union, Tuple\nfrom uuid import uuid4\n\nimport pydantic\nfrom pydantic import BaseModel, Field, field_validator\n\nfrom ..models.contracts import Task, TaskGraph, TaskResult\n\n\n# ============================================================================\n# Review Models\n# ============================================================================\n\nclass ReviewStatus(str, Enum):\n    \"\"\"Review status enumeration.\"\"\"\n    PENDING = \"pending\"\n    IN_PROGRESS = \"in_progress\"\n    PASSED = \"passed\"\n    FAILED = \"failed\"\n    WARNING = \"warning\"\n    ERROR = \"error\"\n\n\nclass CheckType(str, Enum):\n    \"\"\"Type of check being performed.\"\"\"\n    SCHEMA = \"schema\"\n    UNIT = \"unit\"\n    ACCEPTANCE = \"acceptance\"\n\n\nclass ReviewCheck(BaseModel):\n    \"\"\"Individual review check result.\"\"\"\n    check_id: str = Field(default_factory=lambda: str(uuid4()), description=\"Unique check ID\")\n    check_type: CheckType = Field(..., description=\"Type of check\")\n    name: str = Field(..., description=\"Name of the check\")\n    description: str = Field(..., description=\"Description of the check\")\n    status: ReviewStatus = Field(default=ReviewStatus.PENDING, description=\"Check status\")\n    message: Optional[str] = Field(None, description=\"Check result message\")\n    details: Dict[str, Any] = Field(default_factory=dict, description=\"Additional check details\")\n    execution_time: Optional[float] = Field(None, description=\"Execution time in seconds\")\n    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description=\"Check timestamp\")\n    \n    @field_validator('execution_time')\n    @classmethod\n    def validate_execution_time(cls, v):\n        if v is not None and v < 0:\n            raise ValueError(\"Execution time cannot be negative\")\n        return v\n\n\nclass ReviewReport(BaseModel):\n    \"\"\"Comprehensive review report.\"\"\"\n    report_id: str = Field(default_factory=lambda: str(uuid4()), description=\"Unique report ID\")\n    target_path: str = Field(..., description=\"Path to the target being reviewed\")\n    review_type: str = Field(default=\"comprehensive\", description=\"Type of review\")\n    status: ReviewStatus = Field(default=ReviewStatus.PENDING, description=\"Overall review status\")\n    checks: List[ReviewCheck] = Field(default_factory=list, description=\"Individual check results\")\n    summary: Dict[str, Any] = Field(default_factory=dict, description=\"Review summary\")\n    recommendations: List[str] = Field(default_factory=list, description=\"Recommendations\")\n    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description=\"Report creation timestamp\")\n    completed_at: Optional[datetime] = Field(None, description=\"Report completion timestamp\")\n    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Additional metadata\")\n    \n    def get_overall_status(self) -> ReviewStatus:\n        \"\"\"Get overall review status based on individual checks.\"\"\"\n        if not self.checks:\n            return ReviewStatus.PENDING\n        \n        statuses = [check.status for check in self.checks]\n        \n        if ReviewStatus.ERROR in statuses:\n            return ReviewStatus.ERROR\n        elif ReviewStatus.FAILED in statuses:\n            return ReviewStatus.FAILED\n        elif ReviewStatus.WARNING in statuses:\n            return ReviewStatus.WARNING\n        elif all(status == ReviewStatus.PASSED for status in statuses):\n            return ReviewStatus.PASSED\n        else:\n            return ReviewStatus.IN_PROGRESS\n    \n    def get_summary_stats(self) -> Dict[str, int]:\n        \"\"\"Get summary statistics.\"\"\"\n        if not self.checks:\n            return {\"total\": 0, \"passed\": 0, \"failed\": 0, \"warning\": 0, \"error\": 0}\n        \n        stats = {\"total\": len(self.checks)}\n        for status in ReviewStatus:\n            stats[status.value] = sum(1 for check in self.checks if check.status == status)\n        \n        return stats\n\n\nclass ReviewConfig(BaseModel):\n    \"\"\"Configuration for the reviewer.\"\"\"\n    enable_schema_checks: bool = Field(default=True, description=\"Enable schema validation checks\")\n    enable_unit_checks: bool = Field(default=True, description=\"Enable unit test checks\")\n    enable_acceptance_checks: bool = Field(default=True, description=\"Enable acceptance criteria checks\")\n    \n    # Schema check configuration\n    schema_strict_mode: bool = Field(default=True, description=\"Use strict schema validation\")\n    schema_ignore_unknown: bool = Field(default=False, description=\"Ignore unknown fields in schema\")\n    \n    # Unit test configuration\n    test_framework: str = Field(default=\"pytest\", description=\"Test framework to use\")\n    test_timeout: float = Field(default=300.0, ge=1.0, description=\"Test execution timeout in seconds\")\n    test_coverage_threshold: float = Field(default=80.0, ge=0.0, le=100.0, description=\"Minimum test coverage percentage\")\n    \n    # Acceptance criteria configuration\n    acceptance_timeout: float = Field(default=600.0, ge=1.0, description=\"Acceptance test timeout in seconds\")\n    acceptance_retries: int = Field(default=3, ge=0, le=10, description=\"Number of retries for acceptance tests\")\n    \n    # Output configuration\n    output_format: str = Field(default=\"json\", description=\"Output format: json, yaml, html\")\n    include_details: bool = Field(default=True, description=\"Include detailed check results\")\n    include_recommendations: bool = Field(default=True, description=\"Include recommendations\")\n\n\n# ============================================================================\n# Check Implementations\n# ============================================================================\n\nclass ReviewCheckBase(ABC):\n    \"\"\"Abstract base class for review checks.\"\"\"\n    \n    def __init__(self, config: ReviewConfig):\n        self.config = config\n        self.logger = logging.getLogger(self.__class__.__name__)\n    \n    @abstractmethod\n    async def check(self, target_path: str, context: Dict[str, Any]) -> ReviewCheck:\n        \"\"\"Perform the check.\"\"\"\n        pass\n    \n    @abstractmethod\n    def get_check_name(self) -> str:\n        \"\"\"Get the name of this check.\"\"\"\n        pass\n    \n    @abstractmethod\n    def get_check_description(self) -> str:\n        \"\"\"Get the description of this check.\"\"\"\n        pass\n\n\nclass SchemaValidator(ReviewCheckBase):\n    \"\"\"Schema validation checker.\"\"\"\n    \n    def get_check_name(self) -> str:\n        return \"Schema Validation\"\n    \n    def get_check_description(self) -> str:\n        return \"Validates Pydantic schemas and data contracts\"\n    \n    async def check(self, target_path: str, context: Dict[str, Any]) -> ReviewCheck:\n        \"\"\"Perform schema validation check.\"\"\"\n        start_time = datetime.now()\n        \n        try:\n            check = ReviewCheck(\n                check_type=CheckType.SCHEMA,\n                name=self.get_check_name(),\n                description=self.get_check_description(),\n                status=ReviewStatus.IN_PROGRESS\n            )\n            \n            # Find Python files\n            python_files = self._find_python_files(target_path)\n            schema_issues = []\n            \n            for file_path in python_files:\n                issues = await self._validate_file_schemas(file_path)\n                schema_issues.extend(issues)\n            \n            # Determine overall status\n            if not schema_issues:\n                check.status = ReviewStatus.PASSED\n                check.message = f\"All {len(python_files)} files passed schema validation\"\n            else:\n                check.status = ReviewStatus.FAILED\n                check.message = f\"Found {len(schema_issues)} schema issues\"\n            \n            check.details = {\n                \"files_checked\": len(python_files),\n                \"issues_found\": len(schema_issues),\n                \"issues\": schema_issues\n            }\n            \n            execution_time = (datetime.now() - start_time).total_seconds()\n            check.execution_time = execution_time\n            \n            return check\n            \n        except Exception as e:\n            execution_time = (datetime.now() - start_time).total_seconds()\n            return ReviewCheck(\n                check_type=CheckType.SCHEMA,\n                name=self.get_check_name(),\n                description=self.get_check_description(),\n                status=ReviewStatus.ERROR,\n                message=f\"Schema validation failed: {str(e)}\",\n                execution_time=execution_time\n            )\n    \n    def _find_python_files(self, target_path: str) -> List[str]:\n        \"\"\"Find Python files in the target path.\"\"\"\n        python_files = []\n        path = Path(target_path)\n        \n        if path.is_file() and path.suffix == '.py':\n            python_files.append(str(path))\n        elif path.is_dir():\n            for py_file in path.rglob('*.py'):\n                # Skip __pycache__ and test files for now\n                if '__pycache__' not in str(py_file) and 'test_' not in py_file.name:\n                    python_files.append(str(py_file))\n        \n        return python_files\n    \n    async def _validate_file_schemas(self, file_path: str) -> List[Dict[str, Any]]:\n        \"\"\"Validate schemas in a Python file.\"\"\"\n        issues = []\n        \n        try:\n            # Read file content\n            with open(file_path, 'r', encoding='utf-8') as f:\n                content = f.read()\n            \n            # Check for Pydantic models\n            if 'BaseModel' in content or 'pydantic' in content:\n                # Try to import and validate the module\n                try:\n                    # This is a simplified check - in practice, you'd use AST parsing\n                    if 'class ' in content and 'BaseModel' in content:\n                        # Check for common schema issues\n                        if 'Field(' in content:\n                            # Check for proper Field usage\n                            if 'Field(...)' in content and 'required' not in content:\n                                issues.append({\n                                    \"file\": file_path,\n                                    \"line\": \"unknown\",\n                                    \"issue\": \"Required field without explicit required=True\",\n                                    \"severity\": \"warning\"\n                                })\n                        \n                        # Check for validator usage\n                        if '@validator' in content and 'pydantic' in content:\n                            if 'pydantic.v1' in content:\n                                issues.append({\n                                    \"file\": file_path,\n                                    \"line\": \"unknown\", \n                                    \"issue\": \"Using deprecated pydantic v1 validator\",\n                                    \"severity\": \"error\"\n                                })\n                \n                except Exception as e:\n                    issues.append({\n                        \"file\": file_path,\n                        \"line\": \"unknown\",\n                        \"issue\": f\"Could not parse schema: {str(e)}\",\n                        \"severity\": \"error\"\n                    })\n        \n        except Exception as e:\n            issues.append({\n                \"file\": file_path,\n                \"line\": \"unknown\",\n                \"issue\": f\"Could not read file: {str(e)}\",\n                \"severity\": \"error\"\n            })\n        \n        return issues\n\n\nclass UnitTestRunner(ReviewCheckBase):\n    \"\"\"Unit test runner checker.\"\"\"\n    \n    def get_check_name(self) -> str:\n        return \"Unit Tests\"\n    \n    def get_check_description(self) -> str:\n        return \"Runs unit tests and checks coverage\"\n    \n    async def check(self, target_path: str, context: Dict[str, Any]) -> ReviewCheck:\n        \"\"\"Perform unit test check.\"\"\"\n        start_time = datetime.now()\n        \n        try:\n            check = ReviewCheck(\n                check_type=CheckType.UNIT,\n                name=self.get_check_name(),\n                description=self.get_check_description(),\n                status=ReviewStatus.IN_PROGRESS\n            )\n            \n            # Find test files\n            test_files = self._find_test_files(target_path)\n            \n            if not test_files:\n                check.status = ReviewStatus.WARNING\n                check.message = \"No test files found\"\n                check.details = {\"test_files_found\": 0}\n                return check\n            \n            # Run tests\n            test_results = await self._run_tests(test_files)\n            \n            # Check coverage\n            coverage_results = await self._check_coverage(target_path)\n            \n            # Determine overall status\n            if test_results[\"passed\"] and coverage_results[\"coverage\"] >= self.config.test_coverage_threshold:\n                check.status = ReviewStatus.PASSED\n                check.message = f\"All tests passed with {coverage_results['coverage']:.1f}% coverage\"\n            elif test_results[\"passed\"]:\n                check.status = ReviewStatus.WARNING\n                check.message = f\"Tests passed but coverage ({coverage_results['coverage']:.1f}%) below threshold ({self.config.test_coverage_threshold}%)\"\n            else:\n                check.status = ReviewStatus.FAILED\n                check.message = f\"{test_results['failed']} tests failed\"\n            \n            check.details = {\n                \"test_files\": test_files,\n                \"test_results\": test_results,\n                \"coverage_results\": coverage_results\n            }\n            \n            execution_time = (datetime.now() - start_time).total_seconds()\n            check.execution_time = execution_time\n            \n            return check\n            \n        except Exception as e:\n            execution_time = (datetime.now() - start_time).total_seconds()\n            return ReviewCheck(\n                check_type=CheckType.UNIT,\n                name=self.get_check_name(),\n                description=self.get_check_description(),\n                status=ReviewStatus.ERROR,\n                message=f\"Unit test execution failed: {str(e)}\",\n                execution_time=execution_time\n            )\n    \n    def _find_test_files(self, target_path: str) -> List[str]:\n        \"\"\"Find test files in the target path.\"\"\"\n        test_files = []\n        path = Path(target_path)\n        \n        if path.is_file() and ('test_' in path.name or path.name.endswith('_test.py')):\n            test_files.append(str(path))\n        elif path.is_dir():\n            for test_file in path.rglob('test_*.py'):\n                test_files.append(str(test_file))\n            for test_file in path.rglob('*_test.py'):\n                test_files.append(str(test_file))\n        \n        return test_files\n    \n    async def _run_tests(self, test_files: List[str]) -> Dict[str, Any]:\n        \"\"\"Run unit tests.\"\"\"\n        try:\n            # Run pytest on test files\n            cmd = [self.config.test_framework, \"-v\", \"--tb=short\"] + test_files\n            \n            result = await asyncio.create_subprocess_exec(\n                *cmd,\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE\n            )\n            \n            stdout, stderr = await asyncio.wait_for(\n                result.communicate(),\n                timeout=self.config.test_timeout\n            )\n            \n            # Parse pytest output (simplified)\n            output = stdout.decode() if stdout else \"\"\n            error_output = stderr.decode() if stderr else \"\"\n            \n            # Count passed/failed tests (simplified parsing)\n            passed = output.count(\"PASSED\")\n            failed = output.count(\"FAILED\")\n            \n            return {\n                \"passed\": passed,\n                \"failed\": failed,\n                \"total\": passed + failed,\n                \"output\": output,\n                \"error_output\": error_output,\n                \"return_code\": result.returncode\n            }\n            \n        except asyncio.TimeoutError:\n            return {\n                \"passed\": 0,\n                \"failed\": 0,\n                \"total\": 0,\n                \"output\": \"\",\n                \"error_output\": \"Test execution timed out\",\n                \"return_code\": -1\n            }\n        except Exception as e:\n            return {\n                \"passed\": 0,\n                \"failed\": 0,\n                \"total\": 0,\n                \"output\": \"\",\n                \"error_output\": str(e),\n                \"return_code\": -1\n            }\n    \n    async def _check_coverage(self, target_path: str) -> Dict[str, Any]:\n        \"\"\"Check test coverage.\"\"\"\n        try:\n            # Run coverage analysis\n            cmd = [\"coverage\", \"run\", \"-m\", \"pytest\", target_path]\n            \n            result = await asyncio.create_subprocess_exec(\n                *cmd,\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE\n            )\n            \n            await asyncio.wait_for(\n                result.communicate(),\n                timeout=self.config.test_timeout\n            )\n            \n            # Get coverage report\n            report_cmd = [\"coverage\", \"report\", \"--show-missing\"]\n            \n            report_result = await asyncio.create_subprocess_exec(\n                *report_cmd,\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE\n            )\n            \n            stdout, stderr = await report_result.communicate()\n            \n            # Parse coverage percentage (simplified)\n            output = stdout.decode() if stdout else \"\"\n            coverage = 0.0\n            \n            if \"TOTAL\" in output:\n                # Extract coverage percentage from pytest-cov output\n                lines = output.split('\\n')\n                for line in lines:\n                    if \"TOTAL\" in line:\n                        parts = line.split()\n                        for part in parts:\n                            if part.endswith('%'):\n                                try:\n                                    coverage = float(part[:-1])\n                                    break\n                                except ValueError:\n                                    continue\n            \n            return {\n                \"coverage\": coverage,\n                \"output\": output,\n                \"error_output\": stderr.decode() if stderr else \"\"\n            }\n            \n        except Exception as e:\n            return {\n                \"coverage\": 0.0,\n                \"output\": \"\",\n                \"error_output\": str(e)\n            }\n\n\nclass AcceptanceCriteriaValidator(ReviewCheckBase):\n    \"\"\"Acceptance criteria validator.\"\"\"\n    \n    def get_check_name(self) -> str:\n        return \"Acceptance Criteria\"\n    \n    def get_check_description(self) -> str:\n        return \"Validates acceptance criteria and requirements\"\n    \n    async def check(self, target_path: str, context: Dict[str, Any]) -> ReviewCheck:\n        \"\"\"Perform acceptance criteria check.\"\"\"\n        start_time = datetime.now()\n        \n        try:\n            check = ReviewCheck(\n                check_type=CheckType.ACCEPTANCE,\n                name=self.get_check_name(),\n                description=self.get_check_description(),\n                status=ReviewStatus.IN_PROGRESS\n            )\n            \n            # Look for acceptance criteria files\n            criteria_files = self._find_acceptance_files(target_path)\n            \n            if not criteria_files:\n                check.status = ReviewStatus.WARNING\n                check.message = \"No acceptance criteria files found\"\n                check.details = {\"criteria_files_found\": 0}\n                return check\n            \n            # Validate acceptance criteria\n            validation_results = await self._validate_acceptance_criteria(criteria_files, target_path)\n            \n            # Determine overall status\n            if validation_results[\"passed\"]:\n                check.status = ReviewStatus.PASSED\n                check.message = f\"All {validation_results['total']} acceptance criteria validated\"\n            else:\n                check.status = ReviewStatus.FAILED\n                check.message = f\"{validation_results['failed']} acceptance criteria failed\"\n            \n            check.details = validation_results\n            \n            execution_time = (datetime.now() - start_time).total_seconds()\n            check.execution_time = execution_time\n            \n            return check\n            \n        except Exception as e:\n            execution_time = (datetime.now() - start_time).total_seconds()\n            return ReviewCheck(\n                check_type=CheckType.ACCEPTANCE,\n                name=self.get_check_name(),\n                description=self.get_check_description(),\n                status=ReviewStatus.ERROR,\n                message=f\"Acceptance criteria validation failed: {str(e)}\",\n                execution_time=execution_time\n            )\n    \n    def _find_acceptance_files(self, target_path: str) -> List[str]:\n        \"\"\"Find acceptance criteria files.\"\"\"\n        criteria_files = []\n        path = Path(target_path)\n        \n        # Look for common acceptance criteria file patterns\n        patterns = [\n            \"acceptance_criteria.md\",\n            \"acceptance_criteria.json\",\n            \"requirements.md\",\n            \"requirements.json\",\n            \"criteria.md\",\n            \"criteria.json\"\n        ]\n        \n        if path.is_file():\n            if any(pattern in path.name for pattern in patterns):\n                criteria_files.append(str(path))\n        elif path.is_dir():\n            for pattern in patterns:\n                for file_path in path.rglob(pattern):\n                    criteria_files.append(str(file_path))\n        \n        return criteria_files\n    \n    async def _validate_acceptance_criteria(self, criteria_files: List[str], target_path: str) -> Dict[str, Any]:\n        \"\"\"Validate acceptance criteria.\"\"\"\n        results = {\n            \"total\": 0,\n            \"passed\": 0,\n            \"failed\": 0,\n            \"criteria\": []\n        }\n        \n        for file_path in criteria_files:\n            try:\n                criteria = await self._parse_criteria_file(file_path)\n                \n                for criterion in criteria:\n                    results[\"total\"] += 1\n                    \n                    # Validate individual criterion\n                    validation_result = await self._validate_criterion(criterion, target_path)\n                    \n                    if validation_result[\"passed\"]:\n                        results[\"passed\"] += 1\n                    else:\n                        results[\"failed\"] += 1\n                    \n                    results[\"criteria\"].append({\n                        \"file\": file_path,\n                        \"criterion\": criterion,\n                        \"result\": validation_result\n                    })\n            \n            except Exception as e:\n                results[\"criteria\"].append({\n                    \"file\": file_path,\n                    \"error\": str(e)\n                })\n        \n        return results\n    \n    async def _parse_criteria_file(self, file_path: str) -> List[Dict[str, Any]]:\n        \"\"\"Parse acceptance criteria file.\"\"\"\n        path = Path(file_path)\n        \n        if path.suffix == '.json':\n            with open(file_path, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n                return data.get(\"criteria\", [])\n        else:\n            # Parse markdown file (simplified)\n            with open(file_path, 'r', encoding='utf-8') as f:\n                content = f.read()\n            \n            # Simple parsing - look for criteria patterns\n            criteria = []\n            lines = content.split('\\n')\n            \n            for line in lines:\n                line = line.strip()\n                if line.startswith('- ') or line.startswith('* '):\n                    criteria.append({\n                        \"description\": line[2:],\n                        \"type\": \"functional\"\n                    })\n                elif line.startswith('##') or line.startswith('###'):\n                    criteria.append({\n                        \"description\": line[2:].strip(),\n                        \"type\": \"requirement\"\n                    })\n            \n            return criteria\n    \n    async def _validate_criterion(self, criterion: Dict[str, Any], target_path: str) -> Dict[str, Any]:\n        \"\"\"Validate individual acceptance criterion.\"\"\"\n        # This is a simplified validation - in practice, you'd have more sophisticated logic\n        description = criterion.get(\"description\", \"\")\n        \n        # Check if criterion is testable\n        testable_keywords = [\"should\", \"must\", \"will\", \"can\", \"able to\", \"support\", \"handle\"]\n        is_testable = any(keyword in description.lower() for keyword in testable_keywords)\n        \n        # Check if criterion has clear success criteria\n        has_success_criteria = any(word in description.lower() for word in [\"pass\", \"fail\", \"success\", \"error\", \"exception\"])\n        \n        return {\n            \"passed\": is_testable and has_success_criteria,\n            \"is_testable\": is_testable,\n            \"has_success_criteria\": has_success_criteria,\n            \"description\": description\n        }\n\n\n# ============================================================================\n# Main Agent Reviewer\n# ============================================================================\n\nclass AgentReviewer:\n    \"\"\"Main agent reviewer that orchestrates all review checks.\"\"\"\n    \n    def __init__(self, config: Optional[ReviewConfig] = None):\n        self.config = config or ReviewConfig()\n        self.logger = logging.getLogger(__name__)\n        \n        # Initialize checkers\n        self.checkers: List[ReviewCheckBase] = []\n        \n        if self.config.enable_schema_checks:\n            self.checkers.append(SchemaValidator(self.config))\n        \n        if self.config.enable_unit_checks:\n            self.checkers.append(UnitTestRunner(self.config))\n        \n        if self.config.enable_acceptance_checks:\n            self.checkers.append(AcceptanceCriteriaValidator(self.config))\n        \n        # Statistics\n        self.total_reviews = 0\n        self.successful_reviews = 0\n        self.failed_reviews = 0\n    \n    async def review(self, target_path: str, context: Optional[Dict[str, Any]] = None) -> ReviewReport:\n        \"\"\"Perform comprehensive review of the target.\"\"\"\n        context = context or {}\n        \n        self.logger.info(f\"Starting review of: {target_path}\")\n        \n        # Create report\n        report = ReviewReport(\n            target_path=target_path,\n            review_type=\"comprehensive\",\n            status=ReviewStatus.IN_PROGRESS,\n            metadata=context\n        )\n        \n        try:\n            # Run all checks\n            for checker in self.checkers:\n                self.logger.info(f\"Running {checker.get_check_name()} check\")\n                \n                check_result = await checker.check(target_path, context)\n                report.checks.append(check_result)\n            \n            # Update overall status\n            report.status = report.get_overall_status()\n            report.completed_at = datetime.now(timezone.utc)\n            \n            # Generate summary and recommendations\n            report.summary = report.get_summary_stats()\n            report.recommendations = await self._generate_recommendations(report)\n            \n            # Update statistics\n            self.total_reviews += 1\n            if report.status == ReviewStatus.PASSED:\n                self.successful_reviews += 1\n            else:\n                self.failed_reviews += 1\n            \n            self.logger.info(f\"Review completed with status: {report.status}\")\n            \n            return report\n            \n        except Exception as e:\n            self.logger.error(f\"Review failed: {e}\")\n            report.status = ReviewStatus.ERROR\n            report.completed_at = datetime.now(timezone.utc)\n            report.summary = {\"error\": str(e)}\n            \n            self.failed_reviews += 1\n            \n            return report\n    \n    async def _generate_recommendations(self, report: ReviewReport) -> List[str]:\n        \"\"\"Generate recommendations based on review results.\"\"\"\n        recommendations = []\n        \n        # Analyze check results\n        failed_checks = [check for check in report.checks if check.status == ReviewStatus.FAILED]\n        warning_checks = [check for check in report.checks if check.status == ReviewStatus.WARNING]\n        \n        # Schema recommendations\n        schema_checks = [check for check in failed_checks if check.check_type == CheckType.SCHEMA]\n        if schema_checks:\n            recommendations.append(\"Fix schema validation issues by ensuring all Pydantic models have proper field definitions and validators\")\n        \n        # Unit test recommendations\n        unit_checks = [check for check in failed_checks if check.check_type == CheckType.UNIT]\n        if unit_checks:\n            recommendations.append(\"Improve test coverage and fix failing unit tests\")\n        \n        # Acceptance criteria recommendations\n        acceptance_checks = [check for check in failed_checks if check.check_type == CheckType.ACCEPTANCE]\n        if acceptance_checks:\n            recommendations.append(\"Define clear acceptance criteria with testable requirements\")\n        \n        # General recommendations\n        if not failed_checks and not warning_checks:\n            recommendations.append(\"All checks passed! Consider adding more comprehensive tests for edge cases\")\n        \n        return recommendations\n    \n    async def save_report(self, report: ReviewReport, output_path: str) -> None:\n        \"\"\"Save review report to file.\"\"\"\n        try:\n            if self.config.output_format == \"json\":\n                with open(output_path, 'w', encoding='utf-8') as f:\n                    json.dump(report.dict(), f, indent=2, default=str)\n            else:\n                # For other formats, save as JSON for now\n                with open(output_path, 'w', encoding='utf-8') as f:\n                    json.dump(report.dict(), f, indent=2, default=str)\n            \n            self.logger.info(f\"Review report saved to: {output_path}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to save report: {e}\")\n            raise\n    \n    def get_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get reviewer statistics.\"\"\"\n        success_rate = (self.successful_reviews / self.total_reviews * 100) if self.total_reviews > 0 else 0\n        \n        return {\n            \"total_reviews\": self.total_reviews,\n            \"successful_reviews\": self.successful_reviews,\n            \"failed_reviews\": self.failed_reviews,\n            \"success_rate\": success_rate,\n            \"enabled_checks\": [checker.get_check_name() for checker in self.checkers]\n        }\n\n\n# ============================================================================\n# Factory Functions\n# ============================================================================\n\ndef create_agent_reviewer(\n    enable_schema: bool = True,\n    enable_unit: bool = True,\n    enable_acceptance: bool = True,\n    **kwargs\n) -> AgentReviewer:\n    \"\"\"Create an agent reviewer with specified configuration.\"\"\"\n    config = ReviewConfig(\n        enable_schema_checks=enable_schema,\n        enable_unit_checks=enable_unit,\n        enable_acceptance_checks=enable_acceptance,\n        **kwargs\n    )\n    return AgentReviewer(config)\n\n\ndef create_review_config(\n    schema_strict: bool = True,\n    test_coverage_threshold: float = 80.0,\n    test_timeout: float = 300.0,\n    **kwargs\n) -> ReviewConfig:\n    \"\"\"Create a review configuration.\"\"\"\n    return ReviewConfig(\n        schema_strict_mode=schema_strict,\n        test_coverage_threshold=test_coverage_threshold,\n        test_timeout=test_timeout,\n        **kwargs\n    )\n\n\n# ============================================================================\n# Export all classes and functions\n# ============================================================================\n\n__all__ = [\n    # Enums\n    \"ReviewStatus\",\n    \"CheckType\",\n    \n    # Models\n    \"ReviewCheck\",\n    \"ReviewReport\",\n    \"ReviewConfig\",\n    \n    # Check implementations\n    \"ReviewCheckBase\",\n    \"SchemaValidator\",\n    \"UnitTestRunner\",\n    \"AcceptanceCriteriaValidator\",\n    \n    # Main reviewer\n    \"AgentReviewer\",\n    \n    # Factory functions\n    \"create_agent_reviewer\",\n    \"create_review_config\",\n]\n", "source": "src/core/runtime/reviewer.py"}, {"instruction": "Explain the following code from src/core/mcp/test_runner.py:", "response": "This code is from src/core/mcp/test_runner.py:\n\n\"\"\"\nMCP Test Runner for Agentic LLM Core v0.1\n\nThis module provides comprehensive testing capabilities for MCP tools and servers.\n\nCreated: 2024-09-24\nStatus: Draft\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport time\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Union\nfrom uuid import uuid4\n\nimport aiohttp\nimport jsonschema\nfrom pydantic import BaseModel, Field, validator\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n\n# ============================================================================\n# Data Models\n# ============================================================================\n\nclass TestStatus(str, Enum):\n    \"\"\"Test execution status.\"\"\"\n    PENDING = \"pending\"\n    RUNNING = \"running\"\n    PASSED = \"passed\"\n    FAILED = \"failed\"\n    ERROR = \"error\"\n    SKIPPED = \"skipped\"\n\n\nclass TestResult(BaseModel):\n    \"\"\"Test execution result.\"\"\"\n    test_id: str = Field(default_factory=lambda: str(uuid4()), description=\"Unique test identifier\")\n    test_name: str = Field(..., description=\"Test name\")\n    status: TestStatus = Field(default=TestStatus.PENDING, description=\"Test status\")\n    start_time: Optional[datetime] = Field(None, description=\"Test start time\")\n    end_time: Optional[datetime] = Field(None, description=\"Test end time\")\n    duration_ms: Optional[float] = Field(None, description=\"Test duration in milliseconds\")\n    input_data: Dict[str, Any] = Field(default_factory=dict, description=\"Test input data\")\n    output_data: Optional[Dict[str, Any]] = Field(None, description=\"Test output data\")\n    error_message: Optional[str] = Field(None, description=\"Error message if test failed\")\n    validation_results: Dict[str, Any] = Field(default_factory=dict, description=\"Validation results\")\n    retry_count: int = Field(default=0, description=\"Number of retries attempted\")\n    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Additional test metadata\")\n\n\nclass TestConfig(BaseModel):\n    \"\"\"Test configuration.\"\"\"\n    timeout: float = Field(default=30.0, description=\"Test timeout in seconds\")\n    retries: int = Field(default=3, description=\"Number of retries on failure\")\n    validate_schema: bool = Field(default=True, description=\"Enable schema validation\")\n    validate_response: bool = Field(default=True, description=\"Enable response validation\")\n    mock_mode: bool = Field(default=False, description=\"Run in mock mode\")\n    verbose: bool = Field(default=False, description=\"Verbose output\")\n\n\nclass SupabaseTestInput(BaseModel):\n    \"\"\"Supabase test input data.\"\"\"\n    method: str = Field(..., description=\"HTTP method\")\n    endpoint: str = Field(..., description=\"API endpoint\")\n    headers: Dict[str, str] = Field(default_factory=dict, description=\"HTTP headers\")\n    params: Dict[str, Any] = Field(default_factory=dict, description=\"Query parameters\")\n    body: Optional[Dict[str, Any]] = Field(None, description=\"Request body\")\n    timeout: int = Field(default=30, description=\"Request timeout\")\n\n\nclass SupabaseTestResponse(BaseModel):\n    \"\"\"Supabase test response.\"\"\"\n    status: str = Field(..., description=\"Response status\")\n    data: List[Dict[str, Any]] = Field(default_factory=list, description=\"Response data\")\n    message: Optional[str] = Field(None, description=\"Response message\")\n    status_code: int = Field(..., description=\"HTTP status code\")\n    headers: Dict[str, str] = Field(default_factory=dict, description=\"Response headers\")\n\n\n# ============================================================================\n# Schema Validator\n# ============================================================================\n\nclass SchemaValidator:\n    \"\"\"JSON Schema validator for test responses.\"\"\"\n    \n    def __init__(self):\n        self.logger = logging.getLogger(__name__)\n    \n    def validate_response(self, response_data: Dict[str, Any], schema: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate response data against schema.\"\"\"\n        try:\n            jsonschema.validate(response_data, schema)\n            return {\n                \"valid\": True,\n                \"errors\": [],\n                \"message\": \"Schema validation passed\"\n            }\n        except jsonschema.ValidationError as e:\n            return {\n                \"valid\": False,\n                \"errors\": [str(e)],\n                \"message\": f\"Schema validation failed: {e.message}\"\n            }\n        except Exception as e:\n            return {\n                \"valid\": False,\n                \"errors\": [str(e)],\n                \"message\": f\"Schema validation error: {str(e)}\"\n            }\n    \n    def validate_input(self, input_data: Dict[str, Any], schema: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate input data against schema.\"\"\"\n        return self.validate_response(input_data, schema)\n\n\n# ============================================================================\n# Supabase REST Test Runner\n# ============================================================================\n\nclass SupabaseRestTestRunner:\n    \"\"\"Test runner for Supabase REST API.\"\"\"\n    \n    def __init__(self, config: TestConfig):\n        self.config = config\n        self.logger = logging.getLogger(__name__)\n        self.schema_validator = SchemaValidator()\n    \n    async def run_test(self, test_input: SupabaseTestInput, expected_schema: Optional[Dict[str, Any]] = None, input_schema: Optional[Dict[str, Any]] = None) -> TestResult:\n        \"\"\"Run a Supabase REST test.\"\"\"\n        test_result = TestResult(\n            test_name=\"supabase-rest:postgrestRequest\",\n            input_data=test_input.model_dump()\n        )\n        \n        try:\n            test_result.start_time = datetime.now(timezone.utc)\n            self.logger.info(f\"Starting test: {test_result.test_name}\")\n            \n            # Validate input if schema provided\n            if input_schema and self.config.validate_schema:\n                input_validation = self.schema_validator.validate_input(\n                    test_input.model_dump(), \n                    input_schema\n                )\n                test_result.validation_results[\"input_validation\"] = input_validation\n                \n                if not input_validation[\"valid\"]:\n                    test_result.status = TestStatus.FAILED\n                    test_result.error_message = f\"Input validation failed: {input_validation['message']}\"\n                    return test_result\n            \n            # Execute test\n            if self.config.mock_mode:\n                output_data = await self._mock_request(test_input)\n            else:\n                output_data = await self._execute_request(test_input)\n            \n            test_result.output_data = output_data\n            \n            # Validate response if schema provided\n            if expected_schema and self.config.validate_response:\n                response_validation = self.schema_validator.validate_response(\n                    output_data, \n                    expected_schema\n                )\n                test_result.validation_results[\"response_validation\"] = response_validation\n                \n                if not response_validation[\"valid\"]:\n                    test_result.status = TestStatus.FAILED\n                    test_result.error_message = f\"Response validation failed: {response_validation['message']}\"\n                    return test_result\n            \n            # Check response status\n            if output_data.get(\"status_code\", 0) >= 200 and output_data.get(\"status_code\", 0) < 300:\n                test_result.status = TestStatus.PASSED\n                test_result.metadata[\"message\"] = \"Test passed successfully\"\n            else:\n                test_result.status = TestStatus.FAILED\n                test_result.error_message = f\"HTTP error: {output_data.get('status_code', 'unknown')}\"\n            \n        except Exception as e:\n            test_result.status = TestStatus.ERROR\n            test_result.error_message = str(e)\n            self.logger.error(f\"Test execution error: {e}\")\n        \n        finally:\n            test_result.end_time = datetime.now(timezone.utc)\n            if test_result.start_time:\n                duration = (test_result.end_time - test_result.start_time).total_seconds() * 1000\n                test_result.duration_ms = duration\n        \n        return test_result\n    \n    async def _execute_request(self, test_input: SupabaseTestInput) -> Dict[str, Any]:\n        \"\"\"Execute actual HTTP request.\"\"\"\n        try:\n            async with aiohttp.ClientSession() as session:\n                # Prepare request\n                url = f\"https://test-project.supabase.co{test_input.endpoint}\"\n                \n                # Add query parameters\n                params = test_input.params if test_input.params else {}\n                \n                # Make request\n                async with session.request(\n                    method=test_input.method,\n                    url=url,\n                    headers=test_input.headers,\n                    params=params,\n                    json=test_input.body,\n                    timeout=aiohttp.ClientTimeout(total=test_input.timeout)\n                ) as response:\n                    # Parse response\n                    response_data = await response.json() if response.content_type == 'application/json' else {}\n                    \n                    return {\n                        \"status\": \"success\" if response.status < 400 else \"error\",\n                        \"data\": response_data,\n                        \"message\": f\"Request completed with status {response.status}\",\n                        \"status_code\": response.status,\n                        \"headers\": dict(response.headers)\n                    }\n                    \n        except aiohttp.ClientError as e:\n            return {\n                \"status\": \"error\",\n                \"data\": [],\n                \"message\": f\"Client error: {str(e)}\",\n                \"status_code\": 0,\n                \"headers\": {}\n            }\n        except Exception as e:\n            return {\n                \"status\": \"error\",\n                \"data\": [],\n                \"message\": f\"Request error: {str(e)}\",\n                \"status_code\": 0,\n                \"headers\": {}\n            }\n    \n    async def _mock_request(self, test_input: SupabaseTestInput) -> Dict[str, Any]:\n        \"\"\"Execute mock request for testing.\"\"\"\n        # Simulate network delay\n        await asyncio.sleep(0.1)\n        \n        # Return mock response based on endpoint\n        if test_input.endpoint == \"/rest/v1/\":\n            return {\n                \"status\": \"success\",\n                \"data\": [\n                    {\n                        \"table_name\": \"users\",\n                        \"table_schema\": \"public\",\n                        \"table_type\": \"BASE TABLE\"\n                    },\n                    {\n                        \"table_name\": \"posts\",\n                        \"table_schema\": \"public\",\n                        \"table_type\": \"BASE TABLE\"\n                    },\n                    {\n                        \"table_name\": \"comments\",\n                        \"table_schema\": \"public\",\n                        \"table_type\": \"BASE TABLE\"\n                    }\n                ],\n                \"message\": \"Tables retrieved successfully\",\n                \"status_code\": 200,\n                \"headers\": {\n                    \"Content-Type\": \"application/json\",\n                    \"X-Total-Count\": \"3\"\n                }\n            }\n        else:\n            return {\n                \"status\": \"error\",\n                \"data\": [],\n                \"message\": f\"Mock endpoint not found: {test_input.endpoint}\",\n                \"status_code\": 404,\n                \"headers\": {}\n            }\n\n\n# ============================================================================\n# MCP Test Manager\n# ============================================================================\n\nclass MCPTestManager:\n    \"\"\"Manages MCP tool tests.\"\"\"\n    \n    def __init__(self):\n        self.logger = logging.getLogger(__name__)\n        self.test_runners = {\n            \"supabase-rest\": SupabaseRestTestRunner\n        }\n    \n    def load_test_config(self, config_path: Union[str, Path]) -> Dict[str, Any]:\n        \"\"\"Load test configuration from file.\"\"\"\n        try:\n            with open(config_path, 'r', encoding='utf-8') as f:\n                config_data = json.load(f)\n            \n            self.logger.info(f\"Loaded test configuration from {config_path}\")\n            return config_data\n            \n        except FileNotFoundError:\n            raise FileNotFoundError(f\"Test configuration file not found: {config_path}\")\n        except json.JSONDecodeError as e:\n            raise ValueError(f\"Invalid JSON in test configuration: {e}\")\n        except Exception as e:\n            raise RuntimeError(f\"Failed to load test configuration: {e}\")\n    \n    async def run_test_from_config(self, config_path: Union[str, Path], mock_mode: bool = False) -> TestResult:\n        \"\"\"Run test from configuration file.\"\"\"\n        config_data = self.load_test_config(config_path)\n        \n        # Extract test information\n        test_name = config_data.get(\"test_name\", \"unknown\")\n        server_type = config_data.get(\"server_type\", \"unknown\")\n        tool_name = config_data.get(\"tool_name\", \"unknown\")\n        input_data = config_data.get(\"input_data\", {})\n        expected_response = config_data.get(\"expected_response\", {})\n        test_config_data = config_data.get(\"test_config\", {})\n        \n        # Create test configuration\n        test_config_data[\"mock_mode\"] = mock_mode\n        test_config = TestConfig(**test_config_data)\n        \n        # Get schemas\n        input_schema = config_data.get(\"input_schema\")\n        expected_schema = expected_response.get(\"schema\") if expected_response else None\n        \n        # Create appropriate test runner\n        if server_type not in self.test_runners:\n            raise ValueError(f\"Unsupported server type: {server_type}\")\n        \n        runner_class = self.test_runners[server_type]\n        runner = runner_class(test_config)\n        \n        # Create test input\n        if server_type == \"supabase-rest\":\n            test_input = SupabaseTestInput(**input_data)\n        else:\n            raise ValueError(f\"Unsupported server type for test input: {server_type}\")\n        \n        # Run test\n        self.logger.info(f\"Running test: {test_name}\")\n        result = await runner.run_test(test_input, expected_schema, input_schema)\n        \n        return result\n    \n    def print_test_result(self, result: TestResult):\n        \"\"\"Print test result in formatted way.\"\"\"\n        status_emoji = {\n            TestStatus.PASSED: \"\u2705\",\n            TestStatus.FAILED: \"\u274c\",\n            TestStatus.ERROR: \"\ud83d\udca5\",\n            TestStatus.SKIPPED: \"\u23ed\ufe0f\",\n            TestStatus.PENDING: \"\u23f3\",\n            TestStatus.RUNNING: \"\ud83d\udd04\"\n        }.get(result.status, \"\u2753\")\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"MCP TEST RESULT\")\n        print(\"=\"*80)\n        \n        print(f\"\\n{status_emoji} Test: {result.test_name}\")\n        print(f\"   Status: {result.status.value}\")\n        print(f\"   Duration: {result.duration_ms:.2f}ms\" if result.duration_ms else \"   Duration: N/A\")\n        print(f\"   Retries: {result.retry_count}\")\n        \n        if result.error_message:\n            print(f\"   Error: {result.error_message}\")\n        \n        # Print validation results\n        if result.validation_results:\n            print(f\"\\n\ud83d\udd0d Validation Results:\")\n            for validation_type, validation_result in result.validation_results.items():\n                validation_emoji = \"\u2705\" if validation_result.get(\"valid\", False) else \"\u274c\"\n                print(f\"   {validation_emoji} {validation_type}: {validation_result.get('message', 'N/A')}\")\n                if not validation_result.get(\"valid\", True) and validation_result.get(\"errors\"):\n                    for error in validation_result[\"errors\"]:\n                        print(f\"      - {error}\")\n        \n        # Print input data summary\n        if result.input_data:\n            print(f\"\\n\ud83d\udce5 Input Data:\")\n            for key, value in result.input_data.items():\n                if isinstance(value, dict) and len(str(value)) > 100:\n                    print(f\"   {key}: {type(value).__name__} ({len(value)} items)\")\n                else:\n                    print(f\"   {key}: {value}\")\n        \n        # Print output data summary\n        if result.output_data:\n            print(f\"\\n\ud83d\udce4 Output Data:\")\n            for key, value in result.output_data.items():\n                if isinstance(value, (list, dict)) and len(str(value)) > 100:\n                    print(f\"   {key}: {type(value).__name__} ({len(value)} items)\")\n                else:\n                    print(f\"   {key}: {value}\")\n        \n        print()\n\n\n# ============================================================================\n# Main Function\n# ============================================================================\n\nasync def main():\n    \"\"\"Main function for command-line usage.\"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"MCP Test Runner\")\n    parser.add_argument(\"--name\", required=True, help=\"Test name (server:tool)\")\n    parser.add_argument(\"--sample-input\", required=True, help=\"Path to sample input JSON file\")\n    parser.add_argument(\"--expect-schema\", action=\"store_true\", help=\"Expect schema validation\")\n    parser.add_argument(\"--mock\", action=\"store_true\", help=\"Run in mock mode\")\n    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Verbose output\")\n    \n    args = parser.parse_args()\n    \n    if args.verbose:\n        logging.getLogger().setLevel(logging.DEBUG)\n    \n    try:\n        # Create test manager\n        manager = MCPTestManager()\n        \n        # Run test\n        result = await manager.run_test_from_config(args.sample_input, mock_mode=args.mock)\n        \n        # Print results\n        manager.print_test_result(result)\n        \n        # Exit with appropriate code\n        if result.status == TestStatus.PASSED:\n            print(\"\ud83c\udf89 Test completed successfully!\")\n            return 0\n        else:\n            print(\"\ud83d\udca5 Test failed!\")\n            return 1\n            \n    except Exception as e:\n        logger.error(f\"Test execution failed: {e}\")\n        return 1\n\n\nif __name__ == \"__main__\":\n    exit(asyncio.run(main()))\n", "source": "src/core/mcp/test_runner.py"}, {"instruction": "Explain the following code from src/core/mcp/connection_manager.py:", "response": "This code is from src/core/mcp/connection_manager.py:\n\n\"\"\"\nMCP Connection Manager for Agentic LLM Core v0.1\n\nThis module provides secure MCP server connections with policy-based access control.\n\nCreated: 2024-09-24\nStatus: Draft\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport ssl\nimport subprocess\nimport uuid\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Set, Union\nfrom urllib.parse import urlparse\n\nimport aiohttp\nimport asyncpg\nfrom pydantic import BaseModel, Field, validator\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n\n# ============================================================================\n# Data Models\n# ============================================================================\n\nclass ServerType(str, Enum):\n    \"\"\"MCP server types.\"\"\"\n    SUPABASE_REST = \"supabase-rest\"\n    FILESYSTEM = \"filesystem\"\n    GIT = \"git\"\n    WEB = \"web\"\n    DATABASE = \"database\"\n    CUSTOM = \"custom\"\n\n\nclass ConnectionStatus(str, Enum):\n    \"\"\"Connection status.\"\"\"\n    DISCONNECTED = \"disconnected\"\n    CONNECTING = \"connecting\"\n    CONNECTED = \"connected\"\n    ERROR = \"error\"\n    AUTHENTICATING = \"authenticating\"\n\n\nclass SecurityPolicy(str, Enum):\n    \"\"\"Security policy types.\"\"\"\n    DENY_BY_DEFAULT = \"deny by default\"\n    ALLOW_BY_DEFAULT = \"allow by default\"\n    ALLOWLIST_TOOLS = \"allowlist tools\"\n    BLOCKLIST_TOOLS = \"blocklist tools\"\n\n\nclass MCPTool(BaseModel):\n    \"\"\"MCP tool definition.\"\"\"\n    name: str = Field(..., description=\"Tool name\")\n    description: str = Field(..., description=\"Tool description\")\n    server: str = Field(..., description=\"Server identifier\")\n    category: str = Field(..., description=\"Tool category\")\n    allowed: bool = Field(default=False, description=\"Whether tool is allowed\")\n    requires_auth: bool = Field(default=True, description=\"Whether tool requires authentication\")\n\n\nclass MCPServer(BaseModel):\n    \"\"\"MCP server configuration.\"\"\"\n    server_id: str = Field(..., description=\"Unique server identifier\")\n    server_type: ServerType = Field(..., description=\"Server type\")\n    name: str = Field(..., description=\"Human-readable server name\")\n    endpoint: str = Field(..., description=\"Server endpoint URL or path\")\n    auth_config: Optional[Dict[str, Any]] = Field(None, description=\"Authentication configuration\")\n    tools: List[MCPTool] = Field(default_factory=list, description=\"Available tools\")\n    status: ConnectionStatus = Field(default=ConnectionStatus.DISCONNECTED, description=\"Connection status\")\n    last_connected: Optional[datetime] = Field(None, description=\"Last connection timestamp\")\n    error_message: Optional[str] = Field(None, description=\"Last error message\")\n    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Additional metadata\")\n\n\nclass SecurityConfig(BaseModel):\n    \"\"\"Security configuration.\"\"\"\n    policy: SecurityPolicy = Field(default=SecurityPolicy.DENY_BY_DEFAULT, description=\"Security policy\")\n    allowlist_tools: Set[str] = Field(default_factory=set, description=\"Allowed tool names\")\n    blocklist_tools: Set[str] = Field(default_factory=set, description=\"Blocked tool names\")\n    allowed_servers: Set[str] = Field(default_factory=set, description=\"Allowed server IDs\")\n    blocked_servers: Set[str] = Field(default_factory=set, description=\"Blocked server IDs\")\n    require_verification: bool = Field(default=True, description=\"Require connection verification\")\n    max_connections: int = Field(default=10, description=\"Maximum concurrent connections\")\n    connection_timeout: float = Field(default=30.0, description=\"Connection timeout in seconds\")\n\n\nclass ConnectionResult(BaseModel):\n    \"\"\"Connection result.\"\"\"\n    server_id: str = Field(..., description=\"Server identifier\")\n    success: bool = Field(..., description=\"Whether connection succeeded\")\n    status: ConnectionStatus = Field(..., description=\"Connection status\")\n    message: str = Field(..., description=\"Result message\")\n    tools_discovered: int = Field(default=0, description=\"Number of tools discovered\")\n    verification_passed: bool = Field(default=False, description=\"Whether verification passed\")\n    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description=\"Connection timestamp\")\n\n\n# ============================================================================\n# Server Implementations\n# ============================================================================\n\nclass MCPServerConnector:\n    \"\"\"Base class for MCP server connectors.\"\"\"\n    \n    def __init__(self, server: MCPServer):\n        self.server = server\n        self.logger = logging.getLogger(f\"{__name__}.{server.server_id}\")\n    \n    async def connect(self) -> bool:\n        \"\"\"Connect to the server.\"\"\"\n        raise NotImplementedError\n    \n    async def disconnect(self) -> bool:\n        \"\"\"Disconnect from the server.\"\"\"\n        raise NotImplementedError\n    \n    async def discover_tools(self) -> List[MCPTool]:\n        \"\"\"Discover available tools.\"\"\"\n        raise NotImplementedError\n    \n    async def verify_connection(self) -> bool:\n        \"\"\"Verify the connection is working.\"\"\"\n        raise NotImplementedError\n\n\nclass SupabaseRestConnector(MCPServerConnector):\n    \"\"\"Supabase REST API connector.\"\"\"\n    \n    async def connect(self) -> bool:\n        \"\"\"Connect to Supabase REST API.\"\"\"\n        try:\n            self.server.status = ConnectionStatus.CONNECTING\n            self.logger.info(f\"Connecting to Supabase REST API: {self.server.endpoint}\")\n            \n            # Parse endpoint\n            parsed_url = urlparse(self.server.endpoint)\n            if not parsed_url.scheme:\n                self.server.endpoint = f\"https://{self.server.endpoint}\"\n            \n            # Test connection\n            async with aiohttp.ClientSession() as session:\n                async with session.get(\n                    f\"{self.server.endpoint}/rest/v1/\",\n                    headers=self._get_headers(),\n                    timeout=aiohttp.ClientTimeout(total=10)\n                ) as response:\n                    if response.status == 200:\n                        self.server.status = ConnectionStatus.CONNECTED\n                        self.server.last_connected = datetime.now(timezone.utc)\n                        self.logger.info(f\"Successfully connected to Supabase REST API\")\n                        return True\n                    else:\n                        self.server.status = ConnectionStatus.ERROR\n                        self.server.error_message = f\"HTTP {response.status}: {await response.text()}\"\n                        return False\n                        \n        except Exception as e:\n            self.server.status = ConnectionStatus.ERROR\n            self.server.error_message = str(e)\n            self.logger.error(f\"Failed to connect to Supabase REST API: {e}\")\n            return False\n    \n    async def disconnect(self) -> bool:\n        \"\"\"Disconnect from Supabase REST API.\"\"\"\n        self.server.status = ConnectionStatus.DISCONNECTED\n        self.logger.info(\"Disconnected from Supabase REST API\")\n        return True\n    \n    async def discover_tools(self) -> List[MCPTool]:\n        \"\"\"Discover Supabase REST tools.\"\"\"\n        tools = [\n            MCPTool(\n                name=\"supabase_query\",\n                description=\"Execute SQL queries on Supabase database\",\n                server=self.server.server_id,\n                category=\"database\",\n                allowed=False\n            ),\n            MCPTool(\n                name=\"supabase_insert\",\n                description=\"Insert data into Supabase tables\",\n                server=self.server.server_id,\n                category=\"database\",\n                allowed=False\n            ),\n            MCPTool(\n                name=\"supabase_update\",\n                description=\"Update data in Supabase tables\",\n                server=self.server.server_id,\n                category=\"database\",\n                allowed=False\n            ),\n            MCPTool(\n                name=\"supabase_delete\",\n                description=\"Delete data from Supabase tables\",\n                server=self.server.server_id,\n                category=\"database\",\n                allowed=False\n            ),\n            MCPTool(\n                name=\"supabase_auth\",\n                description=\"Authenticate with Supabase Auth\",\n                server=self.server.server_id,\n                category=\"auth\",\n                allowed=False\n            )\n        ]\n        self.server.tools = tools\n        return tools\n    \n    async def verify_connection(self) -> bool:\n        \"\"\"Verify Supabase connection.\"\"\"\n        try:\n            async with aiohttp.ClientSession() as session:\n                async with session.get(\n                    f\"{self.server.endpoint}/rest/v1/\",\n                    headers=self._get_headers(),\n                    timeout=aiohttp.ClientTimeout(total=5)\n                ) as response:\n                    return response.status == 200\n        except Exception:\n            return False\n    \n    def _get_headers(self) -> Dict[str, str]:\n        \"\"\"Get authentication headers.\"\"\"\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\"\n        }\n        \n        if self.server.auth_config:\n            if \"api_key\" in self.server.auth_config:\n                headers[\"apikey\"] = self.server.auth_config[\"api_key\"]\n            if \"bearer_token\" in self.server.auth_config:\n                headers[\"Authorization\"] = f\"Bearer {self.server.auth_config['bearer_token']}\"\n        \n        return headers\n\n\nclass FilesystemConnector(MCPServerConnector):\n    \"\"\"Filesystem connector.\"\"\"\n    \n    async def connect(self) -> bool:\n        \"\"\"Connect to filesystem.\"\"\"\n        try:\n            self.server.status = ConnectionStatus.CONNECTING\n            self.logger.info(f\"Connecting to filesystem: {self.server.endpoint}\")\n            \n            # Verify path exists and is accessible\n            path = Path(self.server.endpoint)\n            if not path.exists():\n                path.mkdir(parents=True, exist_ok=True)\n            \n            if not path.is_dir():\n                self.server.status = ConnectionStatus.ERROR\n                self.server.error_message = \"Endpoint must be a directory\"\n                return False\n            \n            # Test write access\n            test_file = path / f\".mcp_test_{uuid.uuid4().hex[:8]}\"\n            try:\n                test_file.write_text(\"test\")\n                test_file.unlink()\n            except PermissionError:\n                self.server.status = ConnectionStatus.ERROR\n                self.server.error_message = \"No write permission to filesystem\"\n                return False\n            \n            self.server.status = ConnectionStatus.CONNECTED\n            self.server.last_connected = datetime.now(timezone.utc)\n            self.logger.info(\"Successfully connected to filesystem\")\n            return True\n            \n        except Exception as e:\n            self.server.status = ConnectionStatus.ERROR\n            self.server.error_message = str(e)\n            self.logger.error(f\"Failed to connect to filesystem: {e}\")\n            return False\n    \n    async def disconnect(self) -> bool:\n        \"\"\"Disconnect from filesystem.\"\"\"\n        self.server.status = ConnectionStatus.DISCONNECTED\n        self.logger.info(\"Disconnected from filesystem\")\n        return True\n    \n    async def discover_tools(self) -> List[MCPTool]:\n        \"\"\"Discover filesystem tools.\"\"\"\n        tools = [\n            MCPTool(\n                name=\"fs_read\",\n                description=\"Read file contents\",\n                server=self.server.server_id,\n                category=\"file\",\n                allowed=False\n            ),\n            MCPTool(\n                name=\"fs_write\",\n                description=\"Write content to file\",\n                server=self.server.server_id,\n                category=\"file\",\n                allowed=False\n            ),\n            MCPTool(\n                name=\"fs_list\",\n                description=\"List directory contents\",\n                server=self.server.server_id,\n                category=\"file\",\n                allowed=False\n            ),\n            MCPTool(\n                name=\"fs_create_dir\",\n                description=\"Create directory\",\n                server=self.server.server_id,\n                category=\"file\",\n                allowed=False\n            ),\n            MCPTool(\n                name=\"fs_delete\",\n                description=\"Delete file or directory\",\n                server=self.server.server_id,\n                category=\"file\",\n                allowed=False\n            ),\n            MCPTool(\n                name=\"fs_move\",\n                description=\"Move or rename file/directory\",\n                server=self.server.server_id,\n                category=\"file\",\n                allowed=False\n            )\n        ]\n        self.server.tools = tools\n        return tools\n    \n    async def verify_connection(self) -> bool:\n        \"\"\"Verify filesystem connection.\"\"\"\n        try:\n            path = Path(self.server.endpoint)\n            return path.exists() and path.is_dir()\n        except Exception:\n            return False\n\n\nclass GitConnector(MCPServerConnector):\n    \"\"\"Git connector.\"\"\"\n    \n    async def connect(self) -> bool:\n        \"\"\"Connect to Git repository.\"\"\"\n        try:\n            self.server.status = ConnectionStatus.CONNECTING\n            self.logger.info(f\"Connecting to Git repository: {self.server.endpoint}\")\n            \n            # Verify git repository\n            path = Path(self.server.endpoint)\n            if not path.exists():\n                self.server.status = ConnectionStatus.ERROR\n                self.server.error_message = \"Git repository path does not exist\"\n                return False\n            \n            # Check if it's a git repository\n            git_dir = path / \".git\"\n            if not git_dir.exists():\n                self.server.status = ConnectionStatus.ERROR\n                self.server.error_message = \"Path is not a Git repository\"\n                return False\n            \n            # Test git command\n            try:\n                result = subprocess.run(\n                    [\"git\", \"status\", \"--porcelain\"],\n                    cwd=path,\n                    capture_output=True,\n                    text=True,\n                    timeout=10\n                )\n                if result.returncode != 0:\n                    self.server.status = ConnectionStatus.ERROR\n                    self.server.error_message = f\"Git command failed: {result.stderr}\"\n                    return False\n            except subprocess.TimeoutExpired:\n                self.server.status = ConnectionStatus.ERROR\n                self.server.error_message = \"Git command timed out\"\n                return False\n            \n            self.server.status = ConnectionStatus.CONNECTED\n            self.server.last_connected = datetime.now(timezone.utc)\n            self.logger.info(\"Successfully connected to Git repository\")\n            return True\n            \n        except Exception as e:\n            self.server.status = ConnectionStatus.ERROR\n            self.server.error_message = str(e)\n            self.logger.error(f\"Failed to connect to Git repository: {e}\")\n            return False\n    \n    async def disconnect(self) -> bool:\n        \"\"\"Disconnect from Git repository.\"\"\"\n        self.server.status = ConnectionStatus.DISCONNECTED\n        self.logger.info(\"Disconnected from Git repository\")\n        return True\n    \n    async def discover_tools(self) -> List[MCPTool]:\n        \"\"\"Discover Git tools.\"\"\"\n        tools = [\n            MCPTool(\n                name=\"git_status\",\n                description=\"Get Git repository status\",\n                server=self.server.server_id,\n                category=\"git\",\n                allowed=False\n            ),\n            MCPTool(\n                name=\"git_log\",\n                description=\"Get Git commit history\",\n                server=self.server.server_id,\n                category=\"git\",\n                allowed=False\n            ),\n            MCPTool(\n                name=\"git_diff\",\n                description=\"Get Git diff\",\n                server=self.server.server_id,\n                category=\"git\",\n                allowed=False\n            ),\n            MCPTool(\n                name=\"git_add\",\n                description=\"Stage files for commit\",\n                server=self.server.server_id,\n                category=\"git\",\n                allowed=False\n            ),\n            MCPTool(\n                name=\"git_commit\",\n                description=\"Commit staged changes\",\n                server=self.server.server_id,\n                category=\"git\",\n                allowed=False\n            ),\n            MCPTool(\n                name=\"git_push\",\n                description=\"Push commits to remote\",\n                server=self.server.server_id,\n                category=\"git\",\n                allowed=False\n            ),\n            MCPTool(\n                name=\"git_pull\",\n                description=\"Pull changes from remote\",\n                server=self.server.server_id,\n                category=\"git\",\n                allowed=False\n            )\n        ]\n        self.server.tools = tools\n        return tools\n    \n    async def verify_connection(self) -> bool:\n        \"\"\"Verify Git connection.\"\"\"\n        try:\n            path = Path(self.server.endpoint)\n            git_dir = path / \".git\"\n            return path.exists() and git_dir.exists()\n        except Exception:\n            return False\n\n\nclass WebConnector(MCPServerConnector):\n    \"\"\"Web connector.\"\"\"\n    \n    async def connect(self) -> bool:\n        \"\"\"Connect to web service.\"\"\"\n        try:\n            self.server.status = ConnectionStatus.CONNECTING\n            self.logger.info(f\"Connecting to web service: {self.server.endpoint}\")\n            \n            # Parse endpoint\n            parsed_url = urlparse(self.server.endpoint)\n            if not parsed_url.scheme:\n                self.server.endpoint = f\"https://{self.server.endpoint}\"\n            \n            # Test connection\n            async with aiohttp.ClientSession() as session:\n                async with session.get(\n                    self.server.endpoint,\n                    headers=self._get_headers(),\n                    timeout=aiohttp.ClientTimeout(total=10)\n                ) as response:\n                    if response.status in [200, 201, 202]:\n                        self.server.status = ConnectionStatus.CONNECTED\n                        self.server.last_connected = datetime.now(timezone.utc)\n                        self.logger.info(\"Successfully connected to web service\")\n                        return True\n                    else:\n                        self.server.status = ConnectionStatus.ERROR\n                        self.server.error_message = f\"HTTP {response.status}: {await response.text()}\"\n                        return False\n                        \n        except Exception as e:\n            self.server.status = ConnectionStatus.ERROR\n            self.server.error_message = str(e)\n            self.logger.error(f\"Failed to connect to web service: {e}\")\n            return False\n    \n    async def disconnect(self) -> bool:\n        \"\"\"Disconnect from web service.\"\"\"\n        self.server.status = ConnectionStatus.DISCONNECTED\n        self.logger.info(\"Disconnected from web service\")\n        return True\n    \n    async def discover_tools(self) -> List[MCPTool]:\n        \"\"\"Discover web tools.\"\"\"\n        tools = [\n            MCPTool(\n                name=\"web_get\",\n                description=\"Make HTTP GET request\",\n                server=self.server.server_id,\n                category=\"web\",\n                allowed=False\n            ),\n            MCPTool(\n                name=\"web_post\",\n                description=\"Make HTTP POST request\",\n                server=self.server.server_id,\n                category=\"web\",\n                allowed=False\n            ),\n            MCPTool(\n                name=\"web_put\",\n                description=\"Make HTTP PUT request\",\n                server=self.server.server_id,\n                category=\"web\",\n                allowed=False\n            ),\n            MCPTool(\n                name=\"web_delete\",\n                description=\"Make HTTP DELETE request\",\n                server=self.server.server_id,\n                category=\"web\",\n                allowed=False\n            ),\n            MCPTool(\n                name=\"web_scrape\",\n                description=\"Scrape web content\",\n                server=self.server.server_id,\n                category=\"web\",\n                allowed=False\n            )\n        ]\n        self.server.tools = tools\n        return tools\n    \n    async def verify_connection(self) -> bool:\n        \"\"\"Verify web connection.\"\"\"\n        try:\n            async with aiohttp.ClientSession() as session:\n                async with session.get(\n                    self.server.endpoint,\n                    headers=self._get_headers(),\n                    timeout=aiohttp.ClientTimeout(total=5)\n                ) as response:\n                    return response.status in [200, 201, 202]\n        except Exception:\n            return False\n    \n    def _get_headers(self) -> Dict[str, str]:\n        \"\"\"Get authentication headers.\"\"\"\n        headers = {\n            \"User-Agent\": \"Agentic-LLM-Core/0.1.0\",\n            \"Accept\": \"application/json, text/html, */*\"\n        }\n        \n        if self.server.auth_config:\n            if \"api_key\" in self.server.auth_config:\n                headers[\"X-API-Key\"] = self.server.auth_config[\"api_key\"]\n            if \"bearer_token\" in self.server.auth_config:\n                headers[\"Authorization\"] = f\"Bearer {self.server.auth_config['bearer_token']}\"\n        \n        return headers\n\n\n# ============================================================================\n# MCP Connection Manager\n# ============================================================================\n\nclass MCPConnectionManager:\n    \"\"\"Manages MCP server connections with security policies.\"\"\"\n    \n    def __init__(self, security_config: SecurityConfig):\n        self.security_config = security_config\n        self.servers: Dict[str, MCPServer] = {}\n        self.connectors: Dict[str, MCPServerConnector] = {}\n        self.logger = logging.getLogger(__name__)\n    \n    def register_server(self, server: MCPServer) -> bool:\n        \"\"\"Register a server.\"\"\"\n        # Extract server type from server_id for allowlist checking\n        server_type = server.server_id.split('_')[0]\n        \n        if self.security_config.policy == SecurityPolicy.DENY_BY_DEFAULT:\n            if server_type not in self.security_config.allowed_servers:\n                self.logger.warning(f\"Server type {server_type} not in allowlist, denying registration\")\n                return False\n        \n        if server_type in self.security_config.blocked_servers:\n            self.logger.warning(f\"Server type {server_type} is blocked, denying registration\")\n            return False\n        \n        self.servers[server.server_id] = server\n        self.logger.info(f\"Registered server: {server.name} ({server.server_id})\")\n        return True\n    \n    def _create_connector(self, server: MCPServer) -> MCPServerConnector:\n        \"\"\"Create appropriate connector for server type.\"\"\"\n        if server.server_type == ServerType.SUPABASE_REST:\n            return SupabaseRestConnector(server)\n        elif server.server_type == ServerType.FILESYSTEM:\n            return FilesystemConnector(server)\n        elif server.server_type == ServerType.GIT:\n            return GitConnector(server)\n        elif server.server_type == ServerType.WEB:\n            return WebConnector(server)\n        else:\n            raise ValueError(f\"Unsupported server type: {server.server_type}\")\n    \n    async def connect_server(self, server_id: str) -> ConnectionResult:\n        \"\"\"Connect to a specific server.\"\"\"\n        if server_id not in self.servers:\n            return ConnectionResult(\n                server_id=server_id,\n                success=False,\n                status=ConnectionStatus.ERROR,\n                message=f\"Server {server_id} not registered\"\n            )\n        \n        server = self.servers[server_id]\n        \n        try:\n            # Create connector\n            connector = self._create_connector(server)\n            self.connectors[server_id] = connector\n            \n            # Connect\n            success = await connector.connect()\n            \n            if success:\n                # Discover tools\n                tools = await connector.discover_tools()\n                \n                # Apply security policy to tools\n                self._apply_tool_policy(tools)\n                \n                # Verify connection if required\n                verification_passed = True\n                if self.security_config.require_verification:\n                    verification_passed = await connector.verify_connection()\n                \n                return ConnectionResult(\n                    server_id=server_id,\n                    success=True,\n                    status=ConnectionStatus.CONNECTED,\n                    message=f\"Successfully connected to {server.name}\",\n                    tools_discovered=len(tools),\n                    verification_passed=verification_passed\n                )\n            else:\n                return ConnectionResult(\n                    server_id=server_id,\n                    success=False,\n                    status=ConnectionStatus.ERROR,\n                    message=f\"Failed to connect: {server.error_message}\"\n                )\n                \n        except Exception as e:\n            self.logger.error(f\"Error connecting to server {server_id}: {e}\")\n            return ConnectionResult(\n                server_id=server_id,\n                success=False,\n                status=ConnectionStatus.ERROR,\n                message=f\"Connection error: {str(e)}\"\n            )\n    \n    async def connect_servers(self, server_ids: List[str]) -> List[ConnectionResult]:\n        \"\"\"Connect to multiple servers.\"\"\"\n        results = []\n        \n        # Limit concurrent connections\n        semaphore = asyncio.Semaphore(self.security_config.max_connections)\n        \n        async def connect_with_semaphore(server_id: str):\n            async with semaphore:\n                return await self.connect_server(server_id)\n        \n        # Connect to all servers concurrently\n        tasks = [connect_with_semaphore(server_id) for server_id in server_ids]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        \n        # Handle exceptions\n        processed_results = []\n        for i, result in enumerate(results):\n            if isinstance(result, Exception):\n                processed_results.append(ConnectionResult(\n                    server_id=server_ids[i],\n                    success=False,\n                    status=ConnectionStatus.ERROR,\n                    message=f\"Connection exception: {str(result)}\"\n                ))\n            else:\n                processed_results.append(result)\n        \n        return processed_results\n    \n    def _apply_tool_policy(self, tools: List[MCPTool]):\n        \"\"\"Apply security policy to tools.\"\"\"\n        for tool in tools:\n            if self.security_config.policy == SecurityPolicy.DENY_BY_DEFAULT:\n                # Deny by default, allow only if in allowlist\n                tool.allowed = tool.name in self.security_config.allowlist_tools\n            elif self.security_config.policy == SecurityPolicy.ALLOW_BY_DEFAULT:\n                # Allow by default, deny only if in blocklist\n                tool.allowed = tool.name not in self.security_config.blocklist_tools\n            else:\n                # Custom policy\n                tool.allowed = False\n    \n    def get_allowed_tools(self) -> List[MCPTool]:\n        \"\"\"Get all allowed tools across all servers.\"\"\"\n        allowed_tools = []\n        for server in self.servers.values():\n            allowed_tools.extend([tool for tool in server.tools if tool.allowed])\n        return allowed_tools\n    \n    def get_server_status(self) -> Dict[str, Dict[str, Any]]:\n        \"\"\"Get status of all servers.\"\"\"\n        status = {}\n        for server_id, server in self.servers.items():\n            status[server_id] = {\n                \"name\": server.name,\n                \"type\": server.server_type.value,\n                \"status\": server.status.value,\n                \"last_connected\": server.last_connected.isoformat() if server.last_connected else None,\n                \"error_message\": server.error_message,\n                \"tools_count\": len(server.tools),\n                \"allowed_tools_count\": len([t for t in server.tools if t.allowed])\n            }\n        return status\n    \n    async def disconnect_all(self):\n        \"\"\"Disconnect from all servers.\"\"\"\n        for connector in self.connectors.values():\n            await connector.disconnect()\n        self.connectors.clear()\n        self.logger.info(\"Disconnected from all servers\")\n\n\n# ============================================================================\n# Main Function\n# ============================================================================\n\nasync def main():\n    \"\"\"Main function for command-line usage.\"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"MCP Connection Manager\")\n    parser.add_argument(\"--servers\", required=True, help=\"Comma-separated list of servers to connect\")\n    parser.add_argument(\"--policy\", default=\"deny by default\", help=\"Security policy\")\n    parser.add_argument(\"--verify\", action=\"store_true\", help=\"Verify connections\")\n    parser.add_argument(\"--allowlist\", help=\"Comma-separated list of allowed tools\")\n    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Verbose output\")\n    \n    args = parser.parse_args()\n    \n    if args.verbose:\n        logging.getLogger().setLevel(logging.DEBUG)\n    \n    # Parse servers\n    server_list = [s.strip() for s in args.servers.split(\",\")]\n    \n    # Create security config\n    policy_mapping = {\n        \"deny by default\": SecurityPolicy.DENY_BY_DEFAULT,\n        \"allow by default\": SecurityPolicy.ALLOW_BY_DEFAULT,\n        \"allowlist tools\": SecurityPolicy.ALLOWLIST_TOOLS,\n        \"blocklist tools\": SecurityPolicy.BLOCKLIST_TOOLS\n    }\n    \n    policy = policy_mapping.get(args.policy.lower(), SecurityPolicy.DENY_BY_DEFAULT)\n    \n    security_config = SecurityConfig(\n        policy=policy,\n        require_verification=args.verify\n    )\n    \n    if args.allowlist:\n        security_config.allowlist_tools = set(s.strip() for s in args.allowlist.split(\",\"))\n    \n    # Allow all requested servers\n    for server_name in server_list:\n        security_config.allowed_servers.add(server_name)\n    \n    # Create connection manager\n    manager = MCPConnectionManager(security_config)\n    \n    # Register servers\n    servers_to_connect = []\n    for server_name in server_list:\n        server_id = f\"{server_name}_{uuid.uuid4().hex[:8]}\"\n        \n        if server_name == \"supabase-rest\":\n            server = MCPServer(\n                server_id=server_id,\n                server_type=ServerType.SUPABASE_REST,\n                name=\"Supabase REST API\",\n                endpoint=\"https://your-project.supabase.co\",\n                auth_config={\"api_key\": \"your-api-key\"}\n            )\n        elif server_name == \"filesystem\":\n            server = MCPServer(\n                server_id=server_id,\n                server_type=ServerType.FILESYSTEM,\n                name=\"Local Filesystem\",\n                endpoint=\"/tmp/mcp_workspace\"\n            )\n        elif server_name == \"git\":\n            server = MCPServer(\n                server_id=server_id,\n                server_type=ServerType.GIT,\n                name=\"Git Repository\",\n                endpoint=\".\"\n            )\n        elif server_name == \"web\":\n            server = MCPServer(\n                server_id=server_id,\n                server_type=ServerType.WEB,\n                name=\"Web Service\",\n                endpoint=\"https://httpbin.org\"\n            )\n        else:\n            print(f\"Unknown server type: {server_name}\")\n            continue\n        \n        if manager.register_server(server):\n            servers_to_connect.append(server_id)\n    \n    # Connect to servers\n    print(f\"Connecting to {len(servers_to_connect)} servers...\")\n    results = await manager.connect_servers(servers_to_connect)\n    \n    # Print results\n    print(\"\\n\" + \"=\"*80)\n    print(\"MCP CONNECTION RESULTS\")\n    print(\"=\"*80)\n    \n    for result in results:\n        status_emoji = \"\u2705\" if result.success else \"\u274c\"\n        print(f\"\\n{status_emoji} {result.server_id}\")\n        print(f\"   Status: {result.status.value}\")\n        print(f\"   Message: {result.message}\")\n        print(f\"   Tools Discovered: {result.tools_discovered}\")\n        if result.verification_passed is not None:\n            print(f\"   Verification: {'\u2705 Passed' if result.verification_passed else '\u274c Failed'}\")\n    \n    # Print allowed tools\n    allowed_tools = manager.get_allowed_tools()\n    print(f\"\\n\ud83d\udd27 Allowed Tools: {len(allowed_tools)}\")\n    for tool in allowed_tools:\n        print(f\"   - {tool.name} ({tool.server})\")\n    \n    # Print server status\n    print(f\"\\n\ud83d\udcca Server Status:\")\n    status = manager.get_server_status()\n    for server_id, info in status.items():\n        print(f\"   {server_id}: {info['status']} ({info['allowed_tools_count']}/{info['tools_count']} tools allowed)\")\n    \n    # Disconnect\n    await manager.disconnect_all()\n    print(\"\\n\ud83d\udd0c Disconnected from all servers\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n", "source": "src/core/mcp/connection_manager.py"}, {"instruction": "Explain the following code from src/core/models/policy_manager.py:", "response": "This code is from src/core/models/policy_manager.py:\n\n\"\"\"\nModel Policy Manager for Agentic LLM Core v0.1\n\nThis module implements intelligent model selection and routing based on policies.\n\nCreated: 2024-09-24\nStatus: Draft\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nimport time\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Union\nfrom uuid import uuid4\n\nimport psutil\nimport torch\nimport yaml\nfrom pydantic import BaseModel, Field, validator\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n\n# ============================================================================\n# Data Models\n# ============================================================================\n\nclass ModelType(str, Enum):\n    \"\"\"Model types.\"\"\"\n    MULTIMODAL = \"multimodal\"\n    TEXT_ONLY = \"text_only\"\n    EMBEDDING = \"embedding\"\n    CODE = \"code\"\n\n\nclass RoutingAction(str, Enum):\n    \"\"\"Routing actions.\"\"\"\n    USE_PRIMARY = \"use_primary\"\n    USE_FALLBACK = \"use_fallback\"\n    USE_PRIMARY_WITH_STREAMING = \"use_primary_with_streaming\"\n    USE_CACHED = \"use_cached\"\n    ERROR_RESPONSE = \"error_response\"\n\n\nclass ModelCapability(str, Enum):\n    \"\"\"Model capabilities.\"\"\"\n    TEXT_GENERATION = \"text_generation\"\n    TEXT_EMBEDDING = \"text_embedding\"\n    VISION_TO_TEXT = \"vision_to_text\"\n    MULTIMODAL_REASONING = \"multimodal_reasoning\"\n    CODE_GENERATION = \"code_generation\"\n    MATHEMATICAL_REASONING = \"mathematical_reasoning\"\n    INSTRUCTION_FOLLOWING = \"instruction_following\"\n\n\nclass ModelPerformance(BaseModel):\n    \"\"\"Model performance characteristics.\"\"\"\n    context_length: int = Field(..., description=\"Maximum context length\")\n    max_output_tokens: int = Field(..., description=\"Maximum output tokens\")\n    latency_ms: float = Field(..., description=\"Expected latency in milliseconds\")\n    memory_gb: float = Field(..., description=\"Memory requirement in GB\")\n    gpu_required: bool = Field(..., description=\"Whether GPU is required\")\n\n\nclass ModelOptimization(BaseModel):\n    \"\"\"Model optimization settings.\"\"\"\n    precision: str = Field(..., description=\"Precision (float16, int8, etc.)\")\n    quantization: str = Field(..., description=\"Quantization method\")\n    batch_size: int = Field(..., description=\"Batch size\")\n    use_cache: bool = Field(..., description=\"Whether to use caching\")\n\n\nclass ModelDefinition(BaseModel):\n    \"\"\"Model definition.\"\"\"\n    name: str = Field(..., description=\"Model name\")\n    type: ModelType = Field(..., description=\"Model type\")\n    description: str = Field(..., description=\"Model description\")\n    capabilities: List[ModelCapability] = Field(..., description=\"Model capabilities\")\n    performance: ModelPerformance = Field(..., description=\"Performance characteristics\")\n    optimization: ModelOptimization = Field(..., description=\"Optimization settings\")\n\n\nclass RoutingRule(BaseModel):\n    \"\"\"Routing rule definition.\"\"\"\n    name: str = Field(..., description=\"Rule name\")\n    condition: str = Field(..., description=\"Condition expression\")\n    action: RoutingAction = Field(..., description=\"Action to take\")\n    priority: int = Field(..., description=\"Rule priority (lower = higher priority)\")\n    description: str = Field(..., description=\"Rule description\")\n    parameters: Dict[str, Any] = Field(default_factory=dict, description=\"Action parameters\")\n\n\nclass ModelRequest(BaseModel):\n    \"\"\"Model request.\"\"\"\n    request_id: str = Field(default_factory=lambda: str(uuid4()), description=\"Request ID\")\n    input_type: str = Field(..., description=\"Input type (text, image, multimodal)\")\n    input_tokens: int = Field(..., description=\"Number of input tokens\")\n    output_tokens: int = Field(default=512, description=\"Expected output tokens\")\n    latency_requirement: Optional[float] = Field(None, description=\"Latency requirement in ms\")\n    memory_constraint: Optional[float] = Field(None, description=\"Memory constraint in GB\")\n    capabilities_required: List[ModelCapability] = Field(default_factory=list, description=\"Required capabilities\")\n    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Additional metadata\")\n\n\nclass ModelSelection(BaseModel):\n    \"\"\"Model selection result.\"\"\"\n    selected_model: str = Field(..., description=\"Selected model name\")\n    action: RoutingAction = Field(..., description=\"Routing action\")\n    confidence: float = Field(..., description=\"Selection confidence\")\n    reasoning: str = Field(..., description=\"Selection reasoning\")\n    parameters: Dict[str, Any] = Field(default_factory=dict, description=\"Action parameters\")\n    fallback_available: bool = Field(default=True, description=\"Whether fallback is available\")\n    estimated_latency: float = Field(..., description=\"Estimated latency in ms\")\n    estimated_memory: float = Field(..., description=\"Estimated memory usage in GB\")\n\n\nclass SystemMetrics(BaseModel):\n    \"\"\"System performance metrics.\"\"\"\n    available_memory_gb: float = Field(..., description=\"Available memory in GB\")\n    gpu_available: bool = Field(..., description=\"Whether GPU is available\")\n    gpu_memory_gb: Optional[float] = Field(None, description=\"GPU memory in GB\")\n    cpu_usage_percent: float = Field(..., description=\"CPU usage percentage\")\n    memory_usage_percent: float = Field(..., description=\"Memory usage percentage\")\n    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description=\"Metrics timestamp\")\n\n\n# ============================================================================\n# Model Policy Manager\n# ============================================================================\n\nclass ModelPolicyManager:\n    \"\"\"Manages model selection and routing policies.\"\"\"\n    \n    def __init__(self, config_path: Optional[Union[str, Path]] = None):\n        self.logger = logging.getLogger(__name__)\n        self.models: Dict[str, ModelDefinition] = {}\n        self.routing_rules: List[RoutingRule] = []\n        self.system_metrics: Optional[SystemMetrics] = None\n        \n        if config_path:\n            self.load_config(config_path)\n        else:\n            self._load_default_config()\n    \n    def load_config(self, config_path: Union[str, Path]):\n        \"\"\"Load model policy configuration.\"\"\"\n        try:\n            with open(config_path, 'r', encoding='utf-8') as f:\n                if str(config_path).endswith('.yaml') or str(config_path).endswith('.yml'):\n                    config_data = yaml.safe_load(f)\n                else:\n                    config_data = json.load(f)\n            \n            model_policy = config_data.get('model_policy', {})\n            \n            # Load models\n            models_config = model_policy.get('models', {})\n            for model_type, model_data in models_config.items():\n                model_def = ModelDefinition(**model_data)\n                self.models[model_type] = model_def\n            \n            # Load routing rules\n            rules_config = model_policy.get('routing_rules', [])\n            for rule_data in rules_config:\n                rule = RoutingRule(**rule_data)\n                self.routing_rules.append(rule)\n            \n            # Sort rules by priority\n            self.routing_rules.sort(key=lambda r: r.priority)\n            \n            self.logger.info(f\"Loaded model policy configuration from {config_path}\")\n            self.logger.info(f\"Loaded {len(self.models)} models and {len(self.routing_rules)} routing rules\")\n            \n        except FileNotFoundError:\n            raise FileNotFoundError(f\"Configuration file not found: {config_path}\")\n        except (json.JSONDecodeError, yaml.YAMLError) as e:\n            raise ValueError(f\"Invalid configuration format: {e}\")\n        except Exception as e:\n            raise RuntimeError(f\"Failed to load configuration: {e}\")\n    \n    def _load_default_config(self):\n        \"\"\"Load default configuration.\"\"\"\n        # Default models\n        self.models = {\n            \"primary\": ModelDefinition(\n                name=\"qwen3-omni\",\n                type=ModelType.MULTIMODAL,\n                description=\"Primary multimodal model\",\n                capabilities=[\n                    ModelCapability.TEXT_GENERATION,\n                    ModelCapability.VISION_TO_TEXT,\n                    ModelCapability.MULTIMODAL_REASONING\n                ],\n                performance=ModelPerformance(\n                    context_length=128000,\n                    max_output_tokens=4096,\n                    latency_ms=2000,\n                    memory_gb=16,\n                    gpu_required=True\n                ),\n                optimization=ModelOptimization(\n                    precision=\"float16\",\n                    quantization=\"none\",\n                    batch_size=1,\n                    use_cache=True\n                )\n            ),\n            \"fallback\": ModelDefinition(\n                name=\"qwen3-7b-instruct\",\n                type=ModelType.TEXT_ONLY,\n                description=\"Fallback text-only model\",\n                capabilities=[\n                    ModelCapability.TEXT_GENERATION,\n                    ModelCapability.INSTRUCTION_FOLLOWING\n                ],\n                performance=ModelPerformance(\n                    context_length=32000,\n                    max_output_tokens=2048,\n                    latency_ms=500,\n                    memory_gb=8,\n                    gpu_required=False\n                ),\n                optimization=ModelOptimization(\n                    precision=\"int8\",\n                    quantization=\"dynamic\",\n                    batch_size=4,\n                    use_cache=True\n                )\n            )\n        }\n        \n        # Default routing rules\n        self.routing_rules = [\n            RoutingRule(\n                name=\"multimodal_priority\",\n                condition=\"input_type in ['image', 'multimodal', 'vision']\",\n                action=RoutingAction.USE_PRIMARY,\n                priority=1,\n                description=\"Use primary model for multimodal inputs\"\n            ),\n            RoutingRule(\n                name=\"large_context_streaming\",\n                condition=\"input_tokens > 32000\",\n                action=RoutingAction.USE_PRIMARY_WITH_STREAMING,\n                priority=2,\n                description=\"Use primary model with streaming for large contexts\",\n                parameters={\"chunk_size\": 1024, \"overlap\": 128, \"stream_mode\": True}\n            ),\n            RoutingRule(\n                name=\"low_latency_fallback\",\n                condition=\"latency_requirement < 1000\",\n                action=RoutingAction.USE_FALLBACK,\n                priority=3,\n                description=\"Use fallback model for low latency requirements\"\n            ),\n            RoutingRule(\n                name=\"default_primary\",\n                condition=\"true\",\n                action=RoutingAction.USE_PRIMARY,\n                priority=10,\n                description=\"Default to primary model\"\n            )\n        ]\n    \n    async def update_system_metrics(self):\n        \"\"\"Update system performance metrics.\"\"\"\n        try:\n            # Get memory info\n            memory = psutil.virtual_memory()\n            available_memory_gb = memory.available / (1024**3)\n            memory_usage_percent = memory.percent\n            \n            # Get CPU info\n            cpu_usage_percent = psutil.cpu_percent(interval=1)\n            \n            # Check GPU availability\n            gpu_available = torch.cuda.is_available()\n            gpu_memory_gb = None\n            if gpu_available:\n                gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n            \n            self.system_metrics = SystemMetrics(\n                available_memory_gb=available_memory_gb,\n                gpu_available=gpu_available,\n                gpu_memory_gb=gpu_memory_gb,\n                cpu_usage_percent=cpu_usage_percent,\n                memory_usage_percent=memory_usage_percent\n            )\n            \n            self.logger.debug(f\"Updated system metrics: {self.system_metrics}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to update system metrics: {e}\")\n            # Use default metrics\n            self.system_metrics = SystemMetrics(\n                available_memory_gb=8.0,\n                gpu_available=False,\n                cpu_usage_percent=50.0,\n                memory_usage_percent=50.0\n            )\n    \n    def _evaluate_condition(self, condition: str, request: ModelRequest) -> bool:\n        \"\"\"Evaluate routing rule condition.\"\"\"\n        try:\n            # Create evaluation context\n            context = {\n                \"input_type\": request.input_type,\n                \"input_tokens\": request.input_tokens,\n                \"output_tokens\": request.output_tokens,\n                \"latency_requirement\": request.latency_requirement or 0,\n                \"memory_constraint\": request.memory_constraint or 0,\n                \"capabilities_required\": [cap.value for cap in request.capabilities_required]\n            }\n            \n            # Add system metrics if available\n            if self.system_metrics:\n                context.update({\n                    \"available_memory_gb\": self.system_metrics.available_memory_gb,\n                    \"gpu_available\": self.system_metrics.gpu_available,\n                    \"gpu_memory_gb\": self.system_metrics.gpu_memory_gb or 0,\n                    \"cpu_usage_percent\": self.system_metrics.cpu_usage_percent,\n                    \"memory_usage_percent\": self.system_metrics.memory_usage_percent\n                })\n            \n            # Evaluate condition safely\n            return eval(condition, {\"__builtins__\": {}}, context)\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to evaluate condition '{condition}': {e}\")\n            return False\n    \n    async def select_model(self, request: ModelRequest) -> ModelSelection:\n        \"\"\"Select appropriate model for request.\"\"\"\n        # Update system metrics\n        await self.update_system_metrics()\n        \n        # Evaluate routing rules\n        for rule in self.routing_rules:\n            if self._evaluate_condition(rule.condition, request):\n                self.logger.info(f\"Matched routing rule: {rule.name}\")\n                \n                # Determine selected model and action\n                if rule.action == RoutingAction.USE_PRIMARY:\n                    selected_model = self.models[\"primary\"].name\n                    action = RoutingAction.USE_PRIMARY\n                elif rule.action == RoutingAction.USE_FALLBACK:\n                    selected_model = self.models[\"fallback\"].name\n                    action = RoutingAction.USE_FALLBACK\n                elif rule.action == RoutingAction.USE_PRIMARY_WITH_STREAMING:\n                    selected_model = self.models[\"primary\"].name\n                    action = RoutingAction.USE_PRIMARY_WITH_STREAMING\n                else:\n                    selected_model = self.models[\"primary\"].name\n                    action = rule.action\n                \n                # Calculate confidence and reasoning\n                confidence = self._calculate_confidence(rule, request)\n                reasoning = self._generate_reasoning(rule, request)\n                \n                # Estimate performance\n                model_def = self.models[\"primary\"] if \"primary\" in selected_model else self.models[\"fallback\"]\n                estimated_latency = model_def.performance.latency_ms\n                estimated_memory = model_def.performance.memory_gb\n                \n                # Check fallback availability\n                fallback_available = self.models[\"fallback\"] is not None\n                \n                return ModelSelection(\n                    selected_model=selected_model,\n                    action=action,\n                    confidence=confidence,\n                    reasoning=reasoning,\n                    parameters=rule.parameters,\n                    fallback_available=fallback_available,\n                    estimated_latency=estimated_latency,\n                    estimated_memory=estimated_memory\n                )\n        \n        # Fallback if no rules match\n        self.logger.warning(\"No routing rules matched, using default\")\n        return ModelSelection(\n            selected_model=self.models[\"primary\"].name,\n            action=RoutingAction.USE_PRIMARY,\n            confidence=0.5,\n            reasoning=\"No specific rules matched, using default primary model\",\n            fallback_available=True,\n            estimated_latency=self.models[\"primary\"].performance.latency_ms,\n            estimated_memory=self.models[\"primary\"].performance.memory_gb\n        )\n    \n    def _calculate_confidence(self, rule: RoutingRule, request: ModelRequest) -> float:\n        \"\"\"Calculate selection confidence.\"\"\"\n        base_confidence = 0.8\n        \n        # Adjust based on rule priority (lower priority = higher confidence)\n        priority_factor = max(0.1, 1.0 - (rule.priority / 10.0))\n        \n        # Adjust based on system conditions\n        system_factor = 1.0\n        if self.system_metrics:\n            if rule.action == RoutingAction.USE_PRIMARY and not self.system_metrics.gpu_available:\n                system_factor *= 0.5\n            if self.system_metrics.available_memory_gb < 8:\n                system_factor *= 0.7\n        \n        return min(1.0, base_confidence * priority_factor * system_factor)\n    \n    def _generate_reasoning(self, rule: RoutingRule, request: ModelRequest) -> str:\n        \"\"\"Generate human-readable reasoning.\"\"\"\n        reasoning_parts = [f\"Rule '{rule.name}' matched\"]\n        \n        if rule.action == RoutingAction.USE_PRIMARY:\n            reasoning_parts.append(\"using primary model (qwen3-omni) for optimal capabilities\")\n        elif rule.action == RoutingAction.USE_FALLBACK:\n            reasoning_parts.append(\"using fallback model (qwen3-7b-instruct) for efficiency\")\n        elif rule.action == RoutingAction.USE_PRIMARY_WITH_STREAMING:\n            reasoning_parts.append(\"using primary model with streaming for large context\")\n        \n        if self.system_metrics:\n            if not self.system_metrics.gpu_available and rule.action == RoutingAction.USE_PRIMARY:\n                reasoning_parts.append(\"(note: GPU unavailable, may impact performance)\")\n        \n        return \". \".join(reasoning_parts) + \".\"\n    \n    def get_model_info(self, model_name: str) -> Optional[ModelDefinition]:\n        \"\"\"Get model information.\"\"\"\n        for model_def in self.models.values():\n            if model_def.name == model_name:\n                return model_def\n        return None\n    \n    def list_available_models(self) -> List[str]:\n        \"\"\"List available model names.\"\"\"\n        return [model_def.name for model_def in self.models.values()]\n    \n    def print_policy_summary(self):\n        \"\"\"Print policy summary.\"\"\"\n        print(\"\\n\" + \"=\"*80)\n        print(\"MODEL POLICY SUMMARY\")\n        print(\"=\"*80)\n        \n        print(f\"\\n\ud83e\udd16 Available Models ({len(self.models)}):\")\n        for model_type, model_def in self.models.items():\n            capability_emoji = \"\ud83c\udfaf\" if model_def.type == ModelType.MULTIMODAL else \"\ud83d\udcdd\"\n            gpu_emoji = \"\ud83d\udda5\ufe0f\" if model_def.performance.gpu_required else \"\ud83d\udcbb\"\n            \n            print(f\"   {capability_emoji} {model_def.name} ({model_type})\")\n            print(f\"     Type: {model_def.type.value}\")\n            print(f\"     Context: {model_def.performance.context_length:,} tokens\")\n            print(f\"     Latency: {model_def.performance.latency_ms:.0f}ms\")\n            print(f\"     Memory: {model_def.performance.memory_gb:.1f}GB\")\n            print(f\"     {gpu_emoji} GPU: {'Required' if model_def.performance.gpu_required else 'Optional'}\")\n            print(f\"     Capabilities: {', '.join([cap.value for cap in model_def.capabilities])}\")\n        \n        print(f\"\\n\ud83d\udd04 Routing Rules ({len(self.routing_rules)}):\")\n        for rule in self.routing_rules:\n            action_emoji = {\n                RoutingAction.USE_PRIMARY: \"\ud83c\udfaf\",\n                RoutingAction.USE_FALLBACK: \"\u26a1\",\n                RoutingAction.USE_PRIMARY_WITH_STREAMING: \"\ud83c\udf0a\",\n                RoutingAction.USE_CACHED: \"\ud83d\udcbe\",\n                RoutingAction.ERROR_RESPONSE: \"\u274c\"\n            }.get(rule.action, \"\u2753\")\n            \n            print(f\"   {action_emoji} {rule.name} (Priority: {rule.priority})\")\n            print(f\"     Condition: {rule.condition}\")\n            print(f\"     Action: {rule.action.value}\")\n            print(f\"     Description: {rule.description}\")\n        \n        if self.system_metrics:\n            print(f\"\\n\ud83d\udcca System Status:\")\n            print(f\"   Available Memory: {self.system_metrics.available_memory_gb:.1f}GB\")\n            print(f\"   GPU Available: {'\u2705' if self.system_metrics.gpu_available else '\u274c'}\")\n            print(f\"   CPU Usage: {self.system_metrics.cpu_usage_percent:.1f}%\")\n            print(f\"   Memory Usage: {self.system_metrics.memory_usage_percent:.1f}%\")\n\n\n# ============================================================================\n# Main Function\n# ============================================================================\n\nasync def main():\n    \"\"\"Main function for command-line usage.\"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"Model Policy Manager\")\n    parser.add_argument(\"--config\", help=\"Path to policy configuration file\")\n    parser.add_argument(\"--test-request\", help=\"Test request JSON\")\n    parser.add_argument(\"--list-models\", action=\"store_true\", help=\"List available models\")\n    parser.add_argument(\"--show-policy\", action=\"store_true\", help=\"Show policy summary\")\n    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Verbose output\")\n    \n    args = parser.parse_args()\n    \n    if args.verbose:\n        logging.getLogger().setLevel(logging.DEBUG)\n    \n    try:\n        # Create policy manager\n        manager = ModelPolicyManager(args.config)\n        \n        if args.list_models:\n            models = manager.list_available_models()\n            print(\"Available models:\")\n            for model in models:\n                print(f\"  - {model}\")\n        \n        if args.show_policy:\n            manager.print_policy_summary()\n        \n        if args.test_request:\n            # Load test request\n            with open(args.test_request, 'r') as f:\n                request_data = json.load(f)\n            \n            request = ModelRequest(**request_data)\n            \n            # Select model\n            selection = await manager.select_model(request)\n            \n            print(f\"\\n\ud83c\udfaf Model Selection Result:\")\n            print(f\"   Selected Model: {selection.selected_model}\")\n            print(f\"   Action: {selection.action.value}\")\n            print(f\"   Confidence: {selection.confidence:.2f}\")\n            print(f\"   Reasoning: {selection.reasoning}\")\n            print(f\"   Estimated Latency: {selection.estimated_latency:.0f}ms\")\n            print(f\"   Estimated Memory: {selection.estimated_memory:.1f}GB\")\n            if selection.parameters:\n                print(f\"   Parameters: {selection.parameters}\")\n        \n    except Exception as e:\n        logger.error(f\"Policy manager error: {e}\")\n        return 1\n    \n    return 0\n\n\nif __name__ == \"__main__\":\n    exit(asyncio.run(main()))\n", "source": "src/core/models/policy_manager.py"}, {"instruction": "Explain the following code from src/core/models/contracts.py:", "response": "This code is from src/core/models/contracts.py:\n\n\"\"\"\nContracts module for Agentic LLM Core v0.1\n\nThis module defines core data contracts and types for the system.\nNo business logic - types and schemas only.\n\nCreated: 2024-09-24\nStatus: Draft\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Union, Literal\nfrom uuid import UUID, uuid4\n\nfrom pydantic import BaseModel, Field, field_validator\n\n\n# ============================================================================\n# Core Enums\n# ============================================================================\n\nclass InputType(str, Enum):\n    \"\"\"Supported input types for the system.\"\"\"\n    TEXT = \"text\"\n    IMAGE = \"image\"\n    DOCUMENT = \"document\"\n\n\nclass OutputFormat(str, Enum):\n    \"\"\"Supported output formats.\"\"\"\n    TEXT = \"text\"\n    JSON = \"json\"\n    STRUCTURED = \"structured\"\n    VISUAL = \"visual\"\n\n\nclass TaskStatus(str, Enum):\n    \"\"\"Task execution status.\"\"\"\n    PENDING = \"pending\"\n    IN_PROGRESS = \"in_progress\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    CANCELLED = \"cancelled\"\n\n\nclass ToolExecutionStatus(str, Enum):\n    \"\"\"Tool execution status.\"\"\"\n    PENDING = \"pending\"\n    EXECUTING = \"executing\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    TIMEOUT = \"timeout\"\n\n\n# ============================================================================\n# Tool Specification Contracts\n# ============================================================================\n\nclass ToolParameter(BaseModel):\n    \"\"\"Definition of a tool parameter.\"\"\"\n    name: str = Field(..., description=\"Parameter name\")\n    type: str = Field(..., description=\"Parameter type\")\n    required: bool = Field(default=True, description=\"Whether parameter is required\")\n    description: Optional[str] = Field(None, description=\"Parameter description\")\n    default: Optional[Any] = Field(None, description=\"Default value\")\n    constraints: Optional[Dict[str, Any]] = Field(None, description=\"Parameter constraints\")\n\n\nclass ToolSchema(BaseModel):\n    \"\"\"Tool input/output schema definition.\"\"\"\n    input_schema: Dict[str, ToolParameter] = Field(..., description=\"Input parameter schema\")\n    output_schema: Dict[str, Any] = Field(..., description=\"Output schema definition\")\n    version: str = Field(default=\"1.0\", description=\"Schema version\")\n    description: Optional[str] = Field(None, description=\"Schema description\")\n\n\nclass ToolSpec(BaseModel):\n    \"\"\"Complete tool specification.\"\"\"\n    name: str = Field(..., description=\"Tool name\")\n    description: str = Field(..., description=\"Tool description\")\n    category: str = Field(..., description=\"Tool category\")\n    schema: ToolSchema = Field(..., description=\"Tool schema\")\n    version: str = Field(default=\"1.0\", description=\"Tool version\")\n    author: Optional[str] = Field(None, description=\"Tool author\")\n    tags: List[str] = Field(default_factory=list, description=\"Tool tags\")\n    requirements: List[str] = Field(default_factory=list, description=\"Tool requirements\")\n    timeout: Optional[int] = Field(None, description=\"Execution timeout in seconds\")\n    max_retries: int = Field(default=3, description=\"Maximum retry attempts\")\n    \n    @field_validator('name')\n    @classmethod\n    def validate_name(cls, v):\n        if not v or not v.strip():\n            raise ValueError(\"Tool name cannot be empty\")\n        if not v.replace('_', '').replace('-', '').isalnum():\n            raise ValueError(\"Tool name must be alphanumeric with underscores or hyphens\")\n        return v.strip().lower()\n\n\n# ============================================================================\n# Task Contracts\n# ============================================================================\n\nclass TaskDependency(BaseModel):\n    \"\"\"Task dependency definition.\"\"\"\n    task_id: UUID = Field(..., description=\"Dependent task ID\")\n    dependency_type: Literal[\"sequential\", \"parallel\", \"conditional\"] = Field(\n        default=\"sequential\", description=\"Dependency type\"\n    )\n    condition: Optional[str] = Field(None, description=\"Condition for conditional dependencies\")\n\n\nclass TaskMetadata(BaseModel):\n    \"\"\"Task metadata and configuration.\"\"\"\n    priority: int = Field(default=5, ge=1, le=10, description=\"Task priority (1-10)\")\n    timeout: Optional[int] = Field(None, description=\"Task timeout in seconds\")\n    retry_count: int = Field(default=0, ge=0, description=\"Current retry count\")\n    max_retries: int = Field(default=3, ge=0, description=\"Maximum retry attempts\")\n    tags: List[str] = Field(default_factory=list, description=\"Task tags\")\n    created_by: Optional[str] = Field(None, description=\"Task creator\")\n    environment: Optional[str] = Field(None, description=\"Execution environment\")\n\n\nclass Task(BaseModel):\n    \"\"\"Core task definition.\"\"\"\n    id: UUID = Field(default_factory=uuid4, description=\"Unique task ID\")\n    name: str = Field(..., description=\"Task name\")\n    description: Optional[str] = Field(None, description=\"Task description\")\n    task_type: str = Field(..., description=\"Task type identifier\")\n    status: TaskStatus = Field(default=TaskStatus.PENDING, description=\"Task status\")\n    \n    # Task execution\n    input_data: Dict[str, Any] = Field(..., description=\"Task input data\")\n    output_data: Optional[Dict[str, Any]] = Field(None, description=\"Task output data\")\n    error_message: Optional[str] = Field(None, description=\"Error message if failed\")\n    \n    # Task relationships\n    dependencies: List[TaskDependency] = Field(default_factory=list, description=\"Task dependencies\")\n    parent_task_id: Optional[UUID] = Field(None, description=\"Parent task ID\")\n    child_task_ids: List[UUID] = Field(default_factory=list, description=\"Child task IDs\")\n    \n    # Task metadata\n    metadata: TaskMetadata = Field(default_factory=TaskMetadata, description=\"Task metadata\")\n    \n    # Timestamps\n    created_at: datetime = Field(default_factory=datetime.utcnow, description=\"Creation timestamp\")\n    started_at: Optional[datetime] = Field(None, description=\"Start timestamp\")\n    completed_at: Optional[datetime] = Field(None, description=\"Completion timestamp\")\n    updated_at: datetime = Field(default_factory=datetime.utcnow, description=\"Last update timestamp\")\n    \n    @field_validator('name')\n    @classmethod\n    def validate_name(cls, v):\n        if not v or not v.strip():\n            raise ValueError(\"Task name cannot be empty\")\n        return v.strip()\n    \n    @field_validator('task_type')\n    @classmethod\n    def validate_task_type(cls, v):\n        if not v or not v.strip():\n            raise ValueError(\"Task type cannot be empty\")\n        return v.strip().lower()\n\n\n# ============================================================================\n# Task Graph Contracts\n# ============================================================================\n\nclass TaskGraphNode(BaseModel):\n    \"\"\"Node in a task graph.\"\"\"\n    task_id: UUID = Field(..., description=\"Task ID\")\n    level: int = Field(..., ge=0, description=\"Graph level\")\n    dependencies: List[UUID] = Field(default_factory=list, description=\"Dependency task IDs\")\n    dependents: List[UUID] = Field(default_factory=list, description=\"Dependent task IDs\")\n\n\nclass TaskGraphEdge(BaseModel):\n    \"\"\"Edge in a task graph.\"\"\"\n    from_task_id: UUID = Field(..., description=\"Source task ID\")\n    to_task_id: UUID = Field(..., description=\"Target task ID\")\n    edge_type: Literal[\"sequential\", \"parallel\", \"conditional\"] = Field(\n        default=\"sequential\", description=\"Edge type\"\n    )\n    condition: Optional[str] = Field(None, description=\"Condition for conditional edges\")\n\n\nclass TaskGraph(BaseModel):\n    \"\"\"Complete task graph definition.\"\"\"\n    id: UUID = Field(default_factory=uuid4, description=\"Graph ID\")\n    name: str = Field(..., description=\"Graph name\")\n    description: Optional[str] = Field(None, description=\"Graph description\")\n    \n    # Graph structure\n    nodes: List[TaskGraphNode] = Field(default_factory=list, description=\"Graph nodes\")\n    edges: List[TaskGraphEdge] = Field(default_factory=list, description=\"Graph edges\")\n    \n    # Graph metadata\n    root_task_ids: List[UUID] = Field(default_factory=list, description=\"Root task IDs\")\n    leaf_task_ids: List[UUID] = Field(default_factory=list, description=\"Leaf task IDs\")\n    max_depth: int = Field(default=0, ge=0, description=\"Maximum graph depth\")\n    \n    # Execution metadata\n    status: TaskStatus = Field(default=TaskStatus.PENDING, description=\"Graph execution status\")\n    created_at: datetime = Field(default_factory=datetime.utcnow, description=\"Creation timestamp\")\n    started_at: Optional[datetime] = Field(None, description=\"Start timestamp\")\n    completed_at: Optional[datetime] = Field(None, description=\"Completion timestamp\")\n    \n    @field_validator('name')\n    @classmethod\n    def validate_name(cls, v):\n        if not v or not v.strip():\n            raise ValueError(\"Graph name cannot be empty\")\n        return v.strip()\n\n\n# ============================================================================\n# Task Result Contracts\n# ============================================================================\n\nclass TaskExecutionMetrics(BaseModel):\n    \"\"\"Task execution metrics.\"\"\"\n    start_time: datetime = Field(..., description=\"Execution start time\")\n    end_time: Optional[datetime] = Field(None, description=\"Execution end time\")\n    duration: Optional[float] = Field(None, description=\"Execution duration in seconds\")\n    memory_usage: Optional[float] = Field(None, description=\"Memory usage in MB\")\n    cpu_usage: Optional[float] = Field(None, description=\"CPU usage percentage\")\n    error_count: int = Field(default=0, ge=0, description=\"Number of errors\")\n    retry_count: int = Field(default=0, ge=0, description=\"Number of retries\")\n    \n    @field_validator('duration')\n    @classmethod\n    def calculate_duration(cls, v, info):\n        if v is None and info.data.get('end_time') is not None:\n            start_time = info.data.get('start_time')\n            if start_time is not None:\n                return (info.data['end_time'] - start_time).total_seconds()\n        return v\n\n\nclass TaskValidationResult(BaseModel):\n    \"\"\"Task validation result.\"\"\"\n    is_valid: bool = Field(..., description=\"Whether task is valid\")\n    errors: List[str] = Field(default_factory=list, description=\"Validation errors\")\n    warnings: List[str] = Field(default_factory=list, description=\"Validation warnings\")\n    validated_at: datetime = Field(default_factory=datetime.utcnow, description=\"Validation timestamp\")\n\n\nclass TaskResult(BaseModel):\n    \"\"\"Complete task execution result.\"\"\"\n    task_id: UUID = Field(..., description=\"Task ID\")\n    status: TaskStatus = Field(..., description=\"Final task status\")\n    \n    # Execution results\n    output_data: Optional[Dict[str, Any]] = Field(None, description=\"Task output data\")\n    error_message: Optional[str] = Field(None, description=\"Error message if failed\")\n    error_code: Optional[str] = Field(None, description=\"Error code if failed\")\n    \n    # Execution metadata\n    metrics: TaskExecutionMetrics = Field(..., description=\"Execution metrics\")\n    validation_result: Optional[TaskValidationResult] = Field(\n        None, description=\"Task validation result\"\n    )\n    \n    # Result metadata\n    result_type: str = Field(..., description=\"Type of result\")\n    confidence_score: Optional[float] = Field(\n        None, ge=0.0, le=1.0, description=\"Result confidence score\"\n    )\n    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Additional metadata\")\n    \n    # Timestamps\n    created_at: datetime = Field(default_factory=datetime.utcnow, description=\"Result creation time\")\n    completed_at: datetime = Field(default_factory=datetime.utcnow, description=\"Completion time\")\n    \n    @field_validator('confidence_score')\n    @classmethod\n    def validate_confidence(cls, v):\n        if v is not None and (v < 0.0 or v > 1.0):\n            raise ValueError(\"Confidence score must be between 0.0 and 1.0\")\n        return v\n\n\n# ============================================================================\n# Utility Types\n# ============================================================================\n\nclass TaskTemplate(BaseModel):\n    \"\"\"Template for creating tasks.\"\"\"\n    name: str = Field(..., description=\"Template name\")\n    description: Optional[str] = Field(None, description=\"Template description\")\n    task_type: str = Field(..., description=\"Task type\")\n    input_schema: Dict[str, Any] = Field(..., description=\"Input schema\")\n    default_metadata: TaskMetadata = Field(\n        default_factory=TaskMetadata, description=\"Default task metadata\"\n    )\n    created_at: datetime = Field(default_factory=datetime.utcnow, description=\"Creation timestamp\")\n\n\nclass TaskBatch(BaseModel):\n    \"\"\"Batch of tasks for execution.\"\"\"\n    id: UUID = Field(default_factory=uuid4, description=\"Batch ID\")\n    name: str = Field(..., description=\"Batch name\")\n    description: Optional[str] = Field(None, description=\"Batch description\")\n    tasks: List[Task] = Field(..., description=\"Tasks in batch\")\n    batch_type: str = Field(default=\"parallel\", description=\"Batch execution type\")\n    created_at: datetime = Field(default_factory=datetime.utcnow, description=\"Creation timestamp\")\n    \n    @field_validator('tasks')\n    @classmethod\n    def validate_tasks(cls, v):\n        if not v:\n            raise ValueError(\"Batch must contain at least one task\")\n        return v\n\n\n# ============================================================================\n# MCP and Tool Execution Models\n# ============================================================================\n\nclass ToolCall(BaseModel):\n    \"\"\"Tool call request.\"\"\"\n    tool_name: str = Field(..., description=\"Name of the tool to call\")\n    parameters: Dict[str, Any] = Field(default_factory=dict, description=\"Tool parameters\")\n    call_id: str = Field(default_factory=lambda: str(uuid4()), description=\"Unique call identifier\")\n    timestamp: datetime = Field(default_factory=datetime.utcnow, description=\"Call timestamp\")\n    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Additional call metadata\")\n\n\nclass ToolResult(BaseModel):\n    \"\"\"Tool execution result.\"\"\"\n    call_id: str = Field(..., description=\"Corresponding tool call ID\")\n    success: bool = Field(..., description=\"Whether the tool call succeeded\")\n    result: Optional[Any] = Field(None, description=\"Tool execution result\")\n    error: Optional[str] = Field(None, description=\"Error message if failed\")\n    execution_time_ms: float = Field(0.0, description=\"Execution time in milliseconds\")\n    timestamp: datetime = Field(default_factory=datetime.utcnow, description=\"Result timestamp\")\n    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Additional result metadata\")\n\n\n# ============================================================================\n# Input/Output Processing Models\n# ============================================================================\n\nclass ProcessedInput(BaseModel):\n    \"\"\"Processed input data.\"\"\"\n    input_id: str = Field(default_factory=lambda: str(uuid4()), description=\"Unique input identifier\")\n    input_type: InputType = Field(..., description=\"Type of input\")\n    content: Union[str, bytes, Dict[str, Any]] = Field(..., description=\"Input content\")\n    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Input metadata\")\n    processed_at: datetime = Field(default_factory=datetime.utcnow, description=\"Processing timestamp\")\n    confidence_score: float = Field(default=1.0, ge=0.0, le=1.0, description=\"Processing confidence\")\n\n\nclass UnifiedContext(BaseModel):\n    \"\"\"Unified context from multiple inputs.\"\"\"\n    context_id: str = Field(default_factory=lambda: str(uuid4()), description=\"Unique context identifier\")\n    text_content: str = Field(default=\"\", description=\"Extracted text content\")\n    image_features: List[float] = Field(default_factory=list, description=\"Image feature vectors\")\n    document_content: str = Field(default=\"\", description=\"Document content\")\n    combined_embedding: List[float] = Field(default_factory=list, description=\"Combined embedding vector\")\n    confidence_scores: Dict[str, float] = Field(default_factory=dict, description=\"Confidence scores per component\")\n    created_at: datetime = Field(default_factory=datetime.utcnow, description=\"Context creation timestamp\")\n    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Additional context metadata\")\n\n\nclass FinalAnswer(BaseModel):\n    \"\"\"Final answer from the system.\"\"\"\n    answer_id: str = Field(default_factory=lambda: str(uuid4()), description=\"Unique answer identifier\")\n    answer: str = Field(..., description=\"The final answer text\")\n    confidence: float = Field(..., ge=0.0, le=1.0, description=\"Answer confidence score\")\n    tools_used: List[str] = Field(default_factory=list, description=\"Tools used in generation\")\n    processing_time_ms: float = Field(0.0, description=\"Total processing time in milliseconds\")\n    output_format: OutputFormat = Field(default=OutputFormat.TEXT, description=\"Output format\")\n    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Additional answer metadata\")\n    created_at: datetime = Field(default_factory=datetime.utcnow, description=\"Answer creation timestamp\")\n\n\n# ============================================================================\n# Export all contracts\n# ============================================================================\n\n__all__ = [\n    # Enums\n    \"InputType\",\n    \"OutputFormat\", \n    \"TaskStatus\",\n    \"ToolExecutionStatus\",\n    \n    # Tool contracts\n    \"ToolParameter\",\n    \"ToolSchema\", \n    \"ToolSpec\",\n    \n    # Task contracts\n    \"TaskDependency\",\n    \"TaskMetadata\",\n    \"Task\",\n    \n    # Task graph contracts\n    \"TaskGraphNode\",\n    \"TaskGraphEdge\", \n    \"TaskGraph\",\n    \n    # Task result contracts\n    \"TaskExecutionMetrics\",\n    \"TaskValidationResult\",\n    \"TaskResult\",\n    \n    # Utility types\n    \"TaskTemplate\",\n    \"TaskBatch\",\n    \n    # MCP and tool execution\n    \"ToolCall\",\n    \"ToolResult\",\n    \n    # Input/output processing\n    \"ProcessedInput\",\n    \"UnifiedContext\",\n    \"FinalAnswer\",\n]\n", "source": "src/core/models/contracts.py"}, {"instruction": "Explain the following code from src/core/vision/description_system.py:", "response": "This code is from src/core/vision/description_system.py:\n\n\"\"\"\nVision Description System for Agentic LLM Core v0.1\n\nThis module provides intelligent image analysis and description capabilities\nwith support for specific hints, token limits, and normalization.\n\nCreated: 2024-09-24\nStatus: Draft\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport base64\nimport logging\nimport re\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Union\nfrom uuid import uuid4\n\nimport torch\nfrom PIL import Image\nfrom pydantic import BaseModel, Field, validator\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n\n# ============================================================================\n# Data Models\n# ============================================================================\n\nclass AnalysisMode(str, Enum):\n    \"\"\"Analysis modes for vision description.\"\"\"\n    GENERAL = \"general\"\n    TECHNICAL = \"technical\"\n    DEFECT_DETECTION = \"defect_detection\"\n    QUALITY_INSPECTION = \"quality_inspection\"\n    COMPONENT_IDENTIFICATION = \"component_identification\"\n\n\nclass ImageFormat(str, Enum):\n    \"\"\"Supported image formats.\"\"\"\n    JPEG = \"jpeg\"\n    PNG = \"png\"\n    WEBP = \"webp\"\n    BMP = \"bmp\"\n    TIFF = \"tiff\"\n\n\nclass VisionDescriptionRequest(BaseModel):\n    \"\"\"Vision description request.\"\"\"\n    request_id: str = Field(default_factory=lambda: str(uuid4()), description=\"Request ID\")\n    image_path: str = Field(..., description=\"Path to the image file\")\n    hints: Optional[List[str]] = Field(None, description=\"Specific hints for analysis\")\n    max_tokens: int = Field(512, ge=50, le=2048, description=\"Maximum tokens in response\")\n    normalize: bool = Field(True, description=\"Normalize output format\")\n    analysis_mode: AnalysisMode = Field(AnalysisMode.GENERAL, description=\"Analysis mode\")\n    include_metadata: bool = Field(True, description=\"Include image metadata\")\n    confidence_threshold: float = Field(0.7, ge=0.0, le=1.0, description=\"Confidence threshold\")\n\n\nclass ImageMetadata(BaseModel):\n    \"\"\"Image metadata.\"\"\"\n    width: int = Field(..., description=\"Image width in pixels\")\n    height: int = Field(..., description=\"Image height in pixels\")\n    format: ImageFormat = Field(..., description=\"Image format\")\n    file_size_bytes: int = Field(..., description=\"File size in bytes\")\n    color_mode: str = Field(..., description=\"Color mode (RGB, RGBA, etc.)\")\n    has_transparency: bool = Field(False, description=\"Has transparency\")\n    dpi: Optional[tuple] = Field(None, description=\"DPI resolution\")\n\n\nclass DefectAnalysis(BaseModel):\n    \"\"\"Defect analysis results.\"\"\"\n    defect_type: str = Field(..., description=\"Type of defect\")\n    severity: str = Field(..., description=\"Severity level\")\n    location: str = Field(..., description=\"Location description\")\n    confidence: float = Field(..., description=\"Confidence score\")\n    ipc_class: Optional[str] = Field(None, description=\"IPC-A-610 class\")\n    recommendation: str = Field(..., description=\"Recommendation\")\n\n\nclass ComponentAnalysis(BaseModel):\n    \"\"\"Component analysis results.\"\"\"\n    component_type: str = Field(..., description=\"Type of component\")\n    identification: str = Field(..., description=\"Component identification\")\n    condition: str = Field(..., description=\"Component condition\")\n    location: str = Field(..., description=\"Location in image\")\n    confidence: float = Field(..., description=\"Confidence score\")\n\n\nclass VisionDescriptionResponse(BaseModel):\n    \"\"\"Vision description response.\"\"\"\n    response_id: str = Field(default_factory=lambda: str(uuid4()), description=\"Response ID\")\n    request_id: str = Field(..., description=\"Original request ID\")\n    description: str = Field(..., description=\"Main description\")\n    analysis_mode: AnalysisMode = Field(..., description=\"Analysis mode used\")\n    confidence: float = Field(..., description=\"Overall confidence score\")\n    token_count: int = Field(..., description=\"Actual token count\")\n    metadata: Optional[ImageMetadata] = Field(None, description=\"Image metadata\")\n    defects: List[DefectAnalysis] = Field(default_factory=list, description=\"Defect analysis\")\n    components: List[ComponentAnalysis] = Field(default_factory=list, description=\"Component analysis\")\n    technical_details: Dict[str, Any] = Field(default_factory=dict, description=\"Technical details\")\n    processing_time_ms: float = Field(..., description=\"Processing time in milliseconds\")\n    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description=\"Response timestamp\")\n\n\n# ============================================================================\n# Vision Description System\n# ============================================================================\n\nclass VisionDescriptionSystem:\n    \"\"\"Intelligent vision description system.\"\"\"\n    \n    def __init__(self, model_provider=None):\n        self.logger = logging.getLogger(__name__)\n        self.model_provider = model_provider\n        self.supported_formats = {ImageFormat.JPEG, ImageFormat.PNG, ImageFormat.WEBP, ImageFormat.BMP}\n        \n        # IPC-A-610 Class 3 defect patterns\n        self.ipc_defect_patterns = {\n            \"solder_bridge\": {\n                \"keywords\": [\"bridge\", \"short\", \"connection\", \"solder\"],\n                \"severity\": \"critical\",\n                \"ipc_class\": \"Class 3\"\n            },\n            \"insufficient_solder\": {\n                \"keywords\": [\"insufficient\", \"low\", \"solder\", \"joint\"],\n                \"severity\": \"major\",\n                \"ipc_class\": \"Class 3\"\n            },\n            \"excess_solder\": {\n                \"keywords\": [\"excess\", \"too much\", \"solder\", \"blob\"],\n                \"severity\": \"major\",\n                \"ipc_class\": \"Class 3\"\n            },\n            \"cold_solder_joint\": {\n                \"keywords\": [\"cold\", \"dull\", \"solder\", \"joint\"],\n                \"severity\": \"critical\",\n                \"ipc_class\": \"Class 3\"\n            },\n            \"component_misalignment\": {\n                \"keywords\": [\"misaligned\", \"crooked\", \"component\", \"placement\"],\n                \"severity\": \"major\",\n                \"ipc_class\": \"Class 3\"\n            },\n            \"missing_component\": {\n                \"keywords\": [\"missing\", \"absent\", \"component\", \"empty\"],\n                \"severity\": \"critical\",\n                \"ipc_class\": \"Class 3\"\n            },\n            \"damaged_trace\": {\n                \"keywords\": [\"damaged\", \"broken\", \"trace\", \"conductor\"],\n                \"severity\": \"critical\",\n                \"ipc_class\": \"Class 3\"\n            },\n            \"contamination\": {\n                \"keywords\": [\"contamination\", \"dirt\", \"foreign\", \"material\"],\n                \"severity\": \"minor\",\n                \"ipc_class\": \"Class 3\"\n            }\n        }\n        \n        # Component identification patterns\n        self.component_patterns = {\n            \"resistor\": {\n                \"keywords\": [\"resistor\", \"resistance\", \"ohm\", \"band\"],\n                \"identification\": \"R\"\n            },\n            \"capacitor\": {\n                \"keywords\": [\"capacitor\", \"cap\", \"electrolytic\", \"ceramic\"],\n                \"identification\": \"C\"\n            },\n            \"inductor\": {\n                \"keywords\": [\"inductor\", \"coil\", \"choke\", \"magnetic\"],\n                \"identification\": \"L\"\n            },\n            \"transistor\": {\n                \"keywords\": [\"transistor\", \"fet\", \"mosfet\", \"bjt\"],\n                \"identification\": \"Q\"\n            },\n            \"ic\": {\n                \"keywords\": [\"ic\", \"chip\", \"integrated\", \"circuit\"],\n                \"identification\": \"U\"\n            },\n            \"connector\": {\n                \"keywords\": [\"connector\", \"header\", \"socket\", \"plug\"],\n                \"identification\": \"J\"\n            },\n            \"diode\": {\n                \"keywords\": [\"diode\", \"led\", \"rectifier\", \"zener\"],\n                \"identification\": \"D\"\n            }\n        }\n    \n    async def describe_image(self, request: VisionDescriptionRequest) -> VisionDescriptionResponse:\n        \"\"\"Generate intelligent description of an image.\"\"\"\n        start_time = datetime.now()\n        \n        try:\n            # Validate image file\n            image_path = Path(request.image_path)\n            if not image_path.exists():\n                raise FileNotFoundError(f\"Image file not found: {request.image_path}\")\n            \n            # Load and analyze image\n            image_metadata = await self._extract_image_metadata(image_path)\n            \n            # Generate description based on mode and hints\n            if request.analysis_mode == AnalysisMode.DEFECT_DETECTION:\n                description, defects, confidence = await self._analyze_defects(image_path, request.hints)\n            elif request.analysis_mode == AnalysisMode.COMPONENT_IDENTIFICATION:\n                description, components, confidence = await self._identify_components(image_path, request.hints)\n            elif request.analysis_mode == AnalysisMode.TECHNICAL:\n                description, technical_details, confidence = await self._technical_analysis(image_path, request.hints)\n            else:\n                description, confidence = await self._general_analysis(image_path, request.hints)\n                defects = []\n                components = []\n                technical_details = {}\n            \n            # Normalize output if requested\n            if request.normalize:\n                description = self._normalize_description(description, request.max_tokens)\n            \n            # Count tokens (simplified estimation)\n            token_count = len(description.split())\n            \n            # Truncate if exceeds max_tokens\n            if token_count > request.max_tokens:\n                words = description.split()[:request.max_tokens]\n                description = \" \".join(words) + \"...\"\n                token_count = request.max_tokens\n            \n            processing_time = (datetime.now() - start_time).total_seconds() * 1000\n            \n            return VisionDescriptionResponse(\n                request_id=request.request_id,\n                description=description,\n                analysis_mode=request.analysis_mode,\n                confidence=confidence,\n                token_count=token_count,\n                metadata=image_metadata if request.include_metadata else None,\n                defects=defects if request.analysis_mode == AnalysisMode.DEFECT_DETECTION else [],\n                components=components if request.analysis_mode == AnalysisMode.COMPONENT_IDENTIFICATION else [],\n                technical_details=technical_details if request.analysis_mode == AnalysisMode.TECHNICAL else {},\n                processing_time_ms=processing_time\n            )\n            \n        except Exception as e:\n            self.logger.error(f\"Vision description failed: {e}\")\n            processing_time = (datetime.now() - start_time).total_seconds() * 1000\n            \n            return VisionDescriptionResponse(\n                request_id=request.request_id,\n                description=f\"Error analyzing image: {str(e)}\",\n                analysis_mode=request.analysis_mode,\n                confidence=0.0,\n                token_count=len(str(e).split()),\n                processing_time_ms=processing_time\n            )\n    \n    async def _extract_image_metadata(self, image_path: Path) -> ImageMetadata:\n        \"\"\"Extract metadata from image file.\"\"\"\n        try:\n            with Image.open(image_path) as img:\n                format_name = img.format.lower() if img.format else \"unknown\"\n                return ImageMetadata(\n                    width=img.width,\n                    height=img.height,\n                    format=ImageFormat(format_name) if format_name in [f.value for f in ImageFormat] else ImageFormat.JPEG,\n                    file_size_bytes=image_path.stat().st_size,\n                    color_mode=img.mode,\n                    has_transparency=img.mode in ['RGBA', 'LA', 'P'] and 'transparency' in img.info,\n                    dpi=img.info.get('dpi')\n                )\n        except Exception as e:\n            self.logger.warning(f\"Failed to extract metadata: {e}\")\n            return ImageMetadata(\n                width=0, height=0, format=ImageFormat.JPEG,\n                file_size_bytes=image_path.stat().st_size,\n                color_mode=\"unknown\"\n            )\n    \n    async def _analyze_defects(self, image_path: Path, hints: Optional[List[str]]) -> tuple[str, List[DefectAnalysis], float]:\n        \"\"\"Analyze image for defects, especially IPC-A-610 Class 3.\"\"\"\n        defects = []\n        description_parts = []\n        confidence_scores = []\n        \n        # Simulate defect analysis based on hints\n        if hints and any(\"IPC-A-610\" in hint for hint in hints):\n            description_parts.append(\"IPC-A-610 Class 3 compliance analysis:\")\n            \n            # Check for common Class 3 defects\n            for defect_type, pattern in self.ipc_defect_patterns.items():\n                if hints and any(keyword in \" \".join(hints).lower() for keyword in pattern[\"keywords\"]):\n                    defect = DefectAnalysis(\n                        defect_type=defect_type.replace(\"_\", \" \").title(),\n                        severity=pattern[\"severity\"],\n                        location=\"Multiple locations detected\",\n                        confidence=0.85,\n                        ipc_class=pattern[\"ipc_class\"],\n                        recommendation=f\"Address {defect_type.replace('_', ' ')} for Class 3 compliance\"\n                    )\n                    defects.append(defect)\n                    description_parts.append(f\"- {defect_type.replace('_', ' ').title()}: {pattern['severity']} severity\")\n                    confidence_scores.append(0.85)\n        \n        if not defects:\n            description_parts.append(\"No obvious defects detected in IPC-A-610 Class 3 analysis.\")\n            confidence_scores.append(0.7)\n        \n        description = \" \".join(description_parts)\n        avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0.7\n        \n        return description, defects, avg_confidence\n    \n    async def _identify_components(self, image_path: Path, hints: Optional[List[str]]) -> tuple[str, List[ComponentAnalysis], float]:\n        \"\"\"Identify components in the image.\"\"\"\n        components = []\n        description_parts = []\n        confidence_scores = []\n        \n        description_parts.append(\"Component identification analysis:\")\n        \n        # Simulate component identification\n        for comp_type, pattern in self.component_patterns.items():\n            if hints and any(keyword in \" \".join(hints).lower() for keyword in pattern[\"keywords\"]):\n                component = ComponentAnalysis(\n                    component_type=comp_type.title(),\n                    identification=pattern[\"identification\"],\n                    condition=\"Good\",\n                    location=\"Various locations\",\n                    confidence=0.8\n                )\n                components.append(component)\n                description_parts.append(f\"- {comp_type.title()} ({pattern['identification']}): Good condition\")\n                confidence_scores.append(0.8)\n        \n        if not components:\n            description_parts.append(\"Standard electronic components detected: resistors, capacitors, ICs, connectors.\")\n            confidence_scores.append(0.6)\n        \n        description = \" \".join(description_parts)\n        avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0.6\n        \n        return description, components, avg_confidence\n    \n    async def _technical_analysis(self, image_path: Path, hints: Optional[List[str]]) -> tuple[str, Dict[str, Any], float]:\n        \"\"\"Perform technical analysis of the image.\"\"\"\n        technical_details = {}\n        description_parts = []\n        \n        description_parts.append(\"Technical analysis:\")\n        \n        # Analyze image characteristics\n        with Image.open(image_path) as img:\n            technical_details = {\n                \"resolution\": f\"{img.width}x{img.height}\",\n                \"aspect_ratio\": round(img.width / img.height, 2),\n                \"color_depth\": img.mode,\n                \"file_format\": img.format\n            }\n            \n            description_parts.append(f\"Image resolution: {img.width}x{img.height}\")\n            description_parts.append(f\"Color mode: {img.mode}\")\n            \n            if hints and \"wiring\" in \" \".join(hints).lower():\n                description_parts.append(\"Wiring analysis: Circuit traces and connections visible.\")\n                technical_details[\"wiring_density\"] = \"Medium\"\n                technical_details[\"trace_visibility\"] = \"Good\"\n        \n        description = \" \".join(description_parts)\n        return description, technical_details, 0.8\n    \n    async def _general_analysis(self, image_path: Path, hints: Optional[List[str]]) -> tuple[str, float]:\n        \"\"\"Perform general image analysis.\"\"\"\n        description_parts = []\n        \n        with Image.open(image_path) as img:\n            description_parts.append(f\"Image analysis of {img.width}x{img.height} {img.format} image.\")\n            \n            if hints:\n                hint_text = \" \".join(hints)\n                description_parts.append(f\"Analysis focus: {hint_text}\")\n                \n                if \"wiring\" in hint_text.lower():\n                    description_parts.append(\"Electronic circuit board with visible wiring traces and components.\")\n                if \"defect\" in hint_text.lower():\n                    description_parts.append(\"Inspecting for potential manufacturing defects.\")\n                if \"quality\" in hint_text.lower():\n                    description_parts.append(\"Quality inspection analysis.\")\n        \n        description = \" \".join(description_parts)\n        return description, 0.7\n    \n    def _normalize_description(self, description: str, max_tokens: int) -> str:\n        \"\"\"Normalize description format.\"\"\"\n        # Clean up text\n        description = re.sub(r'\\s+', ' ', description.strip())\n        \n        # Ensure proper sentence structure\n        if not description.endswith('.'):\n            description += '.'\n        \n        # Capitalize first letter\n        description = description[0].upper() + description[1:]\n        \n        return description\n    \n    def create_sample_wiring_image(self, output_path: str = \"samples/wiring.jpg\"):\n        \"\"\"Create a sample wiring image for testing.\"\"\"\n        try:\n            # Create a simple circuit board representation\n            from PIL import Image, ImageDraw\n            \n            # Create image\n            width, height = 800, 600\n            img = Image.new('RGB', (width, height), color='white')\n            draw = ImageDraw.Draw(img)\n            \n            # Draw circuit board background\n            draw.rectangle([50, 50, width-50, height-50], fill='green', outline='black', width=2)\n            \n            # Draw traces\n            draw.line([100, 100, 300, 100], fill='gold', width=3)\n            draw.line([300, 100, 300, 200], fill='gold', width=3)\n            draw.line([300, 200, 500, 200], fill='gold', width=3)\n            draw.line([100, 300, 400, 300], fill='gold', width=3)\n            draw.line([400, 300, 400, 400], fill='gold', width=3)\n            \n            # Draw components\n            # Resistors\n            draw.rectangle([150, 80, 170, 120], fill='black', outline='gold', width=2)\n            draw.rectangle([250, 180, 270, 220], fill='black', outline='gold', width=2)\n            \n            # Capacitors\n            draw.rectangle([200, 280, 220, 320], fill='blue', outline='gold', width=2)\n            draw.rectangle([350, 280, 370, 320], fill='blue', outline='gold', width=2)\n            \n            # ICs\n            draw.rectangle([400, 150, 480, 250], fill='black', outline='gold', width=2)\n            \n            # Connectors\n            draw.rectangle([50, 250, 80, 350], fill='gray', outline='gold', width=2)\n            draw.rectangle([720, 250, 750, 350], fill='gray', outline='gold', width=2)\n            \n            # Add some potential defects\n            draw.circle([320, 120], 5, fill='red')  # Solder bridge\n            draw.rectangle([180, 180, 190, 190], fill='red')  # Excess solder\n            \n            # Save image\n            img.save(output_path, 'JPEG', quality=95)\n            self.logger.info(f\"Sample wiring image created: {output_path}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to create sample image: {e}\")\n\n\n# ============================================================================\n# Main Function\n# ============================================================================\n\nasync def main():\n    \"\"\"Main function for command-line usage.\"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"Vision Description System\")\n    parser.add_argument(\"--image\", help=\"Path to image file\")\n    parser.add_argument(\"--hints\", nargs=\"*\", help=\"Analysis hints\")\n    parser.add_argument(\"--max-tokens\", type=int, default=512, help=\"Maximum tokens\")\n    parser.add_argument(\"--normalize\", action=\"store_true\", help=\"Normalize output\")\n    parser.add_argument(\"--mode\", choices=[m.value for m in AnalysisMode], default=\"general\", help=\"Analysis mode\")\n    parser.add_argument(\"--create-sample\", action=\"store_true\", help=\"Create sample wiring image\")\n    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Verbose output\")\n    \n    args = parser.parse_args()\n    \n    if args.verbose:\n        logging.getLogger().setLevel(logging.DEBUG)\n    \n    try:\n        # Create vision system\n        vision_system = VisionDescriptionSystem()\n        \n        if args.create_sample:\n            vision_system.create_sample_wiring_image()\n            print(\"Sample wiring image created: samples/wiring.jpg\")\n            return 0\n        \n        if not args.image:\n            print(\"Error: --image argument is required when not creating sample\")\n            return 1\n        \n        # Create request\n        request = VisionDescriptionRequest(\n            image_path=args.image,\n            hints=args.hints,\n            max_tokens=args.max_tokens,\n            normalize=args.normalize,\n            analysis_mode=AnalysisMode(args.mode)\n        )\n        \n        # Generate description\n        response = await vision_system.describe_image(request)\n        \n        # Print results\n        print(\"\\n\" + \"=\"*80)\n        print(\"VISION DESCRIPTION RESULT\")\n        print(\"=\"*80)\n        print(f\"Request ID: {request.request_id}\")\n        print(f\"Response ID: {response.response_id}\")\n        print(f\"Analysis Mode: {response.analysis_mode.value}\")\n        print(f\"Confidence: {response.confidence:.2f}\")\n        print(f\"Token Count: {response.token_count}\")\n        print(f\"Processing Time: {response.processing_time_ms:.2f}ms\")\n        \n        if response.metadata:\n            print(f\"\\n\ud83d\udcca Image Metadata:\")\n            print(f\"   Resolution: {response.metadata.width}x{response.metadata.height}\")\n            print(f\"   Format: {response.metadata.format.value}\")\n            print(f\"   Size: {response.metadata.file_size_bytes:,} bytes\")\n            print(f\"   Color Mode: {response.metadata.color_mode}\")\n        \n        print(f\"\\n\ud83d\udcdd Description:\")\n        print(f\"   {response.description}\")\n        \n        if response.defects:\n            print(f\"\\n\ud83d\udd0d Defect Analysis ({len(response.defects)} defects):\")\n            for defect in response.defects:\n                print(f\"   - {defect.defect_type}: {defect.severity} severity\")\n                print(f\"     Location: {defect.location}\")\n                print(f\"     Confidence: {defect.confidence:.2f}\")\n                print(f\"     IPC Class: {defect.ipc_class}\")\n                print(f\"     Recommendation: {defect.recommendation}\")\n        \n        if response.components:\n            print(f\"\\n\ud83d\udd27 Component Analysis ({len(response.components)} components):\")\n            for component in response.components:\n                print(f\"   - {component.component_type} ({component.identification})\")\n                print(f\"     Condition: {component.condition}\")\n                print(f\"     Location: {component.location}\")\n                print(f\"     Confidence: {component.confidence:.2f}\")\n        \n        if response.technical_details:\n            print(f\"\\n\u2699\ufe0f Technical Details:\")\n            for key, value in response.technical_details.items():\n                print(f\"   - {key}: {value}\")\n        \n        return 0\n        \n    except Exception as e:\n        logger.error(f\"Vision description failed: {e}\")\n        return 1\n\n\nif __name__ == \"__main__\":\n    exit(asyncio.run(main()))\n", "source": "src/core/vision/description_system.py"}, {"instruction": "Explain the following code from src/core/schemas/__init__.py:", "response": "This code is from src/core/schemas/__init__.py:\n\n# Input schemas for Agentic LLM Core\nfrom .input_schemas import (\n    TextInput,\n    ImageInput,\n    DocumentInput,\n    ProcessedInput,\n    ValidationResult\n)\n\n__all__ = [\n    \"TextInput\",\n    \"ImageInput\", \n    \"DocumentInput\",\n    \"ProcessedInput\",\n    \"ValidationResult\"\n]\n", "source": "src/core/schemas/__init__.py"}, {"instruction": "Explain the following code from src/core/schemas/input_schemas.py:", "response": "This code is from src/core/schemas/input_schemas.py:\n\n#!/usr/bin/env python3\n\"\"\"\nInput Schemas for Agentic LLM Core\n\nThis module implements the Pydantic schemas for all input types as specified in\nmilestone_1_core_pipeline.md Task 1.1.1: Design Pydantic Input Schemas\n\nComplies with:\n- System Specification: Agentic LLM Core v0.1 (specs/system.md)\n- Milestone 1: Core Pipeline Foundation (tasks/milestone_1_core_pipeline.md)\n\"\"\"\n\nfrom pydantic import BaseModel, Field, validator\nfrom typing import Optional, Dict, Any, Union, List\nfrom datetime import datetime\nimport re\n\nclass TextInput(BaseModel):\n    \"\"\"Text input schema with validation rules\"\"\"\n    content: str = Field(..., max_length=10000, description=\"Text content to process\")\n    language: Optional[str] = Field(default=\"en\", description=\"Language of the text\")\n    encoding: str = Field(default=\"utf-8\", description=\"Text encoding\")\n    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict, description=\"Additional metadata\")\n    \n    @validator('content')\n    def validate_content(cls, v):\n        if not v or not v.strip():\n            raise ValueError(\"Content cannot be empty\")\n        return v.strip()\n    \n    @validator('language')\n    def validate_language(cls, v):\n        if v and not re.match(r'^[a-z]{2}(-[A-Z]{2})?$', v):\n            raise ValueError(\"Language must be in ISO 639-1 format (e.g., 'en', 'en-US')\")\n        return v\n\nclass ImageInput(BaseModel):\n    \"\"\"Image input schema with size/format constraints\"\"\"\n    data: bytes = Field(..., description=\"Image data as bytes\")\n    format: str = Field(..., pattern=\"^(jpg|jpeg|png|gif|webp)$\", description=\"Image format\")\n    max_size: tuple[int, int] = Field(default=(2048, 2048), description=\"Maximum image dimensions\")\n    quality: int = Field(default=85, ge=1, le=100, description=\"Image quality (1-100)\")\n    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict, description=\"Additional metadata\")\n    \n    @validator('data')\n    def validate_data_size(cls, v):\n        if len(v) > 10 * 1024 * 1024:  # 10MB limit\n            raise ValueError(\"Image data too large (max 10MB)\")\n        return v\n    \n    @validator('format')\n    def validate_format(cls, v):\n        allowed_formats = ['jpg', 'jpeg', 'png', 'gif', 'webp']\n        if v.lower() not in allowed_formats:\n            raise ValueError(f\"Format must be one of: {allowed_formats}\")\n        return v.lower()\n\nclass DocumentInput(BaseModel):\n    \"\"\"Document input schema with file type validation\"\"\"\n    file_path: str = Field(..., description=\"Path to the document file\")\n    file_type: str = Field(..., pattern=\"^(pdf|docx|txt|md)$\", description=\"Document type\")\n    max_size: int = Field(default=50 * 1024 * 1024, description=\"Maximum file size in bytes\")\n    encoding: str = Field(default=\"utf-8\", description=\"Text encoding for text files\")\n    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict, description=\"Additional metadata\")\n    \n    @validator('file_path')\n    def validate_file_path(cls, v):\n        if not v or not v.strip():\n            raise ValueError(\"File path cannot be empty\")\n        return v.strip()\n    \n    @validator('file_type')\n    def validate_file_type(cls, v):\n        allowed_types = ['pdf', 'docx', 'txt', 'md']\n        if v.lower() not in allowed_types:\n            raise ValueError(f\"File type must be one of: {allowed_types}\")\n        return v.lower()\n\nclass ProcessedInput(BaseModel):\n    \"\"\"Unified schema for processed inputs in the pipeline\"\"\"\n    input_type: str = Field(..., pattern=\"^(text|image|document)$\", description=\"Type of input\")\n    content: Union[str, bytes, dict] = Field(..., description=\"Processed content\")\n    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Processing metadata\")\n    timestamp: float = Field(default_factory=lambda: datetime.now().timestamp(), description=\"Processing timestamp\")\n    confidence: float = Field(default=1.0, ge=0.0, le=1.0, description=\"Processing confidence score\")\n    \n    @validator('input_type')\n    def validate_input_type(cls, v):\n        allowed_types = ['text', 'image', 'document']\n        if v not in allowed_types:\n            raise ValueError(f\"Input type must be one of: {allowed_types}\")\n        return v\n    \n    @validator('content')\n    def validate_content_not_empty(cls, v):\n        if isinstance(v, str) and not v.strip():\n            raise ValueError(\"Content cannot be empty\")\n        elif isinstance(v, bytes) and len(v) == 0:\n            raise ValueError(\"Content cannot be empty\")\n        elif isinstance(v, dict) and not v:\n            raise ValueError(\"Content cannot be empty\")\n        return v\n\nclass ValidationResult(BaseModel):\n    \"\"\"Result of input validation\"\"\"\n    valid: bool = Field(..., description=\"Whether validation passed\")\n    errors: List[str] = Field(default_factory=list, description=\"Validation errors\")\n    warnings: List[str] = Field(default_factory=list, description=\"Validation warnings\")\n    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Validation metadata\")\n\n# Example usage and testing\nif __name__ == \"__main__\":\n    # Test text input\n    try:\n        text_input = TextInput(\n            content=\"Hello, how are you?\",\n            language=\"en\",\n            metadata={\"source\": \"test\"}\n        )\n        print(\"\u2705 Text input validation passed\")\n        print(f\"Content: {text_input.content}\")\n        print(f\"Language: {text_input.language}\")\n    except Exception as e:\n        print(f\"\u274c Text input validation failed: {e}\")\n    \n    # Test image input\n    try:\n        image_data = b\"fake_image_data\"\n        image_input = ImageInput(\n            data=image_data,\n            format=\"png\",\n            metadata={\"source\": \"test\"}\n        )\n        print(\"\u2705 Image input validation passed\")\n        print(f\"Format: {image_input.format}\")\n        print(f\"Data size: {len(image_input.data)} bytes\")\n    except Exception as e:\n        print(f\"\u274c Image input validation failed: {e}\")\n    \n    # Test document input\n    try:\n        doc_input = DocumentInput(\n            file_path=\"/path/to/document.pdf\",\n            file_type=\"pdf\",\n            metadata={\"source\": \"test\"}\n        )\n        print(\"\u2705 Document input validation passed\")\n        print(f\"File path: {doc_input.file_path}\")\n        print(f\"File type: {doc_input.file_type}\")\n    except Exception as e:\n        print(f\"\u274c Document input validation failed: {e}\")\n    \n    # Test processed input\n    try:\n        processed = ProcessedInput(\n            input_type=\"text\",\n            content=\"Processed text content\",\n            metadata={\"original_type\": \"text\", \"processing_time\": 0.1}\n        )\n        print(\"\u2705 Processed input validation passed\")\n        print(f\"Input type: {processed.input_type}\")\n        print(f\"Timestamp: {processed.timestamp}\")\n    except Exception as e:\n        print(f\"\u274c Processed input validation failed: {e}\")\n    \n    print(\"\\n\ud83c\udf89 All input schema tests completed!\")\n", "source": "src/core/schemas/input_schemas.py"}, {"instruction": "Explain the following code from src/core/knowledge/code_generator.py:", "response": "This code is from src/core/knowledge/code_generator.py:\n\n\"\"\"\nKnowledge Base Code Generator for Agentic LLM Core v0.1\n\nThis module provides intelligent code generation using the knowledge base\nto improve code quality, maintain consistency, and leverage existing patterns.\n\nCreated: 2024-09-24\nStatus: Enhanced\n\"\"\"\n\nfrom __future__ import annotations\n\nimport ast\nimport json\nimport logging\nimport re\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional, Union, Tuple, Set\nfrom uuid import uuid4\n\nimport pydantic\nfrom pydantic import BaseModel, Field, field_validator\n\nfrom ..memory.vector_pg import VectorStore, Document, DocumentType\nfrom ..models.contracts import Task, TaskResult, TaskStatus\nfrom ..providers.llm_qwen3 import Qwen3Provider\n\n\n# ============================================================================\n# Code Generation Models\n# ============================================================================\n\nclass CodePattern(BaseModel):\n    \"\"\"Code pattern extracted from knowledge base.\"\"\"\n    pattern_id: str = Field(default_factory=lambda: str(uuid4()), description=\"Pattern ID\")\n    name: str = Field(..., description=\"Pattern name\")\n    description: str = Field(..., description=\"Pattern description\")\n    code_template: str = Field(..., description=\"Code template\")\n    language: str = Field(default=\"python\", description=\"Programming language\")\n    category: str = Field(..., description=\"Pattern category\")\n    tags: List[str] = Field(default_factory=list, description=\"Pattern tags\")\n    parameters: Dict[str, Any] = Field(default_factory=dict, description=\"Template parameters\")\n    examples: List[str] = Field(default_factory=list, description=\"Usage examples\")\n    quality_score: float = Field(default=0.0, ge=0.0, le=1.0, description=\"Pattern quality score\")\n    usage_count: int = Field(default=0, description=\"Number of times used\")\n    created_at: datetime = Field(default_factory=datetime.utcnow, description=\"Creation timestamp\")\n    \n    @field_validator('language')\n    @classmethod\n    def validate_language(cls, v):\n        allowed_languages = ['python', 'javascript', 'typescript', 'go', 'rust', 'java', 'cpp']\n        if v not in allowed_languages:\n            raise ValueError(f\"Language must be one of: {allowed_languages}\")\n        return v\n\n\nclass CodeGenerationRequest(BaseModel):\n    \"\"\"Request for code generation.\"\"\"\n    request_id: str = Field(default_factory=lambda: str(uuid4()), description=\"Request ID\")\n    description: str = Field(..., description=\"What to generate\")\n    language: str = Field(default=\"python\", description=\"Target language\")\n    context: Optional[str] = Field(None, description=\"Additional context\")\n    requirements: List[str] = Field(default_factory=list, description=\"Specific requirements\")\n    patterns_to_use: Optional[List[str]] = Field(None, description=\"Specific patterns to use\")\n    quality_level: str = Field(default=\"high\", description=\"Quality level\")\n    include_tests: bool = Field(default=True, description=\"Include unit tests\")\n    include_docs: bool = Field(default=True, description=\"Include documentation\")\n    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Additional metadata\")\n    \n    @field_validator('quality_level')\n    @classmethod\n    def validate_quality_level(cls, v):\n        allowed_levels = ['basic', 'standard', 'high', 'enterprise']\n        if v not in allowed_levels:\n            raise ValueError(f\"Quality level must be one of: {allowed_levels}\")\n        return v\n\n\nclass CodeGenerationResult(BaseModel):\n    \"\"\"Result of code generation.\"\"\"\n    request_id: str = Field(..., description=\"Corresponding request ID\")\n    generated_code: str = Field(..., description=\"Generated code\")\n    test_code: Optional[str] = Field(None, description=\"Generated test code\")\n    documentation: Optional[str] = Field(None, description=\"Generated documentation\")\n    patterns_used: List[str] = Field(default_factory=list, description=\"Patterns used\")\n    knowledge_base_matches: List[Dict[str, Any]] = Field(default_factory=list, description=\"KB matches\")\n    quality_score: float = Field(default=0.0, ge=0.0, le=1.0, description=\"Generated code quality\")\n    suggestions: List[str] = Field(default_factory=list, description=\"Improvement suggestions\")\n    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Additional metadata\")\n    generated_at: datetime = Field(default_factory=datetime.utcnow, description=\"Generation timestamp\")\n\n\nclass CodeAnalysisResult(BaseModel):\n    \"\"\"Result of code analysis.\"\"\"\n    code_id: str = Field(default_factory=lambda: str(uuid4()), description=\"Analysis ID\")\n    code: str = Field(..., description=\"Code to analyze\")\n    language: str = Field(default=\"python\", description=\"Code language\")\n    analysis_type: str = Field(default=\"comprehensive\", description=\"Type of analysis\")\n    issues_found: List[Dict[str, Any]] = Field(default_factory=list, description=\"Issues found\")\n    suggestions: List[str] = Field(default_factory=list, description=\"Improvement suggestions\")\n    patterns_detected: List[str] = Field(default_factory=list, description=\"Patterns detected\")\n    quality_metrics: Dict[str, float] = Field(default_factory=dict, description=\"Quality metrics\")\n    complexity_score: float = Field(default=0.0, ge=0.0, description=\"Complexity score\")\n    maintainability_score: float = Field(default=0.0, ge=0.0, le=1.0, description=\"Maintainability score\")\n    analyzed_at: datetime = Field(default_factory=datetime.utcnow, description=\"Analysis timestamp\")\n\n\n# ============================================================================\n# Knowledge Base Code Generator\n# ============================================================================\n\nclass KnowledgeBaseCodeGenerator:\n    \"\"\"Intelligent code generator using knowledge base patterns.\"\"\"\n    \n    def __init__(\n        self,\n        vector_store: VectorStore,\n        llm_provider: Qwen3Provider,\n        generator_name: str = \"kb_code_generator\"\n    ):\n        self.vector_store = vector_store\n        self.llm_provider = llm_provider\n        self.generator_name = generator_name\n        self.logger = logging.getLogger(__name__)\n        \n        # Pattern registry\n        self.pattern_registry: Dict[str, CodePattern] = {}\n        self._load_default_patterns()\n        \n        # Code templates\n        self.templates = self._load_code_templates()\n    \n    def _load_default_patterns(self):\n        \"\"\"Load default code patterns.\"\"\"\n        default_patterns = [\n            CodePattern(\n                name=\"async_function\",\n                description=\"Async function with error handling\",\n                code_template=\"\"\"\nasync def {function_name}({parameters}):\n    \\\"\\\"\\\"{description}\\\"\\\"\\\"\n    try:\n        # Implementation here\n        {implementation}\n        return result\n    except Exception as e:\n        logger.error(f\"Error in {function_name}: {e}\")\n        raise\n\"\"\",\n                category=\"function\",\n                tags=[\"async\", \"error_handling\", \"logging\"],\n                parameters={\n                    \"function_name\": {\"type\": \"string\", \"required\": True},\n                    \"parameters\": {\"type\": \"string\", \"default\": \"\"},\n                    \"description\": {\"type\": \"string\", \"default\": \"Function description\"},\n                    \"implementation\": {\"type\": \"string\", \"required\": True}\n                },\n                quality_score=0.9\n            ),\n            CodePattern(\n                name=\"pydantic_model\",\n                description=\"Pydantic model with validation\",\n                code_template=\"\"\"\nclass {class_name}(BaseModel):\n    \\\"\\\"\\\"{description}\\\"\\\"\\\"\n    {fields}\n    \n    @field_validator('{field_name}')\n    @classmethod\n    def validate_{field_name}(cls, v):\n        {validation_logic}\n        return v\n    \n    class Config:\n        {config_options}\n\"\"\",\n                category=\"model\",\n                tags=[\"pydantic\", \"validation\", \"data_model\"],\n                parameters={\n                    \"class_name\": {\"type\": \"string\", \"required\": True},\n                    \"description\": {\"type\": \"string\", \"default\": \"Model description\"},\n                    \"fields\": {\"type\": \"string\", \"required\": True},\n                    \"field_name\": {\"type\": \"string\", \"required\": True},\n                    \"validation_logic\": {\"type\": \"string\", \"required\": True},\n                    \"config_options\": {\"type\": \"string\", \"default\": \"pass\"}\n                },\n                quality_score=0.95\n            ),\n            CodePattern(\n                name=\"test_function\",\n                description=\"Unit test function with fixtures\",\n                code_template=\"\"\"\n@pytest.mark.asyncio\nasync def test_{function_name}({fixtures}):\n    \\\"\\\"\\\"Test {function_name}.\\\"\\\"\\\"\n    # Arrange\n    {arrange_code}\n    \n    # Act\n    result = await {function_name}({test_parameters})\n    \n    # Assert\n    {assertions}\n\"\"\",\n                category=\"test\",\n                tags=[\"pytest\", \"async\", \"unit_test\"],\n                parameters={\n                    \"function_name\": {\"type\": \"string\", \"required\": True},\n                    \"fixtures\": {\"type\": \"string\", \"default\": \"\"},\n                    \"arrange_code\": {\"type\": \"string\", \"required\": True},\n                    \"test_parameters\": {\"type\": \"string\", \"required\": True},\n                    \"assertions\": {\"type\": \"string\", \"required\": True}\n                },\n                quality_score=0.85\n            ),\n            CodePattern(\n                name=\"mcp_tool\",\n                description=\"MCP tool implementation\",\n                code_template=\"\"\"\nasync def {tool_name}({parameters}) -> Dict[str, Any]:\n    \\\"\\\"\\\"{description}\\\"\\\"\\\"\n    try:\n        # Tool implementation\n        {implementation}\n        \n        return {{\n            \"success\": True,\n            \"result\": result,\n            \"metadata\": {{\n                \"execution_time\": execution_time,\n                \"tool_name\": \"{tool_name}\"\n            }}\n        }}\n    except Exception as e:\n        logger.error(f\"Tool {tool_name} failed: {e}\")\n        return {{\n            \"success\": False,\n            \"error\": str(e),\n            \"metadata\": {{\n                \"tool_name\": \"{tool_name}\",\n                \"error_type\": type(e).__name__\n            }}\n        }}\n\"\"\",\n                category=\"tool\",\n                tags=[\"mcp\", \"async\", \"error_handling\"],\n                parameters={\n                    \"tool_name\": {\"type\": \"string\", \"required\": True},\n                    \"parameters\": {\"type\": \"string\", \"required\": True},\n                    \"description\": {\"type\": \"string\", \"default\": \"Tool description\"},\n                    \"implementation\": {\"type\": \"string\", \"required\": True}\n                },\n                quality_score=0.9\n            )\n        ]\n        \n        for pattern in default_patterns:\n            self.pattern_registry[pattern.name] = pattern\n    \n    def _load_code_templates(self) -> Dict[str, str]:\n        \"\"\"Load code templates for different scenarios.\"\"\"\n        return {\n            \"imports\": \"\"\"\nfrom __future__ import annotations\n\nimport asyncio\nimport logging\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional, Union\nfrom uuid import uuid4\n\nimport pydantic\nfrom pydantic import BaseModel, Field, field_validator\n\"\"\",\n            \"class_header\": \"\"\"\nclass {class_name}(BaseModel):\n    \\\"\\\"\\\"{description}\\\"\\\"\\\"\n\"\"\",\n            \"async_method\": \"\"\"\n    async def {method_name}(self, {parameters}) -> {return_type}:\n        \\\"\\\"\\\"{description}\\\"\\\"\\\"\n        try:\n            {implementation}\n        except Exception as e:\n            self.logger.error(f\"Error in {method_name}: {e}\")\n            raise\n\"\"\",\n            \"test_template\": \"\"\"\nimport pytest\nfrom unittest.mock import AsyncMock, Mock\n\nfrom {module_path} import {class_name}\n\n\n@pytest.mark.asyncio\nclass Test{class_name}:\n    \\\"\\\"\\\"Test suite for {class_name}.\\\"\\\"\\\"\n    \n    async def test_{method_name}(self):\n        \\\"\\\"\\\"Test {method_name}.\\\"\\\"\\\"\n        # Arrange\n        {arrange_code}\n        \n        # Act\n        result = await instance.{method_name}({test_parameters})\n        \n        # Assert\n        {assertions}\n\"\"\"\n        }\n    \n    async def generate_code(\n        self,\n        request: CodeGenerationRequest\n    ) -> CodeGenerationResult:\n        \"\"\"Generate code using knowledge base patterns.\"\"\"\n        try:\n            # Search knowledge base for relevant patterns\n            kb_matches = await self._search_knowledge_base(request)\n            \n            # Select appropriate patterns\n            selected_patterns = self._select_patterns(request, kb_matches)\n            \n            # Generate code using patterns\n            generated_code = await self._generate_with_patterns(request, selected_patterns)\n            \n            # Generate tests if requested\n            test_code = None\n            if request.include_tests:\n                test_code = await self._generate_tests(request, generated_code)\n            \n            # Generate documentation if requested\n            documentation = None\n            if request.include_docs:\n                documentation = await self._generate_documentation(request, generated_code)\n            \n            # Calculate quality score\n            quality_score = self._calculate_quality_score(generated_code, selected_patterns)\n            \n            # Generate suggestions\n            suggestions = self._generate_suggestions(generated_code, quality_score)\n            \n            return CodeGenerationResult(\n                request_id=request.request_id,\n                generated_code=generated_code,\n                test_code=test_code,\n                documentation=documentation,\n                patterns_used=[p.name for p in selected_patterns],\n                knowledge_base_matches=kb_matches,\n                quality_score=quality_score,\n                suggestions=suggestions,\n                metadata={\n                    \"generator\": self.generator_name,\n                    \"language\": request.language,\n                    \"quality_level\": request.quality_level\n                }\n            )\n            \n        except Exception as e:\n            self.logger.error(f\"Code generation failed: {e}\")\n            raise\n    \n    async def _search_knowledge_base(\n        self,\n        request: CodeGenerationRequest\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Search knowledge base for relevant code patterns.\"\"\"\n        if not self.vector_store:\n            return []\n        \n        # Create search query\n        query_parts = [request.description]\n        if request.context:\n            query_parts.append(request.context)\n        query_parts.extend(request.requirements)\n        \n        search_query = \" \".join(query_parts)\n        \n        try:\n            # Search for relevant documents\n            results = await self.vector_store.search(\n                query=search_query,\n                limit=10,\n                similarity_threshold=0.6,\n                document_types=[DocumentType.CODE, DocumentType.DOCUMENTATION]\n            )\n            \n            return [\n                {\n                    \"content\": result.document.content,\n                    \"metadata\": result.document.metadata,\n                    \"similarity\": result.similarity,\n                    \"source\": result.document.source\n                }\n                for result in results\n            ]\n            \n        except Exception as e:\n            self.logger.error(f\"Knowledge base search failed: {e}\")\n            return []\n    \n    def _select_patterns(\n        self,\n        request: CodeGenerationRequest,\n        kb_matches: List[Dict[str, Any]]\n    ) -> List[CodePattern]:\n        \"\"\"Select appropriate patterns for code generation.\"\"\"\n        selected_patterns = []\n        \n        # If specific patterns requested, use them\n        if request.patterns_to_use:\n            for pattern_name in request.patterns_to_use:\n                if pattern_name in self.pattern_registry:\n                    selected_patterns.append(self.pattern_registry[pattern_name])\n            return selected_patterns\n        \n        # Otherwise, select based on description and requirements\n        description_lower = request.description.lower()\n        \n        # Pattern selection logic\n        if \"async\" in description_lower or \"await\" in description_lower:\n            if \"async_function\" in self.pattern_registry:\n                selected_patterns.append(self.pattern_registry[\"async_function\"])\n        \n        if \"model\" in description_lower or \"pydantic\" in description_lower:\n            if \"pydantic_model\" in self.pattern_registry:\n                selected_patterns.append(self.pattern_registry[\"pydantic_model\"])\n        \n        if \"test\" in description_lower or \"unit test\" in description_lower:\n            if \"test_function\" in self.pattern_registry:\n                selected_patterns.append(self.pattern_registry[\"test_function\"])\n        \n        if \"tool\" in description_lower or \"mcp\" in description_lower:\n            if \"mcp_tool\" in self.pattern_registry:\n                selected_patterns.append(self.pattern_registry[\"mcp_tool\"])\n        \n        # If no patterns selected, use default async function pattern\n        if not selected_patterns and \"async_function\" in self.pattern_registry:\n            selected_patterns.append(self.pattern_registry[\"async_function\"])\n        \n        return selected_patterns\n    \n    async def _generate_with_patterns(\n        self,\n        request: CodeGenerationRequest,\n        patterns: List[CodePattern]\n    ) -> str:\n        \"\"\"Generate code using selected patterns.\"\"\"\n        if not patterns:\n            return await self._generate_basic_code(request)\n        \n        # Use the highest quality pattern\n        best_pattern = max(patterns, key=lambda p: p.quality_score)\n        \n        # Generate code using LLM with pattern as context\n        prompt = self._create_generation_prompt(request, best_pattern)\n        \n        try:\n            response = await self.llm_provider.generate_text(\n                prompt=prompt,\n                max_tokens=2000,\n                temperature=0.3\n            )\n            \n            # Extract code from response\n            code = self._extract_code_from_response(response)\n            \n            # Apply pattern template if needed\n            if best_pattern.code_template:\n                code = self._apply_pattern_template(best_pattern, code, request)\n            \n            return code\n            \n        except Exception as e:\n            self.logger.error(f\"LLM generation failed: {e}\")\n            return await self._generate_basic_code(request)\n    \n    def _create_generation_prompt(\n        self,\n        request: CodeGenerationRequest,\n        pattern: CodePattern\n    ) -> str:\n        \"\"\"Create prompt for code generation.\"\"\"\n        return f\"\"\"\nGenerate {request.language} code for: {request.description}\n\nContext: {request.context or \"No additional context\"}\n\nRequirements:\n{chr(10).join(f\"- {req}\" for req in request.requirements)}\n\nUse this pattern as reference:\nName: {pattern.name}\nDescription: {pattern.description}\nTemplate: {pattern.code_template}\n\nQuality Level: {request.quality_level}\n\nGenerate clean, well-documented, production-ready code.\nInclude proper error handling, logging, and type hints.\n\"\"\"\n    \n    def _extract_code_from_response(self, response: str) -> str:\n        \"\"\"Extract code from LLM response.\"\"\"\n        # Look for code blocks\n        code_pattern = r'```(?:python|py)?\\n(.*?)\\n```'\n        matches = re.findall(code_pattern, response, re.DOTALL)\n        \n        if matches:\n            return matches[0].strip()\n        \n        # If no code blocks, return the response as-is\n        return response.strip()\n    \n    def _apply_pattern_template(\n        self,\n        pattern: CodePattern,\n        code: str,\n        request: CodeGenerationRequest\n    ) -> str:\n        \"\"\"Apply pattern template to generated code.\"\"\"\n        # Extract function/class name from description\n        function_name = self._extract_name_from_description(request.description)\n        \n        # Apply template substitutions\n        template = pattern.code_template\n        substitutions = {\n            \"function_name\": function_name,\n            \"class_name\": function_name.title().replace(\"_\", \"\"),\n            \"description\": request.description,\n            \"parameters\": self._extract_parameters_from_description(request.description),\n            \"implementation\": code,\n            \"field_name\": \"name\",  # Default field name\n            \"validation_logic\": \"if not v:\\n            raise ValueError('Value cannot be empty')\",\n            \"config_options\": \"pass\"\n        }\n        \n        # Apply substitutions\n        for key, value in substitutions.items():\n            template = template.replace(f\"{{{key}}}\", str(value))\n        \n        return template\n    \n    def _extract_name_from_description(self, description: str) -> str:\n        \"\"\"Extract function/class name from description.\"\"\"\n        # Simple extraction - look for \"function\", \"class\", etc.\n        words = description.lower().split()\n        \n        if \"function\" in words:\n            idx = words.index(\"function\")\n            if idx > 0:\n                return words[idx - 1].replace(\"_\", \"_\")\n        \n        if \"class\" in words:\n            idx = words.index(\"class\")\n            if idx > 0:\n                return words[idx - 1].replace(\"_\", \"_\")\n        \n        # Default to first word\n        return words[0].replace(\"_\", \"_\") if words else \"generated_code\"\n    \n    def _extract_parameters_from_description(self, description: str) -> str:\n        \"\"\"Extract parameters from description.\"\"\"\n        # Simple parameter extraction\n        if \"no parameters\" in description.lower():\n            return \"\"\n        \n        # Look for parameter hints\n        if \"with parameters\" in description.lower():\n            # Extract parameters after \"with parameters\"\n            parts = description.lower().split(\"with parameters\")\n            if len(parts) > 1:\n                param_text = parts[1].split(\".\")[0]\n                return param_text.strip()\n        \n        return \"self\"\n    \n    async def _generate_basic_code(self, request: CodeGenerationRequest) -> str:\n        \"\"\"Generate basic code without patterns.\"\"\"\n        prompt = f\"\"\"\nGenerate {request.language} code for: {request.description}\n\nContext: {request.context or \"No additional context\"}\n\nRequirements:\n{chr(10).join(f\"- {req}\" for req in request.requirements)}\n\nGenerate clean, well-documented, production-ready code.\n\"\"\"\n        \n        try:\n            response = await self.llm_provider.generate_text(\n                prompt=prompt,\n                max_tokens=1500,\n                temperature=0.3\n            )\n            return self._extract_code_from_response(response)\n        except Exception as e:\n            self.logger.error(f\"Basic code generation failed: {e}\")\n            return f\"# Generated code for: {request.description}\\n# Implementation needed\"\n    \n    async def _generate_tests(\n        self,\n        request: CodeGenerationRequest,\n        code: str\n    ) -> str:\n        \"\"\"Generate unit tests for the code.\"\"\"\n        prompt = f\"\"\"\nGenerate unit tests for this {request.language} code:\n\n{code}\n\nGenerate comprehensive pytest tests with:\n- Test cases for normal operation\n- Test cases for error conditions\n- Proper mocking where needed\n- Clear test names and documentation\n\"\"\"\n        \n        try:\n            response = await self.llm_provider.generate_text(\n                prompt=prompt,\n                max_tokens=1000,\n                temperature=0.2\n            )\n            return self._extract_code_from_response(response)\n        except Exception as e:\n            self.logger.error(f\"Test generation failed: {e}\")\n            return f\"# Tests for: {request.description}\\n# Test implementation needed\"\n    \n    async def _generate_documentation(\n        self,\n        request: CodeGenerationRequest,\n        code: str\n    ) -> str:\n        \"\"\"Generate documentation for the code.\"\"\"\n        prompt = f\"\"\"\nGenerate documentation for this {request.language} code:\n\n{code}\n\nGenerate:\n- Module-level documentation\n- Function/class documentation\n- Usage examples\n- API reference\n\"\"\"\n        \n        try:\n            response = await self.llm_provider.generate_text(\n                prompt=prompt,\n                max_tokens=800,\n                temperature=0.2\n            )\n            return response.strip()\n        except Exception as e:\n            self.logger.error(f\"Documentation generation failed: {e}\")\n            return f\"# Documentation for: {request.description}\\n# Documentation needed\"\n    \n    def _calculate_quality_score(\n        self,\n        code: str,\n        patterns_used: List[CodePattern]\n    ) -> float:\n        \"\"\"Calculate quality score for generated code.\"\"\"\n        score = 0.0\n        \n        # Base score from patterns\n        if patterns_used:\n            pattern_score = sum(p.quality_score for p in patterns_used) / len(patterns_used)\n            score += pattern_score * 0.4\n        \n        # Code quality checks\n        if \"async def\" in code:\n            score += 0.1  # Async functions are good\n        \n        if \"try:\" in code and \"except\" in code:\n            score += 0.1  # Error handling\n        \n        if \"logger\" in code:\n            score += 0.1  # Logging\n        \n        if \"type:\" in code or \"->\" in code:\n            score += 0.1  # Type hints\n        \n        if '\"\"\"' in code:\n            score += 0.1  # Documentation\n        \n        if \"pytest\" in code or \"test_\" in code:\n            score += 0.1  # Tests\n        \n        return min(score, 1.0)\n    \n    def _generate_suggestions(\n        self,\n        code: str,\n        quality_score: float\n    ) -> List[str]:\n        \"\"\"Generate improvement suggestions.\"\"\"\n        suggestions = []\n        \n        if quality_score < 0.7:\n            suggestions.append(\"Consider adding more error handling\")\n            suggestions.append(\"Add comprehensive documentation\")\n        \n        if \"async def\" not in code and \"def \" in code:\n            suggestions.append(\"Consider making functions async for better performance\")\n        \n        if \"logger\" not in code:\n            suggestions.append(\"Add logging for better debugging\")\n        \n        if \"type:\" not in code and \"->\" not in code:\n            suggestions.append(\"Add type hints for better code clarity\")\n        \n        if \"test_\" not in code:\n            suggestions.append(\"Add unit tests for better reliability\")\n        \n        return suggestions\n    \n    async def analyze_code(self, code: str, language: str = \"python\") -> CodeAnalysisResult:\n        \"\"\"Analyze existing code for quality and patterns.\"\"\"\n        try:\n            # Basic analysis\n            issues_found = []\n            suggestions = []\n            patterns_detected = []\n            quality_metrics = {}\n            \n            # Check for common issues\n            if \"TODO\" in code or \"FIXME\" in code:\n                issues_found.append({\n                    \"type\": \"todo\",\n                    \"message\": \"Contains TODO or FIXME comments\",\n                    \"severity\": \"low\"\n                })\n            \n            if \"print(\" in code:\n                issues_found.append({\n                    \"type\": \"logging\",\n                    \"message\": \"Uses print() instead of logger\",\n                    \"severity\": \"medium\"\n                })\n                suggestions.append(\"Replace print() statements with proper logging\")\n            \n            if \"except:\" in code:\n                issues_found.append({\n                    \"type\": \"error_handling\",\n                    \"message\": \"Bare except clause\",\n                    \"severity\": \"high\"\n                })\n                suggestions.append(\"Specify exception types in except clauses\")\n            \n            # Detect patterns\n            if \"async def\" in code:\n                patterns_detected.append(\"async_function\")\n            \n            if \"class \" in code and \"BaseModel\" in code:\n                patterns_detected.append(\"pydantic_model\")\n            \n            if \"pytest\" in code or \"test_\" in code:\n                patterns_detected.append(\"test_function\")\n            \n            # Calculate metrics\n            lines_of_code = len(code.split('\\n'))\n            complexity_score = self._calculate_complexity(code)\n            maintainability_score = self._calculate_maintainability(code, issues_found)\n            \n            quality_metrics = {\n                \"lines_of_code\": lines_of_code,\n                \"complexity_score\": complexity_score,\n                \"maintainability_score\": maintainability_score,\n                \"documentation_coverage\": self._calculate_doc_coverage(code),\n                \"test_coverage\": self._calculate_test_coverage(code)\n            }\n            \n            return CodeAnalysisResult(\n                code=code,\n                language=language,\n                issues_found=issues_found,\n                suggestions=suggestions,\n                patterns_detected=patterns_detected,\n                quality_metrics=quality_metrics,\n                complexity_score=complexity_score,\n                maintainability_score=maintainability_score\n            )\n            \n        except Exception as e:\n            self.logger.error(f\"Code analysis failed: {e}\")\n            raise\n    \n    def _calculate_complexity(self, code: str) -> float:\n        \"\"\"Calculate code complexity score.\"\"\"\n        # Simple complexity calculation\n        complexity_indicators = [\n            \"if \", \"for \", \"while \", \"try:\", \"except\", \"def \", \"class \",\n            \"and \", \"or \", \"not \", \"lambda\", \"yield\", \"async def\"\n        ]\n        \n        complexity_count = sum(code.count(indicator) for indicator in complexity_indicators)\n        lines = len(code.split('\\n'))\n        \n        return complexity_count / max(lines, 1)\n    \n    def _calculate_maintainability(self, code: str, issues: List[Dict[str, Any]]) -> float:\n        \"\"\"Calculate maintainability score.\"\"\"\n        score = 1.0\n        \n        # Deduct for issues\n        for issue in issues:\n            severity = issue.get(\"severity\", \"low\")\n            if severity == \"high\":\n                score -= 0.2\n            elif severity == \"medium\":\n                score -= 0.1\n            else:\n                score -= 0.05\n        \n        # Bonus for good practices\n        if \"logger\" in code:\n            score += 0.1\n        if '\"\"\"' in code:\n            score += 0.1\n        if \"type:\" in code or \"->\" in code:\n            score += 0.1\n        \n        return max(0.0, min(score, 1.0))\n    \n    def _calculate_doc_coverage(self, code: str) -> float:\n        \"\"\"Calculate documentation coverage.\"\"\"\n        functions = len(re.findall(r'def\\s+\\w+', code))\n        documented_functions = len(re.findall(r'def\\s+\\w+.*?\\n\\s*\"\"\".*?\"\"\"', code, re.DOTALL))\n        \n        if functions == 0:\n            return 1.0\n        \n        return documented_functions / functions\n    \n    def _calculate_test_coverage(self, code: str) -> float:\n        \"\"\"Calculate test coverage estimate.\"\"\"\n        functions = len(re.findall(r'def\\s+\\w+', code))\n        test_functions = len(re.findall(r'def\\s+test_\\w+', code))\n        \n        if functions == 0:\n            return 1.0\n        \n        return min(test_functions / functions, 1.0)\n    \n    def register_pattern(self, pattern: CodePattern):\n        \"\"\"Register a new code pattern.\"\"\"\n        self.pattern_registry[pattern.name] = pattern\n        self.logger.info(f\"Registered pattern: {pattern.name}\")\n    \n    def get_pattern_catalog(self) -> Dict[str, CodePattern]:\n        \"\"\"Get the complete pattern catalog.\"\"\"\n        return self.pattern_registry.copy()\n\n\n# ============================================================================\n# Factory Functions\n# ============================================================================\n\nasync def create_code_generator(\n    vector_store: VectorStore,\n    llm_provider: Qwen3Provider,\n    generator_name: str = \"kb_code_generator\"\n) -> KnowledgeBaseCodeGenerator:\n    \"\"\"Create a new knowledge base code generator.\"\"\"\n    return KnowledgeBaseCodeGenerator(vector_store, llm_provider, generator_name)\n\n\ndef create_generation_request(\n    description: str,\n    language: str = \"python\",\n    context: Optional[str] = None,\n    requirements: Optional[List[str]] = None,\n    quality_level: str = \"high\"\n) -> CodeGenerationRequest:\n    \"\"\"Create a code generation request.\"\"\"\n    return CodeGenerationRequest(\n        description=description,\n        language=language,\n        context=context,\n        requirements=requirements or [],\n        quality_level=quality_level\n    )\n\n\n# ============================================================================\n# Example Usage\n# ============================================================================\n\nasync def example_usage():\n    \"\"\"Example usage of the knowledge base code generator.\"\"\"\n    # This would be used in actual implementation\n    pass\n", "source": "src/core/knowledge/code_generator.py"}, {"instruction": "Explain the following code from src/core/engines/qwen3_omni_engine.py:", "response": "This code is from src/core/engines/qwen3_omni_engine.py:\n\n#!/usr/bin/env python3\n\"\"\"\nQwen3-Omni Engine for Agentic LLM Core\n\nThis module implements the Qwen3OmniEngine interface as specified in the project architecture.\nSince the Qwen3-Omni model is very new, this implementation provides a framework that can be\neasily updated when full support becomes available.\n\nComplies with:\n- System Specification: Agentic LLM Core v0.1 (specs/system.md)\n- Architecture Plan: Agentic LLM Core v0.1 (plans/architecture.md)\n- Milestone 1: Core Pipeline Foundation (tasks/milestone_1_core_pipeline.md)\n\"\"\"\n\nimport os\nimport time\nimport logging\nfrom typing import Dict, List, Any, Optional, Union\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom datetime import datetime\nimport json\n\nlogger = logging.getLogger(__name__)\n\n# Data structures as specified in the project architecture\n@dataclass\nclass ProcessedInput:\n    \"\"\"Processed input data structure - matches system.md specification\"\"\"\n    input_type: str  # \"text\", \"image\", \"document\"\n    content: Union[str, bytes, dict]\n    metadata: Dict[str, Any]\n    timestamp: float\n\n@dataclass\nclass UnifiedContext:\n    \"\"\"Unified context from multimodal processing - matches system.md specification\"\"\"\n    text_content: str\n    image_features: List[float]\n    document_content: str\n    combined_embedding: List[float]\n    confidence_scores: Dict[str, float]\n\n@dataclass\nclass ContextAnalysis:\n    \"\"\"Context analysis results - matches system.md specification\"\"\"\n    intent: str\n    entities: List[str]\n    required_tools: List[str]\n    confidence: float\n    reasoning: str\n\n@dataclass\nclass FinalAnswer:\n    \"\"\"Final answer structure - matches system.md specification\"\"\"\n    answer: str\n    confidence: float\n    tools_used: List[str]\n    processing_time: float\n    metadata: Dict[str, Any]\n\nclass Qwen3OmniEngine:\n    \"\"\"\n    Qwen3-Omni Engine for Agentic LLM Core\n    \n    This class implements the Qwen3OmniEngine interface specified in the project architecture.\n    Currently provides a framework implementation that can be enhanced when full Qwen3-Omni\n    support becomes available in transformers/MLX.\n    \n    Key Features:\n    - Framework for Apple Silicon optimization\n    - Memory management and cleanup\n    - Context analysis and understanding\n    - Answer generation with tool integration\n    - Multimodal input processing support\n    - Deterministic caching for reproducibility\n    \n    Performance Requirements (from system.md):\n    - < 5 seconds total processing time for multimodal inputs\n    - < 2GB memory footprint per model instance\n    - Support for concurrent inference requests\n    - Automatic memory cleanup and garbage collection\n    \"\"\"\n    \n    def __init__(self, model_path: str, device: str = \"auto\"):\n        self.model_path = Path(model_path)\n        self.device = device\n        self.model = None\n        self.tokenizer = None\n        self.processor = None\n        self.initialized = False\n        \n        # Performance tracking\n        self.performance_metrics = {\n            \"load_time\": 0,\n            \"inference_times\": [],\n            \"memory_usage\": [],\n            \"context_cache_hits\": 0,\n            \"context_cache_misses\": 0\n        }\n        \n        # Context cache for deterministic results (as specified in architecture.md)\n        self.context_cache = {}\n        self.cache_size_limit = 1000\n        \n        # Mock responses for testing framework\n        self.mock_responses = {\n            \"analyze_context\": {\n                \"intent\": \"information_request\",\n                \"entities\": [\"artificial intelligence\", \"technology\"],\n                \"required_tools\": [],\n                \"confidence\": 0.85,\n                \"reasoning\": \"User is asking for information about AI, no tools needed for basic explanation.\"\n            },\n            \"generate_answer\": {\n                \"answer\": \"Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think and learn like humans. It encompasses various technologies including machine learning, natural language processing, computer vision, and robotics. AI systems can perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation.\",\n                \"confidence\": 0.9\n            }\n        }\n        \n    async def initialize(self) -> bool:\n        \"\"\"\n        Initialize the Qwen3-Omni model and processor\n        \n        Implements the initialization as specified in milestone_1_core_pipeline.md\n        Task 2.1.1: Qwen3-Omni Engine Integration Setup\n        \n        Note: This is a framework implementation that will be enhanced when\n        full Qwen3-Omni support becomes available.\n        \"\"\"\n        try:\n            if not self.model_path.exists():\n                logger.error(f\"Model not found at {self.model_path}\")\n                return False\n            \n            logger.info(\"Initializing Qwen3-Omni engine framework...\")\n            logger.info(\"Note: Using framework implementation until full Qwen3-Omni support is available\")\n            \n            start_time = time.time()\n            \n            # For now, we'll use a mock implementation\n            # In the future, this will load the actual Qwen3-Omni model\n            try:\n                # Try to load tokenizer if possible\n                from transformers import AutoTokenizer\n                self.tokenizer = AutoTokenizer.from_pretrained(\n                    str(self.model_path), \n                    trust_remote_code=True\n                )\n                logger.info(\"\u2705 Tokenizer loaded successfully\")\n            except Exception as e:\n                logger.warning(f\"Could not load tokenizer: {e}\")\n                logger.info(\"Using mock tokenizer for framework testing\")\n            \n            # Record initialization metrics\n            self.performance_metrics[\"load_time\"] = time.time() - start_time\n            \n            self.initialized = True\n            logger.info(f\"\ud83c\udf89 Qwen3-Omni engine framework initialized in {self.performance_metrics['load_time']:.2f} seconds\")\n            logger.info(\"Ready for integration with Agentic LLM Core project\")\n            \n            return True\n            \n        except Exception as e:\n            logger.error(f\"Failed to initialize Qwen3-Omni engine framework: {e}\")\n            import traceback\n            traceback.print_exc()\n            return False\n    \n    async def analyze_context(self, context: UnifiedContext) -> ContextAnalysis:\n        \"\"\"\n        Analyze context using Qwen3-Omni\n        \n        Implements Task 2.1.2: ContextAnalysisEngine from milestone_1_core_pipeline.md\n        \n        Acceptance Criteria:\n        - [ ] Analyzes context with > 90% accuracy\n        - [ ] Extracts intent and entities correctly\n        - [ ] Provides confidence scores\n        - [ ] Performance < 2 seconds per analysis\n        \"\"\"\n        if not self.initialized:\n            raise RuntimeError(\"Engine not initialized\")\n        \n        try:\n            # Check cache for deterministic results\n            cache_key = self._generate_cache_key(context)\n            if cache_key in self.context_cache:\n                self.performance_metrics[\"context_cache_hits\"] += 1\n                logger.debug(f\"Context cache hit for key: {cache_key}\")\n                return self.context_cache[cache_key]\n            \n            self.performance_metrics[\"context_cache_misses\"] += 1\n            \n            start_time = time.time()\n            \n            # Mock analysis for framework testing\n            # In the future, this will use the actual Qwen3-Omni model\n            analysis_data = self.mock_responses[\"analyze_context\"]\n            \n            analysis = ContextAnalysis(\n                intent=analysis_data[\"intent\"],\n                entities=analysis_data[\"entities\"],\n                required_tools=analysis_data[\"required_tools\"],\n                confidence=analysis_data[\"confidence\"],\n                reasoning=analysis_data[\"reasoning\"]\n            )\n            \n            analysis_time = time.time() - start_time\n            self.performance_metrics[\"inference_times\"].append(analysis_time)\n            \n            # Cache the result for deterministic behavior\n            self._cache_analysis_result(cache_key, analysis)\n            \n            logger.info(f\"Context analysis completed in {analysis_time:.2f}s\")\n            return analysis\n            \n        except Exception as e:\n            logger.error(f\"Context analysis failed: {e}\")\n            raise\n    \n    async def generate_answer(self, analysis: ContextAnalysis, tools: List[Any] = None) -> FinalAnswer:\n        \"\"\"\n        Generate answer based on context analysis\n        \n        Implements Task 2.2.1: AnswerGenerationService from milestone_1_core_pipeline.md\n        \n        Acceptance Criteria:\n        - [ ] Generates coherent answers\n        - [ ] Structures answers appropriately\n        - [ ] Calculates confidence scores\n        - [ ] Performance < 3 seconds per answer\n        \"\"\"\n        if not self.initialized:\n            raise RuntimeError(\"Engine not initialized\")\n        \n        try:\n            start_time = time.time()\n            \n            # Mock answer generation for framework testing\n            # In the future, this will use the actual Qwen3-Omni model\n            answer_data = self.mock_responses[\"generate_answer\"]\n            \n            answer = FinalAnswer(\n                answer=answer_data[\"answer\"],\n                confidence=answer_data[\"confidence\"],\n                tools_used=analysis.required_tools,\n                processing_time=time.time() - start_time,\n                metadata={\n                    \"model\": \"qwen3-omni-framework\",\n                    \"timestamp\": time.time(),\n                    \"framework_version\": \"0.1.0\",\n                    \"cache_hits\": self.performance_metrics[\"context_cache_hits\"],\n                    \"cache_misses\": self.performance_metrics[\"context_cache_misses\"]\n                }\n            )\n            \n            processing_time = time.time() - start_time\n            self.performance_metrics[\"inference_times\"].append(processing_time)\n            \n            logger.info(f\"Answer generated in {processing_time:.2f}s\")\n            return answer\n            \n        except Exception as e:\n            logger.error(f\"Answer generation failed: {e}\")\n            raise\n    \n    async def determine_tool_usage(self, analysis: ContextAnalysis) -> List[Dict[str, Any]]:\n        \"\"\"\n        Determine which tools should be used based on context analysis\n        \n        Implements tool selection logic for MCP tool integration\n        \"\"\"\n        tool_calls = []\n        \n        # Simple tool selection logic (to be enhanced with actual MCP tool integration)\n        if \"file\" in analysis.intent.lower() or \"document\" in analysis.intent.lower():\n            tool_calls.append({\n                \"tool_name\": \"file_system_tools\",\n                \"parameters\": {\"action\": \"read\"},\n                \"priority\": 1\n            })\n        \n        if \"database\" in analysis.intent.lower() or \"query\" in analysis.intent.lower():\n            tool_calls.append({\n                \"tool_name\": \"database_tools\", \n                \"parameters\": {\"action\": \"query\"},\n                \"priority\": 2\n            })\n        \n        return tool_calls\n    \n    async def process_multimodal_input(self, input_data: ProcessedInput) -> UnifiedContext:\n        \"\"\"\n        Process multimodal input and return unified context\n        \n        Implements multimodal processing as specified in system.md\n        \"\"\"\n        if not self.initialized:\n            raise RuntimeError(\"Engine not initialized\")\n        \n        try:\n            # Process input based on type\n            if input_data.input_type == \"text\":\n                text_content = str(input_data.content)\n                document_content = \"\"\n            elif input_data.input_type == \"document\":\n                text_content = \"\"\n                document_content = str(input_data.content)\n            else:\n                text_content = str(input_data.content) if isinstance(input_data.content, str) else \"\"\n                document_content = \"\"\n            \n            unified_context = UnifiedContext(\n                text_content=text_content,\n                image_features=[],\n                document_content=document_content,\n                combined_embedding=[],\n                confidence_scores={\"overall\": 0.8}\n            )\n            \n            logger.debug(f\"Multimodal input processed: {input_data.input_type}\")\n            return unified_context\n            \n        except Exception as e:\n            logger.error(f\"Multimodal processing failed: {e}\")\n            raise\n    \n    def _generate_cache_key(self, context: UnifiedContext) -> str:\n        \"\"\"\n        Generate deterministic cache key for context\n        \n        Implements deterministic caching as specified in architecture.md\n        \"\"\"\n        import hashlib\n        \n        # Create deterministic key based on context content\n        content_hash = hashlib.sha256(\n            f\"{context.text_content}_{context.document_content}_{context.confidence_scores}\".encode()\n        ).hexdigest()\n        \n        return f\"context_{content_hash}\"\n    \n    def _cache_analysis_result(self, cache_key: str, analysis: ContextAnalysis) -> None:\n        \"\"\"\n        Cache analysis result with size management\n        \n        Implements cache management as specified in architecture.md\n        \"\"\"\n        # Check cache size limit\n        if len(self.context_cache) >= self.cache_size_limit:\n            # Remove oldest entry (simple FIFO)\n            oldest_key = next(iter(self.context_cache))\n            del self.context_cache[oldest_key]\n        \n        # Cache the result\n        self.context_cache[cache_key] = analysis\n        \n        logger.debug(f\"Cached analysis result for key: {cache_key}\")\n    \n    def get_performance_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get performance metrics for monitoring\"\"\"\n        avg_inference_time = (\n            sum(self.performance_metrics[\"inference_times\"]) / len(self.performance_metrics[\"inference_times\"])\n            if self.performance_metrics[\"inference_times\"] else 0\n        )\n        \n        cache_hit_rate = (\n            self.performance_metrics[\"context_cache_hits\"] / \n            (self.performance_metrics[\"context_cache_hits\"] + self.performance_metrics[\"context_cache_misses\"])\n            if (self.performance_metrics[\"context_cache_hits\"] + self.performance_metrics[\"context_cache_misses\"]) > 0 else 0\n        )\n        \n        return {\n            \"load_time\": self.performance_metrics[\"load_time\"],\n            \"average_inference_time\": avg_inference_time,\n            \"total_inferences\": len(self.performance_metrics[\"inference_times\"]),\n            \"cache_hit_rate\": cache_hit_rate,\n            \"cache_size\": len(self.context_cache),\n            \"device\": self.device,\n            \"initialized\": self.initialized\n        }\n    \n    def cleanup(self):\n        \"\"\"Clean up resources and memory\"\"\"\n        if self.model:\n            del self.model\n        if self.tokenizer:\n            del self.tokenizer\n        if self.processor:\n            del self.processor\n        \n        # Clear cache\n        self.context_cache.clear()\n        \n        logger.info(\"\u2705 Qwen3-Omni engine cleanup completed\")\n\n# Example usage and testing\nasync def main():\n    \"\"\"Example usage of Qwen3OmniEngine\"\"\"\n    model_path = \"./Qwen3-Omni-30B-A3B-Instruct\"\n    \n    logger.info(\"\ud83d\ude80 Testing Qwen3-Omni Engine Framework\")\n    logger.info(\"=\" * 50)\n    \n    engine = Qwen3OmniEngine(model_path)\n    \n    try:\n        if await engine.initialize():\n            logger.info(\"\u2705 Engine framework initialized successfully\")\n            \n            # Create test input\n            test_input = ProcessedInput(\n                input_type=\"text\",\n                content=\"What is artificial intelligence?\",\n                metadata={\"source\": \"test\"},\n                timestamp=time.time()\n            )\n            \n            # Process input\n            context = await engine.process_multimodal_input(test_input)\n            logger.info(f\"\u2705 Context processed: {context.text_content[:50]}...\")\n            \n            # Analyze context\n            analysis = await engine.analyze_context(context)\n            logger.info(f\"\u2705 Context analyzed: {analysis.intent}\")\n            logger.info(f\"   Entities: {analysis.entities}\")\n            logger.info(f\"   Confidence: {analysis.confidence}\")\n            \n            # Determine tool usage\n            tool_calls = await engine.determine_tool_usage(analysis)\n            logger.info(f\"\u2705 Tool calls determined: {len(tool_calls)} tools\")\n            \n            # Generate answer\n            answer = await engine.generate_answer(analysis)\n            logger.info(f\"\u2705 Answer generated: {answer.answer[:100]}...\")\n            logger.info(f\"Processing time: {answer.processing_time:.2f}s\")\n            \n            # Get performance metrics\n            metrics = engine.get_performance_metrics()\n            logger.info(f\"Performance metrics: {json.dumps(metrics, indent=2)}\")\n            \n            return True\n        else:\n            logger.error(\"\u274c Failed to initialize engine framework\")\n            return False\n            \n    finally:\n        engine.cleanup()\n\nif __name__ == \"__main__\":\n    import asyncio\n    success = asyncio.run(main())\n    exit(0 if success else 1)\n", "source": "src/core/engines/qwen3_omni_engine.py"}, {"instruction": "Explain the following code from src/core/engines/qwen3_omni_mlx_engine.py:", "response": "This code is from src/core/engines/qwen3_omni_mlx_engine.py:\n\n#!/usr/bin/env python3\n\"\"\"\nMLX-Optimized Qwen3-Omni Engine for Agentic LLM Core\n\nThis module implements the Qwen3OmniEngine interface as specified in the project architecture,\nwith MLX optimizations for Apple Silicon (M1/M2/M3) chips.\n\nComplies with:\n- System Specification: Agentic LLM Core v0.1 (specs/system.md)\n- Architecture Plan: Agentic LLM Core v0.1 (plans/architecture.md)\n- Milestone 1: Core Pipeline Foundation (tasks/milestone_1_core_pipeline.md)\n\"\"\"\n\nimport os\nimport time\nimport logging\nfrom typing import Dict, List, Any, Optional, Union\nfrom pathlib import Path\nimport torch\nimport mlx.core as mx\nimport mlx.nn as nn\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\n# Data structures as specified in the project architecture\n@dataclass\nclass ProcessedInput:\n    \"\"\"Processed input data structure - matches system.md specification\"\"\"\n    input_type: str  # \"text\", \"image\", \"document\"\n    content: Union[str, bytes, dict]\n    metadata: Dict[str, Any]\n    timestamp: float\n\n@dataclass\nclass UnifiedContext:\n    \"\"\"Unified context from multimodal processing - matches system.md specification\"\"\"\n    text_content: str\n    image_features: List[float]\n    document_content: str\n    combined_embedding: List[float]\n    confidence_scores: Dict[str, float]\n\n@dataclass\nclass ContextAnalysis:\n    \"\"\"Context analysis results - matches system.md specification\"\"\"\n    intent: str\n    entities: List[str]\n    required_tools: List[str]\n    confidence: float\n    reasoning: str\n\n@dataclass\nclass FinalAnswer:\n    \"\"\"Final answer structure - matches system.md specification\"\"\"\n    answer: str\n    confidence: float\n    tools_used: List[str]\n    processing_time: float\n    metadata: Dict[str, Any]\n\nclass Qwen3OmniEngine:\n    \"\"\"\n    MLX-Optimized Qwen3-Omni Engine for Agentic LLM Core\n    \n    This class implements the Qwen3OmniEngine interface specified in the project architecture\n    with MLX optimizations for Apple Silicon performance.\n    \n    Key Features:\n    - Apple Silicon (MPS) optimization using MLX\n    - Memory management and cleanup\n    - Context analysis and understanding\n    - Answer generation with tool integration\n    - Multimodal input processing support\n    \n    Performance Requirements (from system.md):\n    - < 5 seconds total processing time for multimodal inputs\n    - < 2GB memory footprint per model instance\n    - Support for concurrent inference requests\n    - Automatic memory cleanup and garbage collection\n    \"\"\"\n    \n    def __init__(self, model_path: str, device: str = \"auto\"):\n        self.model_path = Path(model_path)\n        self.device = device\n        self.model = None\n        self.tokenizer = None\n        self.processor = None\n        self.initialized = False\n        \n        # MLX optimization settings\n        self.mlx_device = mx.default_device()\n        self.use_mlx_optimizations = True\n        \n        # Performance tracking\n        self.performance_metrics = {\n            \"load_time\": 0,\n            \"inference_times\": [],\n            \"memory_usage\": [],\n            \"context_cache_hits\": 0,\n            \"context_cache_misses\": 0\n        }\n        \n        # Context cache for deterministic results (as specified in architecture.md)\n        self.context_cache = {}\n        self.cache_size_limit = 1000\n        \n    async def initialize(self) -> bool:\n        \"\"\"\n        Initialize the Qwen3-Omni model and processor\n        \n        Implements the initialization as specified in milestone_1_core_pipeline.md\n        Task 2.1.1: Qwen3-Omni Engine Integration Setup\n        \n        Acceptance Criteria:\n        - [ ] Model loads successfully on Apple Silicon\n        - [ ] Memory usage < 2GB per model instance\n        - [ ] MPS acceleration working\n        - [ ] Model initialization < 10 seconds\n        \"\"\"\n        try:\n            if not self.model_path.exists():\n                logger.error(f\"Model not found at {self.model_path}\")\n                return False\n            \n            logger.info(\"Initializing Qwen3-Omni engine with MLX optimizations...\")\n            logger.info(f\"MLX Device: {self.mlx_device}\")\n            \n            start_time = time.time()\n            \n            # Load tokenizer\n            from transformers import AutoTokenizer\n            self.tokenizer = AutoTokenizer.from_pretrained(\n                str(self.model_path), \n                trust_remote_code=True\n            )\n            logger.info(\"\u2705 Tokenizer loaded successfully\")\n            \n            # Load model with MLX optimizations\n            try:\n                # Configure MLX device for tensor operations\n                mx.set_default_device(self.mlx_device)\n                \n                # Load model with MLX-compatible settings\n                from transformers import Qwen3OmniMoeForConditionalGeneration\n                \n                self.model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n                    str(self.model_path),\n                    trust_remote_code=True,\n                    torch_dtype=torch.float16,\n                    device_map=self.device,\n                    low_cpu_mem_usage=True,\n                    use_cache=True,\n                    return_dict=True\n                )\n                \n                logger.info(\"\u2705 Model loaded with MLX optimizations\")\n                \n            except Exception as e:\n                logger.warning(f\"MLX optimization failed, falling back to standard loading: {e}\")\n                \n                # Fallback to standard loading\n                from transformers import AutoModelForCausalLM\n                self.model = AutoModelForCausalLM.from_pretrained(\n                    str(self.model_path),\n                    trust_remote_code=True,\n                    torch_dtype=torch.float16,\n                    device_map=self.device,\n                    low_cpu_mem_usage=True\n                )\n                \n                logger.info(\"\u2705 Model loaded with standard configuration\")\n            \n            # Try to load multimodal processor\n            try:\n                from transformers import AutoProcessor\n                self.processor = AutoProcessor.from_pretrained(\n                    str(self.model_path),\n                    trust_remote_code=True\n                )\n                logger.info(\"\u2705 Multimodal processor loaded successfully\")\n            except Exception as e:\n                logger.warning(f\"Could not load multimodal processor: {e}\")\n                self.processor = None\n            \n            # Record initialization metrics\n            self.performance_metrics[\"load_time\"] = time.time() - start_time\n            \n            # Log model information\n            total_params = sum(p.numel() for p in self.model.parameters())\n            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n            logger.info(f\"Total parameters: {total_params:,}\")\n            logger.info(f\"Trainable parameters: {trainable_params:,}\")\n            \n            self.initialized = True\n            logger.info(f\"\ud83c\udf89 Qwen3-Omni MLX engine initialized in {self.performance_metrics['load_time']:.2f} seconds\")\n            \n            return True\n            \n        except Exception as e:\n            logger.error(f\"Failed to initialize Qwen3-Omni MLX engine: {e}\")\n            import traceback\n            traceback.print_exc()\n            return False\n    \n    async def analyze_context(self, context: UnifiedContext) -> ContextAnalysis:\n        \"\"\"\n        Analyze context using Qwen3-Omni\n        \n        Implements Task 2.1.2: ContextAnalysisEngine from milestone_1_core_pipeline.md\n        \n        Acceptance Criteria:\n        - [ ] Analyzes context with > 90% accuracy\n        - [ ] Extracts intent and entities correctly\n        - [ ] Provides confidence scores\n        - [ ] Performance < 2 seconds per analysis\n        \"\"\"\n        if not self.initialized:\n            raise RuntimeError(\"Engine not initialized\")\n        \n        try:\n            # Check cache for deterministic results\n            cache_key = self._generate_cache_key(context)\n            if cache_key in self.context_cache:\n                self.performance_metrics[\"context_cache_hits\"] += 1\n                logger.debug(f\"Context cache hit for key: {cache_key}\")\n                return self.context_cache[cache_key]\n            \n            self.performance_metrics[\"context_cache_misses\"] += 1\n            \n            # Create analysis prompt\n            prompt = f\"\"\"\n            Analyze the following context and provide structured analysis:\n            \n            Text Content: {context.text_content}\n            Document Content: {context.document_content}\n            Confidence Scores: {context.confidence_scores}\n            \n            Please provide:\n            1. Intent: What is the user trying to achieve?\n            2. Entities: What key entities are mentioned?\n            3. Required Tools: What tools might be needed?\n            4. Confidence: How confident are you in this analysis?\n            5. Reasoning: Explain your reasoning.\n            \n            Respond in JSON format.\n            \"\"\"\n            \n            start_time = time.time()\n            \n            # Generate analysis with MLX optimizations\n            response = await self._generate_with_mlx_optimization(prompt, max_tokens=512, temperature=0.3)\n            \n            analysis_time = time.time() - start_time\n            self.performance_metrics[\"inference_times\"].append(analysis_time)\n            \n            # Parse response (simplified - in real implementation, use proper JSON parsing)\n            analysis = ContextAnalysis(\n                intent=\"analyze_context\",\n                entities=[\"context\"],\n                required_tools=[],\n                confidence=0.8,\n                reasoning=response\n            )\n            \n            # Cache the result for deterministic behavior\n            self._cache_analysis_result(cache_key, analysis)\n            \n            logger.info(f\"Context analysis completed in {analysis_time:.2f}s\")\n            return analysis\n            \n        except Exception as e:\n            logger.error(f\"Context analysis failed: {e}\")\n            raise\n    \n    async def generate_answer(self, analysis: ContextAnalysis, tools: List[Any] = None) -> FinalAnswer:\n        \"\"\"\n        Generate answer based on context analysis\n        \n        Implements Task 2.2.1: AnswerGenerationService from milestone_1_core_pipeline.md\n        \n        Acceptance Criteria:\n        - [ ] Generates coherent answers\n        - [ ] Structures answers appropriately\n        - [ ] Calculates confidence scores\n        - [ ] Performance < 3 seconds per answer\n        \"\"\"\n        if not self.initialized:\n            raise RuntimeError(\"Engine not initialized\")\n        \n        try:\n            # Create answer generation prompt\n            prompt = f\"\"\"\n            Based on the following analysis, generate a comprehensive answer:\n            \n            Intent: {analysis.intent}\n            Entities: {analysis.entities}\n            Required Tools: {analysis.required_tools}\n            Reasoning: {analysis.reasoning}\n            \n            Please provide a clear, helpful, and accurate response.\n            \"\"\"\n            \n            start_time = time.time()\n            \n            # Generate answer with MLX optimizations\n            response = await self._generate_with_mlx_optimization(prompt, max_tokens=1024, temperature=0.7)\n            \n            processing_time = time.time() - start_time\n            self.performance_metrics[\"inference_times\"].append(processing_time)\n            \n            answer = FinalAnswer(\n                answer=response,\n                confidence=analysis.confidence,\n                tools_used=analysis.required_tools,\n                processing_time=processing_time,\n                metadata={\n                    \"model\": \"qwen3-omni-mlx\",\n                    \"timestamp\": time.time(),\n                    \"mlx_device\": str(self.mlx_device),\n                    \"cache_hits\": self.performance_metrics[\"context_cache_hits\"],\n                    \"cache_misses\": self.performance_metrics[\"context_cache_misses\"]\n                }\n            )\n            \n            logger.info(f\"Answer generated in {processing_time:.2f}s\")\n            return answer\n            \n        except Exception as e:\n            logger.error(f\"Answer generation failed: {e}\")\n            raise\n    \n    async def determine_tool_usage(self, analysis: ContextAnalysis) -> List[Dict[str, Any]]:\n        \"\"\"\n        Determine which tools should be used based on context analysis\n        \n        Implements tool selection logic for MCP tool integration\n        \"\"\"\n        tool_calls = []\n        \n        # Simple tool selection logic (to be enhanced with actual MCP tool integration)\n        if \"file\" in analysis.intent.lower() or \"document\" in analysis.intent.lower():\n            tool_calls.append({\n                \"tool_name\": \"file_system_tools\",\n                \"parameters\": {\"action\": \"read\"},\n                \"priority\": 1\n            })\n        \n        if \"database\" in analysis.intent.lower() or \"query\" in analysis.intent.lower():\n            tool_calls.append({\n                \"tool_name\": \"database_tools\", \n                \"parameters\": {\"action\": \"query\"},\n                \"priority\": 2\n            })\n        \n        return tool_calls\n    \n    async def process_multimodal_input(self, input_data: ProcessedInput) -> UnifiedContext:\n        \"\"\"\n        Process multimodal input and return unified context\n        \n        Implements multimodal processing as specified in system.md\n        \"\"\"\n        if not self.initialized:\n            raise RuntimeError(\"Engine not initialized\")\n        \n        try:\n            # For now, return a basic unified context\n            # In a full implementation, this would process the multimodal input\n            # using the processor for images, documents, etc.\n            \n            unified_context = UnifiedContext(\n                text_content=str(input_data.content) if isinstance(input_data.content, str) else \"\",\n                image_features=[],\n                document_content=\"\",\n                combined_embedding=[],\n                confidence_scores={\"overall\": 0.8}\n            )\n            \n            logger.debug(f\"Multimodal input processed: {input_data.input_type}\")\n            return unified_context\n            \n        except Exception as e:\n            logger.error(f\"Multimodal processing failed: {e}\")\n            raise\n    \n    async def _generate_with_mlx_optimization(self, prompt: str, max_tokens: int = 512, temperature: float = 0.7) -> str:\n        \"\"\"\n        Generate text with MLX optimizations\n        \n        Private method for MLX-optimized text generation\n        \"\"\"\n        try:\n            # Tokenize input\n            inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n            \n            # Use MLX optimizations if available\n            if self.use_mlx_optimizations:\n                # Convert to MLX tensors for optimization\n                try:\n                    input_ids = mx.array(inputs.input_ids.numpy())\n                    logger.debug(\"Using MLX optimizations for generation\")\n                except Exception as e:\n                    logger.debug(f\"MLX tensor conversion failed, using standard generation: {e}\")\n            \n            # Generate with the model\n            with torch.no_grad():\n                outputs = self.model.generate(\n                    **inputs,\n                    max_new_tokens=max_tokens,\n                    temperature=temperature,\n                    do_sample=True,\n                    pad_token_id=self.tokenizer.eos_token_id,\n                    eos_token_id=self.tokenizer.eos_token_id,\n                    use_cache=True\n                )\n            \n            # Decode response\n            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n            return response[len(prompt):].strip()\n            \n        except Exception as e:\n            logger.error(f\"MLX-optimized generation failed: {e}\")\n            raise\n    \n    def _generate_cache_key(self, context: UnifiedContext) -> str:\n        \"\"\"\n        Generate deterministic cache key for context\n        \n        Implements deterministic caching as specified in architecture.md\n        \"\"\"\n        import hashlib\n        \n        # Create deterministic key based on context content\n        content_hash = hashlib.sha256(\n            f\"{context.text_content}_{context.document_content}_{context.confidence_scores}\".encode()\n        ).hexdigest()\n        \n        return f\"context_{content_hash}\"\n    \n    def _cache_analysis_result(self, cache_key: str, analysis: ContextAnalysis) -> None:\n        \"\"\"\n        Cache analysis result with size management\n        \n        Implements cache management as specified in architecture.md\n        \"\"\"\n        # Check cache size limit\n        if len(self.context_cache) >= self.cache_size_limit:\n            # Remove oldest entry (simple FIFO)\n            oldest_key = next(iter(self.context_cache))\n            del self.context_cache[oldest_key]\n        \n        # Cache the result\n        self.context_cache[cache_key] = analysis\n        \n        logger.debug(f\"Cached analysis result for key: {cache_key}\")\n    \n    def get_performance_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get performance metrics for monitoring\"\"\"\n        avg_inference_time = (\n            sum(self.performance_metrics[\"inference_times\"]) / len(self.performance_metrics[\"inference_times\"])\n            if self.performance_metrics[\"inference_times\"] else 0\n        )\n        \n        cache_hit_rate = (\n            self.performance_metrics[\"context_cache_hits\"] / \n            (self.performance_metrics[\"context_cache_hits\"] + self.performance_metrics[\"context_cache_misses\"])\n            if (self.performance_metrics[\"context_cache_hits\"] + self.performance_metrics[\"context_cache_misses\"]) > 0 else 0\n        )\n        \n        return {\n            \"load_time\": self.performance_metrics[\"load_time\"],\n            \"average_inference_time\": avg_inference_time,\n            \"total_inferences\": len(self.performance_metrics[\"inference_times\"]),\n            \"cache_hit_rate\": cache_hit_rate,\n            \"cache_size\": len(self.context_cache),\n            \"mlx_device\": str(self.mlx_device),\n            \"initialized\": self.initialized\n        }\n    \n    def cleanup(self):\n        \"\"\"Clean up resources and memory\"\"\"\n        if self.model:\n            del self.model\n        if self.tokenizer:\n            del self.tokenizer\n        if self.processor:\n            del self.processor\n        \n        # Clear cache\n        self.context_cache.clear()\n        \n        # MLX cleanup\n        if hasattr(mx, 'eval'):\n            mx.eval(mx.array([]))  # Force evaluation of any pending operations\n        \n        logger.info(\"\u2705 MLX Qwen3-Omni engine cleanup completed\")\n\n# Example usage and testing\nasync def main():\n    \"\"\"Example usage of Qwen3OmniEngine\"\"\"\n    model_path = \"./Qwen3-Omni-30B-A3B-Instruct\"\n    \n    logger.info(\"\ud83d\ude80 Testing MLX-Optimized Qwen3-Omni Engine\")\n    logger.info(\"=\" * 50)\n    \n    engine = Qwen3OmniEngine(model_path)\n    \n    try:\n        if await engine.initialize():\n            logger.info(\"\u2705 MLX Engine initialized successfully\")\n            \n            # Create test input\n            test_input = ProcessedInput(\n                input_type=\"text\",\n                content=\"What is artificial intelligence?\",\n                metadata={\"source\": \"test\"},\n                timestamp=time.time()\n            )\n            \n            # Process input\n            context = await engine.process_multimodal_input(test_input)\n            logger.info(f\"\u2705 Context processed: {context.text_content[:50]}...\")\n            \n            # Analyze context\n            analysis = await engine.analyze_context(context)\n            logger.info(f\"\u2705 Context analyzed: {analysis.intent}\")\n            \n            # Determine tool usage\n            tool_calls = await engine.determine_tool_usage(analysis)\n            logger.info(f\"\u2705 Tool calls determined: {len(tool_calls)} tools\")\n            \n            # Generate answer\n            answer = await engine.generate_answer(analysis)\n            logger.info(f\"\u2705 Answer generated: {answer.answer[:100]}...\")\n            logger.info(f\"Processing time: {answer.processing_time:.2f}s\")\n            \n            # Get performance metrics\n            metrics = engine.get_performance_metrics()\n            logger.info(f\"Performance metrics: {metrics}\")\n            \n            return True\n        else:\n            logger.error(\"\u274c Failed to initialize MLX engine\")\n            return False\n            \n    finally:\n        engine.cleanup()\n\nif __name__ == \"__main__\":\n    import asyncio\n    success = asyncio.run(main())\n    exit(0 if success else 1)\n", "source": "src/core/engines/qwen3_omni_mlx_engine.py"}, {"instruction": "Explain the following code from src/core/logging/log_tailer.py:", "response": "This code is from src/core/logging/log_tailer.py:\n\n\"\"\"\nLog Tailing System for Agentic LLM Core v0.1\n\nThis module provides real-time log monitoring with filtering, level control,\nand follow capabilities for system debugging and monitoring.\n\nCreated: 2024-09-24\nStatus: Draft\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport re\nimport time\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Union, AsyncGenerator\nfrom uuid import uuid4\n\nfrom pydantic import BaseModel, Field, validator\n\n\n# ============================================================================\n# Data Models\n# ============================================================================\n\nclass LogLevel(str, Enum):\n    \"\"\"Log levels in order of severity.\"\"\"\n    DEBUG = \"DEBUG\"\n    INFO = \"INFO\"\n    WARNING = \"WARNING\"\n    ERROR = \"ERROR\"\n    CRITICAL = \"CRITICAL\"\n\n\nclass LogEntry(BaseModel):\n    \"\"\"Parsed log entry.\"\"\"\n    timestamp: datetime = Field(..., description=\"Log timestamp\")\n    level: LogLevel = Field(..., description=\"Log level\")\n    component: str = Field(..., description=\"Component that generated the log\")\n    message: str = Field(..., description=\"Log message\")\n    raw_line: str = Field(..., description=\"Original log line\")\n    line_number: int = Field(..., description=\"Line number in file\")\n    file_path: str = Field(..., description=\"Source log file path\")\n\n\nclass LogFilter(BaseModel):\n    \"\"\"Log filtering configuration.\"\"\"\n    level: Optional[LogLevel] = Field(None, description=\"Minimum log level to include\")\n    components: Optional[List[str]] = Field(None, description=\"Components to include (regex patterns)\")\n    exclude_components: Optional[List[str]] = Field(None, description=\"Components to exclude (regex patterns)\")\n    message_pattern: Optional[str] = Field(None, description=\"Message content pattern (regex)\")\n    exclude_pattern: Optional[str] = Field(None, description=\"Message content to exclude (regex)\")\n    time_range: Optional[tuple] = Field(None, description=\"Time range filter (start, end) timestamps\")\n\n\nclass TailConfig(BaseModel):\n    \"\"\"Log tailing configuration.\"\"\"\n    log_files: List[str] = Field(..., description=\"Log files to monitor\")\n    follow: bool = Field(False, description=\"Follow new log entries in real-time\")\n    lines: int = Field(50, ge=1, le=10000, description=\"Number of recent lines to show initially\")\n    filter_config: Optional[LogFilter] = Field(None, description=\"Log filtering configuration\")\n    refresh_interval: float = Field(1.0, ge=0.1, le=10.0, description=\"Refresh interval for follow mode\")\n    max_buffer_size: int = Field(1000, ge=100, le=10000, description=\"Maximum buffer size for log entries\")\n\n\nclass TailResult(BaseModel):\n    \"\"\"Log tailing result.\"\"\"\n    result_id: str = Field(default_factory=lambda: str(uuid4()), description=\"Result ID\")\n    config: TailConfig = Field(..., description=\"Configuration used\")\n    entries: List[LogEntry] = Field(default_factory=list, description=\"Matching log entries\")\n    total_entries: int = Field(0, description=\"Total entries processed\")\n    filtered_entries: int = Field(0, description=\"Entries that matched filters\")\n    processing_time_ms: float = Field(..., description=\"Processing time in milliseconds\")\n    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description=\"Result timestamp\")\n\n\n# ============================================================================\n# Log Tailing System\n# ============================================================================\n\nclass LogTailer:\n    \"\"\"Real-time log monitoring and filtering system.\"\"\"\n    \n    def __init__(self, config: TailConfig):\n        self.config = config\n        self.log_pattern = re.compile(\n            r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3}) - (\\w+) - (\\w+) - (.+)'\n        )\n        self._running = False\n        self._file_positions: Dict[str, int] = {}\n    \n    async def tail_logs(self) -> TailResult:\n        \"\"\"Tail logs based on configuration.\"\"\"\n        start_time = time.time()\n        \n        try:\n            entries = []\n            total_entries = 0\n            \n            # Read initial lines from each log file\n            for log_file in self.config.log_files:\n                file_entries = await self._read_log_file(log_file)\n                entries.extend(file_entries)\n                total_entries += len(file_entries)\n            \n            # Apply filters\n            filtered_entries = self._apply_filters(entries)\n            \n            # Sort by timestamp\n            filtered_entries.sort(key=lambda x: x.timestamp)\n            \n            # Limit to requested number of lines\n            if len(filtered_entries) > self.config.lines:\n                filtered_entries = filtered_entries[-self.config.lines:]\n            \n            processing_time = (time.time() - start_time) * 1000\n            \n            return TailResult(\n                config=self.config,\n                entries=filtered_entries,\n                total_entries=total_entries,\n                filtered_entries=len(filtered_entries),\n                processing_time_ms=processing_time\n            )\n            \n        except Exception as e:\n            processing_time = (time.time() - start_time) * 1000\n            return TailResult(\n                config=self.config,\n                entries=[],\n                total_entries=0,\n                filtered_entries=0,\n                processing_time_ms=processing_time\n            )\n    \n    async def follow_logs(self) -> AsyncGenerator[LogEntry, None]:\n        \"\"\"Follow logs in real-time.\"\"\"\n        self._running = True\n        \n        try:\n            # Initialize file positions\n            for log_file in self.config.log_files:\n                if Path(log_file).exists():\n                    self._file_positions[log_file] = Path(log_file).stat().st_size\n            \n            while self._running:\n                new_entries = []\n                \n                for log_file in self.config.log_files:\n                    if not Path(log_file).exists():\n                        continue\n                    \n                    # Read new content\n                    file_entries = await self._read_new_content(log_file)\n                    new_entries.extend(file_entries)\n                \n                # Apply filters and yield new entries\n                filtered_entries = self._apply_filters(new_entries)\n                for entry in filtered_entries:\n                    yield entry\n                \n                # Wait for next refresh\n                await asyncio.sleep(self.config.refresh_interval)\n                \n        except Exception as e:\n            # Yield error as a special log entry\n            error_entry = LogEntry(\n                timestamp=datetime.now(timezone.utc),\n                level=LogLevel.ERROR,\n                component=\"log_tailer\",\n                message=f\"Follow mode error: {str(e)}\",\n                raw_line=f\"ERROR - log_tailer - Follow mode error: {str(e)}\",\n                line_number=0,\n                file_path=\"system\"\n            )\n            yield error_entry\n    \n    def stop_follow(self):\n        \"\"\"Stop following logs.\"\"\"\n        self._running = False\n    \n    async def _read_log_file(self, log_file: str) -> List[LogEntry]:\n        \"\"\"Read log file and parse entries.\"\"\"\n        entries = []\n        \n        try:\n            if not Path(log_file).exists():\n                return entries\n            \n            with open(log_file, 'r', encoding='utf-8') as f:\n                lines = f.readlines()\n            \n            for line_num, line in enumerate(lines, 1):\n                entry = self._parse_log_line(line.strip(), log_file, line_num)\n                if entry:\n                    entries.append(entry)\n            \n        except Exception as e:\n            # Create error entry\n            error_entry = LogEntry(\n                timestamp=datetime.now(timezone.utc),\n                level=LogLevel.ERROR,\n                component=\"log_tailer\",\n                message=f\"Failed to read {log_file}: {str(e)}\",\n                raw_line=f\"ERROR - log_tailer - Failed to read {log_file}: {str(e)}\",\n                line_number=0,\n                file_path=log_file\n            )\n            entries.append(error_entry)\n        \n        return entries\n    \n    async def _read_new_content(self, log_file: str) -> List[LogEntry]:\n        \"\"\"Read new content from log file since last position.\"\"\"\n        entries = []\n        \n        try:\n            if not Path(log_file).exists():\n                return entries\n            \n            current_size = Path(log_file).stat().st_size\n            last_position = self._file_positions.get(log_file, 0)\n            \n            if current_size <= last_position:\n                return entries\n            \n            with open(log_file, 'r', encoding='utf-8') as f:\n                f.seek(last_position)\n                new_content = f.read()\n            \n            # Update position\n            self._file_positions[log_file] = current_size\n            \n            # Parse new lines\n            lines = new_content.strip().split('\\n')\n            for line_num, line in enumerate(lines, 1):\n                if line.strip():\n                    entry = self._parse_log_line(line.strip(), log_file, line_num)\n                    if entry:\n                        entries.append(entry)\n            \n        except Exception as e:\n            # Create error entry\n            error_entry = LogEntry(\n                timestamp=datetime.now(timezone.utc),\n                level=LogLevel.ERROR,\n                component=\"log_tailer\",\n                message=f\"Failed to read new content from {log_file}: {str(e)}\",\n                raw_line=f\"ERROR - log_tailer - Failed to read new content from {log_file}: {str(e)}\",\n                line_number=0,\n                file_path=log_file\n            )\n            entries.append(error_entry)\n        \n        return entries\n    \n    def _parse_log_line(self, line: str, file_path: str, line_number: int) -> Optional[LogEntry]:\n        \"\"\"Parse a single log line into LogEntry.\"\"\"\n        try:\n            match = self.log_pattern.match(line)\n            if not match:\n                return None\n            \n            timestamp_str, level_str, component, message = match.groups()\n            \n            # Parse timestamp\n            timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S,%f')\n            \n            # Parse level\n            try:\n                level = LogLevel(level_str)\n            except ValueError:\n                level = LogLevel.INFO\n            \n            return LogEntry(\n                timestamp=timestamp,\n                level=level,\n                component=component,\n                message=message,\n                raw_line=line,\n                line_number=line_number,\n                file_path=file_path\n            )\n            \n        except Exception:\n            return None\n    \n    def _apply_filters(self, entries: List[LogEntry]) -> List[LogEntry]:\n        \"\"\"Apply filters to log entries.\"\"\"\n        if not self.config.filter_config:\n            return entries\n        \n        filter_config = self.config.filter_config\n        filtered_entries = []\n        \n        for entry in entries:\n            # Level filter\n            if filter_config.level:\n                level_order = [LogLevel.DEBUG, LogLevel.INFO, LogLevel.WARNING, LogLevel.ERROR, LogLevel.CRITICAL]\n                if level_order.index(entry.level) < level_order.index(filter_config.level):\n                    continue\n            \n            # Component include filter\n            if filter_config.components:\n                if not any(re.search(pattern, entry.component, re.IGNORECASE) for pattern in filter_config.components):\n                    continue\n            \n            # Component exclude filter\n            if filter_config.exclude_components:\n                if any(re.search(pattern, entry.component, re.IGNORECASE) for pattern in filter_config.exclude_components):\n                    continue\n            \n            # Message pattern filter\n            if filter_config.message_pattern:\n                if not re.search(filter_config.message_pattern, entry.message, re.IGNORECASE):\n                    continue\n            \n            # Exclude pattern filter\n            if filter_config.exclude_pattern:\n                if re.search(filter_config.exclude_pattern, entry.message, re.IGNORECASE):\n                    continue\n            \n            # Time range filter\n            if filter_config.time_range:\n                start_time, end_time = filter_config.time_range\n                if not (start_time <= entry.timestamp <= end_time):\n                    continue\n            \n            filtered_entries.append(entry)\n        \n        return filtered_entries\n\n\n# ============================================================================\n# Main Function\n# ============================================================================\n\nasync def main():\n    \"\"\"Main function for command-line usage.\"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"Log Tailing System\")\n    parser.add_argument(\"--level\", choices=[l.value for l in LogLevel], help=\"Minimum log level\")\n    parser.add_argument(\"--follow\", action=\"store_true\", help=\"Follow logs in real-time\")\n    parser.add_argument(\"--filter\", help=\"Component filter (regex patterns separated by |)\")\n    parser.add_argument(\"--lines\", type=int, default=50, help=\"Number of recent lines to show\")\n    parser.add_argument(\"--files\", nargs=\"*\", default=[\"logs/system.log\", \"logs/error.log\"], help=\"Log files to monitor\")\n    parser.add_argument(\"--refresh\", type=float, default=1.0, help=\"Refresh interval for follow mode\")\n    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Verbose output\")\n    \n    args = parser.parse_args()\n    \n    try:\n        # Parse filter\n        components = None\n        if args.filter:\n            components = args.filter.split(\"|\")\n        \n        # Create filter configuration\n        filter_config = LogFilter(\n            level=LogLevel(args.level) if args.level else None,\n            components=components\n        )\n        \n        # Create tail configuration\n        tail_config = TailConfig(\n            log_files=args.files,\n            follow=args.follow,\n            lines=args.lines,\n            filter_config=filter_config,\n            refresh_interval=args.refresh\n        )\n        \n        # Create log tailer\n        tailer = LogTailer(tail_config)\n        \n        if args.follow:\n            # Follow mode\n            print(f\"Following logs with filter: {args.filter or 'none'}\")\n            print(f\"Level: {args.level or 'all'}\")\n            print(\"Press Ctrl+C to stop...\")\n            print(\"-\" * 80)\n            \n            try:\n                async for entry in tailer.follow_logs():\n                    timestamp = entry.timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")\n                    print(f\"{timestamp} - {entry.level.value} - {entry.component} - {entry.message}\")\n            except KeyboardInterrupt:\n                print(\"\\nStopping log follow...\")\n                tailer.stop_follow()\n        else:\n            # One-time tail\n            result = await tailer.tail_logs()\n            \n            print(\"\\n\" + \"=\"*80)\n            print(\"LOG TAIL RESULT\")\n            print(\"=\"*80)\n            print(f\"Result ID: {result.result_id}\")\n            print(f\"Files: {', '.join(result.config.log_files)}\")\n            print(f\"Total Entries: {result.total_entries}\")\n            print(f\"Filtered Entries: {result.filtered_entries}\")\n            print(f\"Processing Time: {result.processing_time_ms:.2f}ms\")\n            \n            if result.config.filter_config:\n                print(f\"Filter Level: {result.config.filter_config.level or 'all'}\")\n                print(f\"Filter Components: {result.config.filter_config.components or 'all'}\")\n            \n            print(f\"\\n\ud83d\udccb Recent Log Entries:\")\n            print(\"-\" * 80)\n            \n            for entry in result.entries:\n                timestamp = entry.timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")\n                print(f\"{timestamp} - {entry.level.value} - {entry.component} - {entry.message}\")\n        \n        return 0\n        \n    except Exception as e:\n        print(f\"Log tailing failed: {e}\")\n        return 1\n\n\nif __name__ == \"__main__\":\n    exit(asyncio.run(main()))\n", "source": "src/core/logging/log_tailer.py"}, {"instruction": "Explain the following code from src/core/services/input_ingestion_service.py:", "response": "This code is from src/core/services/input_ingestion_service.py:\n\n#!/usr/bin/env python3\n\"\"\"\nInput Ingestion Service for Agentic LLM Core\n\nThis module implements the InputIngestionService as specified in\nmilestone_1_core_pipeline.md Task 1.1.2: Implement InputIngestionService Class\n\nComplies with:\n- System Specification: Agentic LLM Core v0.1 (specs/system.md)\n- Milestone 1: Core Pipeline Foundation (tasks/milestone_1_core_pipeline.md)\n\"\"\"\n\nimport asyncio\nimport logging\nfrom typing import Dict, Any, Optional, List\nfrom pathlib import Path\nfrom datetime import datetime\nimport time\n\nfrom ..schemas.input_schemas import (\n    TextInput,\n    ImageInput,\n    DocumentInput,\n    ProcessedInput,\n    ValidationResult\n)\n\nlogger = logging.getLogger(__name__)\n\nclass InputValidator:\n    \"\"\"Input validation service\"\"\"\n    \n    async def validate_text_input(self, content: str, metadata: Dict[str, Any]) -> ValidationResult:\n        \"\"\"Validate text input\"\"\"\n        try:\n            text_input = TextInput(\n                content=content,\n                language=metadata.get(\"language\", \"en\"),\n                encoding=metadata.get(\"encoding\", \"utf-8\"),\n                metadata=metadata\n            )\n            return ValidationResult(valid=True, metadata={\"validated_input\": text_input})\n        except Exception as e:\n            return ValidationResult(valid=False, errors=[str(e)])\n    \n    async def validate_image_input(self, data: bytes, metadata: Dict[str, Any]) -> ValidationResult:\n        \"\"\"Validate image input\"\"\"\n        try:\n            image_input = ImageInput(\n                data=data,\n                format=metadata.get(\"format\", \"png\"),\n                max_size=metadata.get(\"max_size\", (2048, 2048)),\n                quality=metadata.get(\"quality\", 85),\n                metadata=metadata\n            )\n            return ValidationResult(valid=True, metadata={\"validated_input\": image_input})\n        except Exception as e:\n            return ValidationResult(valid=False, errors=[str(e)])\n    \n    async def validate_document_input(self, path: str, metadata: Dict[str, Any]) -> ValidationResult:\n        \"\"\"Validate document input\"\"\"\n        try:\n            # Check if file exists\n            file_path = Path(path)\n            if not file_path.exists():\n                return ValidationResult(valid=False, errors=[f\"File not found: {path}\"])\n            \n            # Get file extension\n            file_ext = file_path.suffix.lower().lstrip('.')\n            \n            document_input = DocumentInput(\n                file_path=str(file_path),\n                file_type=file_ext,\n                max_size=metadata.get(\"max_size\", 50 * 1024 * 1024),\n                encoding=metadata.get(\"encoding\", \"utf-8\"),\n                metadata=metadata\n            )\n            return ValidationResult(valid=True, metadata={\"validated_input\": document_input})\n        except Exception as e:\n            return ValidationResult(valid=False, errors=[str(e)])\n\nclass InputIngestionService:\n    \"\"\"\n    Core input ingestion service with async processing and validation\n    \n    Implements Task 1.1.2: Implement InputIngestionService Class\n    \n    Acceptance Criteria:\n    - [ ] All input types processed asynchronously\n    - [ ] Validation errors handled gracefully\n    - [ ] Queue backpressure prevents memory issues\n    - [ ] Performance < 1 second per input\n    \"\"\"\n    \n    def __init__(self, validator: InputValidator):\n        self.validator = validator\n        self.input_queue = asyncio.Queue(maxsize=100)\n        self.processing_stats = {\n            \"total_processed\": 0,\n            \"successful_processing\": 0,\n            \"failed_processing\": 0,\n            \"average_processing_time\": 0.0\n        }\n        \n    async def ingest_text(self, content: str, metadata: Optional[Dict[str, Any]] = None) -> ProcessedInput:\n        \"\"\"\n        Ingest and process text input\n        \n        Args:\n            content: Text content to process\n            metadata: Optional metadata for the input\n            \n        Returns:\n            ProcessedInput: Processed text input\n        \"\"\"\n        if metadata is None:\n            metadata = {}\n        \n        start_time = time.time()\n        \n        try:\n            # Validate input\n            validation_result = await self.validator.validate_text_input(content, metadata)\n            \n            if not validation_result.valid:\n                raise ValueError(f\"Text validation failed: {validation_result.errors}\")\n            \n            validated_input = validation_result.metadata[\"validated_input\"]\n            \n            # Process text input\n            processed_input = await self._process_text(validated_input)\n            \n            # Update stats\n            processing_time = time.time() - start_time\n            self._update_stats(True, processing_time)\n            \n            logger.info(f\"Text input processed in {processing_time:.3f}s\")\n            return processed_input\n            \n        except Exception as e:\n            processing_time = time.time() - start_time\n            self._update_stats(False, processing_time)\n            logger.error(f\"Text ingestion failed: {e}\")\n            raise\n    \n    async def ingest_image(self, data: bytes, metadata: Optional[Dict[str, Any]] = None) -> ProcessedInput:\n        \"\"\"\n        Ingest and process image input\n        \n        Args:\n            data: Image data as bytes\n            metadata: Optional metadata for the input\n            \n        Returns:\n            ProcessedInput: Processed image input\n        \"\"\"\n        if metadata is None:\n            metadata = {}\n        \n        start_time = time.time()\n        \n        try:\n            # Validate input\n            validation_result = await self.validator.validate_image_input(data, metadata)\n            \n            if not validation_result.valid:\n                raise ValueError(f\"Image validation failed: {validation_result.errors}\")\n            \n            validated_input = validation_result.metadata[\"validated_input\"]\n            \n            # Process image input\n            processed_input = await self._process_image(validated_input)\n            \n            # Update stats\n            processing_time = time.time() - start_time\n            self._update_stats(True, processing_time)\n            \n            logger.info(f\"Image input processed in {processing_time:.3f}s\")\n            return processed_input\n            \n        except Exception as e:\n            processing_time = time.time() - start_time\n            self._update_stats(False, processing_time)\n            logger.error(f\"Image ingestion failed: {e}\")\n            raise\n    \n    async def ingest_document(self, path: str, metadata: Optional[Dict[str, Any]] = None) -> ProcessedInput:\n        \"\"\"\n        Ingest and process document input\n        \n        Args:\n            path: Path to the document file\n            metadata: Optional metadata for the input\n            \n        Returns:\n            ProcessedInput: Processed document input\n        \"\"\"\n        if metadata is None:\n            metadata = {}\n        \n        start_time = time.time()\n        \n        try:\n            # Validate input\n            validation_result = await self.validator.validate_document_input(path, metadata)\n            \n            if not validation_result.valid:\n                raise ValueError(f\"Document validation failed: {validation_result.errors}\")\n            \n            validated_input = validation_result.metadata[\"validated_input\"]\n            \n            # Process document input\n            processed_input = await self._process_document(validated_input)\n            \n            # Update stats\n            processing_time = time.time() - start_time\n            self._update_stats(True, processing_time)\n            \n            logger.info(f\"Document input processed in {processing_time:.3f}s\")\n            return processed_input\n            \n        except Exception as e:\n            processing_time = time.time() - start_time\n            self._update_stats(False, processing_time)\n            logger.error(f\"Document ingestion failed: {e}\")\n            raise\n    \n    async def _process_text(self, text_input: TextInput) -> ProcessedInput:\n        \"\"\"Process text input\"\"\"\n        # Basic text processing (to be enhanced with actual processing)\n        processed_content = text_input.content\n        \n        # Add processing metadata\n        processing_metadata = {\n            \"original_length\": len(text_input.content),\n            \"language\": text_input.language,\n            \"encoding\": text_input.encoding,\n            \"processing_steps\": [\"validation\", \"normalization\"],\n            \"timestamp\": datetime.now().isoformat()\n        }\n        \n        return ProcessedInput(\n            input_type=\"text\",\n            content=processed_content,\n            metadata=processing_metadata,\n            timestamp=time.time(),\n            confidence=1.0\n        )\n    \n    async def _process_image(self, image_input: ImageInput) -> ProcessedInput:\n        \"\"\"Process image input\"\"\"\n        # Basic image processing (to be enhanced with actual processing)\n        # For now, we'll store the image data as-is\n        \n        # Add processing metadata\n        processing_metadata = {\n            \"original_size\": len(image_input.data),\n            \"format\": image_input.format,\n            \"max_size\": image_input.max_size,\n            \"quality\": image_input.quality,\n            \"processing_steps\": [\"validation\", \"format_check\"],\n            \"timestamp\": datetime.now().isoformat()\n        }\n        \n        return ProcessedInput(\n            input_type=\"image\",\n            content=image_input.data,\n            metadata=processing_metadata,\n            timestamp=time.time(),\n            confidence=1.0\n        )\n    \n    async def _process_document(self, document_input: DocumentInput) -> ProcessedInput:\n        \"\"\"Process document input\"\"\"\n        # Basic document processing (to be enhanced with actual processing)\n        # For now, we'll read the file path and type\n        \n        # Add processing metadata\n        processing_metadata = {\n            \"file_path\": document_input.file_path,\n            \"file_type\": document_input.file_type,\n            \"file_size\": Path(document_input.file_path).stat().st_size,\n            \"encoding\": document_input.encoding,\n            \"processing_steps\": [\"validation\", \"path_check\"],\n            \"timestamp\": datetime.now().isoformat()\n        }\n        \n        return ProcessedInput(\n            input_type=\"document\",\n            content={\"file_path\": document_input.file_path, \"file_type\": document_input.file_type},\n            metadata=processing_metadata,\n            timestamp=time.time(),\n            confidence=1.0\n        )\n    \n    def _update_stats(self, success: bool, processing_time: float):\n        \"\"\"Update processing statistics\"\"\"\n        self.processing_stats[\"total_processed\"] += 1\n        \n        if success:\n            self.processing_stats[\"successful_processing\"] += 1\n        else:\n            self.processing_stats[\"failed_processing\"] += 1\n        \n        # Update average processing time\n        total = self.processing_stats[\"total_processed\"]\n        current_avg = self.processing_stats[\"average_processing_time\"]\n        self.processing_stats[\"average_processing_time\"] = (\n            (current_avg * (total - 1) + processing_time) / total\n        )\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get processing statistics\"\"\"\n        return self.processing_stats.copy()\n    \n    def get_queue_status(self) -> Dict[str, Any]:\n        \"\"\"Get input queue status\"\"\"\n        return {\n            \"queue_size\": self.input_queue.qsize(),\n            \"max_size\": self.input_queue.maxsize,\n            \"is_full\": self.input_queue.full(),\n            \"is_empty\": self.input_queue.empty()\n        }\n\n# Example usage and testing\nasync def main():\n    \"\"\"Example usage of InputIngestionService\"\"\"\n    logger.info(\"\ud83d\ude80 Testing Input Ingestion Service\")\n    logger.info(\"=\" * 50)\n    \n    validator = InputValidator()\n    service = InputIngestionService(validator)\n    \n    try:\n        # Test text ingestion\n        logger.info(\"Testing text ingestion...\")\n        text_input = await service.ingest_text(\n            \"Hello, how are you today?\",\n            {\"source\": \"test\", \"language\": \"en\"}\n        )\n        logger.info(f\"\u2705 Text processed: {text_input.input_type}\")\n        logger.info(f\"Content: {text_input.content[:50]}...\")\n        \n        # Test image ingestion\n        logger.info(\"Testing image ingestion...\")\n        image_data = b\"fake_image_data_for_testing\"\n        image_input = await service.ingest_image(\n            image_data,\n            {\"source\": \"test\", \"format\": \"png\"}\n        )\n        logger.info(f\"\u2705 Image processed: {image_input.input_type}\")\n        logger.info(f\"Data size: {len(image_input.content)} bytes\")\n        \n        # Test document ingestion\n        logger.info(\"Testing document ingestion...\")\n        # Create a temporary file for testing\n        test_file = Path(\"test_document.txt\")\n        test_file.write_text(\"This is a test document.\")\n        \n        try:\n            doc_input = await service.ingest_document(\n                str(test_file),\n                {\"source\": \"test\"}\n            )\n            logger.info(f\"\u2705 Document processed: {doc_input.input_type}\")\n            logger.info(f\"File path: {doc_input.metadata['file_path']}\")\n        finally:\n            # Clean up test file\n            if test_file.exists():\n                test_file.unlink()\n        \n        # Get statistics\n        stats = service.get_stats()\n        logger.info(f\"Processing stats: {stats}\")\n        \n        queue_status = service.get_queue_status()\n        logger.info(f\"Queue status: {queue_status}\")\n        \n        logger.info(\"\ud83c\udf89 Input ingestion service test completed successfully!\")\n        return True\n        \n    except Exception as e:\n        logger.error(f\"\u274c Input ingestion service test failed: {e}\")\n        import traceback\n        traceback.print_exc()\n        return False\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n", "source": "src/core/services/input_ingestion_service.py"}, {"instruction": "Summarize the following document from configs/policies.yaml:", "response": "Document summary from configs/policies.yaml:\n\n# Tool Catalog Configuration for Agentic LLM Core v0.1\n# This file defines policies, tool specifications, and validation rules\n\n# ============================================================================\n# Tool Catalog Policies\n# ============================================================================\n\ntool_catalog:\n  version: \"0.1.0\"\n  last_updated: \"2024-09-24\"\n  description: \"Comprehensive tool catalog for Agentic LLM Core MVP\"\n  \n  # Tool Categories\n  categories:\n    - name: \"file_operations\"\n      description: \"File system operations and management\"\n      priority: \"high\"\n      offline_compatible: true\n      \n    - name: \"database_operations\"\n      description: \"Database queries and data manipulation\"\n      priority: \"high\"\n      offline_compatible: true\n      \n    - name: \"network_operations\"\n      description: \"Network requests and API calls\"\n      priority: \"medium\"\n      offline_compatible: false\n      \n    - name: \"text_processing\"\n      description: \"Text analysis and manipulation\"\n      priority: \"high\"\n      offline_compatible: true\n      \n    - name: \"data_analysis\"\n      description: \"Data processing and analysis\"\n      priority: \"medium\"\n      offline_compatible: true\n      \n    - name: \"security_operations\"\n      description: \"Security and authentication operations\"\n      priority: \"critical\"\n      offline_compatible: true\n      \n    - name: \"utility_operations\"\n      description: \"General utility functions\"\n      priority: \"low\"\n      offline_compatible: true\n\n# ============================================================================\n# Tool Specifications\n# ============================================================================\n\ntools:\n  # File Operations\n  - name: \"file_read\"\n    category: \"file_operations\"\n    description: \"Read contents of a file\"\n    parameters:\n      - name: \"file_path\"\n        type: \"string\"\n        required: true\n        description: \"Path to the file to read\"\n        validation:\n          pattern: \"^[a-zA-Z0-9_/.-]+$\"\n          max_length: 255\n      - name: \"encoding\"\n        type: \"string\"\n        required: false\n        default: \"utf-8\"\n        description: \"File encoding\"\n        validation:\n          enum: [\"utf-8\", \"ascii\", \"latin-1\", \"cp1252\"]\n    returns:\n      content: \"string\"\n      size: \"integer\"\n      encoding: \"string\"\n    safety_level: \"safe\"\n    requires_permissions: [\"read\"]\n    \n  - name: \"file_write\"\n    category: \"file_operations\"\n    description: \"Write content to a file\"\n    parameters:\n      - name: \"file_path\"\n        type: \"string\"\n        required: true\n        description: \"Path to the file to write\"\n        validation:\n          pattern: \"^[a-zA-Z0-9_/.-]+$\"\n          max_length: 255\n      - name: \"content\"\n        type: \"string\"\n        required: true\n        description: \"Content to write\"\n        validation:\n          max_length: 10485760  # 10MB\n      - name: \"encoding\"\n        type: \"string\"\n        required: false\n        default: \"utf-8\"\n        description: \"File encoding\"\n    returns:\n      success: \"boolean\"\n      bytes_written: \"integer\"\n      file_path: \"string\"\n    safety_level: \"moderate\"\n    requires_permissions: [\"write\"]\n    \n  - name: \"file_delete\"\n    category: \"file_operations\"\n    description: \"Delete a file\"\n    parameters:\n      - name: \"file_path\"\n        type: \"string\"\n        required: true\n        description: \"Path to the file to delete\"\n        validation:\n          pattern: \"^[a-zA-Z0-9_/.-]+$\"\n    returns:\n      success: \"boolean\"\n      file_path: \"string\"\n    safety_level: \"dangerous\"\n    requires_permissions: [\"delete\"]\n    \n  # Database Operations\n  - name: \"db_query\"\n    category: \"database_operations\"\n    description: \"Execute a database query\"\n    parameters:\n      - name: \"query\"\n        type: \"string\"\n        required: true\n        description: \"SQL query to execute\"\n        validation:\n          pattern: \"^(SELECT|WITH)\\\\s+\"\n          max_length: 10000\n      - name: \"database\"\n        type: \"string\"\n        required: false\n        default: \"default\"\n        description: \"Database name\"\n      - name: \"parameters\"\n        type: \"object\"\n        required: false\n        description: \"Query parameters\"\n    returns:\n      results: \"array\"\n      row_count: \"integer\"\n      execution_time: \"float\"\n    safety_level: \"moderate\"\n    requires_permissions: [\"read\"]\n    \n  - name: \"db_execute\"\n    category: \"database_operations\"\n    description: \"Execute a database command (INSERT, UPDATE, DELETE)\"\n    parameters:\n      - name: \"command\"\n        type: \"string\"\n        required: true\n        description: \"SQL command to execute\"\n        validation:\n          pattern: \"^(INSERT|UPDATE|DELETE|CREATE|DROP|ALTER)\\\\s+\"\n      - name: \"database\"\n        type: \"string\"\n        required: false\n        default: \"default\"\n        description: \"Database name\"\n      - name: \"parameters\"\n        type: \"object\"\n        required: false\n        description: \"Command parameters\"\n    returns:\n      success: \"boolean\"\n      affected_rows: \"integer\"\n      execution_time: \"float\"\n    safety_level: \"dangerous\"\n    requires_permissions: [\"write\"]\n    \n  # Network Operations\n  - name: \"http_get\"\n    category: \"network_operations\"\n    description: \"Make an HTTP GET request\"\n    parameters:\n      - name: \"url\"\n        type: \"string\"\n        required: true\n        description: \"URL to request\"\n        validation:\n          pattern: \"^https?://\"\n      - name: \"headers\"\n        type: \"object\"\n        required: false\n        description: \"HTTP headers\"\n      - name: \"timeout\"\n        type: \"integer\"\n        required: false\n        default: 30\n        description: \"Request timeout in seconds\"\n    returns:\n      status_code: \"integer\"\n      headers: \"object\"\n      body: \"string\"\n      response_time: \"float\"\n    safety_level: \"moderate\"\n    requires_permissions: [\"network\"]\n    \n  - name: \"http_post\"\n    category: \"network_operations\"\n    description: \"Make an HTTP POST request\"\n    parameters:\n      - name: \"url\"\n        type: \"string\"\n        required: true\n        description: \"URL to request\"\n        validation:\n          pattern: \"^https?://\"\n      - name: \"data\"\n        type: \"object\"\n        required: true\n        description: \"Request body data\"\n      - name: \"headers\"\n        type: \"object\"\n        required: false\n        description: \"HTTP headers\"\n    returns:\n      status_code: \"integer\"\n      headers: \"object\"\n      body: \"string\"\n      response_time: \"float\"\n    safety_level: \"moderate\"\n    requires_permissions: [\"network\"]\n    \n  # Text Processing\n  - name: \"text_extract\"\n    category: \"text_processing\"\n    description: \"Extract text from various formats\"\n    parameters:\n      - name: \"input_data\"\n        type: \"string\"\n        required: true\n        description: \"Input text or file path\"\n      - name: \"format\"\n        type: \"string\"\n        required: false\n        default: \"auto\"\n        description: \"Input format\"\n        validation:\n          enum: [\"auto\", \"text\", \"html\", \"markdown\", \"pdf\", \"docx\"]\n    returns:\n      extracted_text: \"string\"\n      format_detected: \"string\"\n      confidence: \"float\"\n    safety_level: \"safe\"\n    requires_permissions: [\"read\"]\n    \n  - name: \"text_summarize\"\n    category: \"text_processing\"\n    description: \"Generate a summary of text\"\n    parameters:\n      - name: \"text\"\n        type: \"string\"\n        required: true\n        description: \"Text to summarize\"\n        validation:\n          min_length: 100\n          max_length: 100000\n      - name: \"max_length\"\n        type: \"integer\"\n        required: false\n        default: 200\n        description: \"Maximum summary length\"\n      - name: \"style\"\n        type: \"string\"\n        required: false\n        default: \"extractive\"\n        description: \"Summary style\"\n        validation:\n          enum: [\"extractive\", \"abstractive\", \"bullet_points\"]\n    returns:\n      summary: \"string\"\n      original_length: \"integer\"\n      summary_length: \"integer\"\n      compression_ratio: \"float\"\n    safety_level: \"safe\"\n    requires_permissions: []\n    \n  # Data Analysis\n  - name: \"data_analyze\"\n    category: \"data_analysis\"\n    description: \"Analyze structured data\"\n    parameters:\n      - name: \"data\"\n        type: \"array\"\n        required: true\n        description: \"Data to analyze\"\n      - name: \"analysis_type\"\n        type: \"string\"\n        required: true\n        description: \"Type of analysis\"\n        validation:\n          enum: [\"descriptive\", \"correlation\", \"trend\", \"anomaly\", \"clustering\"]\n      - name: \"columns\"\n        type: \"array\"\n        required: false\n        description: \"Columns to analyze\"\n    returns:\n      results: \"object\"\n      insights: \"array\"\n      confidence: \"float\"\n      processing_time: \"float\"\n    safety_level: \"safe\"\n    requires_permissions: []\n    \n  # Security Operations\n  - name: \"encrypt_data\"\n    category: \"security_operations\"\n    description: \"Encrypt sensitive data\"\n    parameters:\n      - name: \"data\"\n        type: \"string\"\n        required: true\n        description: \"Data to encrypt\"\n      - name: \"algorithm\"\n        type: \"string\"\n        required: false\n        default: \"AES-256-GCM\"\n        description: \"Encryption algorithm\"\n        validation:\n          enum: [\"AES-256-GCM\", \"AES-256-CBC\", \"ChaCha20-Poly1305\"]\n      - name: \"key_id\"\n        type: \"string\"\n        required: false\n        description: \"Key identifier\"\n    returns:\n      encrypted_data: \"string\"\n      key_id: \"string\"\n      algorithm: \"string\"\n      iv: \"string\"\n    safety_level: \"critical\"\n    requires_permissions: [\"encrypt\"]\n    \n  - name: \"decrypt_data\"\n    category: \"security_operations\"\n    description: \"Decrypt encrypted data\"\n    parameters:\n      - name: \"encrypted_data\"\n        type: \"string\"\n        required: true\n        description: \"Encrypted data\"\n      - name: \"key_id\"\n        type: \"string\"\n        required: true\n        description: \"Key identifier\"\n    returns:\n      decrypted_data: \"string\"\n      success: \"boolean\"\n    safety_level: \"critical\"\n    requires_permissions: [\"decrypt\"]\n    \n  # Utility Operations\n  - name: \"uuid_generate\"\n    category: \"utility_operations\"\n    description: \"Generate a UUID\"\n    parameters:\n      - name: \"version\"\n        type: \"integer\"\n        required: false\n        default: 4\n        description: \"UUID version\"\n        validation:\n          enum: [1, 3, 4, 5]\n    returns:\n      uuid: \"string\"\n      version: \"integer\"\n    safety_level: \"safe\"\n    requires_permissions: []\n    \n  - name: \"timestamp_now\"\n    category: \"utility_operations\"\n    description: \"Get current timestamp\"\n    parameters:\n      - name: \"format\"\n        type: \"string\"\n        required: false\n        default: \"iso\"\n        description: \"Timestamp format\"\n        validation:\n          enum: [\"iso\", \"unix\", \"rfc2822\", \"custom\"]\n      - name: \"timezone\"\n        type: \"string\"\n        required: false\n        default: \"UTC\"\n        description: \"Timezone for timestamp\"\n    returns:\n      timestamp: \"string\"\n      unix_timestamp: \"integer\"\n      timezone: \"string\"\n    safety_level: \"safe\"\n    requires_permissions: []\n\n# ============================================================================\n# Validation Rules\n# ============================================================================\n\nvalidation:\n  # Global validation rules\n  global_rules:\n    - name: \"parameter_validation\"\n      description: \"All parameters must be validated against their schemas\"\n      severity: \"error\"\n      \n    - name: \"permission_check\"\n      description: \"Tool execution requires appropriate permissions\"\n      severity: \"error\"\n      \n    - name: \"safety_level_check\"\n      description: \"Tools with dangerous/critical safety levels require extra validation\"\n      severity: \"warning\"\n      \n    - name: \"offline_compatibility\"\n      description: \"Tools must be compatible with offline operation\"\n      severity: \"info\"\n  \n  # Tool-specific validation rules\n  tool_rules:\n    file_operations:\n      - name: \"path_sanitization\"\n        description: \"File paths must be sanitized and validated\"\n        severity: \"error\"\n      - name: \"permission_verification\"\n        description: \"File operation permissions must be verified\"\n        severity: \"error\"\n        \n    database_operations:\n      - name: \"sql_injection_prevention\"\n        description: \"SQL queries must be protected against injection\"\n        severity: \"error\"\n      - name: \"query_validation\"\n        description: \"Database queries must be validated\"\n        severity: \"warning\"\n        \n    network_operations:\n      - name: \"url_validation\"\n        description: \"URLs must be validated and sanitized\"\n        severity: \"error\"\n      - name: \"timeout_limits\"\n        description: \"Network timeouts must be within safe limits\"\n        severity: \"warning\"\n        \n    security_operations:\n      - name: \"key_management\"\n        description: \"Encryption keys must be properly managed\"\n        severity: \"error\"\n      - name: \"audit_logging\"\n        description: \"Security operations must be logged\"\n        severity: \"error\"\n\n# ============================================================================\n# Tool Policies\n# ============================================================================\n\npolicies:\n  # Execution policies\n  execution:\n    max_concurrent_tools: 10\n    default_timeout: 30\n    retry_attempts: 3\n    retry_delay: 1.0\n    \n  # Security policies\n  security:\n    require_authentication: true\n    audit_all_operations: true\n    encrypt_sensitive_data: true\n    redact_logs: true\n    \n  # Performance policies\n  performance:\n    max_memory_usage: \"1GB\"\n    max_cpu_usage: \"80%\"\n    rate_limiting: true\n    cache_enabled: true\n    \n  # Offline policies\n  offline:\n    prefer_local_tools: true\n    cache_network_results: true\n    fallback_to_cached: true\n    sync_when_online: true\n\n# ============================================================================\n# Model Policy Configuration\n# ============================================================================\n\nmodel_policy:\n  version: \"0.1.0\"\n  last_updated: \"2024-09-24\"\n  description: \"Model selection and routing policies for Agentic LLM Core\"\n  \n  # Primary and Fallback Models\n  models:\n    primary:\n      name: \"qwen3-omni\"\n      type: \"multimodal\"\n      description: \"Primary multimodal model for complex tasks\"\n      capabilities:\n        - \"text_generation\"\n        - \"text_embedding\"\n        - \"vision_to_text\"\n        - \"multimodal_reasoning\"\n        - \"code_generation\"\n        - \"mathematical_reasoning\"\n      performance:\n        context_length: 128000\n        max_output_tokens: 4096\n        latency_ms: 2000\n        memory_gb: 16\n        gpu_required: true\n      optimization:\n        precision: \"float16\"\n        quantization: \"none\"\n        batch_size: 1\n        use_cache: true\n      \n    fallback:\n      name: \"qwen3-7b-instruct\"\n      type: \"text_only\"\n      description: \"Fallback text-only model for simple tasks\"\n      capabilities:\n        - \"text_generation\"\n        - \"text_embedding\"\n        - \"code_generation\"\n        - \"instruction_following\"\n      performance:\n        context_length: 32000\n        max_output_tokens: 2048\n        latency_ms: 500\n        memory_gb: 8\n        gpu_required: false\n      optimization:\n        precision: \"int8\"\n        quantization: \"dynamic\"\n        batch_size: 4\n        use_cache: true\n\n  # Routing Rules\n  routing_rules:\n    - name: \"multimodal_priority\"\n      condition: \"input_type in ['image', 'multimodal', 'vision']\"\n      action: \"use_primary\"\n      priority: 1\n      description: \"Use primary model for multimodal inputs\"\n      \n    - name: \"large_context_streaming\"\n      condition: \"input_tokens > 32000\"\n      action: \"use_primary_with_streaming\"\n      priority: 2\n      description: \"Use primary model with streaming for large contexts\"\n      parameters:\n        chunk_size: 1024\n        overlap: 128\n        stream_mode: true\n        \n    - name: \"low_latency_fallback\"\n      condition: \"latency_requirement < 1000\"\n      action: \"use_fallback\"\n      priority: 3\n      description: \"Use fallback model for low latency requirements\"\n      \n    - name: \"memory_constraint_fallback\"\n      condition: \"available_memory_gb < 12\"\n      action: \"use_fallback\"\n      priority: 4\n      description: \"Use fallback model when memory is constrained\"\n      \n    - name: \"gpu_unavailable_fallback\"\n      condition: \"gpu_available == false\"\n      action: \"use_fallback\"\n      priority: 5\n      description: \"Use fallback model when GPU is unavailable\"\n      \n    - name: \"default_primary\"\n      condition: \"true\"\n      action: \"use_primary\"\n      priority: 10\n      description: \"Default to primary model for all other cases\"\n\n  # Model Selection Logic\n  selection_logic:\n    algorithm: \"rule_based\"\n    evaluation_order: \"priority_ascending\"\n    conflict_resolution: \"highest_priority_wins\"\n    fallback_strategy: \"graceful_degradation\"\n    \n    # Performance thresholds\n    thresholds:\n      max_latency_ms: 5000\n      min_memory_gb: 4\n      max_context_ratio: 0.8\n      min_confidence_score: 0.7\n      \n    # Load balancing\n    load_balancing:\n      enabled: true\n      strategy: \"round_robin\"\n      health_check_interval: 30\n      max_concurrent_requests: 10\n      \n    # Caching strategy\n    caching:\n      enabled: true\n      cache_size_mb: 1024\n      ttl_seconds: 3600\n      cache_keys:\n        - \"input_hash\"\n        - \"model_name\"\n        - \"parameters_hash\"\n\n  # Model Configuration\n  configuration:\n    # Apple Silicon optimization\n    apple_silicon:\n      use_mps: true\n      mps_fallback: true\n      memory_pressure_handling: true\n      thermal_throttling: true\n      \n    # Precision settings\n    precision:\n      default: \"float16\"\n      fallback_precision: \"int8\"\n      mixed_precision: true\n      \n    # Memory management\n    memory:\n      max_memory_usage: 0.8\n      memory_cleanup_interval: 300\n      swap_enabled: false\n      \n    # Performance monitoring\n    monitoring:\n      enabled: true\n      metrics_collection: true\n      performance_logging: true\n      alert_thresholds:\n        latency_ms: 3000\n        memory_usage_percent: 85\n        error_rate_percent: 5\n\n  # Security and Compliance\n  security:\n    model_validation: true\n    input_sanitization: true\n    output_filtering: true\n    audit_logging: true\n    \n    # Content filtering\n    content_filters:\n      enabled: true\n      categories:\n        - \"harmful_content\"\n        - \"personal_information\"\n        - \"sensitive_data\"\n      action: \"block_and_log\"\n      \n    # Rate limiting\n    rate_limiting:\n      enabled: true\n      requests_per_minute: 60\n      burst_limit: 10\n      cooldown_period: 300\n\n  # Error Handling\n  error_handling:\n    retry_policy:\n      max_retries: 3\n      backoff_strategy: \"exponential\"\n      base_delay_ms: 1000\n      max_delay_ms: 10000\n      \n    fallback_chain:\n      - \"primary_model\"\n      - \"fallback_model\"\n      - \"cached_response\"\n      - \"error_response\"\n      \n    error_recovery:\n      auto_fallback: true\n      graceful_degradation: true\n      user_notification: true\n\n# ============================================================================\n# Security Policy Configuration\n# ============================================================================\n\nsecurity_policy:\n  version: \"0.1.0\"\n  last_updated: \"2024-09-24\"\n  description: \"Comprehensive security policies for Agentic LLM Core\"\n  \n  # Data Redaction Rules\n  redaction:\n    enabled: true\n    mode: \"strict\"  # strict, moderate, lenient\n    \n    # Sensitive Data Patterns\n    patterns:\n      api_keys:\n        enabled: true\n        patterns:\n          - \"api[_-]?key\"\n          - \"apikey\"\n          - \"access[_-]?token\"\n          - \"bearer[_-]?token\"\n          - \"auth[_-]?token\"\n          - \"secret[_-]?key\"\n          - \"private[_-]?key\"\n        replacement: \"[REDACTED_API_KEY]\"\n        case_sensitive: false\n        \n      secrets:\n        enabled: true\n        patterns:\n          - \"password\"\n          - \"passwd\"\n          - \"pwd\"\n          - \"secret\"\n          - \"private[_-]?key\"\n          - \"credential\"\n          - \"auth[_-]?secret\"\n          - \"jwt[_-]?secret\"\n          - \"session[_-]?secret\"\n        replacement: \"[REDACTED_SECRET]\"\n        case_sensitive: false\n        \n      emails:\n        enabled: true\n        patterns:\n          - \"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}\"\n          - \"email[_-]?address\"\n          - \"e[_-]?mail\"\n        replacement: \"[REDACTED_EMAIL]\"\n        case_sensitive: false\n        \n      # Additional sensitive patterns\n      credit_cards:\n        enabled: true\n        patterns:\n          - \"\\\\b(?:4[0-9]{12}(?:[0-9]{3})?|5[1-5][0-9]{14}|3[47][0-9]{13}|3[0-9]{13}|6(?:011|5[0-9]{2})[0-9]{12})\\\\b\"\n        replacement: \"[REDACTED_CC]\"\n        case_sensitive: false\n        \n      ssn:\n        enabled: true\n        patterns:\n          - \"\\\\b\\\\d{3}-\\\\d{2}-\\\\d{4}\\\\b\"\n          - \"\\\\b\\\\d{9}\\\\b\"\n        replacement: \"[REDACTED_SSN]\"\n        case_sensitive: false\n        \n      phone_numbers:\n        enabled: true\n        patterns:\n          - \"\\\\b\\\\d{3}[-.]?\\\\d{3}[-.]?\\\\d{4}\\\\b\"\n          - \"\\\\b\\\\(\\\\d{3}\\\\)\\\\s?\\\\d{3}[-.]?\\\\d{4}\\\\b\"\n        replacement: \"[REDACTED_PHONE]\"\n        case_sensitive: false\n    \n    # Redaction Context\n    context:\n      preserve_context: true\n      log_redactions: true\n      audit_trail: true\n      redaction_count: true\n      \n    # Advanced Redaction\n    advanced:\n      partial_redaction: true\n      redaction_length: 4  # Show last 4 characters\n      hash_redaction: false\n      encryption_redaction: false\n\n  # Tool Allowlist Configuration\n  tool_security:\n    mode: \"allowlist\"  # allowlist, blocklist, open\n    \n    # Allowed Tool Categories\n    allowed_categories:\n      - \"mcp\"\n      - \"langchain\"\n      - \"utility\"\n      - \"file_system\"\n      - \"database\"\n      \n    # Specific Tool Allowlist\n    allowed_tools:\n      # MCP Tools\n      - name: \"mcp:sql\"\n        category: \"mcp\"\n        description: \"SQL database operations via MCP\"\n        permissions:\n          - \"read\"\n          - \"write\"\n          - \"schema_query\"\n        restrictions:\n          - \"no_drop_table\"\n          - \"no_delete_all\"\n          - \"max_query_time: 30s\"\n          \n      - name: \"mcp:fs\"\n        category: \"mcp\"\n        description: \"File system operations via MCP\"\n        permissions:\n          - \"read\"\n          - \"write\"\n          - \"list_directory\"\n        restrictions:\n          - \"no_system_files\"\n          - \"no_executable_files\"\n          - \"max_file_size: 100MB\"\n          \n      # LangChain Tools\n      - name: \"lc:retriever\"\n        category: \"langchain\"\n        description: \"Document retrieval and search\"\n        permissions:\n          - \"search\"\n          - \"retrieve\"\n          - \"embed\"\n        restrictions:\n          - \"no_personal_data\"\n          - \"max_results: 100\"\n          \n      # Utility Tools\n      - name: \"utility:json_parser\"\n        category: \"utility\"\n        description: \"JSON parsing and validation\"\n        permissions:\n          - \"parse\"\n          - \"validate\"\n          - \"format\"\n        restrictions:\n          - \"max_depth: 10\"\n          - \"max_size: 1MB\"\n          \n      - name: \"utility:text_processor\"\n        category: \"utility\"\n        description: \"Text processing and analysis\"\n        permissions:\n          - \"tokenize\"\n          - \"summarize\"\n          - \"extract\"\n        restrictions:\n          - \"no_pii_extraction\"\n          - \"max_length: 1M_chars\"\n\n    # Blocked Tools\n    blocked_tools:\n      - name: \"system:shell\"\n        reason: \"Direct shell access not allowed\"\n      - name: \"network:curl\"\n        reason: \"Direct network access restricted\"\n      - name: \"crypto:encrypt\"\n        reason: \"Encryption operations require special permissions\"\n\n    # Tool Execution Policies\n    execution_policies:\n      timeout_seconds: 30\n      max_memory_mb: 512\n      max_cpu_percent: 80\n      sandbox_mode: true\n      isolation_level: \"strict\"\n      \n      # Resource Limits\n      resource_limits:\n        max_concurrent_tools: 5\n        max_tool_calls_per_session: 100\n        max_tool_calls_per_minute: 20\n        \n      # Security Checks\n      security_checks:\n        input_validation: true\n        output_sanitization: true\n        permission_verification: true\n        resource_monitoring: true\n\n  # Side Effects Configuration\n  side_effects:\n    mode: \"log-only\"  # log-only, block, allow-with-log\n    \n    # Logging Configuration\n    logging:\n      enabled: true\n      level: \"INFO\"  # DEBUG, INFO, WARNING, ERROR\n      format: \"structured\"  # structured, simple, json\n      destination: \"file\"  # file, console, both\n      file_path: \"logs/security.log\"\n      max_file_size_mb: 100\n      backup_count: 5\n      \n    # Side Effect Categories\n    categories:\n      data_access:\n        enabled: true\n        log_level: \"INFO\"\n        include_details: true\n        \n      data_modification:\n        enabled: true\n        log_level: \"WARNING\"\n        include_details: true\n        \n      external_communication:\n        enabled: true\n        log_level: \"WARNING\"\n        include_details: true\n        \n      system_changes:\n        enabled: true\n        log_level: \"ERROR\"\n        include_details: true\n        \n      security_events:\n        enabled: true\n        log_level: \"ERROR\"\n        include_details: true\n\n    # Audit Trail\n    audit_trail:\n      enabled: true\n      retention_days: 90\n      include_user_context: true\n      include_session_info: true\n      include_tool_parameters: true\n      include_results_summary: true\n      \n    # Real-time Monitoring\n    monitoring:\n      enabled: true\n      alert_thresholds:\n        failed_attempts: 5\n        suspicious_patterns: 3\n        resource_exhaustion: 1\n      notification_channels:\n        - \"log\"\n        - \"console\"\n      escalation_policy: \"immediate\"\n\n  # Access Control\n  access_control:\n    # Authentication\n    authentication:\n      required: true\n      methods: [\"api_key\", \"jwt\", \"oauth2\"]\n      session_timeout: 3600  # seconds\n      max_failed_attempts: 3\n      lockout_duration: 900  # seconds\n      \n    # Authorization\n    authorization:\n      model: \"rbac\"  # rbac, abac, simple\n      roles:\n        - name: \"admin\"\n          permissions: [\"*\"]\n        - name: \"user\"\n          permissions: [\"read\", \"execute_allowed_tools\"]\n        - name: \"readonly\"\n          permissions: [\"read\"]\n          \n    # Rate Limiting\n    rate_limiting:\n      enabled: true\n      requests_per_minute: 60\n      requests_per_hour: 1000\n      burst_limit: 10\n      cooldown_period: 300\n      \n    # IP Restrictions\n    ip_restrictions:\n      enabled: false\n      allowed_ips: []\n      blocked_ips: []\n      geo_restrictions: false\n\n  # Data Protection\n  data_protection:\n    # Encryption\n    encryption:\n      at_rest: true\n      in_transit: true\n      algorithm: \"AES-256-GCM\"\n      key_rotation_days: 90\n      \n    # Data Classification\n    classification:\n      levels: [\"public\", \"internal\", \"confidential\", \"restricted\"]\n      default_level: \"internal\"\n      auto_classification: true\n      \n    # Data Retention\n    retention:\n      logs: 90  # days\n      cache: 7  # days\n      temp_files: 1  # days\n      audit_trail: 365  # days\n      \n    # Data Loss Prevention\n    dlp:\n      enabled: true\n      scan_content: true\n      block_sensitive_uploads: true\n      quarantine_suspicious: true\n\n  # Compliance\n  compliance:\n    # Standards\n    standards:\n      - \"GDPR\"\n      - \"CCPA\"\n      - \"SOC2\"\n      - \"ISO27001\"\n      \n    # Privacy\n    privacy:\n      data_minimization: true\n      purpose_limitation: true\n      storage_limitation: true\n      consent_management: true\n      \n    # Reporting\n    reporting:\n      security_incidents: true\n      data_breaches: true\n      compliance_violations: true\n      audit_reports: true\n", "source": "configs/policies.yaml"}, {"instruction": "Summarize the following document from configs/samples/supa_get_tables.json:", "response": "Document summary from configs/samples/supa_get_tables.json:\n\n{\n  \"test_name\": \"supabase-rest:postgrestRequest\",\n  \"description\": \"Test Supabase PostgREST API to get database tables\",\n  \"server_type\": \"supabase-rest\",\n  \"tool_name\": \"postgrestRequest\",\n  \"input_data\": {\n    \"method\": \"GET\",\n    \"endpoint\": \"/rest/v1/\",\n    \"headers\": {\n      \"Accept\": \"application/json\",\n      \"apikey\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InRlc3QtcHJvamVjdCIsInJvbGUiOiJhbm9uIiwiaWF0IjoxNjQwOTk1MjAwLCJleHAiOjE5NTYzNTUyMDB9.test-key\",\n      \"Authorization\": \"Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InRlc3QtcHJvamVjdCIsInJvbGUiOiJhbm9uIiwiaWF0IjoxNjQwOTk1MjAwLCJleHAiOjE5NTYzNTUyMDB9.test-key\"\n    },\n    \"params\": {\n      \"select\": \"*\"\n    },\n    \"timeout\": 30\n  },\n  \"input_schema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"method\": {\n        \"type\": \"string\",\n        \"enum\": [\"GET\", \"POST\", \"PUT\", \"DELETE\", \"PATCH\"]\n      },\n      \"endpoint\": {\n        \"type\": \"string\",\n        \"pattern\": \"^/rest/v1/\"\n      },\n      \"headers\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"Accept\": {\"type\": \"string\"},\n          \"apikey\": {\"type\": \"string\"},\n          \"Authorization\": {\"type\": \"string\"}\n        },\n        \"required\": [\"Accept\", \"apikey\"]\n      },\n      \"params\": {\n        \"type\": \"object\"\n      },\n      \"timeout\": {\n        \"type\": \"integer\",\n        \"minimum\": 1,\n        \"maximum\": 300\n      }\n    },\n    \"required\": [\"method\", \"endpoint\", \"headers\"]\n  },\n  \"expected_response\": {\n    \"schema\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"status\": {\n          \"type\": \"string\",\n          \"enum\": [\"success\", \"error\"]\n        },\n        \"data\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"table_name\": {\n                \"type\": \"string\"\n              },\n              \"table_schema\": {\n                \"type\": \"string\"\n              },\n              \"table_type\": {\n                \"type\": \"string\"\n              }\n            },\n            \"required\": [\"table_name\", \"table_schema\", \"table_type\"]\n          }\n        },\n        \"message\": {\n          \"type\": \"string\"\n        },\n        \"status_code\": {\n          \"type\": \"integer\",\n          \"minimum\": 200,\n          \"maximum\": 299\n        }\n      },\n      \"required\": [\"status\", \"data\", \"status_code\"]\n    },\n    \"sample_data\": {\n      \"status\": \"success\",\n      \"data\": [\n        {\n          \"table_name\": \"users\",\n          \"table_schema\": \"public\",\n          \"table_type\": \"BASE TABLE\"\n        },\n        {\n          \"table_name\": \"posts\",\n          \"table_schema\": \"public\", \n          \"table_type\": \"BASE TABLE\"\n        },\n        {\n          \"table_name\": \"comments\",\n          \"table_schema\": \"public\",\n          \"table_type\": \"BASE TABLE\"\n        }\n      ],\n      \"message\": \"Tables retrieved successfully\",\n      \"status_code\": 200\n    }\n  },\n  \"test_config\": {\n    \"timeout\": 30,\n    \"retries\": 3,\n    \"validate_schema\": true,\n    \"validate_response\": true,\n    \"mock_mode\": false\n  },\n  \"security\": {\n    \"allowed_methods\": [\"GET\"],\n    \"allowed_endpoints\": [\"/rest/v1/\"],\n    \"require_auth\": true,\n    \"rate_limit\": {\n      \"requests_per_minute\": 60\n    }\n  }\n}\n", "source": "configs/samples/supa_get_tables.json"}, {"instruction": "Summarize the following document from README.md:", "response": "Document summary from README.md:\n\n# Agentic LLM Core v0.1\n\nA sophisticated **agentic LLM framework** designed for local-first AI agent systems with comprehensive tool integration, multimodal capabilities, and enterprise-grade security.\n\n## \ud83d\ude80 Overview\n\nThis project implements a complete **agentic LLM core system** with:\n\n- **\ud83e\udde0 Multimodal AI**: Qwen3-Omni integration for text, image, audio, and video processing\n- **\ud83d\udd27 Tool Catalog**: 14 comprehensive tools across 7 categories with safety levels and permissions\n- **\ud83d\udd12 Security-First**: Advanced redaction, audit trails, and access control\n- **\ud83d\udcca MCP Integration**: Model Context Protocol adapter for tool execution\n- **\ud83c\udfd7\ufe0f Agentic Architecture**: Planner, Reviewer, and Runner components for autonomous task execution\n- **\ud83d\udcbe Vector Storage**: PostgreSQL and Weaviate integration for knowledge management\n- **\ud83d\udcc8 Performance**: Apple Silicon optimization with < 50ms latency targets\n\n## \ud83d\udcc1 Project Structure\n\n```\n\u251c\u2500\u2500 src/core/                    # Core system components\n\u2502   \u251c\u2500\u2500 chat/                   # Multimodal chat system\n\u2502   \u251c\u2500\u2500 mcp/                    # Model Context Protocol integration\n\u2502   \u251c\u2500\u2500 memory/                 # Vector storage (PostgreSQL, Weaviate)\n\u2502   \u251c\u2500\u2500 models/                 # Data contracts and schemas\n\u2502   \u251c\u2500\u2500 providers/              # LLM providers (Qwen3)\n\u2502   \u251c\u2500\u2500 runtime/                # Agentic components (Planner, Reviewer, Runner)\n\u2502   \u251c\u2500\u2500 security/               # Security policies and redaction\n\u2502   \u251c\u2500\u2500 tools/                  # Tool catalog and MCP adapter\n\u2502   \u2514\u2500\u2500 vision/                 # Vision processing system\n\u251c\u2500\u2500 configs/                    # Configuration files\n\u2502   \u251c\u2500\u2500 policies.yaml          # Tool catalog and security policies\n\u2502   \u2514\u2500\u2500 samples/               # Sample configurations\n\u251c\u2500\u2500 tests/                      # Comprehensive test suite (318 tests)\n\u251c\u2500\u2500 examples/                   # Usage examples\n\u251c\u2500\u2500 specs/                      # System specifications\n\u251c\u2500\u2500 plans/                      # Architecture and milestone plans\n\u2514\u2500\u2500 tasks/                      # Milestone definitions\n```\n\n## \ud83d\udee0\ufe0f Tool Catalog\n\nThe system includes **14 production-ready tools** across **7 categories**:\n\n### File Operations (3 tools)\n- `file_read` - Read file contents (\ud83d\udfe2 Safe)\n- `file_write` - Write content to files (\ud83d\udfe1 Moderate)  \n- `file_delete` - Delete files (\ud83d\udfe0 Dangerous)\n\n### Database Operations (2 tools)\n- `db_query` - Execute SELECT queries (\ud83d\udfe1 Moderate)\n- `db_execute` - Execute INSERT/UPDATE/DELETE (\ud83d\udfe0 Dangerous)\n\n### Network Operations (2 tools)\n- `http_get` - HTTP GET requests (\ud83d\udfe1 Moderate)\n- `http_post` - HTTP POST requests (\ud83d\udfe1 Moderate)\n\n### Text Processing (2 tools)\n- `text_extract` - Extract text from various formats (\ud83d\udfe2 Safe)\n- `text_summarize` - Generate text summaries (\ud83d\udfe2 Safe)\n\n### Data Analysis (1 tool)\n- `data_analyze` - Analyze structured data (\ud83d\udfe2 Safe)\n\n### Security Operations (2 tools)\n- `encrypt_data` - Encrypt sensitive data (\ud83d\udd34 Critical)\n- `decrypt_data` - Decrypt encrypted data (\ud83d\udd34 Critical)\n\n### Utility Operations (2 tools)\n- `uuid_generate` - Generate UUIDs (\ud83d\udfe2 Safe)\n- `timestamp_now` - Get current timestamps (\ud83d\udfe2 Safe)\n\n## \ud83d\udd27 Quick Start\n\n### Prerequisites\n- Python 3.9+\n- Apple Silicon Mac (optimized for M1/M2/M3)\n- PostgreSQL (for vector storage)\n- 16GB+ RAM (for Qwen3-Omni model)\n\n### Installation\n\n1. **Clone the repository**\n   ```bash\n   git clone <repository-url>\n   cd prompt-engineering\n   ```\n\n2. **Install dependencies**\n   ```bash\n   pip3 install -r requirements.txt\n   pip3 install pytest-asyncio  # For async test support\n   ```\n\n3. **Validate the system**\n   ```bash\n   # Validate tool catalog\n   python3 src/core/tools/catalog.py configs/policies.yaml --validate --print-summaries\n   \n   # Run tests\n   python3 -m pytest tests/ -v\n   ```\n\n4. **Run examples**\n   ```bash\n   # Multimodal chat example\n   python3 examples/multimodal_chat.py\n   \n   # Task execution example\n   python3 examples/runner_example.py\n   ```\n\n## \ud83d\udcca System Status\n\n### \u2705 **Compliance Score: 87%**\n\n| Category | Score | Status |\n|----------|-------|--------|\n| **Specification Compliance** | 100% | \u2705 EXCELLENT |\n| **Security Compliance** | 100% | \u2705 EXCELLENT |\n| **Model Policy Compliance** | 100% | \u2705 EXCELLENT |\n| **Architecture Compliance** | 100% | \u2705 EXCELLENT |\n| **Test Coverage** | 4% | \u274c NEEDS IMPROVEMENT |\n| **Code Quality** | 90% | \u26a0\ufe0f GOOD |\n| **Documentation** | 100% | \u2705 EXCELLENT |\n| **Configuration** | 100% | \u2705 EXCELLENT |\n\n### \ud83d\udea8 **Critical Issues: 0** \ud83d\udfe2\n- All core dependencies available\n- Project structure compliant\n- Security and model policies operational\n- No blocking configuration errors\n\n### \u26a0\ufe0f **Areas for Improvement**\n1. **Test Coverage**: Currently 4% \u2192 Target: 80%\n2. **Pydantic V2 Migration**: Update deprecated `@validator` decorators\n3. **Async Test Support**: Install `pytest-asyncio` for async test execution\n\n## \ud83d\udd12 Security Features\n\n- **Data Redaction**: 6 pattern types (API keys, secrets, emails, credit cards, SSN, phone numbers)\n- **Tool Access Control**: Allowlist/blocklist with permission system\n- **Audit Trail**: 90-day retention with comprehensive logging\n- **Side Effects Monitoring**: Real-time security event detection\n\n## \ud83e\udd16 Model Integration\n\n- **Primary Model**: Qwen3-Omni-30B-A3B (multimodal, 128K context)\n- **Fallback Model**: Qwen3-7B-Instruct (text-only, 32K context)\n- **Intelligent Routing**: Resource-aware model selection\n- **Performance Optimization**: Apple Silicon MPS acceleration\n\n## \ud83d\udcc8 Performance Targets\n\n- **Latency**: < 50ms for critical operations\n- **Memory**: < 16GB for primary model\n- **Throughput**: 10+ concurrent requests\n- **Availability**: 99.9% uptime target\n\n## \ud83e\uddea Testing\n\nThe system includes **318 comprehensive tests** across all components:\n\n```bash\n# Run all tests\npython3 -m pytest tests/ -v\n\n# Run specific test suites\npython3 -m pytest tests/test_contracts.py -v\npython3 -m pytest tests/test_mcp_adapter.py -v\npython3 -m pytest tests/test_planner.py -v\n```\n\n## \ud83d\udcda Documentation\n\n- **System Specifications**: `specs/system.md`\n- **Architecture Plans**: `plans/architecture.md`\n- **Security Policies**: `configs/policies.yaml`\n- **Compliance Report**: `COMPLIANCE_REPORT.md`\n- **Usage Examples**: `examples/` directory\n\n## \ud83d\ude80 Roadmap\n\n### Phase 1: Test Coverage (Priority: HIGH)\n- [ ] Implement comprehensive test suites for all modules\n- [ ] Achieve 80%+ coverage target\n- [ ] Set up automated coverage reporting\n\n### Phase 2: Code Quality (Priority: MEDIUM)\n- [ ] Migrate to Pydantic V2 (`@field_validator`)\n- [ ] Add comprehensive docstrings\n- [ ] Implement code quality checks in CI/CD\n\n### Phase 3: Production Readiness (Priority: LOW)\n- [ ] Performance optimization\n- [ ] Advanced monitoring\n- [ ] Load testing\n\n## \ud83e\udd1d Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Add tests for new functionality\n5. Ensure all tests pass\n6. Submit a pull request\n\n## \ud83d\udcc4 License\n\nThis project is licensed under the Apache 2.0 License - see the LICENSE file for details.\n\n## \ud83c\udd98 Support\n\nFor questions or issues:\n- **Documentation**: Check the `specs/` and `plans/` directories\n- **Examples**: See the `examples/` directory\n- **Tests**: Run `python3 -m pytest tests/ -v` for system validation\n\n---\n\n**Agentic LLM Core v0.1** - Building the future of autonomous AI agents \ud83d\ude80\n", "source": "README.md"}, {"instruction": "Summarize the following document from AGENTIC_LLM_CORE_STATUS.md:", "response": "Document summary from AGENTIC_LLM_CORE_STATUS.md:\n\n# Agentic LLM Core v0.1 - Project Status Report\n\n## \ud83c\udfaf **Project Overview**\n\n**Status**: \u2705 **MILESTONE 1 COMPLETED** - Core Pipeline Foundation  \n**Date**: September 24, 2025  \n**Architecture Compliance**: 100% aligned with project specifications  \n\n## \ud83d\udcca **Completion Summary**\n\n### \u2705 **Completed Components**\n\n| Component | Status | Performance | Compliance |\n|-----------|--------|-------------|------------|\n| **Qwen3-Omni Model Download** | \u2705 Complete | 60GB downloaded | \u2705 Offline-ready |\n| **Input Schemas (Pydantic)** | \u2705 Complete | <1ms validation | \u2705 Type-safe |\n| **Input Ingestion Service** | \u2705 Complete | <1s processing | \u2705 Async pipeline |\n| **Qwen3-Omni Engine Framework** | \u2705 Complete | <3s total pipeline | \u2705 MLX-ready |\n| **Integration Pipeline** | \u2705 Complete | 0.002s total time | \u2705 All requirements met |\n| **Error Handling** | \u2705 Complete | Graceful failures | \u2705 Production-ready |\n| **Performance Monitoring** | \u2705 Complete | Real-time metrics | \u2705 Observable |\n\n### \ud83d\ude80 **Performance Achievements**\n\n- **Total Pipeline Time**: 0.002s (Target: <5s) \u2705 **2500x faster than requirement**\n- **Input Ingestion**: 0.002s (Target: <1s) \u2705 **500x faster than requirement**  \n- **Context Analysis**: 0.000s (Target: <2s) \u2705 **Instant processing**\n- **Answer Generation**: 0.000s (Target: <3s) \u2705 **Instant processing**\n- **Model Load Time**: 2.8s (Target: <10s) \u2705 **3.5x faster than requirement**\n\n## \ud83c\udfd7\ufe0f **Architecture Implementation**\n\n### **Core Components Delivered**\n\n1. **Input Infrastructure** \u2705\n   - `src/core/schemas/input_schemas.py` - Pydantic schemas for all input types\n   - `src/core/services/input_ingestion_service.py` - Async input processing\n   - Support for text, image, and document inputs\n   - Comprehensive validation and error handling\n\n2. **Qwen3-Omni Engine** \u2705\n   - `src/core/engines/qwen3_omni_engine.py` - Framework implementation\n   - `src/core/engines/qwen3_omni_mlx_engine.py` - MLX-optimized version\n   - Deterministic caching for reproducibility\n   - Performance monitoring and metrics\n\n3. **Processing Pipeline** \u2705\n   - `integration_test.py` - Complete end-to-end testing\n   - Context analysis and understanding\n   - Answer generation with confidence scoring\n   - Multimodal input processing support\n\n### **Key Features Implemented**\n\n- **\ud83c\udf4e Apple Silicon Ready**: MLX integration prepared for M1/M2/M3 optimization\n- **\ud83d\udd04 Deterministic Processing**: Caching system ensures reproducible results\n- **\u26a1 High Performance**: All performance targets exceeded by orders of magnitude\n- **\ud83d\udee0\ufe0f Tool Integration Ready**: Framework prepared for MCP tool integration\n- **\ud83d\udcf1 Multimodal Support**: Text, image, and document processing capabilities\n- **\ud83d\udd0d Observable**: Comprehensive metrics and monitoring\n- **\ud83d\udee1\ufe0f Production Ready**: Robust error handling and validation\n\n## \ud83d\udcc1 **Project Structure**\n\n```\n/Users/christianmerrill/Prompt Engineering/\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 core/\n\u2502       \u251c\u2500\u2500 schemas/\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u2514\u2500\u2500 input_schemas.py          # Pydantic schemas\n\u2502       \u251c\u2500\u2500 services/\n\u2502       \u2502   \u2514\u2500\u2500 input_ingestion_service.py # Async input processing\n\u2502       \u2514\u2500\u2500 engines/\n\u2502           \u251c\u2500\u2500 qwen3_omni_engine.py      # Framework implementation\n\u2502           \u2514\u2500\u2500 qwen3_omni_mlx_engine.py  # MLX-optimized version\n\u251c\u2500\u2500 Qwen3-Omni-30B-A3B-Instruct/         # Downloaded model (60GB)\n\u251c\u2500\u2500 integration_test.py                   # Complete pipeline test\n\u251c\u2500\u2500 qwen_*.py                            # Experimentation scripts\n\u2514\u2500\u2500 AGENTIC_LLM_CORE_STATUS.md           # This status report\n```\n\n## \ud83e\uddea **Testing Results**\n\n### **Integration Test Results** \u2705\n\n```\n\ud83e\uddea Test 1: Text Processing Pipeline\n\u2705 Text ingested in 0.002s\n\u2705 Context processed in 0.000s  \n\u2705 Context analyzed in 0.000s\n\u2705 Answer generated in 0.000s\n\ud83d\udcca Total pipeline time: 0.002s\n\n\ud83e\uddea Test 2: Performance Validation\n\u2705 Total time 0.002s meets <5s requirement\n\u2705 Ingestion time 0.002s meets <1s requirement\n\u2705 Analysis time 0.000s meets <2s requirement\n\u2705 Generation time 0.000s meets <3s requirement\n\n\ud83e\uddea Test 3: Multiple Input Types\n\u2705 Image input processed\n\u2705 Document context analyzed\n\u2705 All input types supported\n\n\ud83e\uddea Test 4: Statistics and Metrics\n\ud83d\udcca Ingestion stats: 3 processed, 0 failed\n\ud83d\udcca Engine metrics: 2.8s load time, 100% success rate\n\n\ud83e\uddea Test 5: Error Handling\n\u2705 Empty text input properly rejected\n\u2705 Invalid image format properly rejected\n\u2705 Robust error handling verified\n```\n\n## \ud83c\udfaf **Milestone 1 Achievement**\n\n**Task 1.1: Design Pydantic Input Schemas** \u2705 **COMPLETED**\n- [x] All input types validated with Pydantic\n- [x] Comprehensive validation rules implemented\n- [x] Type-safe input handling\n\n**Task 1.2: Implement InputIngestionService** \u2705 **COMPLETED**\n- [x] Async processing pipeline\n- [x] Queue management with backpressure\n- [x] Performance monitoring\n\n**Task 2.1: Qwen3-Omni Engine Integration** \u2705 **COMPLETED**\n- [x] Framework implementation ready\n- [x] MLX optimization prepared\n- [x] Context analysis and answer generation\n\n**Task 2.2: Context Analysis & Answer Generation** \u2705 **COMPLETED**\n- [x] Context analysis engine\n- [x] Answer generation service\n- [x] Confidence scoring\n\n## \ud83d\udd2e **Next Steps (Milestone 2)**\n\n### **Immediate Priorities**\n\n1. **MCP Tool Integration** \ud83d\udee0\ufe0f\n   - Implement MCP tool execution engine\n   - Create tool selection and routing logic\n   - Add tool result processing\n\n2. **Enhanced Qwen3-Omni Support** \ud83e\udd16\n   - Update to latest transformers when Qwen3-Omni support is added\n   - Implement actual model inference (currently using framework)\n   - Add multimodal processing capabilities\n\n3. **Production Enhancements** \ud83d\ude80\n   - Add comprehensive logging and monitoring\n   - Implement health checks and metrics\n   - Add configuration management\n\n### **Technical Debt**\n\n- **Model Support**: Currently using framework implementation pending full Qwen3-Omni support\n- **Multimodal Processing**: Basic framework ready, needs actual image/document processing\n- **Tool Integration**: Framework prepared, needs MCP tool implementation\n\n## \ud83c\udfc6 **Success Metrics Achieved**\n\n| Metric | Target | Achieved | Status |\n|--------|--------|----------|--------|\n| Pipeline Performance | <5s | 0.002s | \u2705 **2500x better** |\n| Input Processing | <1s | 0.002s | \u2705 **500x better** |\n| Context Analysis | <2s | 0.000s | \u2705 **Instant** |\n| Answer Generation | <3s | 0.000s | \u2705 **Instant** |\n| Model Load Time | <10s | 2.8s | \u2705 **3.5x better** |\n| Error Handling | Graceful | Robust | \u2705 **Production ready** |\n| Code Quality | Type-safe | Pydantic | \u2705 **Type-safe** |\n\n## \ud83c\udf89 **Conclusion**\n\n**Milestone 1 is COMPLETE** with all performance targets exceeded by orders of magnitude. The Agentic LLM Core v0.1 foundation is solid, performant, and ready for Milestone 2 development.\n\n**Key Achievements:**\n- \u2705 Complete pipeline architecture implemented\n- \u2705 All performance requirements exceeded\n- \u2705 Production-ready error handling\n- \u2705 Apple Silicon optimization prepared\n- \u2705 Comprehensive testing completed\n- \u2705 Full project specification compliance\n\n**Ready for:** Milestone 2 - Tool Integration & MCP Implementation\n\n---\n\n*Generated on: September 24, 2025*  \n*Project: Agentic LLM Core v0.1*  \n*Status: Milestone 1 Complete \u2705*\n", "source": "AGENTIC_LLM_CORE_STATUS.md"}]  # Limit for testing

def format_instruction(example):
    return f"<|im_start|>user\n{example['instruction']}<|im_end|>\n<|im_start|>assistant\n{example['response']}<|im_end|>"

formatted_data = [format_instruction(ex) for ex in training_examples]
dataset = Dataset.from_list([{"text": text} for text in formatted_data])

# Training arguments
training_args = TrainingArguments(
    output_dir="./finetuned-qwen2.5-7b",
    num_train_epochs=2,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    warmup_steps=50,
    learning_rate=5e-5,
    logging_steps=10,
    save_steps=100,
    save_total_limit=2,
    prediction_loss_only=True,
    remove_unused_columns=False,
    dataloader_pin_memory=False,
    dataloader_num_workers=0,
    fp16=True,
    gradient_accumulation_steps=4,
)

# Create trainer
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    data_collator=data_collator,
)

# Train
logger.info("Starting training...")
trainer.train()

# Save model
trainer.save_model("./finetuned-qwen2.5-7b")
tokenizer.save_pretrained("./finetuned-qwen2.5-7b")

logger.info(f"Fine-tuning completed! Model saved to ./finetuned-qwen2.5-7b")
