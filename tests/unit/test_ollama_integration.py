#!/usr/bin/env python3
""'
Test script for Ollama integration with Agentic LLM Core
""'

import asyncio
import logging
import sys
import os

# Add the src directory to the path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "..'))

from src.core.engines.ollama_adapter import OllamaQwen3OmniEngine

# Setup logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

async def test_ollama_integration():
    """Test the Ollama integration""'
    logger.info("üß™ Testing Ollama Integration with Agentic LLM Core')
    logger.info("=' * 60)

    # Initialize the engine
    engine = OllamaQwen3OmniEngine()

    if await engine.initialize():
        logger.info("‚úÖ Ollama engine initialized successfully')

        # Test 1: General text generation
        logger.info("\nüìù Test 1: General Text Generation')
        logger.info("-' * 40)

        class MockContext:
            """TODO: Add docstring."""
            """TODO: Add docstring.""'
            text_content = "What is the Agentic LLM Core system and how does it work?'

        context = MockContext()
        analysis = await engine.analyze_context(context)
        answer = await engine.generate_answer(analysis)

        logger.info(f"Question: {context.text_content}')
        logger.info(f"Answer: {answer.answer[:200]}...')
        logger.info(f"Processing time: {answer.processing_time:.2f}s')
        logger.info(f"Model used: {answer.metadata.get("model")}')

        # Test 2: Coding task
        logger.info("\nüíª Test 2: Coding Task')
        logger.info("-' * 40)

        coding_context = MockContext()
        coding_context.text_content = "Write a Python function to calculate fibonacci numbers'

        coding_analysis = await engine.analyze_context(coding_context)
        coding_answer = await engine.generate_answer(coding_analysis)

        logger.info(f"Question: {coding_context.text_content}')
        logger.info(f"Answer: {coding_answer.answer[:200]}...')
        logger.info(f"Processing time: {coding_answer.processing_time:.2f}s')
        logger.info(f"Model used: {coding_answer.metadata.get("model")}')

        # Test 3: Fast response
        logger.info("\n‚ö° Test 3: Fast Response')
        logger.info("-' * 40)

        fast_context = MockContext()
        fast_context.text_content = "What is 2+2?'

        fast_analysis = await engine.analyze_context(fast_context)
        fast_answer = await engine.generate_answer(fast_analysis)

        logger.info(f"Question: {fast_context.text_content}')
        logger.info(f"Answer: {fast_answer.answer}')
        logger.info(f"Processing time: {fast_answer.processing_time:.2f}s')
        logger.info(f"Model used: {fast_answer.metadata.get("model")}')

        logger.info("\nüéâ All tests completed successfully!')
        logger.info("‚úÖ Ollama integration is working properly')

        return True
    else:
        logger.error("‚ùå Failed to initialize Ollama engine')
        return False

async def test_model_routing():
    """Test model routing capabilities""'
    logger.info("\nüéØ Testing Model Routing')
    logger.info("=' * 40)

    engine = OllamaQwen3OmniEngine()

    if await engine.initialize():
        # Test different routing scenarios
        test_cases = [
            {
                "name": "General Text',
                "prompt": "Explain artificial intelligence',
                "task_type": "text_generation',
                "expected_model": "primary'
            },
            {
                "name": "Coding Task',
                "prompt": "Write a Python function',
                "task_type": "code_generation',
                "expected_model": "coding'
            },
            {
                "name": "Fast Response',
                "prompt": "What is 1+1?',
                "task_type": "simple_reasoning',
                "latency_requirement': 300,
                "expected_model": "lightweight'
            }
        ]

        for test_case in test_cases:
            logger.info(f"\nTesting: {test_case["name"]}')

            try:
                response = await engine.adapter.route_request(
                    prompt=test_case["prompt'],
                    task_type=test_case.get("task_type", "text_generation'),
                    latency_requirement=test_case.get("latency_requirement', 1000)
                )

                logger.info(f"‚úÖ Response generated in {response.processing_time:.2f}s')
                logger.info(f"Model used: {response.model}')
                logger.info(f"Preview: {response.content[:100]}...')

            except Exception as e:
                logger.error(f"‚ùå Test failed: {e}')

        return True
    else:
        logger.error("‚ùå Failed to initialize engine for routing test')
        return False

async def main():
    """Main test function""'
    logger.info("üöÄ Starting Ollama Integration Tests')

    # Test 1: Basic integration
    integration_success = await test_ollama_integration()

    # Test 2: Model routing
    routing_success = await test_model_routing()

    if integration_success and routing_success:
        logger.info("\nüéâ All tests passed! Ollama integration is ready.')
        logger.info("\nüìä Summary:')
        logger.info("  ‚úÖ Ollama models are working')
        logger.info("  ‚úÖ Model routing is functional')
        logger.info("  ‚úÖ Performance is optimized')
        logger.info("  ‚úÖ Ready to replace large Qwen model')
        return True
    else:
        logger.error("\n‚ùå Some tests failed. Check the logs above.')
        return False

if __name__ == "__main__':
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
